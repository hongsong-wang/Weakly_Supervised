<!DOCTYPE html>
<html>
<head>
<title>Paper collected by Wang</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">


/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 20px;
  max-width: 960px;
  margin: 0 auto;
}

span#pid {
  color:red;
  
}
span#filename{
  font-style: oblique;
}

span#title{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: italic;
  font-size: 20px;
  border:1px solid #B50;
}
span#abs{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: oblique;
  font-size: 18px;
}
</style>
</head>
<body>

</p></br></br><div id='section'>Paperid: <span id='pid'>1, <a href='https://arxiv.org/pdf/2512.05996.pdf' target='_blank'>https://arxiv.org/pdf/2512.05996.pdf</a></span>   <span><a href='https://umfieldrobotics.github.io/FishDetector-R1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Liu, Jingyu Song, Vedanth Kallakuri, Katherine A. Skinner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.05996">FishDetector-R1: Unified MLLM-Based Framework with Reinforcement Fine-Tuning for Weakly Supervised Fish Detection, Segmentation, and Counting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Analyzing underwater fish imagery is critical for ecological monitoring but remains difficult due to visual degradation and costly annotations. We introduce FishDetector-R1, a unified MLLM-based framework for fish detection, segmentation, and counting under weak supervision. On the DeepFish dataset, our framework achieves substantial gains over baselines, improving AP by 20% and mIoU by 10%, while reducing MAE by 30% and GAME by 35%. These improvements stem from two key components: a novel detect-to-count prompt that enforces spatially consistent detections and counts, and Reinforcement Learning from Verifiable Reward (RLVR) with a complementary scalable paradigm leveraging sparse point labels. Ablation studies further validate the effectiveness of this reward design. Moreover, the improvement generalizes well to other underwater datasets, confirming strong cross-domain robustness. Overall, FishDetector-R1 provides a reliable and scalable solution for accurate marine visual understanding via weak supervision. The project page for FishDetector-R1 is https://umfieldrobotics.github.io/FishDetector-R1.
<div id='section'>Paperid: <span id='pid'>2, <a href='https://arxiv.org/pdf/2512.05113.pdf' target='_blank'>https://arxiv.org/pdf/2512.05113.pdf</a></span>   <span><a href='https://chien90190.github.io/splannequin/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao-Jen Chien, Yi-Chuan Huang, Chung-Ho Wu, Wei-Lun Chao, Yu-Lun Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.05113">Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthesizing high-fidelity frozen 3D scenes from monocular Mannequin-Challenge (MC) videos is a unique problem distinct from standard dynamic scene reconstruction. Instead of focusing on modeling motion, our goal is to create a frozen scene while strategically preserving subtle dynamics to enable user-controlled instant selection. To achieve this, we introduce a novel application of dynamic Gaussian splatting: the scene is modeled dynamically, which retains nearby temporal variation, and a static scene is rendered by fixing the model's time parameter. However, under this usage, monocular capture with sparse temporal supervision introduces artifacts like ghosting and blur for Gaussians that become unobserved or occluded at weakly supervised timestamps. We propose Splannequin, an architecture-agnostic regularization that detects two states of Gaussian primitives, hidden and defective, and applies temporal anchoring. Under predominantly forward camera motion, hidden states are anchored to their recent well-observed past states, while defective states are anchored to future states with stronger supervision. Our method integrates into existing dynamic Gaussian pipelines via simple loss terms, requires no architectural changes, and adds zero inference overhead. This results in markedly improved visual quality, enabling high-fidelity, user-selectable frozen-time renderings, validated by a 96% user preference. Project page: https://chien90190.github.io/splannequin/
<div id='section'>Paperid: <span id='pid'>3, <a href='https://arxiv.org/pdf/2512.02359.pdf' target='_blank'>https://arxiv.org/pdf/2512.02359.pdf</a></span>   <span><a href='https://github.com/zqyq/Weakly-MVCC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bin Li, Daijie Chen, Qi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.02359">WSCF-MVCC: Weakly-supervised Calibration-free Multi-view Crowd Counting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-view crowd counting can effectively mitigate occlusion issues that commonly arise in single-image crowd counting. Existing deep-learning multi-view crowd counting methods project different camera view images onto a common space to obtain ground-plane density maps, requiring abundant and costly crowd annotations and camera calibrations. Hence, calibration-free methods are proposed that do not require camera calibrations and scene-level crowd annotations. However, existing calibration-free methods still require expensive image-level crowd annotations for training the single-view counting module. Thus, in this paper, we propose a weakly-supervised calibration-free multi-view crowd counting method (WSCF-MVCC), directly using crowd count as supervision for the single-view counting module rather than density maps constructed from crowd annotations. Instead, a self-supervised ranking loss that leverages multi-scale priors is utilized to enhance the model's perceptual ability without additional annotation costs. What's more, the proposed model leverages semantic information to achieve a more accurate view matching and, consequently, a more precise scene-level crowd count estimation. The proposed method outperforms the state-of-the-art methods on three widely used multi-view counting datasets under weakly supervised settings, indicating that it is more suitable for practical deployment compared with calibrated methods. Code is released in https://github.com/zqyq/Weakly-MVCC.
<div id='section'>Paperid: <span id='pid'>4, <a href='https://arxiv.org/pdf/2512.01178.pdf' target='_blank'>https://arxiv.org/pdf/2512.01178.pdf</a></span>   <span><a href='https://github.com/Magicboomliu/VSRD_plus_plus' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihua Liu, Hiroki Sakuma, Masatoshi Okutomi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.01178">VSRD++: Autolabeling for 3D Object Detection via Instance-Aware Volumetric Silhouette Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monocular 3D object detection is a fundamental yet challenging task in 3D scene understanding. Existing approaches heavily depend on supervised learning with extensive 3D annotations, which are often acquired from LiDAR point clouds through labor-intensive labeling processes. To tackle this problem, we propose VSRD++, a novel weakly supervised framework for monocular 3D object detection that eliminates the reliance on 3D annotations and leverages neural-field-based volumetric rendering with weak 2D supervision. VSRD++ consists of a two-stage pipeline: multi-view 3D autolabeling and subsequent monocular 3D detector training. In the multi-view autolabeling stage, object surfaces are represented as signed distance fields (SDFs) and rendered as instance masks via the proposed instance-aware volumetric silhouette rendering. To optimize 3D bounding boxes, we decompose each instance's SDF into a cuboid SDF and a residual distance field (RDF) that captures deviations from the cuboid. To address the geometry inconsistency commonly observed in volume rendering methods applied to dynamic objects, we model the dynamic objects by including velocity into bounding box attributes as well as assigning confidence to each pseudo-label. Moreover, we also employ a 3D attribute initialization module to initialize the dynamic bounding box parameters. In the monocular 3D object detection phase, the optimized 3D bounding boxes serve as pseudo labels for training monocular 3D object detectors. Extensive experiments on the KITTI-360 dataset demonstrate that VSRD++ significantly outperforms existing weakly supervised approaches for monocular 3D object detection on both static and dynamic scenes. Code is available at https://github.com/Magicboomliu/VSRD_plus_plus
<div id='section'>Paperid: <span id='pid'>5, <a href='https://arxiv.org/pdf/2512.00130.pdf' target='_blank'>https://arxiv.org/pdf/2512.00130.pdf</a></span>   <span><a href='https://github.com/DanielaPlusPlus/LGCOAMix' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fadi Dornaika, Danyang Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.00130">Local and Global Context-and-Object-part-Aware Superpixel-based Data Augmentation for Deep Visual Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cutmix-based data augmentation, which uses a cut-and-paste strategy, has shown remarkable generalization capabilities in deep learning. However, existing methods primarily consider global semantics with image-level constraints, which excessively reduces attention to the discriminative local context of the class and leads to a performance improvement bottleneck. Moreover, existing methods for generating augmented samples usually involve cutting and pasting rectangular or square regions, resulting in a loss of object part information. To mitigate the problem of inconsistency between the augmented image and the generated mixed label, existing methods usually require double forward propagation or rely on an external pre-trained network for object centering, which is inefficient. To overcome the above limitations, we propose LGCOAMix, an efficient context-aware and object-part-aware superpixel-based grid blending method for data augmentation. To the best of our knowledge, this is the first time that a label mixing strategy using a superpixel attention approach has been proposed for cutmix-based data augmentation. It is the first instance of learning local features from discriminative superpixel-wise regions and cross-image superpixel contrasts. Extensive experiments on various benchmark datasets show that LGCOAMix outperforms state-of-the-art cutmix-based data augmentation methods on classification tasks, {and weakly supervised object location on CUB200-2011.} We have demonstrated the effectiveness of LGCOAMix not only for CNN networks, but also for Transformer networks. Source codes are available at https://github.com/DanielaPlusPlus/LGCOAMix.
<div id='section'>Paperid: <span id='pid'>6, <a href='https://arxiv.org/pdf/2511.14639.pdf' target='_blank'>https://arxiv.org/pdf/2511.14639.pdf</a></span>   <span><a href='https://github.com/Ace95/SLAM-AGS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Marco Acerbis, Swarnadip Chatterjee, Christophe Avenel, Joakim Lindblad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.14639">SLAM-AGS: Slide-Label Aware Multi-Task Pretraining Using Adaptive Gradient Surgery in Computational Cytology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computational cytology faces two major challenges: i) instance-level labels are unreliable and prohibitively costly to obtain, ii) witness rates are extremely low. We propose SLAM-AGS, a Slide-Label-Aware Multitask pretraining framework that jointly optimizes (i) a weakly supervised similarity objective on slide-negative patches and (ii) a self-supervised contrastive objective on slide-positive patches, yielding stronger performance on downstream tasks. To stabilize learning, we apply Adaptive Gradient Surgery to tackle conflicting task gradients and prevent model collapse. We integrate the pretrained encoder into an attention-based Multiple Instance Learning aggregator for bag-level prediction and attention-guided retrieval of the most abnormal instances in a bag. On a publicly available bone-marrow cytology dataset, with simulated witness rates from 10% down to 0.5%, SLAM-AGS improves bag-level F1-Score and Top 400 positive cell retrieval over other pretraining methods, with the largest gains at low witness rates, showing that resolving gradient interference enables stable pretraining and better performance on downstream tasks. To facilitate reproducibility, we share our complete implementation and evaluation framework as open source: https://github.com/Ace95/SLAM-AGS.
<div id='section'>Paperid: <span id='pid'>7, <a href='https://arxiv.org/pdf/2511.14250.pdf' target='_blank'>https://arxiv.org/pdf/2511.14250.pdf</a></span>   <span><a href='https://yoni-yaffe.github.io/count-the-notes' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonathan Yaffe, Ben Maman, Meinard Müller, Amit H. Bermano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.14250">Count The Notes: Histogram-Based Supervision for Automatic Music Transcription</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic Music Transcription (AMT) converts audio recordings into symbolic musical representations. Training deep neural networks (DNNs) for AMT typically requires strongly aligned training pairs with precise frame-level annotations. Since creating such datasets is costly and impractical for many musical contexts, weakly aligned approaches using segment-level annotations have gained traction. However, existing methods often rely on Dynamic Time Warping (DTW) or soft alignment loss functions, both of which still require local semantic correspondences, making them error-prone and computationally expensive. In this article, we introduce CountEM, a novel AMT framework that eliminates the need for explicit local alignment by leveraging note event histograms as supervision, enabling lighter computations and greater flexibility. Using an Expectation-Maximization (EM) approach, CountEM iteratively refines predictions based solely on note occurrence counts, significantly reducing annotation efforts while maintaining high transcription accuracy. Experiments on piano, guitar, and multi-instrument datasets demonstrate that CountEM matches or surpasses existing weakly supervised methods, improving AMT's robustness, scalability, and efficiency. Our project page is available at https://yoni-yaffe.github.io/count-the-notes.
<div id='section'>Paperid: <span id='pid'>8, <a href='https://arxiv.org/pdf/2511.13863.pdf' target='_blank'>https://arxiv.org/pdf/2511.13863.pdf</a></span>   <span><a href='https://krantiparida.github.io/projects/cs3.html' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kranti Kumar Parida, Omar Emara, Hazel Doughty, Dima Damen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13863">Segmenting Collision Sound Sources in Egocentric Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans excel at multisensory perception and can often recognise object properties from the sound of their interactions. Inspired by this, we propose the novel task of Collision Sound Source Segmentation (CS3), where we aim to segment the objects responsible for a collision sound in visual input (i.e. video frames from the collision clip), conditioned on the audio. This task presents unique challenges. Unlike isolated sound events, a collision sound arises from interactions between two objects, and the acoustic signature of the collision depends on both. We focus on egocentric video, where sounds are often clear, but the visual scene is cluttered, objects are small, and interactions are brief. To address these challenges, we propose a weakly-supervised method for audio-conditioned segmentation, utilising foundation models (CLIP and SAM2). We also incorporate egocentric cues, i.e. objects in hands, to find acting objects that can potentially be collision sound sources. Our approach outperforms competitive baselines by $3\times$ and $4.7\times$ in mIoU on two benchmarks we introduce for the CS3 task: EPIC-CS3 and Ego4D-CS3.
<div id='section'>Paperid: <span id='pid'>9, <a href='https://arxiv.org/pdf/2511.10334.pdf' target='_blank'>https://arxiv.org/pdf/2511.10334.pdf</a></span>   <span><a href='https://github.com/lessiYin/DSANet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenti Yin, Huaxin Zhang, Xiang Wang, Yuqing Lu, Yicheng Zhang, Bingquan Gong, Jialong Zuo, Li Yu, Changxin Gao, Nong Sang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.10334">Learning to Tell Apart: Weakly Supervised Video Anomaly Detection via Disentangled Semantic Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in weakly-supervised video anomaly detection have achieved remarkable performance by applying the multiple instance learning paradigm based on multimodal foundation models such as CLIP to highlight anomalous instances and classify categories. However, their objectives may tend to detect the most salient response segments, while neglecting to mine diverse normal patterns separated from anomalies, and are prone to category confusion due to similar appearance, leading to unsatisfactory fine-grained classification results. Therefore, we propose a novel Disentangled Semantic Alignment Network (DSANet) to explicitly separate abnormal and normal features from coarse-grained and fine-grained aspects, enhancing the distinguishability. Specifically, at the coarse-grained level, we introduce a self-guided normality modeling branch that reconstructs input video features under the guidance of learned normal prototypes, encouraging the model to exploit normality cues inherent in the video, thereby improving the temporal separation of normal patterns and anomalous events. At the fine-grained level, we present a decoupled contrastive semantic alignment mechanism, which first temporally decomposes each video into event-centric and background-centric components using frame-level anomaly scores and then applies visual-language contrastive learning to enhance class-discriminative representations. Comprehensive experiments on two standard benchmarks, namely XD-Violence and UCF-Crime, demonstrate that DSANet outperforms existing state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>10, <a href='https://arxiv.org/pdf/2511.10003.pdf' target='_blank'>https://arxiv.org/pdf/2511.10003.pdf</a></span>   <span><a href='https://github.com/liuxuexun/DBGroup' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuexun Liu, Xiaoxu Xu, Qiudan Zhang, Lin Ma, Xu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.10003">DBGroup: Dual-Branch Point Grouping for Weakly Supervised 3D Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised 3D instance segmentation is essential for 3D scene understanding, especially as the growing scale of data and high annotation costs associated with fully supervised approaches. Existing methods primarily rely on two forms of weak supervision: one-thing-one-click annotations and bounding box annotations, both of which aim to reduce labeling efforts. However, these approaches still encounter limitations, including labor-intensive annotation processes, high complexity, and reliance on expert annotators. To address these challenges, we propose \textbf{DBGroup}, a two-stage weakly supervised 3D instance segmentation framework that leverages scene-level annotations as a more efficient and scalable alternative. In the first stage, we introduce a Dual-Branch Point Grouping module to generate pseudo labels guided by semantic and mask cues extracted from multi-view images. To further improve label quality, we develop two refinement strategies: Granularity-Aware Instance Merging and Semantic Selection and Propagation. The second stage involves multi-round self-training on an end-to-end instance segmentation network using the refined pseudo-labels. Additionally, we introduce an Instance Mask Filter strategy to address inconsistencies within the pseudo labels. Extensive experiments demonstrate that DBGroup achieves competitive performance compared to sparse-point-level supervised 3D instance segmentation methods, while surpassing state-of-the-art scene-level supervised 3D semantic segmentation approaches. Code is available at https://github.com/liuxuexun/DBGroup.
<div id='section'>Paperid: <span id='pid'>11, <a href='https://arxiv.org/pdf/2511.05833.pdf' target='_blank'>https://arxiv.org/pdf/2511.05833.pdf</a></span>   <span><a href='https://github.com/Taixi-CHEN/TYrPPG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Taixi Chen, Yiu-ming Cheung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.05833">TYrPPG: Uncomplicated and Enhanced Learning Capability rPPG for Remote Heart Rate Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Remote photoplethysmography (rPPG) can remotely extract physiological signals from RGB video, which has many advantages in detecting heart rate, such as low cost and no invasion to patients. The existing rPPG model is usually based on the transformer module, which has low computation efficiency. Recently, the Mamba model has garnered increasing attention due to its efficient performance in natural language processing tasks, demonstrating potential as a substitute for transformer-based algorithms. However, the Mambaout model and its variants prove that the SSM module, which is the core component of the Mamba model, is unnecessary for the vision task. Therefore, we hope to prove the feasibility of using the Mambaout-based module to remotely learn the heart rate. Specifically, we propose a novel rPPG algorithm called uncomplicated and enhanced learning capability rPPG (TYrPPG). This paper introduces an innovative gated video understanding block (GVB) designed for efficient analysis of RGB videos. Based on the Mambaout structure, this block integrates 2D-CNN and 3D-CNN to enhance video understanding for analysis. In addition, we propose a comprehensive supervised loss function (CSL) to improve the model's learning capability, along with its weakly supervised variants. The experiments show that our TYrPPG can achieve state-of-the-art performance in commonly used datasets, indicating its prospects and superiority in remote heart rate estimation. The source code is available at https://github.com/Taixi-CHEN/TYrPPG.
<div id='section'>Paperid: <span id='pid'>12, <a href='https://arxiv.org/pdf/2511.00456.pdf' target='_blank'>https://arxiv.org/pdf/2511.00456.pdf</a></span>   <span><a href='https://github.com/kiranshahi/pneumonia-analysis' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kiran Shahi, Anup Bagale
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.00456">Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study proposes a weakly supervised deep learning framework for pneumonia classification and localization from chest X-rays, utilizing Grad-CAM explanations. Instead of costly pixel-level annotations, our approach uses image-level labels to generate clinically meaningful heatmaps that highlight regions affected by pneumonia. We evaluate seven pre-trained architectures and the Vision Transformer under identical training conditions, using focal loss and patient-wise splits to prevent data leakage. Experimental results suggest that all models achieved high accuracy (96-98%), with ResNet-18 and EfficientNet-B0 showing the best overall performance and MobileNet-V2 providing an efficient lightweight alternative. Grad-CAM heatmap visualizations confirm that the proposed models focus on clinically relevant lung regions, supporting the use of interpretable AI for radiological diagnostics. This work highlights the potential of weakly supervised, explainable models that enhance the transparency of pneumonia screening and clinical trust in AI-assisted screening.
<div id='section'>Paperid: <span id='pid'>13, <a href='https://arxiv.org/pdf/2510.22055.pdf' target='_blank'>https://arxiv.org/pdf/2510.22055.pdf</a></span>   <span><a href='https://github.com/VenkteshV/QuanTemp_Plus' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>V Venktesh, Deepali Prabhu, Avishek Anand
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.22055">A Benchmark for Open-Domain Numerical Fact-Checking Enhanced by Claim Decomposition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fact-checking numerical claims is critical as the presence of numbers provide mirage of veracity despite being fake potentially causing catastrophic impacts on society. The prior works in automatic fact verification do not primarily focus on natural numerical claims. A typical human fact-checker first retrieves relevant evidence addressing the different numerical aspects of the claim and then reasons about them to predict the veracity of the claim. Hence, the search process of a human fact-checker is a crucial skill that forms the foundation of the verification process. Emulating a real-world setting is essential to aid in the development of automated methods that encompass such skills. However, existing benchmarks employ heuristic claim decomposition approaches augmented with weakly supervised web search to collect evidences for verifying claims. This sometimes results in less relevant evidences and noisy sources with temporal leakage rendering a less realistic retrieval setting for claim verification. Hence, we introduce QuanTemp++: a dataset consisting of natural numerical claims, an open domain corpus, with the corresponding relevant evidence for each claim. The evidences are collected through a claim decomposition process approximately emulating the approach of human fact-checker and veracity labels ensuring there is no temporal leakage. Given this dataset, we also characterize the retrieval performance of key claim decomposition paradigms. Finally, we observe their effect on the outcome of the verification pipeline and draw insights. The code for data pipeline along with link to data can be found at https://github.com/VenkteshV/QuanTemp_Plus
<div id='section'>Paperid: <span id='pid'>14, <a href='https://arxiv.org/pdf/2510.21532.pdf' target='_blank'>https://arxiv.org/pdf/2510.21532.pdf</a></span>   <span><a href='https://github.com/rohban-lab/FrameShield' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mojtaba Nafez, Mobina Poulaei, Nikan Vasei, Bardia Soltani Moakhar, Mohammad Sabokrou, MohammadHossein Rohban
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.21532">FrameShield: Adversarially Robust Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Video Anomaly Detection (WSVAD) has achieved notable advancements, yet existing models remain vulnerable to adversarial attacks, limiting their reliability. Due to the inherent constraints of weak supervision, where only video-level labels are provided despite the need for frame-level predictions, traditional adversarial defense mechanisms, such as adversarial training, are not effective since video-level adversarial perturbations are typically weak and inadequate. To address this limitation, pseudo-labels generated directly from the model can enable frame-level adversarial training; however, these pseudo-labels are inherently noisy, significantly degrading performance. We therefore introduce a novel Pseudo-Anomaly Generation method called Spatiotemporal Region Distortion (SRD), which creates synthetic anomalies by applying severe augmentations to localized regions in normal videos while preserving temporal consistency. Integrating these precisely annotated synthetic anomalies with the noisy pseudo-labels substantially reduces label noise, enabling effective adversarial training. Extensive experiments demonstrate that our method significantly enhances the robustness of WSVAD models against adversarial attacks, outperforming state-of-the-art methods by an average of 71.0\% in overall AUROC performance across multiple benchmarks. The implementation and code are publicly available at https://github.com/rohban-lab/FrameShield.
<div id='section'>Paperid: <span id='pid'>15, <a href='https://arxiv.org/pdf/2510.21449.pdf' target='_blank'>https://arxiv.org/pdf/2510.21449.pdf</a></span>   <span><a href='https://github.com/YsTvT/MoniTor' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengtian Yang, Yue Feng, Yingshi Liu, Jingrou Zhang, Jie Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.21449">MoniTor: Exploiting Large Language Models with Instruction for Online Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Anomaly Detection (VAD) aims to locate unusual activities or behaviors within videos. Recently, offline VAD has garnered substantial research attention, which has been invigorated by the progress in large language models (LLMs) and vision-language models (VLMs), offering the potential for a more nuanced understanding of anomalies. However, online VAD has seldom received attention due to real-time constraints and computational intensity. In this paper, we introduce a novel Memory-based online scoring queue scheme for Training-free VAD (MoniTor), to address the inherent complexities in online VAD. Specifically, MoniTor applies a streaming input to VLMs, leveraging the capabilities of pre-trained large-scale models. To capture temporal dependencies more effectively, we incorporate a novel prediction mechanism inspired by Long Short-Term Memory (LSTM) networks. This ensures the model can effectively model past states and leverage previous predictions to identify anomalous behaviors. Thereby, it better understands the current frame. Moreover, we design a scoring queue and an anomaly prior to dynamically store recent scores and cover all anomalies in the monitoring scenario, providing guidance for LLMs to distinguish between normal and abnormal behaviors over time. We evaluate MoniTor on two large datasets (i.e., UCF-Crime and XD-Violence) containing various surveillance and real-world scenarios. The results demonstrate that MoniTor outperforms state-of-the-art methods and is competitive with weakly supervised methods without training. Code is available at https://github.com/YsTvT/MoniTor.
<div id='section'>Paperid: <span id='pid'>16, <a href='https://arxiv.org/pdf/2510.12385.pdf' target='_blank'>https://arxiv.org/pdf/2510.12385.pdf</a></span>   <span><a href='https://timschoonbeek.github.io/stormpsr' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tim J. Schoonbeek, Shao-Hsuan Hung, Dan Lehman, Hans Onvlee, Jacek Kustra, Peter H. N. de With, Fons van der Sommen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.12385">Learning to Recognize Correctly Completed Procedure Steps in Egocentric Assembly Videos through Spatio-Temporal Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Procedure step recognition (PSR) aims to identify all correctly completed steps and their sequential order in videos of procedural tasks. The existing state-of-the-art models rely solely on detecting assembly object states in individual video frames. By neglecting temporal features, model robustness and accuracy are limited, especially when objects are partially occluded. To overcome these limitations, we propose Spatio-Temporal Occlusion-Resilient Modeling for Procedure Step Recognition (STORM-PSR), a dual-stream framework for PSR that leverages both spatial and temporal features. The assembly state detection stream operates effectively with unobstructed views of the object, while the spatio-temporal stream captures both spatial and temporal features to recognize step completions even under partial occlusion. This stream includes a spatial encoder, pre-trained using a novel weakly supervised approach to capture meaningful spatial representations, and a transformer-based temporal encoder that learns how these spatial features relate over time. STORM-PSR is evaluated on the MECCANO and IndustReal datasets, reducing the average delay between actual and predicted assembly step completions by 11.2% and 26.1%, respectively, compared to prior methods. We demonstrate that this reduction in delay is driven by the spatio-temporal stream, which does not rely on unobstructed views of the object to infer completed steps. The code for STORM-PSR, along with the newly annotated MECCANO labels, is made publicly available at https://timschoonbeek.github.io/stormpsr .
<div id='section'>Paperid: <span id='pid'>17, <a href='https://arxiv.org/pdf/2510.10564.pdf' target='_blank'>https://arxiv.org/pdf/2510.10564.pdf</a></span>   <span><a href='https://github.com/lalunex/MGSD-WSS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liang Li, Zhou Yang, Xiaofei Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10564">Multi-Granularity Sequence Denoising with Weakly Supervised Signal for Sequential Recommendation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sequential recommendation aims to predict the next item based on user interests in historical interaction sequences. Historical interaction sequences often contain irrelevant noisy items, which significantly hinders the performance of recommendation systems. Existing research employs unsupervised methods that indirectly identify item-granularity irrelevant noise by predicting the ground truth item. Since these methods lack explicit noise labels, they are prone to misidentify users' interested items as noise. Additionally, while these methods focus on removing item-granularity noise driven by the ground truth item, they overlook interest-granularity noise, limiting their ability to perform broader denoising based on user interests. To address these issues, we propose Multi-Granularity Sequence Denoising with Weakly Supervised Signal for Sequential Recommendation(MGSD-WSS). MGSD-WSS first introduces the Multiple Gaussian Kernel Perceptron module to map the original and enhance sequence into a common representation space and utilizes weakly supervised signals to accurately identify noisy items in the historical interaction sequence. Subsequently, it employs the item-granularity denoising module with noise-weighted contrastive learning to obtain denoised item representations. Then, it extracts target interest representations from the ground truth item and applies noise-weighted contrastive learning to obtain denoised interest representations. Finally, based on the denoised item and interest representations, MGSD-WSS predicts the next item. Extensive experiments on five datasets demonstrate that the proposed method significantly outperforms state-of-the-art sequence recommendation and denoising models. Our code is available at https://github.com/lalunex/MGSD-WSS.
<div id='section'>Paperid: <span id='pid'>18, <a href='https://arxiv.org/pdf/2510.08052.pdf' target='_blank'>https://arxiv.org/pdf/2510.08052.pdf</a></span>   <span><a href='https://github.com/BheeshmSharma/RASALoRE-BMVC-2025/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bheeshm Sharma, Karthikeyan Jaganathan, Balamurugan Palaniappan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08052">RASALoRE: Region Aware Spatial Attention with Location-based Random Embeddings for Weakly Supervised Anomaly Detection in Brain MRI Scans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Anomaly detection (WSAD) in brain MRI scans is an important challenge useful to obtain quick and accurate detection of brain anomalies when precise pixel-level anomaly annotations are unavailable and only weak labels (e.g., slice-level) are available. In this work, we propose RASALoRE: Region Aware Spatial Attention with Location-based Random Embeddings, a novel two-stage WSAD framework. In the first stage, we introduce a Discriminative Dual Prompt Tuning (DDPT) mechanism that generates high-quality pseudo weak masks based on slice-level labels, serving as coarse localization cues. In the second stage, we propose a segmentation network with a region-aware spatial attention mechanism that relies on fixed location-based random embeddings. This design enables the model to effectively focus on anomalous regions. Our approach achieves state-of-the-art anomaly detection performance, significantly outperforming existing WSAD methods while utilizing less than 8 million parameters. Extensive evaluations on the BraTS20, BraTS21, BraTS23, and MSD datasets demonstrate a substantial performance improvement coupled with a significant reduction in computational complexity. Code is available at: https://github.com/BheeshmSharma/RASALoRE-BMVC-2025/.
<div id='section'>Paperid: <span id='pid'>19, <a href='https://arxiv.org/pdf/2510.05899.pdf' target='_blank'>https://arxiv.org/pdf/2510.05899.pdf</a></span>   <span><a href='https://github.com/jiesihu/Weak-ICL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiesi Hu, Yanwu Yang, Zhiyu Ye, Jinyan Zhou, Jianfeng Cao, Hanyang Peng, Ting Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05899">Efficient Universal Models for Medical Image Segmentation via Weakly Supervised In-Context Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Universal models for medical image segmentation, such as interactive and in-context learning (ICL) models, offer strong generalization but require extensive annotations. Interactive models need repeated user prompts for each image, while ICL relies on dense, pixel-level labels. To address this, we propose Weakly Supervised In-Context Learning (WS-ICL), a new ICL paradigm that leverages weak prompts (e.g., bounding boxes or points) instead of dense labels for context. This approach significantly reduces annotation effort by eliminating the need for fine-grained masks and repeated user prompting for all images. We evaluated the proposed WS-ICL model on three held-out benchmarks. Experimental results demonstrate that WS-ICL achieves performance comparable to regular ICL models at a significantly lower annotation cost. In addition, WS-ICL is highly competitive even under the interactive paradigm. These findings establish WS-ICL as a promising step toward more efficient and unified universal models for medical image segmentation. Our code and model are publicly available at https://github.com/jiesihu/Weak-ICL.
<div id='section'>Paperid: <span id='pid'>20, <a href='https://arxiv.org/pdf/2509.10308.pdf' target='_blank'>https://arxiv.org/pdf/2509.10308.pdf</a></span>   <span><a href='https://github.com/riskaudit/GraphCSVAE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Joshua Dimasaka, Christian GeiÃ, Robert Muir-Wood, Emily So
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10308">GraphCSVAE: Graph Categorical Structured Variational Autoencoder for Spatiotemporal Auditing of Physical Vulnerability Towards Sustainable Post-Disaster Risk Reduction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the aftermath of disasters, many institutions worldwide face challenges in continually monitoring changes in disaster risk, limiting the ability of key decision-makers to assess progress towards the UN Sendai Framework for Disaster Risk Reduction 2015-2030. While numerous efforts have substantially advanced the large-scale modeling of hazard and exposure through Earth observation and data-driven methods, progress remains limited in modeling another equally important yet challenging element of the risk equation: physical vulnerability. To address this gap, we introduce Graph Categorical Structured Variational Autoencoder (GraphCSVAE), a novel probabilistic data-driven framework for modeling physical vulnerability by integrating deep learning, graph representation, and categorical probabilistic inference, using time-series satellite-derived datasets and prior expert belief systems. We introduce a weakly supervised first-order transition matrix that reflects the changes in the spatiotemporal distribution of physical vulnerability in two disaster-stricken and socioeconomically disadvantaged areas: (1) the cyclone-impacted coastal Khurushkul community in Bangladesh and (2) the mudslide-affected city of Freetown in Sierra Leone. Our work reveals post-disaster regional dynamics in physical vulnerability, offering valuable insights into localized spatiotemporal auditing and sustainable strategies for post-disaster risk reduction.
<div id='section'>Paperid: <span id='pid'>21, <a href='https://arxiv.org/pdf/2509.08991.pdf' target='_blank'>https://arxiv.org/pdf/2509.08991.pdf</a></span>   <span><a href='https://github.com/magdalena-wysocki/ultron' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Magdalena Wysocki, Felix Duelmer, Ananya Bal, Nassir Navab, Mohammad Farid Azampour
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08991">UltrON: Ultrasound Occupancy Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In free-hand ultrasound imaging, sonographers rely on expertise to mentally integrate partial 2D views into 3D anatomical shapes. Shape reconstruction can assist clinicians in this process. Central to this task is the choice of shape representation, as it determines how accurately and efficiently the structure can be visualized, analyzed, and interpreted. Implicit representations, such as SDF and occupancy function, offer a powerful alternative to traditional voxel- or mesh-based methods by modeling continuous, smooth surfaces with compact storage, avoiding explicit discretization. Recent studies demonstrate that SDF can be effectively optimized using annotations derived from segmented B-mode ultrasound images. Yet, these approaches hinge on precise annotations, overlooking the rich acoustic information embedded in B-mode intensity. Moreover, implicit representation approaches struggle with the ultrasound's view-dependent nature and acoustic shadowing artifacts, which impair reconstruction. To address the problems resulting from occlusions and annotation dependency, we propose an occupancy-based representation and introduce \gls{UltrON} that leverages acoustic features to improve geometric consistency in weakly-supervised optimization regime. We show that these features can be obtained from B-mode images without additional annotation cost. Moreover, we propose a novel loss function that compensates for view-dependency in the B-mode images and facilitates occupancy optimization from multiview ultrasound. By incorporating acoustic properties, \gls{UltrON} generalizes to shapes of the same anatomy. We show that \gls{UltrON} mitigates the limitations of occlusions and sparse labeling and paves the way for more accurate 3D reconstruction. Code and dataset will be available at https://github.com/magdalena-wysocki/ultron.
<div id='section'>Paperid: <span id='pid'>22, <a href='https://arxiv.org/pdf/2509.08289.pdf' target='_blank'>https://arxiv.org/pdf/2509.08289.pdf</a></span>   <span><a href='https://github.com/gyl2565309278/DTH-CP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuelin Guo, Haoyu He, Zhiyuan Chen, Zitong Huang, Renhao Lu, Lu Shi, Zejun Wang, Weizhe Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08289">Dual-Thresholding Heatmaps to Cluster Proposals for Weakly Supervised Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised object detection (WSOD) has attracted significant attention in recent years, as it does not require box-level annotations. State-of-the-art methods generally adopt a multi-module network, which employs WSDDN as the multiple instance detection network module and multiple instance refinement modules to refine performance. However, these approaches suffer from three key limitations. First, existing methods tend to generate pseudo GT boxes that either focus only on discriminative parts, failing to capture the whole object, or cover the entire object but fail to distinguish between adjacent intra-class instances. Second, the foundational WSDDN architecture lacks a crucial background class representation for each proposal and exhibits a large semantic gap between its branches. Third, prior methods discard ignored proposals during optimization, leading to slow convergence. To address these challenges, we first design a heatmap-guided proposal selector (HGPS) algorithm, which utilizes dual thresholds on heatmaps to pre-select proposals, enabling pseudo GT boxes to both capture the full object extent and distinguish between adjacent intra-class instances. We then present a weakly supervised basic detection network (WSBDN), which augments each proposal with a background class representation and uses heatmaps for pre-supervision to bridge the semantic gap between matrices. At last, we introduce a negative certainty supervision loss on ignored proposals to accelerate convergence. Extensive experiments on the challenging PASCAL VOC 2007 and 2012 datasets demonstrate the effectiveness of our framework. We achieve mAP/mCorLoc scores of 58.5%/81.8% on VOC 2007 and 55.6%/80.5% on VOC 2012, performing favorably against the state-of-the-art WSOD methods. Our code is publicly available at https://github.com/gyl2565309278/DTH-CP.
<div id='section'>Paperid: <span id='pid'>23, <a href='https://arxiv.org/pdf/2509.07507.pdf' target='_blank'>https://arxiv.org/pdf/2509.07507.pdf</a></span>   <span><a href='https://github.com/CEA-LIST/MVAT' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/CEA-LIST/MVAT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Saad Lahlali, Alexandre Fournier Montgieux, Nicolas Granger, HervÃ© Le Borgne, Quoc Cuong Pham
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07507">MVAT: Multi-View Aware Teacher for Weakly Supervised 3D Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Annotating 3D data remains a costly bottleneck for 3D object detection, motivating the development of weakly supervised annotation methods that rely on more accessible 2D box annotations. However, relying solely on 2D boxes introduces projection ambiguities since a single 2D box can correspond to multiple valid 3D poses. Furthermore, partial object visibility under a single viewpoint setting makes accurate 3D box estimation difficult. We propose MVAT, a novel framework that leverages temporal multi-view present in sequential data to address these challenges. Our approach aggregates object-centric point clouds across time to build 3D object representations as dense and complete as possible. A Teacher-Student distillation paradigm is employed: The Teacher network learns from single viewpoints but targets are derived from temporally aggregated static objects. Then the Teacher generates high quality pseudo-labels that the Student learns to predict from a single viewpoint for both static and moving objects. The whole framework incorporates a multi-view 2D projection loss to enforce consistency between predicted 3D boxes and all available 2D annotations. Experiments on the nuScenes and Waymo Open datasets demonstrate that MVAT achieves state-of-the-art performance for weakly supervised 3D object detection, significantly narrowing the gap with fully supervised methods without requiring any 3D box annotations. % \footnote{Code available upon acceptance} Our code is available in our public repository (\href{https://github.com/CEA-LIST/MVAT}{code}).
<div id='section'>Paperid: <span id='pid'>24, <a href='https://arxiv.org/pdf/2508.19060.pdf' target='_blank'>https://arxiv.org/pdf/2508.19060.pdf</a></span>   <span><a href='https://github.com/blaz-r/SuperSimpleNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>BlaÅ¾ Rolih, Matic FuÄka, Danijel SkoÄaj
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19060">No Label Left Behind: A Unified Surface Defect Detection Model for all Supervision Regimes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Surface defect detection is a critical task across numerous industries, aimed at efficiently identifying and localising imperfections or irregularities on manufactured components. While numerous methods have been proposed, many fail to meet industrial demands for high performance, efficiency, and adaptability. Existing approaches are often constrained to specific supervision scenarios and struggle to adapt to the diverse data annotations encountered in real-world manufacturing processes, such as unsupervised, weakly supervised, mixed supervision, and fully supervised settings. To address these challenges, we propose SuperSimpleNet, a highly efficient and adaptable discriminative model built on the foundation of SimpleNet. SuperSimpleNet incorporates a novel synthetic anomaly generation process, an enhanced classification head, and an improved learning procedure, enabling efficient training in all four supervision scenarios, making it the first model capable of fully leveraging all available data annotations. SuperSimpleNet sets a new standard for performance across all scenarios, as demonstrated by its results on four challenging benchmark datasets. Beyond accuracy, it is very fast, achieving an inference time below 10 ms. With its ability to unify diverse supervision paradigms while maintaining outstanding speed and reliability, SuperSimpleNet represents a promising step forward in addressing real-world manufacturing challenges and bridging the gap between academic research and industrial applications. Code: https://github.com/blaz-r/SuperSimpleNet
<div id='section'>Paperid: <span id='pid'>25, <a href='https://arxiv.org/pdf/2508.17186.pdf' target='_blank'>https://arxiv.org/pdf/2508.17186.pdf</a></span>   <span><a href='https://github.com/zhenghuizhao/AdvCP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenghui Zhao, Chen Wu, Di Wang, Hongruixuan Chen, Cuiqun Chen, Zhuo Zheng, Bo Du, Liangpei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17186">Advancing Weakly-Supervised Change Detection in Satellite Images via Adversarial Class Prompting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-Supervised Change Detection (WSCD) aims to distinguish specific object changes (e.g., objects appearing or disappearing) from background variations (e.g., environmental changes due to light, weather, or seasonal shifts) in paired satellite images, relying only on paired image (i.e., image-level) classification labels. This technique significantly reduces the need for dense annotations required in fully-supervised change detection. However, as image-level supervision only indicates whether objects have changed in a scene, WSCD methods often misclassify background variations as object changes, especially in complex remote-sensing scenarios. In this work, we propose an Adversarial Class Prompting (AdvCP) method to address this co-occurring noise problem, including two phases: a) Adversarial Prompt Mining: After each training iteration, we introduce adversarial prompting perturbations, using incorrect one-hot image-level labels to activate erroneous feature mappings. This process reveals co-occurring adversarial samples under weak supervision, namely background variation features that are likely to be misclassified as object changes. b) Adversarial Sample Rectification: We integrate these adversarially prompt-activated pixel samples into training by constructing an online global prototype. This prototype is built from an exponentially weighted moving average of the current batch and all historical training data. Our AdvCP can be seamlessly integrated into current WSCD methods without adding additional inference cost. Experiments on ConvNet, Transformer, and Segment Anything Model (SAM)-based baselines demonstrate significant performance enhancements. Furthermore, we demonstrate the generalizability of AdvCP to other multi-class weakly-supervised dense prediction scenarios. Code is available at https://github.com/zhenghuizhao/AdvCP
<div id='section'>Paperid: <span id='pid'>26, <a href='https://arxiv.org/pdf/2508.16159.pdf' target='_blank'>https://arxiv.org/pdf/2508.16159.pdf</a></span>   <span><a href='https://github.com/jarch-ma/TLG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Ma, Guo-Sen Xie, Fang Zhao, Zechao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16159">Through the Looking Glass: A Dual Perspective on Weakly-Supervised Few-Shot Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Meta-learning aims to uniformly sample homogeneous support-query pairs, characterized by the same categories and similar attributes, and extract useful inductive biases through identical network architectures. However, this identical network design results in over-semantic homogenization. To address this, we propose a novel homologous but heterogeneous network. By treating support-query pairs as dual perspectives, we introduce heterogeneous visual aggregation (HA) modules to enhance complementarity while preserving semantic commonality. To further reduce semantic noise and amplify the uniqueness of heterogeneous semantics, we design a heterogeneous transfer (HT) module. Finally, we propose heterogeneous CLIP (HC) textual information to enhance the generalization capability of multimodal models. In the weakly-supervised few-shot semantic segmentation (WFSS) task, with only 1/24 of the parameters of existing state-of-the-art models, TLG achieves a 13.2\% improvement on Pascal-5\textsuperscript{i} and a 9.7\% improvement on COCO-20\textsuperscript{i}. To the best of our knowledge, TLG is also the first weakly supervised (image-level) model that outperforms fully supervised (pixel-level) models under the same backbone architectures. The code is available at https://github.com/jarch-ma/TLG.
<div id='section'>Paperid: <span id='pid'>27, <a href='https://arxiv.org/pdf/2508.12023.pdf' target='_blank'>https://arxiv.org/pdf/2508.12023.pdf</a></span>   <span><a href='https://github.com/SFI-Visual-Intelligence/wiselvam.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Durgesh Kumar Singh, Qing Cao, Sarina Thomas, AhcÃ¨ne Boubekki, Robert Jenssen, Michael Kampffmeyer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12023">WiseLVAM: A Novel Framework For Left Ventricle Automatic Measurements</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Clinical guidelines recommend performing left ventricular (LV) linear measurements in B-mode echocardiographic images at the basal level -- typically at the mitral valve leaflet tips -- and aligned perpendicular to the LV long axis along a virtual scanline (SL). However, most automated methods estimate landmarks directly from B-mode images for the measurement task, where even small shifts in predicted points along the LV walls can lead to significant measurement errors, reducing their clinical reliability. A recent semi-automatic method, EnLVAM, addresses this limitation by constraining landmark prediction to a clinician-defined SL and training on generated Anatomical Motion Mode (AMM) images to predict LV landmarks along the same. To enable full automation, a contour-aware SL placement approach is proposed in this work, in which the LV contour is estimated using a weakly supervised B-mode landmark detector. SL placement is then performed by inferring the LV long axis and the basal level- mimicking clinical guidelines. Building on this foundation, we introduce \textit{WiseLVAM} -- a novel, fully automated yet manually adaptable framework for automatically placing the SL and then automatically performing the LV linear measurements in the AMM mode. \textit{WiseLVAM} utilizes the structure-awareness from B-mode images and the motion-awareness from AMM mode to enhance robustness and accuracy with the potential to provide a practical solution for the routine clinical application. The source code is publicly available at https://github.com/SFI-Visual-Intelligence/wiselvam.git.
<div id='section'>Paperid: <span id='pid'>28, <a href='https://arxiv.org/pdf/2508.10256.pdf' target='_blank'>https://arxiv.org/pdf/2508.10256.pdf</a></span>   <span><a href='https://github.com/nantonzhang/Awesome-Crack-Detection' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinan Zhang, Haolin Wang, Yung-An Hsieh, Zhongyu Yang, Anthony Yezzi, Yi-Chang Tsai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10256">Deep Learning for Crack Detection: A Review of Learning Paradigms, Generalizability, and Datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Crack detection plays a crucial role in civil infrastructures, including inspection of pavements, buildings, etc., and deep learning has significantly advanced this field in recent years. While numerous technical and review papers exist in this domain, emerging trends are reshaping the landscape. These shifts include transitions in learning paradigms (from fully supervised learning to semi-supervised, weakly-supervised, unsupervised, few-shot, domain adaptation and fine-tuning foundation models), improvements in generalizability (from single-dataset performance to cross-dataset evaluation), and diversification in dataset acquisition (from RGB images to specialized sensor-based data). In this review, we systematically analyze these trends and highlight representative works. Additionally, we introduce a new annotated dataset collected with 3D laser scans, 3DCrack, to support future research and conduct extensive benchmarking experiments to establish baselines for commonly used deep learning methodologies, including recent foundation models. Our findings provide insights into the evolving methodologies and future directions in deep learning-based crack detection. Project page: https://github.com/nantonzhang/Awesome-Crack-Detection
<div id='section'>Paperid: <span id='pid'>29, <a href='https://arxiv.org/pdf/2508.07298.pdf' target='_blank'>https://arxiv.org/pdf/2508.07298.pdf</a></span>   <span><a href='https://github.com/Senyh/SynMatch' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiqiang Shen, Peng Cao, Xiaoli Liu, Jinzhu Yang, Osmar R. Zaiane
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07298">SynMatch: Rethinking Consistency in Medical Image Segmentation with Sparse Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Label scarcity remains a major challenge in deep learning-based medical image segmentation. Recent studies use strong-weak pseudo supervision to leverage unlabeled data. However, performance is often hindered by inconsistencies between pseudo labels and their corresponding unlabeled images. In this work, we propose \textbf{SynMatch}, a novel framework that sidesteps the need for improving pseudo labels by synthesizing images to match them instead. Specifically, SynMatch synthesizes images using texture and shape features extracted from the same segmentation model that generates the corresponding pseudo labels for unlabeled images. This design enables the generation of highly consistent synthesized-image-pseudo-label pairs without requiring any training parameters for image synthesis. We extensively evaluate SynMatch across diverse medical image segmentation tasks under semi-supervised learning (SSL), weakly-supervised learning (WSL), and barely-supervised learning (BSL) settings with increasingly limited annotations. The results demonstrate that SynMatch achieves superior performance, especially in the most challenging BSL setting. For example, it outperforms the recent strong-weak pseudo supervision-based method by 29.71\% and 10.05\% on the polyp segmentation task with 5\% and 10\% scribble annotations, respectively. The code will be released at https://github.com/Senyh/SynMatch.
<div id='section'>Paperid: <span id='pid'>30, <a href='https://arxiv.org/pdf/2508.06485.pdf' target='_blank'>https://arxiv.org/pdf/2508.06485.pdf</a></span>   <span><a href='https://github.com/Sofianebouaziz1/WGAST.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sofiane Bouaziz, Adel Hafiane, Raphael Canals, Rachid Nedjai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06485">WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Urbanization, climate change, and agricultural stress are increasing the demand for precise and timely environmental monitoring. Land Surface Temperature (LST) is a key variable in this context and is retrieved from remote sensing satellites. However, these systems face a trade-off between spatial and temporal resolution. While spatio-temporal fusion methods offer promising solutions, few have addressed the estimation of daily LST at 10 m resolution. In this study, we present WGAST, a Weakly-Supervised Generative Network for Daily 10 m LST Estimation via Spatio-Temporal Fusion of Terra MODIS, Landsat 8, and Sentinel-2. WGAST is the first end-to-end deep learning framework designed for this task. It adopts a conditional generative adversarial architecture, with a generator composed of four stages: feature extraction, fusion, LST reconstruction, and noise suppression. The first stage employs a set of encoders to extract multi-level latent representations from the inputs, which are then fused in the second stage using cosine similarity, normalization, and temporal attention mechanisms. The third stage decodes the fused features into high-resolution LST, followed by a Gaussian filter to suppress high-frequency noise. Training follows a weakly supervised strategy based on physical averaging principles and reinforced by a PatchGAN discriminator. Experiments demonstrate that WGAST outperforms existing methods in both quantitative and qualitative evaluations. Compared to the best-performing baseline, on average, WGAST reduces RMSE by 17.18% and improves SSIM by 11.00%. Furthermore, WGAST is robust to cloud-induced LST and effectively captures fine-scale thermal patterns, as validated against 33 ground-based sensors. The code is available at https://github.com/Sofianebouaziz1/WGAST.git.
<div id='section'>Paperid: <span id='pid'>31, <a href='https://arxiv.org/pdf/2508.04943.pdf' target='_blank'>https://arxiv.org/pdf/2508.04943.pdf</a></span>   <span><a href='https://github.com/XZPKU/TRKT.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhu Xu, Ting Lei, Zhimin Li, Guan Wang, Qingchao Chen, Yuxin Peng, Yang liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04943">TRKT: Weakly Supervised Dynamic Scene Graph Generation with Temporal-enhanced Relation-aware Knowledge Transferring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic Scene Graph Generation (DSGG) aims to create a scene graph for each video frame by detecting objects and predicting their relationships. Weakly Supervised DSGG (WS-DSGG) reduces annotation workload by using an unlocalized scene graph from a single frame per video for training. Existing WS-DSGG methods depend on an off-the-shelf external object detector to generate pseudo labels for subsequent DSGG training. However, detectors trained on static, object-centric images struggle in dynamic, relation-aware scenarios required for DSGG, leading to inaccurate localization and low-confidence proposals. To address the challenges posed by external object detectors in WS-DSGG, we propose a Temporal-enhanced Relation-aware Knowledge Transferring (TRKT) method, which leverages knowledge to enhance detection in relation-aware dynamic scenarios. TRKT is built on two key components:(1)Relation-aware knowledge mining: we first employ object and relation class decoders that generate category-specific attention maps to highlight both object regions and interactive areas. Then we propose an Inter-frame Attention Augmentation strategy that exploits optical flow for neighboring frames to enhance the attention maps, making them motion-aware and robust to motion blur. This step yields relation- and motion-aware knowledge mining for WS-DSGG. (2) we introduce a Dual-stream Fusion Module that integrates category-specific attention maps into external detections to refine object localization and boost confidence scores for object proposals. Extensive experiments demonstrate that TRKT achieves state-of-the-art performance on Action Genome dataset. Our code is avaliable at https://github.com/XZPKU/TRKT.git.
<div id='section'>Paperid: <span id='pid'>32, <a href='https://arxiv.org/pdf/2508.03201.pdf' target='_blank'>https://arxiv.org/pdf/2508.03201.pdf</a></span>   <span><a href='https://github.com/I2-Multimedia-Lab/AlignCAT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yidan Wang, Chenyi Zhuang, Wutao Liu, Pan Gao, Nicu Sebe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03201">AlignCAT: Visual-Linguistic Alignment of Category and Attributefor Weakly Supervised Visual Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised visual grounding (VG) aims to locate objects in images based on text descriptions. Despite significant progress, existing methods lack strong cross-modal reasoning to distinguish subtle semantic differences in text expressions due to category-based and attribute-based ambiguity. To address these challenges, we introduce AlignCAT, a novel query-based semantic matching framework for weakly supervised VG. To enhance visual-linguistic alignment, we propose a coarse-grained alignment module that utilizes category information and global context, effectively mitigating interference from category-inconsistent objects. Subsequently, a fine-grained alignment module leverages descriptive information and captures word-level text features to achieve attribute consistency. By exploiting linguistic cues to their fullest extent, our proposed AlignCAT progressively filters out misaligned visual queries and enhances contrastive learning efficiency. Extensive experiments on three VG benchmarks, namely RefCOCO, RefCOCO+, and RefCOCOg, verify the superiority of AlignCAT against existing weakly supervised methods on two VG tasks. Our code is available at: https://github.com/I2-Multimedia-Lab/AlignCAT.
<div id='section'>Paperid: <span id='pid'>33, <a href='https://arxiv.org/pdf/2508.01310.pdf' target='_blank'>https://arxiv.org/pdf/2508.01310.pdf</a></span>   <span><a href='https://github.com/riskaudit/GraphVSSM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Joshua Dimasaka, Christian GeiÃ, Emily So
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01310">GraphVSSM: Graph Variational State-Space Model for Probabilistic Spatiotemporal Inference of Dynamic Exposure and Vulnerability for Regional Disaster Resilience Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Regional disaster resilience quantifies the changing nature of physical risks to inform policy instruments ranging from local immediate recovery to international sustainable development. While many existing state-of-practice methods have greatly advanced the dynamic mapping of exposure and hazard, our understanding of large-scale physical vulnerability has remained static, costly, limited, region-specific, coarse-grained, overly aggregated, and inadequately calibrated. With the significant growth in the availability of time-series satellite imagery and derived products for exposure and hazard, we focus our work on the equally important yet challenging element of the risk equation: physical vulnerability. We leverage machine learning methods that flexibly capture spatial contextual relationships, limited temporal observations, and uncertainty in a unified probabilistic spatiotemporal inference framework. We therefore introduce Graph Variational State-Space Model (GraphVSSM), a novel modular spatiotemporal approach that uniquely integrates graph deep learning, state-space modeling, and variational inference using time-series data and prior expert belief systems in a weakly supervised or coarse-to-fine-grained manner. We present three major results: a city-wide demonstration in Quezon City, Philippines; an investigation of sudden changes in the cyclone-impacted coastal Khurushkul community (Bangladesh) and mudslide-affected Freetown (Sierra Leone); and an open geospatial dataset, METEOR 2.5D, that spatiotemporally enhances the existing global static dataset for UN Least Developed Countries (2020). Beyond advancing regional disaster resilience assessment and improving our understanding global disaster risk reduction progress, our method also offers a probabilistic deep learning approach, contributing to broader urban studies that require compositional data analysis in weak supervision.
<div id='section'>Paperid: <span id='pid'>34, <a href='https://arxiv.org/pdf/2508.01250.pdf' target='_blank'>https://arxiv.org/pdf/2508.01250.pdf</a></span>   <span><a href='https://github.com/CVI-SZU/DisFaceRep' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/CVI-SZU/DisFaceRep' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoqin Wang, Xianxu Hou, Meidan Ding, Junliang Chen, Kaijun Deng, Jinheng Xie, Linlin Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01250">DisFaceRep: Representation Disentanglement for Co-occurring Facial Components in Weakly Supervised Face Parsing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face parsing aims to segment facial images into key components such as eyes, lips, and eyebrows. While existing methods rely on dense pixel-level annotations, such annotations are expensive and labor-intensive to obtain. To reduce annotation cost, we introduce Weakly Supervised Face Parsing (WSFP), a new task setting that performs dense facial component segmentation using only weak supervision, such as image-level labels and natural language descriptions. WSFP introduces unique challenges due to the high co-occurrence and visual similarity of facial components, which lead to ambiguous activations and degraded parsing performance. To address this, we propose DisFaceRep, a representation disentanglement framework designed to separate co-occurring facial components through both explicit and implicit mechanisms. Specifically, we introduce a co-occurring component disentanglement strategy to explicitly reduce dataset-level bias, and a text-guided component disentanglement loss to guide component separation using language supervision implicitly. Extensive experiments on CelebAMask-HQ, LaPa, and Helen demonstrate the difficulty of WSFP and the effectiveness of DisFaceRep, which significantly outperforms existing weakly supervised semantic segmentation methods. The code will be released at \href{https://github.com/CVI-SZU/DisFaceRep}{\textcolor{cyan}{https://github.com/CVI-SZU/DisFaceRep}}.
<div id='section'>Paperid: <span id='pid'>35, <a href='https://arxiv.org/pdf/2508.00312.pdf' target='_blank'>https://arxiv.org/pdf/2508.00312.pdf</a></span>   <span><a href='https://github.com/Sumutan/GV-VAD.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Suhang Cai, Xiaohao Peng, Chong Wang, Xiaojie Cai, Jiangbo Qian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00312">GV-VAD : Exploring Video Generation for Weakly-Supervised Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video anomaly detection (VAD) plays a critical role in public safety applications such as intelligent surveillance. However, the rarity, unpredictability, and high annotation cost of real-world anomalies make it difficult to scale VAD datasets, which limits the performance and generalization ability of existing models. To address this challenge, we propose a generative video-enhanced weakly-supervised video anomaly detection (GV-VAD) framework that leverages text-conditioned video generation models to produce semantically controllable and physically plausible synthetic videos. These virtual videos are used to augment training data at low cost. In addition, a synthetic sample loss scaling strategy is utilized to control the influence of generated synthetic samples for efficient training. The experiments show that the proposed framework outperforms state-of-the-art methods on UCF-Crime datasets. The code is available at https://github.com/Sumutan/GV-VAD.git.
<div id='section'>Paperid: <span id='pid'>36, <a href='https://arxiv.org/pdf/2507.20976.pdf' target='_blank'>https://arxiv.org/pdf/2507.20976.pdf</a></span>   <span><a href='https://humansensinglab.github.io/AGenDA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Fang, Minhyek Jeon, Zheyang Qin, Stanislav Panev, Celso de Melo, Shuowen Hu, Shayok Chakraborty, Fernando De la Torre
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20976">Adapting Vehicle Detectors for Aerial Imagery to Unseen Domains with Weak Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detecting vehicles in aerial imagery is a critical task with applications in traffic monitoring, urban planning, and defense intelligence. Deep learning methods have provided state-of-the-art (SOTA) results for this application. However, a significant challenge arises when models trained on data from one geographic region fail to generalize effectively to other areas. Variability in factors such as environmental conditions, urban layouts, road networks, vehicle types, and image acquisition parameters (e.g., resolution, lighting, and angle) leads to domain shifts that degrade model performance. This paper proposes a novel method that uses generative AI to synthesize high-quality aerial images and their labels, improving detector training through data augmentation. Our key contribution is the development of a multi-stage, multi-modal knowledge transfer framework utilizing fine-tuned latent diffusion models (LDMs) to mitigate the distribution gap between the source and target environments. Extensive experiments across diverse aerial imagery domains show consistent performance improvements in AP50 over supervised learning on source domain data, weakly supervised adaptation methods, unsupervised domain adaptation methods, and open-set object detectors by 4-23%, 6-10%, 7-40%, and more than 50%, respectively. Furthermore, we introduce two newly annotated aerial datasets from New Zealand and Utah to support further research in this field. Project page is available at: https://humansensinglab.github.io/AGenDA
<div id='section'>Paperid: <span id='pid'>37, <a href='https://arxiv.org/pdf/2507.18677.pdf' target='_blank'>https://arxiv.org/pdf/2507.18677.pdf</a></span>   <span><a href='https://github.com/SiyuMU/Loaded2UnNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyu Mu, Wei Xuan Chan, Choon Hwai Yap
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18677">HeartUnloadNet: A Weakly-Supervised Cycle-Consistent Graph Network for Predicting Unloaded Cardiac Geometry from Diastolic States</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The unloaded cardiac geometry (i.e., the state of the heart devoid of luminal pressure) serves as a valuable zero-stress and zero-strain reference and is critical for personalized biomechanical modeling of cardiac function, to understand both healthy and diseased physiology and to predict the effects of cardiac interventions. However, estimating the unloaded geometry from clinical images remains a challenging task. Traditional approaches rely on inverse finite element (FE) solvers that require iterative optimization and are computationally expensive. In this work, we introduce HeartUnloadNet, a deep learning framework that predicts the unloaded left ventricular (LV) shape directly from the end diastolic (ED) mesh while explicitly incorporating biophysical priors. The network accepts a mesh of arbitrary size along with physiological parameters such as ED pressure, myocardial stiffness scale, and fiber helix orientation, and outputs the corresponding unloaded mesh. It adopts a graph attention architecture and employs a cycle-consistency strategy to enable bidirectional (loading and unloading) prediction, allowing for partial self-supervision that improves accuracy and reduces the need for large training datasets. Trained and tested on 20,700 FE simulations across diverse LV geometries and physiological conditions, HeartUnloadNet achieves sub-millimeter accuracy, with an average DSC of 0.986 and HD of 0.083 cm, while reducing inference time to just 0.02 seconds per case, over 10^5 times faster and significantly more accurate than traditional inverse FE solvers. Ablation studies confirm the effectiveness of the architecture. Notably, the cycle-consistent design enables the model to maintain a DSC of 97% even with as few as 200 training samples. This work thus presents a scalable and accurate surrogate for inverse FE solvers, supporting real-time clinical applications in the future.
<div id='section'>Paperid: <span id='pid'>38, <a href='https://arxiv.org/pdf/2507.10935.pdf' target='_blank'>https://arxiv.org/pdf/2507.10935.pdf</a></span>   <span><a href='https://github.com/tongshw/GeoDistill' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaowen Tong, Zimin Xia, Alexandre Alahi, Xuming He, Yujiao Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10935">GeoDistill: Geometry-Guided Self-Distillation for Weakly Supervised Cross-View Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-view localization, the task of estimating a camera's 3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with satellite images, is crucial for large-scale outdoor applications like autonomous navigation and augmented reality. Existing methods often rely on fully supervised learning, which requires costly ground-truth pose annotations. In this work, we propose GeoDistill, a Geometry guided weakly supervised self distillation framework that uses teacher-student learning with Field-of-View (FoV)-based masking to enhance local feature learning for robust cross-view localization. In GeoDistill, the teacher model localizes a panoramic image, while the student model predicts locations from a limited FoV counterpart created by FoV-based masking. By aligning the student's predictions with those of the teacher, the student focuses on key features like lane lines and ignores textureless regions, such as roads. This results in more accurate predictions and reduced uncertainty, regardless of whether the query images are panoramas or limited FoV images. Our experiments show that GeoDistill significantly improves localization performance across different frameworks. Additionally, we introduce a novel orientation estimation network that predicts relative orientation without requiring precise planar position ground truth. GeoDistill provides a scalable and efficient solution for real-world cross-view localization challenges. Code and model can be found at https://github.com/tongshw/GeoDistill.
<div id='section'>Paperid: <span id='pid'>39, <a href='https://arxiv.org/pdf/2507.07578.pdf' target='_blank'>https://arxiv.org/pdf/2507.07578.pdf</a></span>   <span><a href='https://github.com/ChunyanWang1/DGKD-WLSS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunyan Wang, Dong Zhang, Jinhui Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07578">Diffusion-Guided Knowledge Distillation for Weakly-Supervised Low-Light Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised semantic segmentation aims to assign category labels to each pixel using weak annotations, significantly reducing manual annotation costs. Although existing methods have achieved remarkable progress in well-lit scenarios, their performance significantly degrades in low-light environments due to two fundamental limitations: severe image quality degradation (e.g., low contrast, noise, and color distortion) and the inherent constraints of weak supervision. These factors collectively lead to unreliable class activation maps and semantically ambiguous pseudo-labels, ultimately compromising the model's ability to learn discriminative feature representations. To address these problems, we propose Diffusion-Guided Knowledge Distillation for Weakly-Supervised Low-light Semantic Segmentation (DGKD-WLSS), a novel framework that synergistically combines Diffusion-Guided Knowledge Distillation (DGKD) with Depth-Guided Feature Fusion (DGF2). DGKD aligns normal-light and low-light features via diffusion-based denoising and knowledge distillation, while DGF2 integrates depth maps as illumination-invariant geometric priors to enhance structural feature learning. Extensive experiments demonstrate the effectiveness of DGKD-WLSS, which achieves state-of-the-art performance in weakly supervised semantic segmentation tasks under low-light conditions. The source codes have been released at:https://github.com/ChunyanWang1/DGKD-WLSS.
<div id='section'>Paperid: <span id='pid'>40, <a href='https://arxiv.org/pdf/2507.04839.pdf' target='_blank'>https://arxiv.org/pdf/2507.04839.pdf</a></span>   <span><a href='https://github.com/fraunhoferhhi/RIPE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Johannes KÃ¼nzel, Anna Hilsmann, Peter Eisert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04839">RIPE: Reinforcement Learning on Unlabeled Image Pairs for Robust Keypoint Extraction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce RIPE, an innovative reinforcement learning-based framework for weakly-supervised training of a keypoint extractor that excels in both detection and description tasks. In contrast to conventional training regimes that depend heavily on artificial transformations, pre-generated models, or 3D data, RIPE requires only a binary label indicating whether paired images represent the same scene. This minimal supervision significantly expands the pool of training data, enabling the creation of a highly generalized and robust keypoint extractor.
  RIPE utilizes the encoder's intermediate layers for the description of the keypoints with a hyper-column approach to integrate information from different scales. Additionally, we propose an auxiliary loss to enhance the discriminative capability of the learned descriptors.
  Comprehensive evaluations on standard benchmarks demonstrate that RIPE simplifies data preparation while achieving competitive performance compared to state-of-the-art techniques, marking a significant advancement in robust keypoint extraction and description. To support further research, we have made our code publicly available at https://github.com/fraunhoferhhi/RIPE.
<div id='section'>Paperid: <span id='pid'>41, <a href='https://arxiv.org/pdf/2507.02751.pdf' target='_blank'>https://arxiv.org/pdf/2507.02751.pdf</a></span>   <span><a href='https://github.com/VisionXLab/PWOOD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingxin Liu, Peiyuan Zhang, Yuan Liu, Wei Zhang, Yue Zhou, Ning Liao, Ziyang Gong, Junwei Luo, Zhirui Wang, Yi Yu, Xue Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02751">Partial Weakly-Supervised Oriented Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing demand for oriented object detection (OOD) across various domains has driven significant research in this area. However, the high cost of dataset annotation remains a major concern. Current mainstream OOD algorithms can be mainly categorized into three types: (1) fully supervised methods using complete oriented bounding box (OBB) annotations, (2) semi-supervised methods using partial OBB annotations, and (3) weakly supervised methods using weak annotations such as horizontal boxes or points. However, these algorithms inevitably increase the cost of models in terms of annotation speed or annotation cost. To address this issue, we propose:(1) the first Partial Weakly-Supervised Oriented Object Detection (PWOOD) framework based on partially weak annotations (horizontal boxes or single points), which can efficiently leverage large amounts of unlabeled data, significantly outperforming weakly supervised algorithms trained with partially weak annotations, also offers a lower cost solution; (2) Orientation-and-Scale-aware Student (OS-Student) model capable of learning orientation and scale information with only a small amount of orientation-agnostic or scale-agnostic weak annotations; and (3) Class-Agnostic Pseudo-Label Filtering strategy (CPF) to reduce the model's sensitivity to static filtering thresholds. Comprehensive experiments on DOTA-v1.0/v1.5/v2.0 and DIOR datasets demonstrate that our PWOOD framework performs comparably to, or even surpasses, traditional semi-supervised algorithms.
<div id='section'>Paperid: <span id='pid'>42, <a href='https://arxiv.org/pdf/2507.02743.pdf' target='_blank'>https://arxiv.org/pdf/2507.02743.pdf</a></span>   <span><a href='https://github.com/Minimel/box-prompt-learning-VFM.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>MÃ©lanie Gaillochet, Mehrdad Noori, Sahar Dastani, Christian Desrosiers, HervÃ© Lombaert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02743">Prompt learning with bounding box constraints for medical image segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pixel-wise annotations are notoriously labourious and costly to obtain in the medical domain. To mitigate this burden, weakly supervised approaches based on bounding box annotations-much easier to acquire-offer a practical alternative. Vision foundation models have recently shown noteworthy segmentation performance when provided with prompts such as points or bounding boxes. Prompt learning exploits these models by adapting them to downstream tasks and automating segmentation, thereby reducing user intervention. However, existing prompt learning approaches depend on fully annotated segmentation masks. This paper proposes a novel framework that combines the representational power of foundation models with the annotation efficiency of weakly supervised segmentation. More specifically, our approach automates prompt generation for foundation models using only bounding box annotations. Our proposed optimization scheme integrates multiple constraints derived from box annotations with pseudo-labels generated by the prompted foundation model. Extensive experiments across multimodal datasets reveal that our weakly supervised method achieves an average Dice score of 84.90% in a limited data setting, outperforming existing fully-supervised and weakly-supervised approaches. The code is available at https://github.com/Minimel/box-prompt-learning-VFM.git
<div id='section'>Paperid: <span id='pid'>43, <a href='https://arxiv.org/pdf/2507.01384.pdf' target='_blank'>https://arxiv.org/pdf/2507.01384.pdf</a></span>   <span><a href='https://github.com/WangLY136/MUG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Langyu Wang, Bingke Zhu, Yingying Chen, Yiyuan Zhang, Ming Tang, Jinqiao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01384">MUG: Pseudo Labeling Augmented Audio-Visual Mamba Network for Audio-Visual Video Parsing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The weakly-supervised audio-visual video parsing (AVVP) aims to predict all modality-specific events and locate their temporal boundaries. Despite significant progress, due to the limitations of the weakly-supervised and the deficiencies of the model architecture, existing methods are lacking in simultaneously improving both the segment-level prediction and the event-level prediction. In this work, we propose a audio-visual Mamba network with pseudo labeling aUGmentation (MUG) for emphasising the uniqueness of each segment and excluding the noise interference from the alternate modalities. Specifically, we annotate some of the pseudo-labels based on previous work. Using unimodal pseudo-labels, we perform cross-modal random combinations to generate new data, which can enhance the model's ability to parse various segment-level event combinations. For feature processing and interaction, we employ a audio-visual mamba network. The AV-Mamba enhances the ability to perceive different segments and excludes additional modal noise while sharing similar modal information. Our extensive experiments demonstrate that MUG improves state-of-the-art results on LLP dataset in all metrics (e.g,, gains of 2.1% and 1.2% in terms of visual Segment-level and audio Segment-level metrics). Our code is available at https://github.com/WangLY136/MUG.
<div id='section'>Paperid: <span id='pid'>44, <a href='https://arxiv.org/pdf/2506.23648.pdf' target='_blank'>https://arxiv.org/pdf/2506.23648.pdf</a></span>   <span><a href='https://github.com/cskdstz/MReg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Liu, Yuhao Huang, Lian Liu, Chengrui Zhang, Haotian Lin, Tong Han, Zhiyuan Zhu, Yanlin Chen, Yuerui Chen, Dong Ni, Zhongshan Gou, Xin Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23648">MReg: A Novel Regression Model with MoE-based Video Feature Mining for Mitral Regurgitation Diagnosis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Color Doppler echocardiography is a crucial tool for diagnosing mitral regurgitation (MR). Recent studies have explored intelligent methods for MR diagnosis to minimize user dependence and improve accuracy. However, these approaches often fail to align with clinical workflow and may lead to suboptimal accuracy and interpretability. In this study, we introduce an automated MR diagnosis model (MReg) developed on the 4-chamber cardiac color Doppler echocardiography video (A4C-CDV). It follows comprehensive feature mining strategies to detect MR and assess its severity, considering clinical realities. Our contribution is threefold. First, we formulate the MR diagnosis as a regression task to capture the continuity and ordinal relationships between categories. Second, we design a feature selection and amplification mechanism to imitate the sonographer's diagnostic logic for accurate MR grading. Third, inspired by the Mixture-of-Experts concept, we introduce a feature summary module to extract the category-level features, enhancing the representational capacity for more accurate grading. We trained and evaluated our proposed MReg on a large in-house A4C-CDV dataset comprising 1868 cases with three graded regurgitation labels. Compared to other weakly supervised video anomaly detection and supervised classification methods, MReg demonstrated superior performance in MR diagnosis. Our code is available at: https://github.com/cskdstz/MReg.
<div id='section'>Paperid: <span id='pid'>45, <a href='https://arxiv.org/pdf/2506.22505.pdf' target='_blank'>https://arxiv.org/pdf/2506.22505.pdf</a></span>   <span><a href='GitHub' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/bakerhassan/WSOS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hassan Baker, Matthew S. Emigh, Austin J. Brockmeier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22505">Weakly Supervised Object Segmentation by Background Conditional Divergence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As a computer vision task, automatic object segmentation remains challenging in specialized image domains without massive labeled data, such as synthetic aperture sonar images, remote sensing, biomedical imaging, etc. In any domain, obtaining pixel-wise segmentation masks is expensive. In this work, we propose a method for training a masking network to perform binary object segmentation using weak supervision in the form of image-wise presence or absence of an object of interest, which provides less information but may be obtained more quickly from manual or automatic labeling. A key step in our method is that the segmented objects can be placed into background-only images to create realistic, images of the objects with counterfactual backgrounds. To create a contrast between the original and counterfactual background images, we propose to first cluster the background-only images, and then during learning create counterfactual images that blend objects segmented from their original source backgrounds to backgrounds chosen from a targeted cluster. One term in the training loss is the divergence between these counterfactual images and the real object images with backgrounds of the target cluster. The other term is a supervised loss for background-only images. While an adversarial critic could provide the divergence, we use sample-based divergences. We conduct experiments on side-scan and synthetic aperture sonar in which our approach succeeds compared to previous unsupervised segmentation baselines that were only tested on natural images. Furthermore, to show generality we extend our experiments to natural images, obtaining reasonable performance with our method that avoids pretrained networks, generative networks, and adversarial critics. The basecode for this work can be found at \href{GitHub}{https://github.com/bakerhassan/WSOS}.
<div id='section'>Paperid: <span id='pid'>46, <a href='https://arxiv.org/pdf/2506.09022.pdf' target='_blank'>https://arxiv.org/pdf/2506.09022.pdf</a></span>   <span><a href='https://github.com/mahmoodlab/MIL-Lab' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Shao, Richard J. Chen, Andrew H. Song, Joel Runevic, Ming Y. Lu, Tong Ding, Faisal Mahmood
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09022">Do Multiple Instance Learning Models Transfer?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple Instance Learning (MIL) is a cornerstone approach in computational pathology (CPath) for generating clinically meaningful slide-level embeddings from gigapixel tissue images. However, MIL often struggles with small, weakly supervised clinical datasets. In contrast to fields such as NLP and conventional computer vision, where transfer learning is widely used to address data scarcity, the transferability of MIL models remains poorly understood. In this study, we systematically evaluate the transfer learning capabilities of pretrained MIL models by assessing 11 models across 21 pretraining tasks for morphological and molecular subtype prediction. Our results show that pretrained MIL models, even when trained on different organs than the target task, consistently outperform models trained from scratch. Moreover, pretraining on pancancer datasets enables strong generalization across organs and tasks, outperforming slide foundation models while using substantially less pretraining data. These findings highlight the robust adaptability of MIL models and demonstrate the benefits of leveraging transfer learning to boost performance in CPath. Lastly, we provide a resource which standardizes the implementation of MIL models and collection of pretrained model weights on popular CPath tasks, available at https://github.com/mahmoodlab/MIL-Lab
<div id='section'>Paperid: <span id='pid'>47, <a href='https://arxiv.org/pdf/2506.06589.pdf' target='_blank'>https://arxiv.org/pdf/2506.06589.pdf</a></span>   <span><a href='https://github.com/jacqueline-he/precise-information-control' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jacqueline He, Howard Yen, Margaret Li, Shuyue Stella Li, Zhiyuan Zeng, Weijia Shi, Yulia Tsvetkov, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06589">Precise Information Control in Long-Form Text Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A central challenge in modern language models (LMs) is intrinsic hallucination: the generation of information that is plausible but unsubstantiated relative to input context. To study this problem, we propose Precise Information Control (PIC), a new task formulation that requires models to generate long-form outputs grounded in a provided set of short self-contained statements, known as verifiable claims, without adding any unsupported ones. For comprehensiveness, PIC includes a full setting that tests a model's ability to include exactly all input claims, and a partial setting that requires the model to selectively incorporate only relevant claims. We present PIC-Bench, a benchmark of eight long-form generation tasks (e.g., summarization, biography generation) adapted to the PIC setting, where LMs are supplied with well-formed, verifiable input claims. Our evaluation of a range of open and proprietary LMs on PIC-Bench reveals that, surprisingly, state-of-the-art LMs still intrinsically hallucinate in over 70% of outputs. To alleviate this lack of faithfulness, we introduce a post-training framework, using a weakly supervised preference data construction method, to train an 8B PIC-LM with stronger PIC ability--improving from 69.1% to 91.0% F1 in the full PIC setting. When integrated into end-to-end factual generation pipelines, PIC-LM improves exact match recall by 17.1% on ambiguous QA with retrieval, and factual precision by 30.5% on a birthplace verification task, underscoring the potential of precisely grounded generation.
<div id='section'>Paperid: <span id='pid'>48, <a href='https://arxiv.org/pdf/2506.05895.pdf' target='_blank'>https://arxiv.org/pdf/2506.05895.pdf</a></span>   <span><a href='https://github.com/adrienpetralia/CamAL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Adrien Petralia, Paul Boniol, Philippe Charpentier, Themis Palpanas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05895">Few Labels are all you need: A Weakly Supervised Framework for Appliance Localization in Smart-Meter Series</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Improving smart grid system management is crucial in the fight against climate change, and enabling consumers to play an active role in this effort is a significant challenge for electricity suppliers. In this regard, millions of smart meters have been deployed worldwide in the last decade, recording the main electricity power consumed in individual households. This data produces valuable information that can help them reduce their electricity footprint; nevertheless, the collected signal aggregates the consumption of the different appliances running simultaneously in the house, making it difficult to apprehend. Non-Intrusive Load Monitoring (NILM) refers to the challenge of estimating the power consumption, pattern, or on/off state activation of individual appliances using the main smart meter signal. Recent methods proposed to tackle this task are based on a fully supervised deep-learning approach that requires both the aggregate signal and the ground truth of individual appliance power. However, such labels are expensive to collect and extremely scarce in practice, as they require conducting intrusive surveys in households to monitor each appliance. In this paper, we introduce CamAL, a weakly supervised approach for appliance pattern localization that only requires information on the presence of an appliance in a household to be trained. CamAL merges an ensemble of deep-learning classifiers combined with an explainable classification method to be able to localize appliance patterns. Our experimental evaluation, conducted on 4 real-world datasets, demonstrates that CamAL significantly outperforms existing weakly supervised baselines and that current SotA fully supervised NILM approaches require significantly more labels to reach CamAL performances. The source of our experiments is available at: https://github.com/adrienpetralia/CamAL. This paper appeared in ICDE 2025.
<div id='section'>Paperid: <span id='pid'>49, <a href='https://arxiv.org/pdf/2505.24824.pdf' target='_blank'>https://arxiv.org/pdf/2505.24824.pdf</a></span>   <span><a href='https://github.com/Archiel19/FRAx4.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Marta LÃ³pez-Rauhut, Hongyu Zhou, Mathieu Aubry, Loic Landrieu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24824">Segmenting France Across Four Centuries</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Historical maps offer an invaluable perspective into territory evolution across past centuries--long before satellite or remote sensing technologies existed. Deep learning methods have shown promising results in segmenting historical maps, but publicly available datasets typically focus on a single map type or period, require extensive and costly annotations, and are not suited for nationwide, long-term analyses. In this paper, we introduce a new dataset of historical maps tailored for analyzing large-scale, long-term land use and land cover evolution with limited annotations. Spanning metropolitan France (548,305 km^2), our dataset contains three map collections from the 18th, 19th, and 20th centuries. We provide both comprehensive modern labels and 22,878 km^2 of manually annotated historical labels for the 18th and 19th century maps. Our dataset illustrates the complexity of the segmentation task, featuring stylistic inconsistencies, interpretive ambiguities, and significant landscape changes (e.g., marshlands disappearing in favor of forests). We assess the difficulty of these challenges by benchmarking three approaches: a fully-supervised model trained with historical labels, and two weakly-supervised models that rely only on modern annotations. The latter either use the modern labels directly or first perform image-to-image translation to address the stylistic gap between historical and contemporary maps. Finally, we discuss how these methods can support long-term environment monitoring, offering insights into centuries of landscape transformation. Our official project repository is publicly available at https://github.com/Archiel19/FRAx4.git.
<div id='section'>Paperid: <span id='pid'>50, <a href='https://arxiv.org/pdf/2505.24103.pdf' target='_blank'>https://arxiv.org/pdf/2505.24103.pdf</a></span>   <span><a href='https://github.com/woyut/WSAG-PLSP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Peiran Xu, Yadong Mu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24103">Weakly-Supervised Affordance Grounding Guided by Part-Level Semantic Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we focus on the task of weakly supervised affordance grounding, where a model is trained to identify affordance regions on objects using human-object interaction images and egocentric object images without dense labels. Previous works are mostly built upon class activation maps, which are effective for semantic segmentation but may not be suitable for locating actions and functions. Leveraging recent advanced foundation models, we develop a supervised training pipeline based on pseudo labels. The pseudo labels are generated from an off-the-shelf part segmentation model, guided by a mapping from affordance to part names. Furthermore, we introduce three key enhancements to the baseline model: a label refining stage, a fine-grained feature alignment process, and a lightweight reasoning module. These techniques harness the semantic knowledge of static objects embedded in off-the-shelf foundation models to improve affordance learning, effectively bridging the gap between objects and actions. Extensive experiments demonstrate that the performance of the proposed model has achieved a breakthrough improvement over existing methods. Our codes are available at https://github.com/woyut/WSAG-PLSP .
<div id='section'>Paperid: <span id='pid'>51, <a href='https://arxiv.org/pdf/2505.22028.pdf' target='_blank'>https://arxiv.org/pdf/2505.22028.pdf</a></span>   <span><a href='https://github.com/Speechless-10308/WSC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zi-Hao Zhou, Jun-Jie Wang, Tong Wei, Min-Ling Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22028">Weakly-Supervised Contrastive Learning for Imprecise Class Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contrastive learning has achieved remarkable success in learning effective representations, with supervised contrastive learning often outperforming self-supervised approaches. However, in real-world scenarios, data annotations are often ambiguous or inaccurate, meaning that class labels may not reliably indicate whether two examples belong to the same class. This limitation restricts the applicability of supervised contrastive learning. To address this challenge, we introduce the concept of ``continuous semantic similarity'' to define positive and negative pairs. Instead of directly relying on imprecise class labels, we measure the semantic similarity between example pairs, which quantifies how closely they belong to the same category by iteratively refining weak supervisory signals. Based on this concept, we propose a graph-theoretic framework for weakly-supervised contrastive learning, where semantic similarity serves as the graph weights. Our framework is highly versatile and can be applied to many weakly-supervised learning scenarios. We demonstrate its effectiveness through experiments in two common settings, i.e., noisy label and partial label learning, where existing methods can be easily integrated to significantly improve performance. Theoretically, we establish an error bound for our approach, showing that it can approximate supervised contrastive learning under mild conditions. The implementation code is available at https://github.com/Speechless-10308/WSC.
<div id='section'>Paperid: <span id='pid'>52, <a href='https://arxiv.org/pdf/2505.19190.pdf' target='_blank'>https://arxiv.org/pdf/2505.19190.pdf</a></span>   <span><a href='https://github.com/Raina-Xin/I2MoE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayi Xin, Sukwon Yun, Jie Peng, Inyoung Choi, Jenna L. Ballard, Tianlong Chen, Qi Long
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19190">I2MoE: Interpretable Multimodal Interaction-aware Mixture-of-Experts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modality fusion is a cornerstone of multimodal learning, enabling information integration from diverse data sources. However, vanilla fusion methods are limited by (1) inability to account for heterogeneous interactions between modalities and (2) lack of interpretability in uncovering the multimodal interactions inherent in the data. To this end, we propose I2MoE (Interpretable Multimodal Interaction-aware Mixture of Experts), an end-to-end MoE framework designed to enhance modality fusion by explicitly modeling diverse multimodal interactions, as well as providing interpretation on a local and global level. First, I2MoE utilizes different interaction experts with weakly supervised interaction losses to learn multimodal interactions in a data-driven way. Second, I2MoE deploys a reweighting model that assigns importance scores for the output of each interaction expert, which offers sample-level and dataset-level interpretation. Extensive evaluation of medical and general multimodal datasets shows that I2MoE is flexible enough to be combined with different fusion techniques, consistently improves task performance, and provides interpretation across various real-world scenarios. Code is available at https://github.com/Raina-Xin/I2MoE.
<div id='section'>Paperid: <span id='pid'>53, <a href='https://arxiv.org/pdf/2505.19022.pdf' target='_blank'>https://arxiv.org/pdf/2505.19022.pdf</a></span>   <span><a href='https://github.com/Kamino666/RethinkingVAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihao Liu, Xiaoyu Wu, Wenna Li, Linlin Yang, Shengjin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19022">Rethinking Metrics and Benchmarks of Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Anomaly Detection (VAD), which aims to detect anomalies that deviate from expectation, has attracted increasing attention in recent years. Existing advancements in VAD primarily focus on model architectures and training strategies, while devoting insufficient attention to evaluation metrics and benchmarks. In this paper, we rethink VAD evaluation methods through comprehensive analyses, revealing three critical limitations in current practices: 1) existing metrics are significantly influenced by single annotation bias; 2) current metrics fail to reward early detection of anomalies; 3) available benchmarks lack the capability to evaluate scene overfitting of fully/weakly-supervised algorithms. To address these limitations, we propose three novel evaluation methods: first, we establish probabilistic AUC/AP (Prob-AUC/AP) metrics utlizing multi-round annotations to mitigate single annotation bias; second, we develop a Latency-aware Average Precision (LaAP) metric that rewards early and accurate anomaly detection; and finally, we introduce two hard normal benchmarks (UCF-HN, MSAD-HN) with videos specifically designed to evaluate scene overfitting. We report performance comparisons of ten state-of-the-art VAD approaches using our proposed evaluation methods, providing novel perspectives for future VAD model development. We release our data and code in https://github.com/Kamino666/RethinkingVAD.
<div id='section'>Paperid: <span id='pid'>54, <a href='https://arxiv.org/pdf/2505.18686.pdf' target='_blank'>https://arxiv.org/pdf/2505.18686.pdf</a></span>   <span><a href='https://github.com/MRUIL/WeakMCN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Liu, Silin Cheng, Xinwei He, Sebastien Ourselin, Lei Tan, Gen Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18686">WeakMCN: Multi-task Collaborative Network for Weakly Supervised Referring Expression Comprehension and Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised referring expression comprehension(WREC) and segmentation(WRES) aim to learn object grounding based on a given expression using weak supervision signals like image-text pairs. While these tasks have traditionally been modeled separately, we argue that they can benefit from joint learning in a multi-task framework. To this end, we propose WeakMCN, a novel multi-task collaborative network that effectively combines WREC and WRES with a dual-branch architecture. Specifically, the WREC branch is formulated as anchor-based contrastive learning, which also acts as a teacher to supervise the WRES branch. In WeakMCN, we propose two innovative designs to facilitate multi-task collaboration, namely Dynamic Visual Feature Enhancement(DVFE) and Collaborative Consistency Module(CCM). DVFE dynamically combines various pre-trained visual knowledge to meet different task requirements, while CCM promotes cross-task consistency from the perspective of optimization. Extensive experimental results on three popular REC and RES benchmarks, i.e., RefCOCO, RefCOCO+, and RefCOCOg, consistently demonstrate performance gains of WeakMCN over state-of-the-art single-task alternatives, e.g., up to 3.91% and 13.11% on RefCOCO for WREC and WRES tasks, respectively. Furthermore, experiments also validate the strong generalization ability of WeakMCN in both semi-supervised REC and RES settings against existing methods, e.g., +8.94% for semi-REC and +7.71% for semi-RES on 1% RefCOCO. The code is publicly available at https://github.com/MRUIL/WeakMCN.
<div id='section'>Paperid: <span id='pid'>55, <a href='https://arxiv.org/pdf/2505.17982.pdf' target='_blank'>https://arxiv.org/pdf/2505.17982.pdf</a></span>   <span><a href='https://github.com/bryanwong17/HiVE-MIL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bryan Wong, Jong Woo Kim, Huazhu Fu, Mun Yong Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17982">Few-Shot Learning from Gigapixel Images via Hierarchical Vision-Language Alignment and Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language models (VLMs) have recently been integrated into multiple instance learning (MIL) frameworks to address the challenge of few-shot, weakly supervised classification of whole slide images (WSIs). A key trend involves leveraging multi-scale information to better represent hierarchical tissue structures. However, existing methods often face two key limitations: (1) insufficient modeling of interactions within the same modalities across scales (e.g., 5x and 20x) and (2) inadequate alignment between visual and textual modalities on the same scale. To address these gaps, we propose HiVE-MIL, a hierarchical vision-language framework that constructs a unified graph consisting of (1) parent-child links between coarse (5x) and fine (20x) visual/textual nodes to capture hierarchical relationships, and (2) heterogeneous intra-scale edges linking visual and textual nodes on the same scale. To further enhance semantic consistency, HiVE-MIL incorporates a two-stage, text-guided dynamic filtering mechanism that removes weakly correlated patch-text pairs, and introduces a hierarchical contrastive loss to align textual semantics across scales. Extensive experiments on TCGA breast, lung, and kidney cancer datasets demonstrate that HiVE-MIL consistently outperforms both traditional MIL and recent VLM-based MIL approaches, achieving gains of up to 4.1% in macro F1 under 16-shot settings. Our results demonstrate the value of jointly modeling hierarchical structure and multimodal alignment for efficient and scalable learning from limited pathology data. The code is available at https://github.com/bryanwong17/HiVE-MIL
<div id='section'>Paperid: <span id='pid'>56, <a href='https://arxiv.org/pdf/2505.16399.pdf' target='_blank'>https://arxiv.org/pdf/2505.16399.pdf</a></span>   <span><a href='https://github.com/dengq7/Sketchy-3DIS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qian Deng, Le Hui, Jin Xie, Jian Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16399">Sketchy Bounding-box Supervision for 3D Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Bounding box supervision has gained considerable attention in weakly supervised 3D instance segmentation. While this approach alleviates the need for extensive point-level annotations, obtaining accurate bounding boxes in practical applications remains challenging. To this end, we explore the inaccurate bounding box, named sketchy bounding box, which is imitated through perturbing ground truth bounding box by adding scaling, translation, and rotation. In this paper, we propose Sketchy-3DIS, a novel weakly 3D instance segmentation framework, which jointly learns pseudo labeler and segmentator to improve the performance under the sketchy bounding-box supervisions. Specifically, we first propose an adaptive box-to-point pseudo labeler that adaptively learns to assign points located in the overlapped parts between two sketchy bounding boxes to the correct instance, resulting in compact and pure pseudo instance labels. Then, we present a coarse-to-fine instance segmentator that first predicts coarse instances from the entire point cloud and then learns fine instances based on the region of coarse instances. Finally, by using the pseudo instance labels to supervise the instance segmentator, we can gradually generate high-quality instances through joint training. Extensive experiments show that our method achieves state-of-the-art performance on both the ScanNetV2 and S3DIS benchmarks, and even outperforms several fully supervised methods using sketchy bounding boxes. Code is available at https://github.com/dengq7/Sketchy-3DIS.
<div id='section'>Paperid: <span id='pid'>57, <a href='https://arxiv.org/pdf/2505.13905.pdf' target='_blank'>https://arxiv.org/pdf/2505.13905.pdf</a></span>   <span><a href='https://github.com/CLASS-Lab/4D-ROLLS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruihan Liu, Xiaoyi Wu, Xijun Chen, Liang Hu, Yunjiang Lou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13905">4D-ROLLS: 4D Radar Occupancy Learning via LiDAR Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A comprehensive understanding of 3D scenes is essential for autonomous vehicles (AVs), and among various perception tasks, occupancy estimation plays a central role by providing a general representation of drivable and occupied space. However, most existing occupancy estimation methods rely on LiDAR or cameras, which perform poorly in degraded environments such as smoke, rain, snow, and fog. In this paper, we propose 4D-ROLLS, the first weakly supervised occupancy estimation method for 4D radar using the LiDAR point cloud as the supervisory signal. Specifically, we introduce a method for generating pseudo-LiDAR labels, including occupancy queries and LiDAR height maps, as multi-stage supervision to train the 4D radar occupancy estimation model. Then the model is aligned with the occupancy map produced by LiDAR, fine-tuning its accuracy in occupancy estimation. Extensive comparative experiments validate the exceptional performance of 4D-ROLLS. Its robustness in degraded environments and effectiveness in cross-dataset training are qualitatively demonstrated. The model is also seamlessly transferred to downstream tasks BEV segmentation and point cloud occupancy prediction, highlighting its potential for broader applications. The lightweight network enables 4D-ROLLS model to achieve fast inference speeds at about 30 Hz on a 4060 GPU. The code of 4D-ROLLS will be made available at https://github.com/CLASS-Lab/4D-ROLLS.
<div id='section'>Paperid: <span id='pid'>58, <a href='https://arxiv.org/pdf/2505.12911.pdf' target='_blank'>https://arxiv.org/pdf/2505.12911.pdf</a></span>   <span><a href='https://github.com/sapeirone/hiero' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Simone Alberto Peirone, Francesca Pistilli, Giuseppe Averta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12911">HiERO: understanding the hierarchy of human behavior enhances reasoning on egocentric videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human activities are particularly complex and variable, and this makes challenging for deep learning models to reason about them. However, we note that such variability does have an underlying structure, composed of a hierarchy of patterns of related actions. We argue that such structure can emerge naturally from unscripted videos of human activities, and can be leveraged to better reason about their content. We present HiERO, a weakly-supervised method to enrich video segments features with the corresponding hierarchical activity threads. By aligning video clips with their narrated descriptions, HiERO infers contextual, semantic and temporal reasoning with an hierarchical architecture. We prove the potential of our enriched features with multiple video-text alignment benchmarks (EgoMCQ, EgoNLQ) with minimal additional training, and in zero-shot for procedure learning tasks (EgoProceL and Ego4D Goal-Step). Notably, HiERO achieves state-of-the-art performance in all the benchmarks, and for procedure learning tasks it outperforms fully-supervised methods by a large margin (+12.5% F1 on EgoProceL) in zero shot. Our results prove the relevance of using knowledge of the hierarchy of human activities for multiple reasoning tasks in egocentric vision.
<div id='section'>Paperid: <span id='pid'>59, <a href='https://arxiv.org/pdf/2505.09263.pdf' target='_blank'>https://arxiv.org/pdf/2505.09263.pdf</a></span>   <span><a href='https://github.com/gaobb/AnoGen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guan Gui, Bin-Bin Gao, Jun Liu, Chengjie Wang, Yunsheng Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09263">Few-Shot Anomaly-Driven Generation for Anomaly Classification and Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anomaly detection is a practical and challenging task due to the scarcity of anomaly samples in industrial inspection. Some existing anomaly detection methods address this issue by synthesizing anomalies with noise or external data. However, there is always a large semantic gap between synthetic and real-world anomalies, resulting in weak performance in anomaly detection. To solve the problem, we propose a few-shot Anomaly-driven Generation (AnoGen) method, which guides the diffusion model to generate realistic and diverse anomalies with only a few real anomalies, thereby benefiting training anomaly detection models. Specifically, our work is divided into three stages. In the first stage, we learn the anomaly distribution based on a few given real anomalies and inject the learned knowledge into an embedding. In the second stage, we use the embedding and given bounding boxes to guide the diffusion model to generate realistic and diverse anomalies on specific objects (or textures). In the final stage, we propose a weakly-supervised anomaly detection method to train a more powerful model with generated anomalies. Our method builds upon DRAEM and DesTSeg as the foundation model and conducts experiments on the commonly used industrial anomaly detection dataset, MVTec. The experiments demonstrate that our generated anomalies effectively improve the model performance of both anomaly classification and segmentation tasks simultaneously, \eg, DRAEM and DseTSeg achieved a 5.8\% and 1.5\% improvement in AU-PR metric on segmentation task, respectively. The code and generated anomalous data are available at https://github.com/gaobb/AnoGen.
<div id='section'>Paperid: <span id='pid'>60, <a href='https://arxiv.org/pdf/2505.07817.pdf' target='_blank'>https://arxiv.org/pdf/2505.07817.pdf</a></span>   <span><a href='https://kahnchana.github.io/LangToMo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kanchana Ranasinghe, Xiang Li, E-Ro Nguyen, Cristina Mata, Jongwoo Park, Michael S Ryoo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07817">Pixel Motion as Universal Representation for Robot Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present LangToMo, a vision-language-action framework structured as a dual-system architecture that uses pixel motion forecasts as intermediate representations. Our high-level System 2, an image diffusion model, generates text-conditioned pixel motion sequences from a single frame to guide robot control. Pixel motion-a universal, interpretable, and motion-centric representation-can be extracted from videos in a weakly-supervised manner, enabling diffusion model training on any video-caption data. Treating generated pixel motion as learned universal representations, our low level System 1 module translates these into robot actions via motion-to-action mapping functions, which can be either hand-crafted or learned with minimal supervision. System 2 operates as a high-level policy applied at sparse temporal intervals, while System 1 acts as a low-level policy at dense temporal intervals. This hierarchical decoupling enables flexible, scalable, and generalizable robot control under both unsupervised and supervised settings, bridging the gap between language, motion, and action. Checkout https://kahnchana.github.io/LangToMo
<div id='section'>Paperid: <span id='pid'>61, <a href='https://arxiv.org/pdf/2505.03207.pdf' target='_blank'>https://arxiv.org/pdf/2505.03207.pdf</a></span>   <span><a href='https://github.com/xyt-ml/PLC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yutong Xie, Fuchao Yang, Yuheng Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03207">Partial Label Clustering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partial label learning (PLL) is a significant weakly supervised learning framework, where each training example corresponds to a set of candidate labels and only one label is the ground-truth label. For the first time, this paper investigates the partial label clustering problem, which takes advantage of the limited available partial labels to improve the clustering performance. Specifically, we first construct a weight matrix of examples based on their relationships in the feature space and disambiguate the candidate labels to estimate the ground-truth label based on the weight matrix. Then, we construct a set of must-link and cannot-link constraints based on the disambiguation results. Moreover, we propagate the initial must-link and cannot-link constraints based on an adversarial prior promoted dual-graph learning approach. Finally, we integrate weight matrix construction, label disambiguation, and pairwise constraints propagation into a joint model to achieve mutual enhancement. We also theoretically prove that a better disambiguated label matrix can help improve clustering performance. Comprehensive experiments demonstrate our method realizes superior performance when comparing with state-of-the-art constrained clustering methods, and outperforms PLL and semi-supervised PLL methods when only limited samples are annotated. The code is publicly available at https://github.com/xyt-ml/PLC.
<div id='section'>Paperid: <span id='pid'>62, <a href='https://arxiv.org/pdf/2505.02179.pdf' target='_blank'>https://arxiv.org/pdf/2505.02179.pdf</a></span>   <span><a href='https://github.com/modadundun/ProDisc-VAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Zhu, Qi Yu, Xinru Dong, Shiyu Li, Yue Liu, Jinlong Jiang, Lei Shu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02179">ProDisc-VAD: An Efficient System for Weakly-Supervised Anomaly Detection in Video Surveillance Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised video anomaly detection (WS-VAD) using Multiple Instance Learning (MIL) suffers from label ambiguity, hindering discriminative feature learning. We propose ProDisc-VAD, an efficient framework tackling this via two synergistic components. The Prototype Interaction Layer (PIL) provides controlled normality modeling using a small set of learnable prototypes, establishing a robust baseline without being overwhelmed by dominant normal data. The Pseudo-Instance Discriminative Enhancement (PIDE) loss boosts separability by applying targeted contrastive learning exclusively to the most reliable extreme-scoring instances (highest/lowest scores). ProDisc-VAD achieves strong AUCs (97.98% ShanghaiTech, 87.12% UCF-Crime) using only 0.4M parameters, over 800x fewer than recent ViT-based methods like VadCLIP. Code is available at https://github.com/modadundun/ProDisc-VAD.
<div id='section'>Paperid: <span id='pid'>63, <a href='https://arxiv.org/pdf/2504.19357.pdf' target='_blank'>https://arxiv.org/pdf/2504.19357.pdf</a></span>   <span><a href='https://github.com/diku-dk/credanno' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Lu, Chong Yin, Silvia Ingala, Kenny Erleben, Michael Bachmann Nielsen, Sune Darkner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.19357">MERA: Multimodal and Multiscale Self-Explanatory Model with Considerably Reduced Annotation for Lung Nodule Diagnosis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Lung cancer, a leading cause of cancer-related deaths globally, emphasises the importance of early detection for better patient outcomes. Pulmonary nodules, often early indicators of lung cancer, necessitate accurate, timely diagnosis. Despite Explainable Artificial Intelligence (XAI) advances, many existing systems struggle providing clear, comprehensive explanations, especially with limited labelled data. This study introduces MERA, a Multimodal and Multiscale self-Explanatory model designed for lung nodule diagnosis with considerably Reduced Annotation requirements. MERA integrates unsupervised and weakly supervised learning strategies (self-supervised learning techniques and Vision Transformer architecture for unsupervised feature extraction) and a hierarchical prediction mechanism leveraging sparse annotations via semi-supervised active learning in the learned latent space. MERA explains its decisions on multiple levels: model-level global explanations via semantic latent space clustering, instance-level case-based explanations showing similar instances, local visual explanations via attention maps, and concept explanations using critical nodule attributes. Evaluations on the public LIDC dataset show MERA's superior diagnostic accuracy and self-explainability. With only 1% annotated samples, MERA achieves diagnostic accuracy comparable to or exceeding state-of-the-art methods requiring full annotation. The model's inherent design delivers comprehensive, robust, multilevel explanations aligned closely with clinical practice, enhancing trustworthiness and transparency. Demonstrated viability of unsupervised and weakly supervised learning lowers the barrier to deploying diagnostic AI in broader medical domains. Our complete code is open-source available: https://github.com/diku-dk/credanno.
<div id='section'>Paperid: <span id='pid'>64, <a href='https://arxiv.org/pdf/2504.17379.pdf' target='_blank'>https://arxiv.org/pdf/2504.17379.pdf</a></span>   <span><a href='https://github.com/tueimage/GABMIL' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/tueimage/GABMIL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hassan Keshvarikhojasteh, Mihail Tifrea, Sibylle Hess, Josien P. W. Pluim, Mitko Veta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.17379">A Spatially-Aware Multiple Instance Learning Framework for Digital Pathology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple instance learning (MIL) is a promising approach for weakly supervised classification in pathology using whole slide images (WSIs). However, conventional MIL methods such as Attention-Based Deep Multiple Instance Learning (ABMIL) typically disregard spatial interactions among patches that are crucial to pathological diagnosis. Recent advancements, such as Transformer based MIL (TransMIL), have incorporated spatial context and inter-patch relationships. However, it remains unclear whether explicitly modeling patch relationships yields similar performance gains in ABMIL, which relies solely on Multi-Layer Perceptrons (MLPs). In contrast, TransMIL employs Transformer-based layers, introducing a fundamental architectural shift at the cost of substantially increased computational complexity. In this work, we enhance the ABMIL framework by integrating interaction-aware representations to address this question. Our proposed model, Global ABMIL (GABMIL), explicitly captures inter-instance dependencies while preserving computational efficiency. Experimental results on two publicly available datasets for tumor subtyping in breast and lung cancers demonstrate that GABMIL achieves up to a 7 percentage point improvement in AUPRC and a 5 percentage point increase in the Kappa score over ABMIL, with minimal or no additional computational overhead. These findings underscore the importance of incorporating patch interactions within MIL frameworks. Our code is available at \href{https://github.com/tueimage/GABMIL}{\texttt{GABMIL}}.
<div id='section'>Paperid: <span id='pid'>65, <a href='https://arxiv.org/pdf/2504.14783.pdf' target='_blank'>https://arxiv.org/pdf/2504.14783.pdf</a></span>   <span><a href='https://github.com/ChongQingNoSubway/MILDropout' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhui Zhu, Peijie Qiu, Xiwen Chen, Zhangsihao Yang, Aristeidis Sotiras, Abolfazl Razi, Yalin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14783">How Effective Can Dropout Be in Multiple Instance Learning ?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple Instance Learning (MIL) is a popular weakly-supervised method for various applications, with a particular interest in histological whole slide image (WSI) classification. Due to the gigapixel resolution of WSI, applications of MIL in WSI typically necessitate a two-stage training scheme: first, extract features from the pre-trained backbone and then perform MIL aggregation. However, it is well-known that this suboptimal training scheme suffers from "noisy" feature embeddings from the backbone and inherent weak supervision, hindering MIL from learning rich and generalizable features. However, the most commonly used technique (i.e., dropout) for mitigating this issue has yet to be explored in MIL. In this paper, we empirically explore how effective the dropout can be in MIL. Interestingly, we observe that dropping the top-k most important instances within a bag leads to better performance and generalization even under noise attack. Based on this key observation, we propose a novel MIL-specific dropout method, termed MIL-Dropout, which systematically determines which instances to drop. Experiments on five MIL benchmark datasets and two WSI datasets demonstrate that MIL-Dropout boosts the performance of current MIL methods with a negligible computational cost. The code is available at https://github.com/ChongQingNoSubway/MILDropout.
<div id='section'>Paperid: <span id='pid'>66, <a href='https://arxiv.org/pdf/2504.11368.pdf' target='_blank'>https://arxiv.org/pdf/2504.11368.pdf</a></span>   <span><a href='https://github.com/jingkunchen/FGI.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingkun Chen, Haoran Duan, Xiao Zhang, Boyan Gao, Tao Tan, Vicente Grau, Jungong Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11368">From Gaze to Insight: Bridging Human Visual Attention and Vision Language Model Explanation for Weakly-Supervised Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical image segmentation remains challenging due to the high cost of pixel-level annotations for training. In the context of weak supervision, clinician gaze data captures regions of diagnostic interest; however, its sparsity limits its use for segmentation. In contrast, vision-language models (VLMs) provide semantic context through textual descriptions but lack the explanation precision required. Recognizing that neither source alone suffices, we propose a teacher-student framework that integrates both gaze and language supervision, leveraging their complementary strengths. Our key insight is that gaze data indicates where clinicians focus during diagnosis, while VLMs explain why those regions are significant. To implement this, the teacher model first learns from gaze points enhanced by VLM-generated descriptions of lesion morphology, establishing a foundation for guiding the student model. The teacher then directs the student through three strategies: (1) Multi-scale feature alignment to fuse visual cues with textual semantics; (2) Confidence-weighted consistency constraints to focus on reliable predictions; (3) Adaptive masking to limit error propagation in uncertain areas. Experiments on the Kvasir-SEG, NCI-ISBI, and ISIC datasets show that our method achieves Dice scores of 80.78%, 80.53%, and 84.22%, respectively-improving 3-5% over gaze baselines without increasing the annotation burden. By preserving correlations among predictions, gaze data, and lesion descriptions, our framework also maintains clinical interpretability. This work illustrates how integrating human visual attention with AI-generated semantic context can effectively overcome the limitations of individual weak supervision signals, thereby advancing the development of deployable, annotation-efficient medical AI systems. Code is available at: https://github.com/jingkunchen/FGI.git.
<div id='section'>Paperid: <span id='pid'>67, <a href='https://arxiv.org/pdf/2504.11014.pdf' target='_blank'>https://arxiv.org/pdf/2504.11014.pdf</a></span>   <span><a href='https://ies0411.github.io/GATE3D/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Eunsoo Im, Changhyun Jee, Jung Kwon Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11014">GATE3D: Generalized Attention-based Task-synergized Estimation in 3D*</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emerging trend in computer vision emphasizes developing universal models capable of simultaneously addressing multiple diverse tasks. Such universality typically requires joint training across multi-domain datasets to ensure effective generalization. However, monocular 3D object detection presents unique challenges in multi-domain training due to the scarcity of datasets annotated with accurate 3D ground-truth labels, especially beyond typical road-based autonomous driving contexts. To address this challenge, we introduce a novel weakly supervised framework leveraging pseudo-labels. Current pretrained models often struggle to accurately detect pedestrians in non-road environments due to inherent dataset biases. Unlike generalized image-based 2D object detection models, achieving similar generalization in monocular 3D detection remains largely unexplored. In this paper, we propose GATE3D, a novel framework designed specifically for generalized monocular 3D object detection via weak supervision. GATE3D effectively bridges domain gaps by employing consistency losses between 2D and 3D predictions. Remarkably, our model achieves competitive performance on the KITTI benchmark as well as on an indoor-office dataset collected by us to evaluate the generalization capabilities of our framework. Our results demonstrate that GATE3D significantly accelerates learning from limited annotated data through effective pre-training strategies, highlighting substantial potential for broader impacts in robotics, augmented reality, and virtual reality applications. Project page: https://ies0411.github.io/GATE3D/
<div id='section'>Paperid: <span id='pid'>68, <a href='https://arxiv.org/pdf/2504.09881.pdf' target='_blank'>https://arxiv.org/pdf/2504.09881.pdf</a></span>   <span><a href='https://github.com/chenshunpeng/FoL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Changwei Wang, Shunpeng Chen, Yukun Song, Rongtao Xu, Zherui Zhang, Jiguang Zhang, Haoran Yang, Yu Zhang, Kexue Fu, Shide Du, Zhiwei Xu, Longxiang Gao, Li Guo, Shibiao Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09881">Focus on Local: Finding Reliable Discriminative Regions for Visual Place Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Place Recognition (VPR) is aimed at predicting the location of a query image by referencing a database of geotagged images. For VPR task, often fewer discriminative local regions in an image produce important effects while mundane background regions do not contribute or even cause perceptual aliasing because of easy overlap. However, existing methods lack precisely modeling and full exploitation of these discriminative regions. In this paper, we propose the Focus on Local (FoL) approach to stimulate the performance of image retrieval and re-ranking in VPR simultaneously by mining and exploiting reliable discriminative local regions in images and introducing pseudo-correlation supervision. First, we design two losses, Extraction-Aggregation Spatial Alignment Loss (SAL) and Foreground-Background Contrast Enhancement Loss (CEL), to explicitly model reliable discriminative local regions and use them to guide the generation of global representations and efficient re-ranking. Second, we introduce a weakly-supervised local feature training strategy based on pseudo-correspondences obtained from aggregating global features to alleviate the lack of local correspondences ground truth for the VPR task. Third, we suggest an efficient re-ranking pipeline that is efficiently and precisely based on discriminative region guidance. Finally, experimental results show that our FoL achieves the state-of-the-art on multiple VPR benchmarks in both image retrieval and re-ranking stages and also significantly outperforms existing two-stage VPR methods in terms of computational efficiency. Code and models are available at https://github.com/chenshunpeng/FoL
<div id='section'>Paperid: <span id='pid'>69, <a href='https://arxiv.org/pdf/2504.03096.pdf' target='_blank'>https://arxiv.org/pdf/2504.03096.pdf</a></span>   <span><a href='https://siatheindochinese.github.io/sia_act_page/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhen Hao Sia, Yogesh Singh Rawat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03096">Scaling Open-Vocabulary Action Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we focus on scaling open-vocabulary action detection. Existing approaches for action detection are predominantly limited to closed-set scenarios and rely on complex, parameter-heavy architectures. Extending these models to the open-vocabulary setting poses two key challenges: (1) the lack of large-scale datasets with many action classes for robust training, and (2) parameter-heavy adaptations to a pretrained vision-language contrastive model to convert it for detection, risking overfitting the additional non-pretrained parameters to base action classes. Firstly, we introduce an encoder-only multimodal model for video action detection, reducing the reliance on parameter-heavy additions for video action detection. Secondly, we introduce a simple weakly supervised training strategy to exploit an existing closed-set action detection dataset for pretraining. Finally, we depart from the ill-posed base-to-novel benchmark used by prior works in open-vocabulary action detection and devise a new benchmark to evaluate on existing closed-set action detection datasets without ever using them for training, showing novel results to serve as baselines for future work. Our code is available at https://siatheindochinese.github.io/sia_act_page/ .
<div id='section'>Paperid: <span id='pid'>70, <a href='https://arxiv.org/pdf/2503.11439.pdf' target='_blank'>https://arxiv.org/pdf/2503.11439.pdf</a></span>   <span><a href='https://github.com/shjo-april/COIN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sanghyun Jo, Seo Jin Lee, Seungwoo Lee, Seohyung Hong, Hyungseok Seo, Kyungsu Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11439">COIN: Confidence Score-Guided Distillation for Annotation-Free Cell Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cell instance segmentation (CIS) is crucial for identifying individual cell morphologies in histopathological images, providing valuable insights for biological and medical research. While unsupervised CIS (UCIS) models aim to reduce the heavy reliance on labor-intensive image annotations, they fail to accurately capture cell boundaries, causing missed detections and poor performance. Recognizing the absence of error-free instances as a key limitation, we present COIN (COnfidence score-guided INstance distillation), a novel annotation-free framework with three key steps: (1) Increasing the sensitivity for the presence of error-free instances via unsupervised semantic segmentation with optimal transport, leveraging its ability to discriminate spatially minor instances, (2) Instance-level confidence scoring to measure the consistency between model prediction and refined mask and identify highly confident instances, offering an alternative to ground truth annotations, and (3) Progressive expansion of confidence with recursive self-distillation. Extensive experiments across six datasets show COIN outperforming existing UCIS methods, even surpassing semi- and weakly-supervised approaches across all metrics on the MoNuSeg and TNBC datasets. The code is available at https://github.com/shjo-april/COIN.
<div id='section'>Paperid: <span id='pid'>71, <a href='https://arxiv.org/pdf/2503.11032.pdf' target='_blank'>https://arxiv.org/pdf/2503.11032.pdf</a></span>   <span><a href='https://github.com/zhang-lilin/WSCAT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lilin Zhang, Chengpei Wu, Ning Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11032">Weakly Supervised Contrastive Adversarial Training for Learning Robust Features from Semi-supervised Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing adversarial training (AT) methods often suffer from incomplete perturbation, meaning that not all non-robust features are perturbed when generating adversarial examples (AEs). This results in residual correlations between non-robust features and labels, leading to suboptimal learning of robust features. However, achieving complete perturbation, i.e., perturbing as many non-robust features as possible, is challenging due to the difficulty in distinguishing robust and non-robust features and the sparsity of labeled data. To address these challenges, we propose a novel approach called Weakly Supervised Contrastive Adversarial Training (WSCAT). WSCAT ensures complete perturbation for improved learning of robust features by disrupting correlations between non-robust features and labels through complete AE generation over partially labeled data, grounded in information theory. Extensive theoretical analysis and comprehensive experiments on widely adopted benchmarks validate the superiority of WSCAT. Our code is available at https://github.com/zhang-lilin/WSCAT.
<div id='section'>Paperid: <span id='pid'>72, <a href='https://arxiv.org/pdf/2503.07982.pdf' target='_blank'>https://arxiv.org/pdf/2503.07982.pdf</a></span>   <span><a href='https://github.com/shjo-april/DiffEGG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sanghyun Jo, Ziseok Lee, Wooyeol Lee, Kyungsu Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.07982">DiffEGG: Diffusion-Driven Edge Generation as a Pixel-Annotation-Free Alternative for Instance Annotation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving precise panoptic segmentation relies on pixel-wise instance annotations, but obtaining such datasets is costly. Unsupervised instance segmentation (UIS) eliminates annotation requirements but struggles with adjacent instance merging and single-instance fragmentation, largely due to the limitations of DINO-based backbones which lack strong instance separation cues. Weakly-supervised panoptic segmentation (WPS) reduces annotation costs using sparse labels (e.g., points, boxes), yet these annotations remain expensive and introduce human bias and boundary errors. To address these challenges, we propose DiffEGG (Diffusion-Driven EdGe Generation), a fully annotation-free method that extracts instance-aware features from pretrained diffusion models to generate precise instance edge maps. Unlike DINO-based UIS methods, diffusion models inherently capture fine-grained, instance-aware features, enabling more precise boundary delineation. For WPS, DiffEGG eliminates annotation costs and human bias by operating without any form of manual supervision, addressing the key limitations of prior best methods. Additionally, we introduce RIP, a post-processing technique that fuses DiffEGG's edge maps with segmentation masks in a task-agnostic manner. RIP allows DiffEGG to be seamlessly integrated into various segmentation frameworks. When applied to UIS, DiffEGG and RIP achieve an average $+4.4\text{ AP}$ improvement over prior best UIS methods. When combined with weakly-supervised semantic segmentation (WSS), DiffEGG enables WPS without instance annotations, outperforming prior best point-supervised WPS methods by $+1.7\text{ PQ}$. These results demonstrate that DiffEGG's edge maps serve as a cost-effective, annotation-free alternative to instance annotations, significantly improving segmentation without human intervention. Code is available at https://github.com/shjo-april/DiffEGG.
<div id='section'>Paperid: <span id='pid'>73, <a href='https://arxiv.org/pdf/2503.07635.pdf' target='_blank'>https://arxiv.org/pdf/2503.07635.pdf</a></span>   <span><a href='https://github.com/WissingChen/CRA-GQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weixing Chen, Yang Liu, Binglin Chen, Jiandong Su, Yongsen Zheng, Liang Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.07635">Cross-modal Causal Relation Alignment for Video Question Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video question grounding (VideoQG) requires models to answer the questions and simultaneously infer the relevant video segments to support the answers. However, existing VideoQG methods usually suffer from spurious cross-modal correlations, leading to a failure to identify the dominant visual scenes that align with the intended question. Moreover, vision-language models exhibit unfaithful generalization performance and lack robustness on challenging downstream tasks such as VideoQG. In this work, we propose a novel VideoQG framework named Cross-modal Causal Relation Alignment (CRA), to eliminate spurious correlations and improve the causal consistency between question-answering and video temporal grounding. Our CRA involves three essential components: i) Gaussian Smoothing Grounding (GSG) module for estimating the time interval via cross-modal attention, which is de-noised by an adaptive Gaussian filter, ii) Cross-Modal Alignment (CMA) enhances the performance of weakly supervised VideoQG by leveraging bidirectional contrastive learning between estimated video segments and QA features, iii) Explicit Causal Intervention (ECI) module for multimodal deconfounding, which involves front-door intervention for vision and back-door intervention for language. Extensive experiments on two VideoQG datasets demonstrate the superiority of our CRA in discovering visually grounded content and achieving robust question reasoning. Codes are available at https://github.com/WissingChen/CRA-GQA.
<div id='section'>Paperid: <span id='pid'>74, <a href='https://arxiv.org/pdf/2503.04106.pdf' target='_blank'>https://arxiv.org/pdf/2503.04106.pdf</a></span>   <span><a href='https://github.com/wanghr64/WeakMedSAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Wang, Lian Huai, Wenbin Li, Lei Qi, Xingqun Jiang, Yinghuan Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04106">WeakMedSAM: Weakly-Supervised Medical Image Segmentation via SAM with Sub-Class Exploration and Prompt Affinity Mining</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We have witnessed remarkable progress in foundation models in vision tasks. Currently, several recent works have utilized the segmenting anything model (SAM) to boost the segmentation performance in medical images, where most of them focus on training an adaptor for fine-tuning a large amount of pixel-wise annotated medical images following a fully supervised manner. In this paper, to reduce the labeling cost, we investigate a novel weakly-supervised SAM-based segmentation model, namely WeakMedSAM. Specifically, our proposed WeakMedSAM contains two modules: 1) to mitigate severe co-occurrence in medical images, a sub-class exploration module is introduced to learn accurate feature representations. 2) to improve the quality of the class activation maps, our prompt affinity mining module utilizes the prompt capability of SAM to obtain an affinity map for random-walk refinement. Our method can be applied to any SAM-like backbone, and we conduct experiments with SAMUS and EfficientSAM. The experimental results on three popularly-used benchmark datasets, i.e., BraTS 2019, AbdomenCT-1K, and MSD Cardiac dataset, show the promising results of our proposed WeakMedSAM. Our code is available at https://github.com/wanghr64/WeakMedSAM.
<div id='section'>Paperid: <span id='pid'>75, <a href='https://arxiv.org/pdf/2503.03562.pdf' target='_blank'>https://arxiv.org/pdf/2503.03562.pdf</a></span>   <span><a href='https://guyao2023.github.io/Phys-AD/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenqiao Li, Yao Gu, Xintao Chen, Xiaohao Xu, Ming Hu, Xiaonan Huang, Yingna Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.03562">Towards Visual Discrimination and Reasoning of Real-World Physical Dynamics: Physics-Grounded Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans detect real-world object anomalies by perceiving, interacting, and reasoning based on object-conditioned physical knowledge. The long-term goal of Industrial Anomaly Detection (IAD) is to enable machines to autonomously replicate this skill. However, current IAD algorithms are largely developed and tested on static, semantically simple datasets, which diverge from real-world scenarios where physical understanding and reasoning are essential. To bridge this gap, we introduce the Physics Anomaly Detection (Phys-AD) dataset, the first large-scale, real-world, physics-grounded video dataset for industrial anomaly detection. Collected using a real robot arm and motor, Phys-AD provides a diverse set of dynamic, semantically rich scenarios. The dataset includes more than 6400 videos across 22 real-world object categories, interacting with robot arms and motors, and exhibits 47 types of anomalies. Anomaly detection in Phys-AD requires visual reasoning, combining both physical knowledge and video content to determine object abnormality. We benchmark state-of-the-art anomaly detection methods under three settings: unsupervised AD, weakly-supervised AD, and video-understanding AD, highlighting their limitations in handling physics-grounded anomalies. Additionally, we introduce the Physics Anomaly Explanation (PAEval) metric, designed to assess the ability of visual-language foundation models to not only detect anomalies but also provide accurate explanations for their underlying physical causes. Our project is available at https://guyao2023.github.io/Phys-AD/.
<div id='section'>Paperid: <span id='pid'>76, <a href='https://arxiv.org/pdf/2502.21109.pdf' target='_blank'>https://arxiv.org/pdf/2502.21109.pdf</a></span>   <span><a href='https://github.com/DIAGNijmegen/tumor-percentage-mil-regression' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Marina D'Amato, Jeroen van der Laak, Francesco Ciompi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.21109">"No negatives needed": weakly-supervised regression for interpretable tumor detection in whole-slide histopathology images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate tumor detection in digital pathology whole-slide images (WSIs) is crucial for cancer diagnosis and treatment planning. Multiple Instance Learning (MIL) has emerged as a widely used approach for weakly-supervised tumor detection with large-scale data without the need for manual annotations. However, traditional MIL methods often depend on classification tasks that require tumor-free cases as negative examples, which are challenging to obtain in real-world clinical workflows, especially for surgical resection specimens. We address this limitation by reformulating tumor detection as a regression task, estimating tumor percentages from WSIs, a clinically available target across multiple cancer types. In this paper, we provide an analysis of the proposed weakly-supervised regression framework by applying it to multiple organs, specimen types and clinical scenarios. We characterize the robustness of our framework to tumor percentage as a noisy regression target, and introduce a novel concept of amplification technique to improve tumor detection sensitivity when learning from small tumor regions. Finally, we provide interpretable insights into the model's predictions by analyzing visual attention and logit maps. Our code is available at https://github.com/DIAGNijmegen/tumor-percentage-mil-regression.
<div id='section'>Paperid: <span id='pid'>77, <a href='https://arxiv.org/pdf/2502.20838.pdf' target='_blank'>https://arxiv.org/pdf/2502.20838.pdf</a></span>   <span><a href='https://github.com/Ragib-Amin-Nihal/DSMIL-Loc' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ragib Amin Nihal, Benjamin Yen, Runwu Shi, Kazuhiro Nakadai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20838">Weakly Supervised Multiple Instance Learning for Whale Call Detection and Temporal Localization in Long-Duration Passive Acoustic Monitoring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Marine ecosystem monitoring via Passive Acoustic Monitoring (PAM) generates vast data, but deep learning often requires precise annotations and short segments. We introduce DSMIL-LocNet, a Multiple Instance Learning framework for whale call detection and localization using only bag-level labels. Our dual-stream model processes 2-30 minute audio segments, leveraging spectral and temporal features with attention-based instance selection. Tests on Antarctic whale data show longer contexts improve classification (F1: 0.8-0.9) while medium instances ensure localization precision (0.65-0.70). This suggests MIL can enhance scalable marine monitoring. Code: https://github.com/Ragib-Amin-Nihal/DSMIL-Loc
<div id='section'>Paperid: <span id='pid'>78, <a href='https://arxiv.org/pdf/2502.19707.pdf' target='_blank'>https://arxiv.org/pdf/2502.19707.pdf</a></span>   <span><a href='https://github.com/bluehenglee/MLI-MSC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianning Chi, Zelan Li, Geng Lin, MingYang Sun, Xiaosheng Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19707">Weakly Supervised Segmentation Framework for Thyroid Nodule Based on High-confidence Labels and High-rationality Losses</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised segmentation methods can delineate thyroid nodules in ultrasound images efficiently using training data with coarse labels, but suffer from: 1) low-confidence pseudo-labels that follow topological priors, introducing significant label noise, and 2) low-rationality loss functions that rigidly compare segmentation with labels, ignoring discriminative information for nodules with diverse and complex shapes. To solve these issues, we clarify the objective and references for weakly supervised ultrasound image segmentation, presenting a framework with high-confidence pseudo-labels to represent topological and anatomical information and high-rationality losses to capture multi-level discriminative features. Specifically, we fuse geometric transformations of four-point annotations and MedSAM model results prompted by specific annotations to generate high-confidence box, foreground, and background labels. Our high-rationality learning strategy includes: 1) Alignment loss measuring spatial consistency between segmentation and box label, and topological continuity within the foreground label, guiding the network to perceive nodule location; 2) Contrastive loss pulling features from labeled foreground regions while pushing features from labeled foreground and background regions, guiding the network to learn nodule and background feature distribution; 3) Prototype correlation loss measuring consistency between correlation maps derived by comparing features with foreground and background prototypes, refining uncertain regions to accurate nodule edges. Experimental results show that our method achieves state-of-the-art performance on the TN3K and DDTI datasets. The code is available at https://github.com/bluehenglee/MLI-MSC.
<div id='section'>Paperid: <span id='pid'>79, <a href='https://arxiv.org/pdf/2502.15885.pdf' target='_blank'>https://arxiv.org/pdf/2502.15885.pdf</a></span>   <span><a href='https://github.com/AIGeeksGroup/DOEI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongjie Zhu, Zeyu Zhang, Guansong Pang, Xu Wang, Shimin Wen, Yu Bai, Daji Ergu, Ying Cai, Yang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.15885">DOEI: Dual Optimization of Embedding Information for Attention-Enhanced Class Activation Maps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation (WSSS) typically utilizes limited semantic annotations to obtain initial Class Activation Maps (CAMs). However, due to the inadequate coupling between class activation responses and semantic information in high-dimensional space, the CAM is prone to object co-occurrence or under-activation, resulting in inferior recognition accuracy. To tackle this issue, we propose DOEI, Dual Optimization of Embedding Information, a novel approach that reconstructs embedding representations through semantic-aware attention weight matrices to optimize the expression capability of embedding information. Specifically, DOEI amplifies tokens with high confidence and suppresses those with low confidence during the class-to-patch interaction. This alignment of activation responses with semantic information strengthens the propagation and decoupling of target features, enabling the generated embeddings to more accurately represent target features in high-level semantic space. In addition, we propose a hybrid-feature alignment module in DOEI that combines RGB values, embedding-guided features, and self-attention weights to increase the reliability of candidate tokens. Comprehensive experiments show that DOEI is an effective plug-and-play module that empowers state-of-the-art visual transformer-based WSSS models to significantly improve the quality of CAMs and segmentation performance on popular benchmarks, including PASCAL VOC (+3.6%, +1.5%, +1.2% mIoU) and MS COCO (+1.2%, +1.6% mIoU). Code will be available at https://github.com/AIGeeksGroup/DOEI.
<div id='section'>Paperid: <span id='pid'>80, <a href='https://arxiv.org/pdf/2502.15152.pdf' target='_blank'>https://arxiv.org/pdf/2502.15152.pdf</a></span>   <span><a href='https://github.com/psychofict/CW-BASS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ebenezer Tarubinga, Jenifer Kalafatovich, Seong-Whan Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.15152">CW-BASS: Confidence-Weighted Boundary-Aware Learning for Semi-Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semi-supervised semantic segmentation (SSSS) aims to improve segmentation performance by utilizing large amounts of unlabeled data with limited labeled samples. Existing methods often suffer from coupling, where over-reliance on initial labeled data leads to suboptimal learning; confirmation bias, where incorrect predictions reinforce themselves repeatedly; and boundary blur caused by limited boundary-awareness and ambiguous edge cues. To address these issues, we propose CW-BASS, a novel framework for SSSS. In order to mitigate the impact of incorrect predictions, we assign confidence weights to pseudo-labels. Additionally, we leverage boundary-delineation techniques, which, despite being extensively explored in weakly-supervised semantic segmentation (WSSS), remain underutilized in SSSS. Specifically, our method: (1) reduces coupling via a confidence-weighted loss that adjusts pseudo-label influence based on their predicted confidence scores, (2) mitigates confirmation bias with a dynamic thresholding mechanism that learns to filter out pseudo-labels based on model performance, (3) tackles boundary blur using a boundary-aware module to refine segmentation near object edges, and (4) reduces label noise through a confidence decay strategy that progressively refines pseudo-labels during training. Extensive experiments on Pascal VOC 2012 and Cityscapes demonstrate that CW-BASS achieves state-of-the-art performance. Notably, CW-BASS achieves a 65.9% mIoU on Cityscapes under a challenging and underexplored 1/30 (3.3%) split (100 images), highlighting its effectiveness in limited-label settings. Our code is available at https://github.com/psychofict/CW-BASS.
<div id='section'>Paperid: <span id='pid'>81, <a href='https://arxiv.org/pdf/2502.10294.pdf' target='_blank'>https://arxiv.org/pdf/2502.10294.pdf</a></span>   <span><a href='https://github.com/anpc849/QMaxViT-Unet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Thien B. Nguyen-Tat, Hoang-An Vo, Phuoc-Sang Dang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10294">QMaxViT-Unet+: A Query-Based MaxViT-Unet with Edge Enhancement for Scribble-Supervised Segmentation of Medical Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The deployment of advanced deep learning models for medical image segmentation is often constrained by the requirement for extensively annotated datasets. Weakly-supervised learning, which allows less precise labels, has become a promising solution to this challenge. Building on this approach, we propose QMaxViT-Unet+, a novel framework for scribble-supervised medical image segmentation. This framework is built on the U-Net architecture, with the encoder and decoder replaced by Multi-Axis Vision Transformer (MaxViT) blocks. These blocks enhance the model's ability to learn local and global features efficiently. Additionally, our approach integrates a query-based Transformer decoder to refine features and an edge enhancement module to compensate for the limited boundary information in the scribble label. We evaluate the proposed QMaxViT-Unet+ on four public datasets focused on cardiac structures, colorectal polyps, and breast cancer: ACDC, MS-CMRSeg, SUN-SEG, and BUSI. Evaluation metrics include the Dice similarity coefficient (DSC) and the 95th percentile of Hausdorff distance (HD95). Experimental results show that QMaxViT-Unet+ achieves 89.1\% DSC and 1.316mm HD95 on ACDC, 88.4\% DSC and 2.226mm HD95 on MS-CMRSeg, 71.4\% DSC and 4.996mm HD95 on SUN-SEG, and 69.4\% DSC and 50.122mm HD95 on BUSI. These results demonstrate that our method outperforms existing approaches in terms of accuracy, robustness, and efficiency while remaining competitive with fully-supervised learning approaches. This makes it ideal for medical image analysis, where high-quality annotations are often scarce and require significant effort and expense. The code is available at: https://github.com/anpc849/QMaxViT-Unet
<div id='section'>Paperid: <span id='pid'>82, <a href='https://arxiv.org/pdf/2502.10263.pdf' target='_blank'>https://arxiv.org/pdf/2502.10263.pdf</a></span>   <span><a href='https://github.com/worldbank/ai4data-use' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aivin V. Solatorio, Rafael Macalaba, James Liounis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10263">Large Language Models and Synthetic Data for Monitoring Dataset Mentions in Research Papers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking how data is mentioned and used in research papers provides critical insights for improving data discoverability, quality, and production. However, manually identifying and classifying dataset mentions across vast academic literature is resource-intensive and not scalable. This paper presents a machine learning framework that automates dataset mention detection across research domains by leveraging large language models (LLMs), synthetic data, and a two-stage fine-tuning process. We employ zero-shot extraction from research papers, an LLM-as-a-Judge for quality assessment, and a reasoning agent for refinement to generate a weakly supervised synthetic dataset. The Phi-3.5-mini instruct model is pre-fine-tuned on this dataset, followed by fine-tuning on a manually annotated subset. At inference, a ModernBERT-based classifier efficiently filters dataset mentions, reducing computational overhead while maintaining high recall. Evaluated on a held-out manually annotated sample, our fine-tuned model outperforms NuExtract-v1.5 and GLiNER-large-v2.1 in dataset extraction accuracy. Our results highlight how LLM-generated synthetic data can effectively address training data scarcity, improving generalization in low-resource settings. This framework offers a pathway toward scalable monitoring of dataset usage, enhancing transparency, and supporting researchers, funders, and policymakers in identifying data gaps and strengthening data accessibility for informed decision-making.
<div id='section'>Paperid: <span id='pid'>83, <a href='https://arxiv.org/pdf/2502.09874.pdf' target='_blank'>https://arxiv.org/pdf/2502.09874.pdf</a></span>   <span><a href='https://github.com/LQY404/FrGNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Ling, Wenxiao Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09874">FrGNet: A fourier-guided weakly-supervised framework for nuclear instance segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nuclear instance segmentation has played a critical role in pathology image analysis. The main challenges arise from the difficulty in accurately segmenting instances and the high cost of precise mask-level annotations for fully-supervised training.In this work, we propose a fourier guidance framework for solving the weakly-supervised nuclear instance segmentation problem. In this framework, we construct a fourier guidance module to fuse the priori information into the training process of the model, which facilitates the model to capture the relevant features of the nuclear. Meanwhile, in order to further improve the model's ability to represent the features of nuclear, we propose the guide-based instance level contrastive module. This module makes full use of the framework's own properties and guide information to effectively enhance the representation features of nuclear. We show on two public datasets that our model can outperform current SOTA methods under fully-supervised design, and in weakly-supervised experiments, with only a small amount of labeling our model still maintains close to the performance under full supervision.In addition, we also perform generalization experiments on a private dataset, and without any labeling, our model is able to segment nuclear images that have not been seen during training quite effectively. As open science, all codes and pre-trained models are available at https://github.com/LQY404/FrGNet.
<div id='section'>Paperid: <span id='pid'>84, <a href='https://arxiv.org/pdf/2502.09471.pdf' target='_blank'>https://arxiv.org/pdf/2502.09471.pdf</a></span>   <span><a href='https://github.com/VisionXLab/whollywood' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/VisionXLab/whollywood-jittor' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Yu, Xue Yang, Yansheng Li, Zhenjun Han, Feipeng Da, Junchi Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09471">Wholly-WOOD: Wholly Leveraging Diversified-quality Labels for Weakly-supervised Oriented Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately estimating the orientation of visual objects with compact rotated bounding boxes (RBoxes) has become a prominent demand, which challenges existing object detection paradigms that only use horizontal bounding boxes (HBoxes). To equip the detectors with orientation awareness, supervised regression/classification modules have been introduced at the high cost of rotation annotation. Meanwhile, some existing datasets with oriented objects are already annotated with horizontal boxes or even single points. It becomes attractive yet remains open for effectively utilizing weaker single point and horizontal annotations to train an oriented object detector (OOD). We develop Wholly-WOOD, a weakly-supervised OOD framework, capable of wholly leveraging various labeling forms (Points, HBoxes, RBoxes, and their combination) in a unified fashion. By only using HBox for training, our Wholly-WOOD achieves performance very close to that of the RBox-trained counterpart on remote sensing and other areas, significantly reducing the tedious efforts on labor-intensive annotation for oriented objects. The source codes are available at https://github.com/VisionXLab/whollywood (PyTorch-based) and https://github.com/VisionXLab/whollywood-jittor (Jittor-based).
<div id='section'>Paperid: <span id='pid'>85, <a href='https://arxiv.org/pdf/2502.04268.pdf' target='_blank'>https://arxiv.org/pdf/2502.04268.pdf</a></span>   <span><a href='https://github.com/VisionXLab/point2rbox-v2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Yu, Botao Ren, Peiyuan Zhang, Mingxin Liu, Junwei Luo, Shaofeng Zhang, Feipeng Da, Junchi Yan, Xue Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04268">Point2RBox-v2: Rethinking Point-supervised Oriented Object Detection with Spatial Layout Among Instances</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapidly increasing demand for oriented object detection (OOD), recent research involving weakly-supervised detectors for learning OOD from point annotations has gained great attention. In this paper, we rethink this challenging task setting with the layout among instances and present Point2RBox-v2. At the core are three principles: 1) Gaussian overlap loss. It learns an upper bound for each instance by treating objects as 2D Gaussian distributions and minimizing their overlap. 2) Voronoi watershed loss. It learns a lower bound for each instance through watershed on Voronoi tessellation. 3) Consistency loss. It learns the size/rotation variation between two output sets with respect to an input image and its augmented view. Supplemented by a few devised techniques, e.g. edge loss and copy-paste, the detector is further enhanced. To our best knowledge, Point2RBox-v2 is the first approach to explore the spatial layout among instances for learning point-supervised OOD. Our solution is elegant and lightweight, yet it is expected to give a competitive performance especially in densely packed scenes: 62.61%/86.15%/34.71% on DOTA/HRSC/FAIR1M. Code is available at https://github.com/VisionXLab/point2rbox-v2.
<div id='section'>Paperid: <span id='pid'>86, <a href='https://arxiv.org/pdf/2502.03118.pdf' target='_blank'>https://arxiv.org/pdf/2502.03118.pdf</a></span>   <span><a href='https://github.com/yanwenCi/Tell2Reg.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wen Yan, Qianye Yang, Shiqi Huang, Yipei Wang, Shonit Punwani, Mark Emberton, Vasilis Stavrinides, Yipeng Hu, Dean Barratt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03118">Tell2Reg: Establishing spatial correspondence between images by the same language prompts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spatial correspondence can be represented by pairs of segmented regions, such that the image registration networks aim to segment corresponding regions rather than predicting displacement fields or transformation parameters. In this work, we show that such a corresponding region pair can be predicted by the same language prompt on two different images using the pre-trained large multimodal models based on GroundingDINO and SAM. This enables a fully automated and training-free registration algorithm, potentially generalisable to a wide range of image registration tasks. In this paper, we present experimental results using one of the challenging tasks, registering inter-subject prostate MR images, which involves both highly variable intensity and morphology between patients. Tell2Reg is training-free, eliminating the need for costly and time-consuming data curation and labelling that was previously required for this registration task. This approach outperforms unsupervised learning-based registration methods tested, and has a performance comparable to weakly-supervised methods. Additional qualitative results are also presented to suggest that, for the first time, there is a potential correlation between language semantics and spatial correspondence, including the spatial invariance in language-prompted regions and the difference in language prompts between the obtained local and global correspondences. Code is available at https://github.com/yanwenCi/Tell2Reg.git.
<div id='section'>Paperid: <span id='pid'>87, <a href='https://arxiv.org/pdf/2502.00240.pdf' target='_blank'>https://arxiv.org/pdf/2502.00240.pdf</a></span>   <span><a href='https://github.com/YasminZhang/ADCR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yasi Zhang, Oscar Leong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.00240">Learning Difference-of-Convex Regularizers for Inverse Problems: A Flexible Framework with Theoretical Guarantees</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning effective regularization is crucial for solving ill-posed inverse problems, which arise in a wide range of scientific and engineering applications. While data-driven methods that parameterize regularizers using deep neural networks have demonstrated strong empirical performance, they often result in highly nonconvex formulations that lack theoretical guarantees. Recent work has shown that incorporating structured nonconvexity into neural network-based regularizers, such as weak convexity, can strike a balance between empirical performance and theoretical tractability. In this paper, we demonstrate that a broader class of nonconvex functions, difference-of-convex (DC) functions, can yield improved empirical performance while retaining strong convergence guarantees. The DC structure enables the use of well-established optimization algorithms, such as the Difference-of-Convex Algorithm (DCA) and a Proximal Subgradient Method (PSM), which extend beyond standard gradient descent. Furthermore, we provide theoretical insights into the conditions under which optimal regularizers can be expressed as DC functions. Extensive experiments on computed tomography (CT) reconstruction tasks show that our approach achieves strong performance across sparse and limited-view settings, consistently outperforming other weakly supervised learned regularizers. Our code is available at \url{https://github.com/YasminZhang/ADCR}.
<div id='section'>Paperid: <span id='pid'>88, <a href='https://arxiv.org/pdf/2501.17053.pdf' target='_blank'>https://arxiv.org/pdf/2501.17053.pdf</a></span>   <span><a href='https://akash2907.github.io/cospal_webpage' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Akash Kumar, Zsolt Kira, Yogesh Singh Rawat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.17053">Contextual Self-paced Learning for Weakly Supervised Spatio-Temporal Video Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we focus on Weakly Supervised Spatio-Temporal Video Grounding (WSTVG). It is a multimodal task aimed at localizing specific subjects spatio-temporally based on textual queries without bounding box supervision. Motivated by recent advancements in multi-modal foundation models for grounding tasks, we first explore the potential of state-of-the-art object detection models for WSTVG. Despite their robust zero-shot capabilities, our adaptation reveals significant limitations, including inconsistent temporal predictions, inadequate understanding of complex queries, and challenges in adapting to difficult scenarios. We propose CoSPaL (Contextual Self-Paced Learning), a novel approach which is designed to overcome these limitations. CoSPaL integrates three core components: (1) Tubelet Phrase Grounding (TPG), which introduces spatio-temporal prediction by linking textual queries to tubelets; (2) Contextual Referral Grounding (CRG), which improves comprehension of complex queries by extracting contextual information to refine object identification over time; and (3) Self-Paced Scene Understanding (SPS), a training paradigm that progressively increases task difficulty, enabling the model to adapt to complex scenarios by transitioning from coarse to fine-grained understanding.
<div id='section'>Paperid: <span id='pid'>89, <a href='https://arxiv.org/pdf/2501.15326.pdf' target='_blank'>https://arxiv.org/pdf/2501.15326.pdf</a></span>   <span><a href='https://ntlm1686.github.io/raso' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiajie Li, Brian R Quaranto, Chenhui Xu, Ishan Mishra, Ruiyang Qin, Dancheng Liu, Peter C W Kim, Jinjun Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15326">Recognize Any Surgical Object: Unleashing the Power of Weakly-Supervised Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present RASO, a foundation model designed to Recognize Any Surgical Object, offering robust open-set recognition capabilities across a broad range of surgical procedures and object classes, in both surgical images and videos. RASO leverages a novel weakly-supervised learning framework that generates tag-image-text pairs automatically from large-scale unannotated surgical lecture videos, significantly reducing the need for manual annotations. Our scalable data generation pipeline gathers 2,200 surgical procedures and produces 3.6 million tag annotations across 2,066 unique surgical tags. Our experiments show that RASO achieves improvements of 2.9 mAP, 4.5 mAP, 10.6 mAP, and 7.2 mAP on four standard surgical benchmarks, respectively, in zero-shot settings, and surpasses state-of-the-art models in supervised surgical action recognition tasks. Code, model, and demo are available at https://ntlm1686.github.io/raso.
<div id='section'>Paperid: <span id='pid'>90, <a href='https://arxiv.org/pdf/2501.13426.pdf' target='_blank'>https://arxiv.org/pdf/2501.13426.pdf</a></span>   <span><a href='https://github.com/zxk688' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jian Wang, Xiaokang Zhang, Xianping Ma, Weikang Yu, Pedram Ghamisi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13426">Auto-Prompting SAM for Weakly Supervised Landslide Extraction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised landslide extraction aims to identify landslide regions from remote sensing data using models trained with weak labels, particularly image-level labels. However, it is often challenged by the imprecise boundaries of the extracted objects due to the lack of pixel-wise supervision and the properties of landslide objects. To tackle these issues, we propose a simple yet effective method by auto-prompting the Segment Anything Model (SAM), i.e., APSAM. Instead of depending on high-quality class activation maps (CAMs) for pseudo-labeling or fine-tuning SAM, our method directly yields fine-grained segmentation masks from SAM inference through prompt engineering. Specifically, it adaptively generates hybrid prompts from the CAMs obtained by an object localization network. To provide sufficient information for SAM prompting, an adaptive prompt generation (APG) algorithm is designed to fully leverage the visual patterns of CAMs, enabling the efficient generation of pseudo-masks for landslide extraction. These informative prompts are able to identify the extent of landslide areas (box prompts) and denote the centers of landslide objects (point prompts), guiding SAM in landslide segmentation. Experimental results on high-resolution aerial and satellite datasets demonstrate the effectiveness of our method, achieving improvements of at least 3.0\% in F1 score and 3.69\% in IoU compared to other state-of-the-art methods. The source codes and datasets will be available at https://github.com/zxk688.
<div id='section'>Paperid: <span id='pid'>91, <a href='https://arxiv.org/pdf/2501.07496.pdf' target='_blank'>https://arxiv.org/pdf/2501.07496.pdf</a></span>   <span><a href='https://github.com/xjpp2016/MAVD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenping Jin, Li Zhu, Jing Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07496">Aligning First, Then Fusing: A Novel Weakly Supervised Multimodal Violence Detection Method</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised violence detection refers to the technique of training models to identify violent segments in videos using only video-level labels. Among these approaches, multimodal violence detection, which integrates modalities such as audio and optical flow, holds great potential. Existing methods in this domain primarily focus on designing multimodal fusion models to address modality discrepancies. In contrast, we take a different approach; leveraging the inherent discrepancies across modalities in violence event representation to propose a novel multimodal semantic feature alignment method. This method sparsely maps the semantic features of local, transient, and less informative modalities ( such as audio and optical flow ) into the more informative RGB semantic feature space. Through an iterative process, the method identifies the suitable no-zero feature matching subspace and aligns the modality-specific event representations based on this subspace, enabling the full exploitation of information from all modalities during the subsequent modality fusion stage. Building on this, we design a new weakly supervised violence detection framework that consists of unimodal multiple-instance learning for extracting unimodal semantic features, multimodal alignment, multimodal fusion, and final detection. Experimental results on benchmark datasets demonstrate the effectiveness of our method, achieving an average precision (AP) of 86.07% on the XD-Violence dataset. Our code is available at https://github.com/xjpp2016/MAVD.
<div id='section'>Paperid: <span id='pid'>92, <a href='https://arxiv.org/pdf/2501.04934.pdf' target='_blank'>https://arxiv.org/pdf/2501.04934.pdf</a></span>   <span><a href='https://github.com/zhenghuizhao/Plug-and-Play-DISep-for-Change-Detection' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenghui Zhao, Chen Wu, Lixiang Ru, Di Wang, Hongruixuan Chen, Cuiqun Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04934">Plug-and-Play DISep: Separating Dense Instances for Scene-to-Pixel Weakly-Supervised Change Detection in High-Resolution Remote Sensing Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing Weakly-Supervised Change Detection (WSCD) methods often encounter the problem of "instance lumping" under scene-level supervision, particularly in scenarios with a dense distribution of changed instances (i.e., changed objects). In these scenarios, unchanged pixels between changed instances are also mistakenly identified as changed, causing multiple changes to be mistakenly viewed as one. In practical applications, this issue prevents the accurate quantification of the number of changes. To address this issue, we propose a Dense Instance Separation (DISep) method as a plug-and-play solution, refining pixel features from a unified instance perspective under scene-level supervision. Specifically, our DISep comprises a three-step iterative training process: 1) Instance Localization: We locate instance candidate regions for changed pixels using high-pass class activation maps. 2) Instance Retrieval: We identify and group these changed pixels into different instance IDs through connectivity searching. Then, based on the assigned instance IDs, we extract corresponding pixel-level features on a per-instance basis. 3) Instance Separation: We introduce a separation loss to enforce intra-instance pixel consistency in the embedding space, thereby ensuring separable instance feature representations. The proposed DISep adds only minimal training cost and no inference cost. It can be seamlessly integrated to enhance existing WSCD methods. We achieve state-of-the-art performance by enhancing {three Transformer-based and four ConvNet-based methods} on the LEVIR-CD, WHU-CD, DSIFN-CD, SYSU-CD, and CDD datasets. Additionally, our DISep can be used to improve fully-supervised change detection methods. Code is available at https://github.com/zhenghuizhao/Plug-and-Play-DISep-for-Change-Detection.
<div id='section'>Paperid: <span id='pid'>93, <a href='https://arxiv.org/pdf/2501.04440.pdf' target='_blank'>https://arxiv.org/pdf/2501.04440.pdf</a></span>   <span><a href='https://github.com/zhasion/RSAR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Zhang, Xue Yang, Yuxuan Li, Jian Yang, Ming-Ming Cheng, Xiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04440">RSAR: Restricted State Angle Resolver and Rotated SAR Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rotated object detection has made significant progress in the optical remote sensing. However, advancements in the Synthetic Aperture Radar (SAR) field are laggard behind, primarily due to the absence of a large-scale dataset. Annotating such a dataset is inefficient and costly. A promising solution is to employ a weakly supervised model (e.g., trained with available horizontal boxes only) to generate pseudo-rotated boxes for reference before manual calibration. Unfortunately, the existing weakly supervised models exhibit limited accuracy in predicting the object's angle. Previous works attempt to enhance angle prediction by using angle resolvers that decouple angles into cosine and sine encodings. In this work, we first reevaluate these resolvers from a unified perspective of dimension mapping and expose that they share the same shortcomings: these methods overlook the unit cycle constraint inherent in these encodings, easily leading to prediction biases. To address this issue, we propose the Unit Cycle Resolver, which incorporates a unit circle constraint loss to improve angle prediction accuracy. Our approach can effectively improve the performance of existing state-of-the-art weakly supervised methods and even surpasses fully supervised models on existing optical benchmarks (i.e., DOTA-v1.0 dataset). With the aid of UCR, we further annotate and introduce RSAR, the largest multi-class rotated SAR object detection dataset to date. Extensive experiments on both RSAR and optical datasets demonstrate that our UCR enhances angle prediction accuracy. Our dataset and code can be found at: https://github.com/zhasion/RSAR.
<div id='section'>Paperid: <span id='pid'>94, <a href='https://arxiv.org/pdf/2412.20924.pdf' target='_blank'>https://arxiv.org/pdf/2412.20924.pdf</a></span>   <span><a href='https://github.com/Vison307/HisynSeg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijie Fang, Yifeng Wang, Peizhang Xie, Zhi Wang, Yongbing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20924">HisynSeg: Weakly-Supervised Histopathological Image Segmentation via Image-Mixing Synthesis and Consistency Regularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tissue semantic segmentation is one of the key tasks in computational pathology. To avoid the expensive and laborious acquisition of pixel-level annotations, a wide range of studies attempt to adopt the class activation map (CAM), a weakly-supervised learning scheme, to achieve pixel-level tissue segmentation. However, CAM-based methods are prone to suffer from under-activation and over-activation issues, leading to poor segmentation performance. To address this problem, we propose a novel weakly-supervised semantic segmentation framework for histopathological images based on image-mixing synthesis and consistency regularization, dubbed HisynSeg. Specifically, synthesized histopathological images with pixel-level masks are generated for fully-supervised model training, where two synthesis strategies are proposed based on Mosaic transformation and BÃ©zier mask generation. Besides, an image filtering module is developed to guarantee the authenticity of the synthesized images. In order to further avoid the model overfitting to the occasional synthesis artifacts, we additionally propose a novel self-supervised consistency regularization, which enables the real images without segmentation masks to supervise the training of the segmentation model. By integrating the proposed techniques, the HisynSeg framework successfully transforms the weakly-supervised semantic segmentation problem into a fully-supervised one, greatly improving the segmentation accuracy. Experimental results on three datasets prove that the proposed method achieves a state-of-the-art performance. Code is available at https://github.com/Vison307/HisynSeg.
<div id='section'>Paperid: <span id='pid'>95, <a href='https://arxiv.org/pdf/2412.19418.pdf' target='_blank'>https://arxiv.org/pdf/2412.19418.pdf</a></span>   <span><a href='https://github.com/heyuanpengpku/GUEF/tree/main' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanpeng He, Lijian Li, Tianxiang Zhan, Wenpin Jiao, Chi-Man Pun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19418">Generalized Uncertainty-Based Evidential Fusion with Hybrid Multi-Head Attention for Weak-Supervised Temporal Action Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised temporal action localization (WS-TAL) is a task of targeting at localizing complete action instances and categorizing them with video-level labels. Action-background ambiguity, primarily caused by background noise resulting from aggregation and intra-action variation, is a significant challenge for existing WS-TAL methods. In this paper, we introduce a hybrid multi-head attention (HMHA) module and generalized uncertainty-based evidential fusion (GUEF) module to address the problem. The proposed HMHA effectively enhances RGB and optical flow features by filtering redundant information and adjusting their feature distribution to better align with the WS-TAL task. Additionally, the proposed GUEF adaptively eliminates the interference of background noise by fusing snippet-level evidences to refine uncertainty measurement and select superior foreground feature information, which enables the model to concentrate on integral action instances to achieve better action localization and classification performance. Experimental results conducted on the THUMOS14 dataset demonstrate that our method outperforms state-of-the-art methods. Our code is available in \url{https://github.com/heyuanpengpku/GUEF/tree/main}.
<div id='section'>Paperid: <span id='pid'>96, <a href='https://arxiv.org/pdf/2412.18738.pdf' target='_blank'>https://arxiv.org/pdf/2412.18738.pdf</a></span>   <span><a href='https://github.com/IPMI-NWU/HELPNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Zhang, Shaoxuan Wu, Peilin Zhang, Zhuo Jin, Xiaosong Xiong, Qirong Bu, Jingkun Chen, Jun Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.18738">HELPNet: Hierarchical Perturbations Consistency and Entropy-guided Ensemble for Scribble Supervised Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating fully annotated labels for medical image segmentation is prohibitively time-intensive and costly, emphasizing the necessity for innovative approaches that minimize reliance on detailed annotations. Scribble annotations offer a more cost-effective alternative, significantly reducing the expenses associated with full annotations. However, scribble annotations offer limited and imprecise information, failing to capture the detailed structural and boundary characteristics necessary for accurate organ delineation. To address these challenges, we propose HELPNet, a novel scribble-based weakly supervised segmentation framework, designed to bridge the gap between annotation efficiency and segmentation performance. HELPNet integrates three modules. The Hierarchical perturbations consistency (HPC) module enhances feature learning by employing density-controlled jigsaw perturbations across global, local, and focal views, enabling robust modeling of multi-scale structural representations. Building on this, the Entropy-guided pseudo-label (EGPL) module evaluates the confidence of segmentation predictions using entropy, generating high-quality pseudo-labels. Finally, the structural prior refinement (SPR) module incorporates connectivity and bounded priors to enhance the precision and reliability and pseudo-labels. Experimental results on three public datasets ACDC, MSCMRseg, and CHAOS show that HELPNet significantly outperforms state-of-the-art methods for scribble-based weakly supervised segmentation and achieves performance comparable to fully supervised methods. The code is available at https://github.com/IPMI-NWU/HELPNet.
<div id='section'>Paperid: <span id='pid'>97, <a href='https://arxiv.org/pdf/2412.17601.pdf' target='_blank'>https://arxiv.org/pdf/2412.17601.pdf</a></span>   <span><a href='https://github.com/jarch-ma/AFANet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Ma, Guo-Sen Xie, Fang Zhao, Zechao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.17601">AFANet: Adaptive Frequency-Aware Network for Weakly-Supervised Few-Shot Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Few-shot learning aims to recognize novel concepts by leveraging prior knowledge learned from a few samples. However, for visually intensive tasks such as few-shot semantic segmentation, pixel-level annotations are time-consuming and costly. Therefore, in this paper, we utilize the more challenging image-level annotations and propose an adaptive frequency-aware network (AFANet) for weakly-supervised few-shot semantic segmentation (WFSS). Specifically, we first propose a cross-granularity frequency-aware module (CFM) that decouples RGB images into high-frequency and low-frequency distributions and further optimizes semantic structural information by realigning them. Unlike most existing WFSS methods using the textual information from the multi-modal language-vision model, e.g., CLIP, in an offline learning manner, we further propose a CLIP-guided spatial-adapter module (CSM), which performs spatial domain adaptive transformation on textual information through online learning, thus providing enriched cross-modal semantic information for CFM. Extensive experiments on the Pascal-5\textsuperscript{i} and COCO-20\textsuperscript{i} datasets demonstrate that AFANet has achieved state-of-the-art performance. The code is available at https://github.com/jarch-ma/AFANet.
<div id='section'>Paperid: <span id='pid'>98, <a href='https://arxiv.org/pdf/2412.11076.pdf' target='_blank'>https://arxiv.org/pdf/2412.11076.pdf</a></span>   <span><a href='https://github.com/zwyang6/MoRe' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiwei Yang, Yucong Meng, Kexue Fu, Shuo Wang, Zhijian Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11076">MoRe: Class Patch Attention Needs Regularization for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Semantic Segmentation (WSSS) with image-level labels typically uses Class Activation Maps (CAM) to achieve dense predictions. Recently, Vision Transformer (ViT) has provided an alternative to generate localization maps from class-patch attention. However, due to insufficient constraints on modeling such attention, we observe that the Localization Attention Maps (LAM) often struggle with the artifact issue, i.e., patch regions with minimal semantic relevance are falsely activated by class tokens. In this work, we propose MoRe to address this issue and further explore the potential of LAM. Our findings suggest that imposing additional regularization on class-patch attention is necessary. To this end, we first view the attention as a novel directed graph and propose the Graph Category Representation module to implicitly regularize the interaction among class-patch entities. It ensures that class tokens dynamically condense the related patch information and suppress unrelated artifacts at a graph level. Second, motivated by the observation that CAM from classification weights maintains smooth localization of objects, we devise the Localization-informed Regularization module to explicitly regularize the class-patch attention. It directly mines the token relations from CAM and further supervises the consistency between class and patch tokens in a learnable manner. Extensive experiments are conducted on PASCAL VOC and MS COCO, validating that MoRe effectively addresses the artifact issue and achieves state-of-the-art performance, surpassing recent single-stage and even multi-stage methods. Code is available at https://github.com/zwyang6/MoRe.
<div id='section'>Paperid: <span id='pid'>99, <a href='https://arxiv.org/pdf/2412.06286.pdf' target='_blank'>https://arxiv.org/pdf/2412.06286.pdf</a></span>   <span><a href='https://github.com/patrick-john-ramos/nada' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Patrick Ramos, Nicolas Gonthier, Selina Khan, Yuta Nakashima, Noa Garcia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.06286">No Annotations for Object Detection in Art through Stable Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object detection in art is a valuable tool for the digital humanities, as it allows for faster identification of objects in artistic and historical images compared to humans. However, annotating such images poses significant challenges due to the need for specialized domain expertise. We present NADA (no annotations for detection in art), a pipeline that leverages diffusion models' art-related knowledge for object detection in paintings without the need for full bounding box supervision. Our method, which supports both weakly-supervised and zero-shot scenarios and does not require any fine-tuning of its pretrained components, consists of a class proposer based on large vision-language models and a class-conditioned detector based on Stable Diffusion. NADA is evaluated on two artwork datasets, ArtDL 2.0 and IconArt, outperforming prior work in weakly-supervised detection, while being the first work for zero-shot object detection in art. Code is available at https://github.com/patrick-john-ramos/nada
<div id='section'>Paperid: <span id='pid'>100, <a href='https://arxiv.org/pdf/2412.05876.pdf' target='_blank'>https://arxiv.org/pdf/2412.05876.pdf</a></span>   <span><a href='https://github.com/Xuefeng-Ni/MG-3D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuefeng Ni, Linshan Wu, Jiaxin Zhuang, Qiong Wang, Mingxiang Wu, Varut Vardhanabhuti, Lihai Zhang, Hanyu Gao, Hao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05876">MG-3D: Multi-Grained Knowledge-Enhanced 3D Medical Vision-Language Pre-training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D medical image analysis is pivotal in numerous clinical applications. However, the scarcity of labeled data and limited generalization capabilities hinder the advancement of AI-empowered models. Radiology reports are easily accessible and can serve as weakly-supervised signals. However, large-scale vision-language pre-training (VLP) remains underexplored in 3D medical image analysis. Specifically, the insufficient investigation into multi-grained radiology semantics and their correlations across patients leads to underutilization of large-scale volume-report data.
  Considering intra-patient cross-modal semantic consistency and inter-patient semantic correlations, we propose a multi-task VLP method, MG-3D, pre-trained on large-scale data (47.1K), addressing the challenges by the following two aspects: 1) Establishing the correspondence between volume semantics and multi-grained medical knowledge of each patient with cross-modal global alignment and complementary modality-guided local reconstruction, ensuring intra-patient features of different modalities cohesively represent the same semantic content; 2) Correlating inter-patient visual semantics based on fine-grained report correlations across patients, and keeping sensitivity to global individual differences via contrastive learning, enhancing the discriminative feature representation. Furthermore, we delve into the scaling law to explore potential performance improvements. Comprehensive evaluations across nine uni- and cross-modal clinical tasks are carried out to assess model efficacy. Extensive experiments on both internal and external datasets demonstrate the superior transferability, scalability, and generalization of MG-3D, showcasing its potential in advancing feature representation for 3D medical image analysis. Code will be available: https://github.com/Xuefeng-Ni/MG-3D.
<div id='section'>Paperid: <span id='pid'>101, <a href='https://arxiv.org/pdf/2412.04473.pdf' target='_blank'>https://arxiv.org/pdf/2412.04473.pdf</a></span>   <span><a href='https://github.com/woshixiaobai2019/nids-gpt.gi' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04473">Take Package as Language: Anomaly Detection Using Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Network data packet anomaly detection faces numerous challenges, including exploring new anomaly supervision signals, researching weakly supervised anomaly detection, and improving model interpretability. This paper proposes NIDS-GPT, a GPT-based causal language model for network intrusion detection. Unlike previous work, NIDS-GPT innovatively treats each number in the packet as an independent "word" rather than packet fields, enabling a more fine-grained data representation. We adopt an improved GPT-2 model and design special tokenizers and embedding layers to better capture the structure and semantics of network data. NIDS-GPT has good scalability, supports unsupervised pre-training, and enhances model interpretability through attention weight visualization. Experiments on the CICIDS2017 and car-hacking datasets show that NIDS-GPT achieves 100\% accuracy under extreme imbalance conditions, far surpassing traditional methods; it also achieves over 90\% accuracy in one-shot learning. These results demonstrate NIDS-GPT's excellent performance and potential in handling complex network anomaly detection tasks, especially in data-imbalanced and resource-constrained scenarios. The code is available at \url{https://github.com/woshixiaobai2019/nids-gpt.gi
<div id='section'>Paperid: <span id='pid'>102, <a href='https://arxiv.org/pdf/2412.03968.pdf' target='_blank'>https://arxiv.org/pdf/2412.03968.pdf</a></span>   <span><a href='https://github.com/MiSsU-HH/Exact' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Zhu, Yan Zhu, Jiayu Xiao, Tianxiang Xiao, Yike Ma, Yucheng Zhang, Feng Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.03968">Exact: Exploring Space-Time Perceptive Clues for Weakly Supervised Satellite Image Time Series Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automated crop mapping through Satellite Image Time Series (SITS) has emerged as a crucial avenue for agricultural monitoring and management. However, due to the low resolution and unclear parcel boundaries, annotating pixel-level masks is exceptionally complex and time-consuming in SITS. This paper embraces the weakly supervised paradigm (i.e., only image-level categories available) to liberate the crop mapping task from the exhaustive annotation burden. The unique characteristics of SITS give rise to several challenges in weakly supervised learning: (1) noise perturbation from spatially neighboring regions, and (2) erroneous semantic bias from anomalous temporal periods. To address the above difficulties, we propose a novel method, termed exploring space-time perceptive clues (Exact). First, we introduce a set of spatial clues to explicitly capture the representative patterns of different crops from the most class-relative regions. Besides, we leverage the temporal-to-class interaction of the model to emphasize the contributions of pivotal clips, thereby enhancing the model perception for crop regions. Build upon the space-time perceptive clues, we derive the clue-based CAMs to effectively supervise the SITS segmentation network. Our method demonstrates impressive performance on various SITS benchmarks. Remarkably, the segmentation network trained on Exact-generated masks achieves 95% of its fully supervised performance, showing the bright promise of weakly supervised paradigm in crop mapping scenario. Our code will be publicly available.
<div id='section'>Paperid: <span id='pid'>103, <a href='https://arxiv.org/pdf/2412.02012.pdf' target='_blank'>https://arxiv.org/pdf/2412.02012.pdf</a></span>   <span><a href='https://zhangdylan83.github.io/ewsmia/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenbo Zhang, Junyu Chen, Christopher Kanan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02012">INSIGHT: Explainable Weakly-Supervised Medical Image Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to their large sizes, volumetric scans and whole-slide pathology images (WSIs) are often processed by extracting embeddings from local regions and then an aggregator makes predictions from this set. However, current methods require post-hoc visualization techniques (e.g., Grad-CAM) and often fail to localize small yet clinically crucial details. To address these limitations, we introduce INSIGHT, a novel weakly-supervised aggregator that integrates heatmap generation as an inductive bias. Starting from pre-trained feature maps, INSIGHT employs a detection module with small convolutional kernels to capture fine details and a context module with a broader receptive field to suppress local false positives. The resulting internal heatmap highlights diagnostically relevant regions. On CT and WSI benchmarks, INSIGHT achieves state-of-the-art classification results and high weakly-labeled semantic segmentation performance. Project website and code are available at: https://zhangdylan83.github.io/ewsmia/
<div id='section'>Paperid: <span id='pid'>104, <a href='https://arxiv.org/pdf/2411.19067.pdf' target='_blank'>https://arxiv.org/pdf/2411.19067.pdf</a></span>   <span><a href='https://github.com/naver-ai/maskris' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Minhyun Lee, Seungho Lee, Song Park, Dongyoon Han, Byeongho Heo, Hyunjung Shim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19067">MaskRIS: Semantic Distortion-aware Data Augmentation for Referring Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring Image Segmentation (RIS) is an advanced vision-language task that involves identifying and segmenting objects within an image as described by free-form text descriptions. While previous studies focused on aligning visual and language features, exploring training techniques, such as data augmentation, remains underexplored. In this work, we explore effective data augmentation for RIS and propose a novel training framework called Masked Referring Image Segmentation (MaskRIS). We observe that the conventional image augmentations fall short of RIS, leading to performance degradation, while simple random masking significantly enhances the performance of RIS. MaskRIS uses both image and text masking, followed by Distortion-aware Contextual Learning (DCL) to fully exploit the benefits of the masking strategy. This approach can improve the model's robustness to occlusions, incomplete information, and various linguistic complexities, resulting in a significant performance improvement. Experiments demonstrate that MaskRIS can easily be applied to various RIS models, outperforming existing methods in both fully supervised and weakly supervised settings. Finally, MaskRIS achieves new state-of-the-art performance on RefCOCO, RefCOCO+, and RefCOCOg datasets. Code is available at https://github.com/naver-ai/maskris.
<div id='section'>Paperid: <span id='pid'>105, <a href='https://arxiv.org/pdf/2411.17376.pdf' target='_blank'>https://arxiv.org/pdf/2411.17376.pdf</a></span>   <span><a href='https://fujiry0.github.io/RealTraj-project-page' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryo Fujii, Hideo Saito, Ryo Hachiuma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17376">RealTraj: Towards Real-World Pedestrian Trajectory Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper jointly addresses three key limitations in conventional pedestrian trajectory forecasting: pedestrian perception errors, real-world data collection costs, and person ID annotation costs. We propose a novel framework, RealTraj, that enhances the real-world applicability of trajectory forecasting. Our approach includes two training phases -- self-supervised pretraining on synthetic data and weakly-supervised fine-tuning with limited real-world data -- to minimize data collection efforts. To improve robustness to real-world errors, we focus on both model design and training objectives. Specifically, we present Det2TrajFormer, a trajectory forecasting model that remains invariant to tracking noise by using past detections as inputs. Additionally, we pretrain the model using multiple pretext tasks, which enhance robustness and improve forecasting performance based solely on detection data. Unlike previous trajectory forecasting methods, our approach fine-tunes the model using only ground-truth detections, reducing the need for costly person ID annotations. In the experiments, we comprehensively verify the effectiveness of the proposed method against the limitations, and the method outperforms state-of-the-art trajectory forecasting methods on multiple datasets. The code will be released at https://fujiry0.github.io/RealTraj-project-page.
<div id='section'>Paperid: <span id='pid'>106, <a href='https://arxiv.org/pdf/2411.16219.pdf' target='_blank'>https://arxiv.org/pdf/2411.16219.pdf</a></span>   <span><a href='https://github.com/manuelknott/banana-defect-segmentation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Manuel Knott, Divinefavour Odion, Sameer Sontakke, Anup Karwa, Thijs Defraeye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16219">Weakly Supervised Panoptic Segmentation for Defect-Based Grading of Fresh Produce</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual inspection for defect grading in agricultural supply chains is crucial but traditionally labor-intensive and error-prone. Automated computer vision methods typically require extensively annotated datasets, which are often unavailable in decentralized supply chains. We address this challenge by evaluating the Segment Anything Model (SAM) to generate dense panoptic segmentation masks from sparse annotations. These dense predictions are then used to train a supervised panoptic segmentation model. Focusing on banana surface defects (bruises and scars), we validate our approach using 476 field images annotated with 1440 defects. While SAM-generated masks generally align with human annotations, substantially reducing annotation effort, we explicitly identify failure cases associated with specific defect sizes and shapes. Despite these limitations, our approach offers practical estimates of defect number and relative size from panoptic masks, underscoring the potential and current boundaries of foundation models for defect quantification in low-data agricultural scenarios. GitHub: https://github.com/manuelknott/banana-defect-segmentation
<div id='section'>Paperid: <span id='pid'>107, <a href='https://arxiv.org/pdf/2411.15763.pdf' target='_blank'>https://arxiv.org/pdf/2411.15763.pdf</a></span>   <span><a href='https://github.com/arvindmvepa/al-seg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Arvind Murari Vepa, Zukang Yang, Andrew Choi, Jungseock Joo, Fabien Scalzo, Yizhou Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15763">Integrating Deep Metric Learning with Coreset for Active Learning in 3D Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning has seen remarkable advancements in machine learning, yet it often demands extensive annotated data. Tasks like 3D semantic segmentation impose a substantial annotation burden, especially in domains like medicine, where expert annotations drive up the cost. Active learning (AL) holds great potential to alleviate this annotation burden in 3D medical segmentation. The majority of existing AL methods, however, are not tailored to the medical domain. While weakly-supervised methods have been explored to reduce annotation burden, the fusion of AL with weak supervision remains unexplored, despite its potential to significantly reduce annotation costs. Additionally, there is little focus on slice-based AL for 3D segmentation, which can also significantly reduce costs in comparison to conventional volume-based AL. This paper introduces a novel metric learning method for Coreset to perform slice-based active learning in 3D medical segmentation. By merging contrastive learning with inherent data groupings in medical imaging, we learn a metric that emphasizes the relevant differences in samples for training 3D medical segmentation models. We perform comprehensive evaluations using both weak and full annotations across four datasets (medical and non-medical). Our findings demonstrate that our approach surpasses existing active learning techniques on both weak and full annotations and obtains superior performance with low-annotation budgets which is crucial in medical imaging. Source code for this project is available in the supplementary materials and on GitHub: https://github.com/arvindmvepa/al-seg.
<div id='section'>Paperid: <span id='pid'>108, <a href='https://arxiv.org/pdf/2411.14429.pdf' target='_blank'>https://arxiv.org/pdf/2411.14429.pdf</a></span>   <span><a href='https://github.com/rayleizhu/GLMix' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Zhu, Xinjiang Wang, Wayne Zhang, Rynson W. H. Lau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.14429">Revisiting the Integration of Convolution and Attention for Vision Backbone</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Convolutions (Convs) and multi-head self-attentions (MHSAs) are typically considered alternatives to each other for building vision backbones. Although some works try to integrate both, they apply the two operators simultaneously at the finest pixel granularity. With Convs responsible for per-pixel feature extraction already, the question is whether we still need to include the heavy MHSAs at such a fine-grained level. In fact, this is the root cause of the scalability issue w.r.t. the input resolution for vision transformers. To address this important problem, we propose in this work to use MSHAs and Convs in parallel \textbf{at different granularity levels} instead. Specifically, in each layer, we use two different ways to represent an image: a fine-grained regular grid and a coarse-grained set of semantic slots. We apply different operations to these two representations: Convs to the grid for local features, and MHSAs to the slots for global features. A pair of fully differentiable soft clustering and dispatching modules is introduced to bridge the grid and set representations, thus enabling local-global fusion. Through extensive experiments on various vision tasks, we empirically verify the potential of the proposed integration scheme, named \textit{GLMix}: by offloading the burden of fine-grained features to light-weight Convs, it is sufficient to use MHSAs in a few (e.g., 64) semantic slots to match the performance of recent state-of-the-art backbones, while being more efficient. Our visualization results also demonstrate that the soft clustering module produces a meaningful semantic grouping effect with only IN1k classification supervision, which may induce better interpretability and inspire new weakly-supervised semantic segmentation approaches. Code will be available at \url{https://github.com/rayleizhu/GLMix}.
<div id='section'>Paperid: <span id='pid'>109, <a href='https://arxiv.org/pdf/2411.10364.pdf' target='_blank'>https://arxiv.org/pdf/2411.10364.pdf</a></span>   <span><a href='https://github.com/TianhaoMa5/LLP-AHIL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianhao Ma, Han Chen, Juncheng Hu, Yungang Zhu, Ximing Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10364">Forming Auxiliary High-confident Instance-level Loss to Promote Learning from Label Proportions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning from label proportions (LLP), i.e., a challenging weakly-supervised learning task, aims to train a classifier by using bags of instances and the proportions of classes within bags, rather than annotated labels for each instance. Beyond the traditional bag-level loss, the mainstream methodology of LLP is to incorporate an auxiliary instance-level loss with pseudo-labels formed by predictions. Unfortunately, we empirically observed that the pseudo-labels are are often inaccurate due to over-smoothing, especially for the scenarios with large bag sizes, hurting the classifier induction. To alleviate this problem, we suggest a novel LLP method, namely Learning from Label Proportions with Auxiliary High-confident Instance-level Loss (L^2P-AHIL). Specifically, we propose a dual entropy-based weight (DEW) method to adaptively measure the confidences of pseudo-labels. It simultaneously emphasizes accurate predictions at the bag level and avoids overly smoothed predictions. We then form high-confident instance-level loss with DEW, and jointly optimize it with the bag-level loss in a self-training manner. The experimental results on benchmark datasets show that L^2P-AHIL can surpass the existing baseline methods, and the performance gain can be more significant as the bag size increases. The implementation of our method is available at https://github.com/TianhaoMa5/LLP-AHIL.
<div id='section'>Paperid: <span id='pid'>110, <a href='https://arxiv.org/pdf/2411.06652.pdf' target='_blank'>https://arxiv.org/pdf/2411.06652.pdf</a></span>   <span><a href='https://github.com/liuzywen/LFScribble' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengyi Liu, Longzhen Wang, Xianyong Fang, Zhengzheng Tu, Linbo Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06652">LFSamba: Marry SAM with Mamba for Light Field Salient Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A light field camera can reconstruct 3D scenes using captured multi-focus images that contain rich spatial geometric information, enhancing applications in stereoscopic photography, virtual reality, and robotic vision. In this work, a state-of-the-art salient object detection model for multi-focus light field images, called LFSamba, is introduced to emphasize four main insights: (a) Efficient feature extraction, where SAM is used to extract modality-aware discriminative features; (b) Inter-slice relation modeling, leveraging Mamba to capture long-range dependencies across multiple focal slices, thus extracting implicit depth cues; (c) Inter-modal relation modeling, utilizing Mamba to integrate all-focus and multi-focus images, enabling mutual enhancement; (d) Weakly supervised learning capability, developing a scribble annotation dataset from an existing pixel-level mask dataset, establishing the first scribble-supervised baseline for light field salient object detection.https://github.com/liuzywen/LFScribble
<div id='section'>Paperid: <span id='pid'>111, <a href='https://arxiv.org/pdf/2410.14083.pdf' target='_blank'>https://arxiv.org/pdf/2410.14083.pdf</a></span>   <span><a href='https://github.com/sqhuang0103/SAMReg.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiqi Huang, Tingfa Xu, Ziyi Shen, Shaheer Ullah Saeed, Wen Yan, Dean Barratt, Yipeng Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14083">SAMReg: SAM-enabled Image Registration with ROI-based Correspondence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper describes a new spatial correspondence representation based on paired regions-of-interest (ROIs), for medical image registration. The distinct properties of the proposed ROI-based correspondence are discussed, in the context of potential benefits in clinical applications following image registration, compared with alternative correspondence-representing approaches, such as those based on sampled displacements and spatial transformation functions. These benefits include a clear connection between learning-based image registration and segmentation, which in turn motivates two cases of image registration approaches using (pre-)trained segmentation networks. Based on the segment anything model (SAM), a vision foundation model for segmentation, we develop a new registration algorithm SAMReg, which does not require any training (or training data), gradient-based fine-tuning or prompt engineering. The proposed SAMReg models are evaluated across five real-world applications, including intra-subject registration tasks with cardiac MR and lung CT, challenging inter-subject registration scenarios with prostate MR and retinal imaging, and an additional evaluation with a non-clinical example with aerial image registration. The proposed methods outperform both intensity-based iterative algorithms and DDF-predicting learning-based networks across tested metrics including Dice and target registration errors on anatomical structures, and further demonstrates competitive performance compared to weakly-supervised registration approaches that rely on fully-segmented training data. Open source code and examples are available at: https://github.com/sqhuang0103/SAMReg.git.
<div id='section'>Paperid: <span id='pid'>112, <a href='https://arxiv.org/pdf/2410.13621.pdf' target='_blank'>https://arxiv.org/pdf/2410.13621.pdf</a></span>   <span><a href='https://github.com/QI-NemoSong/EP-SAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Joonhyeon Song, Seohwan Yun, Seongho Yoon, Joohyeok Kim, Sangmin Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13621">EP-SAM: Weakly Supervised Histopathology Segmentation via Enhanced Prompt with Segment Anything</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work proposes a novel approach beyond supervised learning for effective pathological image analysis, addressing the challenge of limited robust labeled data. Pathological diagnosis of diseases like cancer has conventionally relied on the evaluation of morphological features by physicians and pathologists. However, recent advancements in compute-aided diagnosis (CAD) systems are gaining significant attention as diagnostic support tools. Although the advancement of deep learning has improved CAD significantly, segmentation models typically require large pixel-level annotated dataset, and such labeling is expensive. Existing studies not based on supervised approaches still struggle with limited generalization, and no practical approach has emerged yet. To address this issue, we present a weakly supervised semantic segmentation (WSSS) model by combining class activation map and Segment Anything Model (SAM)-based pseudo-labeling. For effective pretraining, we adopt the SAM-a foundation model that is pretrained on large datasets and operates in zero-shot configurations using only coarse prompts. The proposed approach transfer enhanced Attention Dropout Layer's knowledge to SAM, thereby generating pseudo-labels. To demonstrate the superiority of the proposed method, experimental studies are conducted on histopathological breast cancer datasets. The proposed method outperformed other WSSS methods across three datasets, demonstrating its efficiency by achieving this with only 12GB of GPU memory during training. Our code is available at : https://github.com/QI-NemoSong/EP-SAM
<div id='section'>Paperid: <span id='pid'>113, <a href='https://arxiv.org/pdf/2410.08509.pdf' target='_blank'>https://arxiv.org/pdf/2410.08509.pdf</a></span>   <span><a href='https://github.com/MoriLabNU/Bayesian_WSS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhou Zheng, Yuichiro Hayashi, Masahiro Oda, Takayuki Kitasaka, Kensaku Mori
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08509">A Bayesian Approach to Weakly-supervised Laparoscopic Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we study weakly-supervised laparoscopic image segmentation with sparse annotations. We introduce a novel Bayesian deep learning approach designed to enhance both the accuracy and interpretability of the model's segmentation, founded upon a comprehensive Bayesian framework, ensuring a robust and theoretically validated method. Our approach diverges from conventional methods that directly train using observed images and their corresponding weak annotations. Instead, we estimate the joint distribution of both images and labels given the acquired data. This facilitates the sampling of images and their high-quality pseudo-labels, enabling the training of a generalizable segmentation model. Each component of our model is expressed through probabilistic formulations, providing a coherent and interpretable structure. This probabilistic nature benefits accurate and practical learning from sparse annotations and equips our model with the ability to quantify uncertainty. Extensive evaluations with two public laparoscopic datasets demonstrated the efficacy of our method, which consistently outperformed existing methods. Furthermore, our method was adapted for scribble-supervised cardiac multi-structure segmentation, presenting competitive performance compared to previous methods. The code is available at https://github.com/MoriLabNU/Bayesian_WSS.
<div id='section'>Paperid: <span id='pid'>114, <a href='https://arxiv.org/pdf/2410.02212.pdf' target='_blank'>https://arxiv.org/pdf/2410.02212.pdf</a></span>   <span><a href='https://github.com/winston52/HNM-WSI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wentao Huang, Xiaoling Hu, Shahira Abousamra, Prateek Prasanna, Chao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02212">Hard Negative Sample Mining for Whole Slide Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised whole slide image (WSI) classification is challenging due to the lack of patch-level labels and high computational costs. State-of-the-art methods use self-supervised patch-wise feature representations for multiple instance learning (MIL). Recently, methods have been proposed to fine-tune the feature representation on the downstream task using pseudo labeling, but mostly focusing on selecting high-quality positive patches. In this paper, we propose to mine hard negative samples during fine-tuning. This allows us to obtain better feature representations and reduce the training cost. Furthermore, we propose a novel patch-wise ranking loss in MIL to better exploit these hard negative samples. Experiments on two public datasets demonstrate the efficacy of these proposed ideas. Our codes are available at https://github.com/winston52/HNM-WSI
<div id='section'>Paperid: <span id='pid'>115, <a href='https://arxiv.org/pdf/2410.01341.pdf' target='_blank'>https://arxiv.org/pdf/2410.01341.pdf</a></span>   <span><a href='https://github.com/ZhaofengSHI/CTDN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaofeng Shi, Heqian Qiu, Lanxiao Wang, Fanman Meng, Qingbo Wu, Hongliang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01341">Cognition Transferring and Decoupling for Text-supervised Egocentric Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we explore a novel Text-supervised Egocentic Semantic Segmentation (TESS) task that aims to assign pixel-level categories to egocentric images weakly supervised by texts from image-level labels. In this task with prospective potential, the egocentric scenes contain dense wearer-object relations and inter-object interference. However, most recent third-view methods leverage the frozen Contrastive Language-Image Pre-training (CLIP) model, which is pre-trained on the semantic-oriented third-view data and lapses in the egocentric view due to the ``relation insensitive" problem. Hence, we propose a Cognition Transferring and Decoupling Network (CTDN) that first learns the egocentric wearer-object relations via correlating the image and text. Besides, a Cognition Transferring Module (CTM) is developed to distill the cognitive knowledge from the large-scale pre-trained model to our model for recognizing egocentric objects with various semantics. Based on the transferred cognition, the Foreground-background Decoupling Module (FDM) disentangles the visual representations to explicitly discriminate the foreground and background regions to mitigate false activation areas caused by foreground-background interferential objects during egocentric relation learning. Extensive experiments on four TESS benchmarks demonstrate the effectiveness of our approach, which outperforms many recent related methods by a large margin. Code will be available at https://github.com/ZhaofengSHI/CTDN.
<div id='section'>Paperid: <span id='pid'>116, <a href='https://arxiv.org/pdf/2409.19720.pdf' target='_blank'>https://arxiv.org/pdf/2409.19720.pdf</a></span>   <span><a href='https://github.com/fukexue/FAST' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kexue Fu, Xiaoyuan Luo, Linhao Qu, Shuo Wang, Ying Xiong, Ilias Maglogiannis, Longxiang Gao, Manning Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19720">FAST: A Dual-tier Few-Shot Learning Paradigm for Whole Slide Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The expensive fine-grained annotation and data scarcity have become the primary obstacles for the widespread adoption of deep learning-based Whole Slide Images (WSI) classification algorithms in clinical practice. Unlike few-shot learning methods in natural images that can leverage the labels of each image, existing few-shot WSI classification methods only utilize a small number of fine-grained labels or weakly supervised slide labels for training in order to avoid expensive fine-grained annotation. They lack sufficient mining of available WSIs, severely limiting WSI classification performance. To address the above issues, we propose a novel and efficient dual-tier few-shot learning paradigm for WSI classification, named FAST. FAST consists of a dual-level annotation strategy and a dual-branch classification framework. Firstly, to avoid expensive fine-grained annotation, we collect a very small number of WSIs at the slide level, and annotate an extremely small number of patches. Then, to fully mining the available WSIs, we use all the patches and available patch labels to build a cache branch, which utilizes the labeled patches to learn the labels of unlabeled patches and through knowledge retrieval for patch classification. In addition to the cache branch, we also construct a prior branch that includes learnable prompt vectors, using the text encoder of visual-language models for patch classification. Finally, we integrate the results from both branches to achieve WSI classification. Extensive experiments on binary and multi-class datasets demonstrate that our proposed method significantly surpasses existing few-shot classification methods and approaches the accuracy of fully supervised methods with only 0.22$\%$ annotation costs. All codes and models will be publicly available on https://github.com/fukexue/FAST.
<div id='section'>Paperid: <span id='pid'>117, <a href='https://arxiv.org/pdf/2409.19483.pdf' target='_blank'>https://arxiv.org/pdf/2409.19483.pdf</a></span>   <span><a href='https://github.com/HealthX-Lab/MedCLIP-SAMv2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Taha Koleilat, Hojat Asgariandehkordi, Hassan Rivaz, Yiming Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19483">MedCLIP-SAMv2: Towards Universal Text-Driven Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Segmentation of anatomical structures and pathological regions in medical images is essential for modern clinical diagnosis, disease research, and treatment planning. While significant advancements have been made in deep learning-based segmentation techniques, many of these methods still suffer from limitations in data efficiency, generalizability, and interactivity. As a result, developing precise segmentation methods that require fewer labeled datasets remains a critical challenge in medical image analysis. Recently, the introduction of foundation models like CLIP and Segment-Anything-Model (SAM), with robust cross-domain representations, has paved the way for interactive and universal image segmentation. However, further exploration of these models for data-efficient segmentation in medical imaging is still needed and highly relevant. In this paper, we introduce MedCLIP-SAMv2, a novel framework that integrates the CLIP and SAM models to perform segmentation on clinical scans using text prompts, in both zero-shot and weakly supervised settings. Our approach includes fine-tuning the BiomedCLIP model with a new Decoupled Hard Negative Noise Contrastive Estimation (DHN-NCE) loss, and leveraging the Multi-modal Information Bottleneck (M2IB) to create visual prompts for generating segmentation masks from SAM in the zero-shot setting. We also investigate using zero-shot segmentation labels within a weakly supervised paradigm to enhance segmentation quality further. Extensive testing across four diverse segmentation tasks and medical imaging modalities (breast tumor ultrasound, brain tumor MRI, lung X-ray, and lung CT) demonstrates the high accuracy of our proposed framework. Our code is available at https://github.com/HealthX-Lab/MedCLIP-SAMv2.
<div id='section'>Paperid: <span id='pid'>118, <a href='https://arxiv.org/pdf/2409.19370.pdf' target='_blank'>https://arxiv.org/pdf/2409.19370.pdf</a></span>   <span><a href='https://github.com/GtLinyer/MambaEviScrib' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoxiang Han, Xinyu Li, Jiang Shang, Yiman Liu, Keyan Chen, Shugong Xu, Qiaohong Liu, Qi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19370">MambaEviScrib: Mamba and Evidence-Guided Consistency Enhance CNN Robustness for Scribble-Based Weakly Supervised Ultrasound Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Segmenting anatomical structures and lesions from ultrasound images contributes to disease assessment. Weakly supervised learning (WSL) based on sparse annotation has achieved encouraging performance and demonstrated the potential to reduce annotation costs. This study attempts to introduce scribble-based WSL into ultrasound image segmentation tasks. However, ultrasound images often suffer from poor contrast and unclear edges, coupled with insufficient supervison signals for edges, posing challenges to edge prediction. Uncertainty modeling has been proven to facilitate models in dealing with these issues. Nevertheless, existing uncertainty estimation paradigms are not robust enough and often filter out predictions near decision boundaries, resulting in unstable edge predictions. Therefore, we propose leveraging predictions near decision boundaries effectively. Specifically, we introduce Dempster-Shafer Theory (DST) of evidence to design an Evidence-Guided Consistency strategy. This strategy utilizes high-evidence predictions, which are more likely to occur near high-density regions, to guide the optimization of low-evidence predictions that may appear near decision boundaries. Furthermore, the diverse sizes and locations of lesions in ultrasound images pose a challenge for CNNs with local receptive fields, as they struggle to model global information. Therefore, we introduce Visual Mamba based on structured state space sequence models, which achieves long-range dependency with linear computational complexity, and we construct a novel hybrid CNN-Mamba framework. During training, the collaboration between the CNN branch and the Mamba branch in the proposed framework draws inspiration from each other based on the EGC strategy. Experiments demonstrate the competitiveness of the proposed method. Dataset and code will be available on https://github.com/GtLinyer/MambaEviScrib.
<div id='section'>Paperid: <span id='pid'>119, <a href='https://arxiv.org/pdf/2409.16213.pdf' target='_blank'>https://arxiv.org/pdf/2409.16213.pdf</a></span>   <span><a href='https://github.com/Harry-Rogers/PSIE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Harry Rogers, Tahmina Zebin, Grzegorz Cielniak, Beatriz De La Iglesia, Ben Magri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16213">Deep Learning for Precision Agriculture: Post-Spraying Evaluation and Deposition Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Precision spraying evaluation requires automation primarily in post-spraying imagery. In this paper we propose an eXplainable Artificial Intelligence (XAI) computer vision pipeline to evaluate a precision spraying system post-spraying without the need for traditional agricultural methods. The developed system can semantically segment potential targets such as lettuce, chickweed, and meadowgrass and correctly identify if targets have been sprayed. Furthermore, this pipeline evaluates using a domain-specific Weakly Supervised Deposition Estimation task, allowing for class-specific quantification of spray deposit weights in Î¼L. Estimation of coverage rates of spray deposition in a class-wise manner allows for further understanding of effectiveness of precision spraying systems. Our study evaluates different Class Activation Mapping techniques, namely AblationCAM and ScoreCAM, to determine which is more effective and interpretable for these tasks. In the pipeline, inference-only feature fusion is used to allow for further interpretability and to enable the automation of precision spraying evaluation post-spray. Our findings indicate that a Fully Convolutional Network with an EfficientNet-B0 backbone and inference-only feature fusion achieves an average absolute difference in deposition values of 156.8 Î¼L across three classes in our test set. The dataset curated in this paper is publicly available at https://github.com/Harry-Rogers/PSIE
<div id='section'>Paperid: <span id='pid'>120, <a href='https://arxiv.org/pdf/2409.14319.pdf' target='_blank'>https://arxiv.org/pdf/2409.14319.pdf</a></span>   <span><a href='https://github.com/zhousheng97/ViTXT-GQA.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sheng Zhou, Junbin Xiao, Xun Yang, Peipei Song, Dan Guo, Angela Yao, Meng Wang, Tat-Seng Chua
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14319">Scene-Text Grounding for Text-Based Video Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing efforts in text-based video question answering (TextVideoQA) are criticized for their opaque decisionmaking and heavy reliance on scene-text recognition. In this paper, we propose to study Grounded TextVideoQA by forcing models to answer questions and spatio-temporally localize the relevant scene-text regions, thus decoupling QA from scenetext recognition and promoting research towards interpretable QA. The task has three-fold significance. First, it encourages scene-text evidence versus other short-cuts for answer predictions. Second, it directly accepts scene-text regions as visual answers, thus circumventing the problem of ineffective answer evaluation by stringent string matching. Third, it isolates the challenges inherited in VideoQA and scene-text recognition. This enables the diagnosis of the root causes for failure predictions, e.g., wrong QA or wrong scene-text recognition? To achieve Grounded TextVideoQA, we propose the T2S-QA model that highlights a disentangled temporal-to-spatial contrastive learning strategy for weakly-supervised scene-text grounding and grounded TextVideoQA. To facilitate evaluation, we construct a new dataset ViTXT-GQA which features 52K scene-text bounding boxes within 2.2K temporal segments related to 2K questions and 729 videos. With ViTXT-GQA, we perform extensive experiments and demonstrate the severe limitations of existing techniques in Grounded TextVideoQA. While T2S-QA achieves superior results, the large performance gap with human leaves ample space for improvement. Our further analysis of oracle scene-text inputs posits that the major challenge is scene-text recognition. To advance the research of Grounded TextVideoQA, our dataset and code are at https://github.com/zhousheng97/ViTXT-GQA.git
<div id='section'>Paperid: <span id='pid'>121, <a href='https://arxiv.org/pdf/2409.13431.pdf' target='_blank'>https://arxiv.org/pdf/2409.13431.pdf</a></span>   <span><a href='https://github.com/wzx99/TMIM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixiao Wang, Hongtao Xie, YuXin Wang, Yadong Qu, Fengjun Guo, Pengwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.13431">Leveraging Text Localization for Scene Text Removal via Text-aware Masked Image Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing scene text removal (STR) task suffers from insufficient training data due to the expensive pixel-level labeling. In this paper, we aim to address this issue by introducing a Text-aware Masked Image Modeling algorithm (TMIM), which can pretrain STR models with low-cost text detection labels (e.g., text bounding box). Different from previous pretraining methods that use indirect auxiliary tasks only to enhance the implicit feature extraction ability, our TMIM first enables the STR task to be directly trained in a weakly supervised manner, which explores the STR knowledge explicitly and efficiently. In TMIM, first, a Background Modeling stream is built to learn background generation rules by recovering the masked non-text region. Meanwhile, it provides pseudo STR labels on the masked text region. Second, a Text Erasing stream is proposed to learn from the pseudo labels and equip the model with end-to-end STR ability. Benefiting from the two collaborative streams, our STR model can achieve impressive performance only with the public text detection datasets, which greatly alleviates the limitation of the high-cost STR labels. Experiments demonstrate that our method outperforms other pretrain methods and achieves state-of-the-art performance (37.35 PSNR on SCUT-EnsText). Code will be available at https://github.com/wzx99/TMIM.
<div id='section'>Paperid: <span id='pid'>122, <a href='https://arxiv.org/pdf/2409.10291.pdf' target='_blank'>https://arxiv.org/pdf/2409.10291.pdf</a></span>   <span><a href='https://github.com/mishgon/ape' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mikhail Goncharov, Valentin Samokhin, Eugenia Soboleva, Roman Sokolov, Boris Shirokikh, Mikhail Belyaev, Anvar Kurmukov, Ivan Oseledets
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10291">Anatomical Positional Embeddings</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a self-supervised model producing 3D anatomical positional embeddings (APE) of individual medical image voxels. APE encodes voxels' anatomical closeness, i.e., voxels of the same organ or nearby organs always have closer positional embeddings than the voxels of more distant body parts. In contrast to the existing models of anatomical positional embeddings, our method is able to efficiently produce a map of voxel-wise embeddings for a whole volumetric input image, which makes it an optimal choice for different downstream applications. We train our APE model on 8400 publicly available CT images of abdomen and chest regions. We demonstrate its superior performance compared with the existing models on anatomical landmark retrieval and weakly-supervised few-shot localization of 13 abdominal organs. As a practical application, we show how to cheaply train APE to crop raw CT images to different anatomical regions of interest with 0.99 recall, while reducing the image volume by 10-100 times. The code and the pre-trained APE model are available at https://github.com/mishgon/ape .
<div id='section'>Paperid: <span id='pid'>123, <a href='https://arxiv.org/pdf/2409.09369.pdf' target='_blank'>https://arxiv.org/pdf/2409.09369.pdf</a></span>   <span><a href='https://github.com/liupei101/VLSA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pei Liu, Luping Ji, Jiaxiang Gou, Bo Fu, Mao Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09369">Interpretable Vision-Language Survival Analysis with Ordinal Inductive Bias for Computational Pathology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Histopathology Whole-Slide Images (WSIs) provide an important tool to assess cancer prognosis in computational pathology (CPATH). While existing survival analysis (SA) approaches have made exciting progress, they are generally limited to adopting highly-expressive network architectures and only coarse-grained patient-level labels to learn visual prognostic representations from gigapixel WSIs. Such learning paradigm suffers from critical performance bottlenecks, when facing present scarce training data and standard multi-instance learning (MIL) framework in CPATH. To overcome it, this paper, for the first time, proposes a new Vision-Language-based SA (VLSA) paradigm. Concretely, (1) VLSA is driven by pathology VL foundation models. It no longer relies on high-capability networks and shows the advantage of data efficiency. (2) In vision-end, VLSA encodes textual prognostic prior and then employs it as auxiliary signals to guide the aggregating of visual prognostic features at instance level, thereby compensating for the weak supervision in MIL. Moreover, given the characteristics of SA, we propose i) ordinal survival prompt learning to transform continuous survival labels into textual prompts; and ii) ordinal incidence function as prediction target to make SA compatible with VL-based prediction. Notably, VLSA's predictions can be interpreted intuitively by our Shapley values-based method. The extensive experiments on five datasets confirm the effectiveness of our scheme. Our VLSA could pave a new way for SA in CPATH by offering weakly-supervised MIL an effective means to learn valuable prognostic clues from gigapixel WSIs. Our source code is available at https://github.com/liupei101/VLSA.
<div id='section'>Paperid: <span id='pid'>124, <a href='https://arxiv.org/pdf/2409.01691.pdf' target='_blank'>https://arxiv.org/pdf/2409.01691.pdf</a></span>   <span><a href='https://github.com/CUHK-AIM-Group/SAMTooth' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Liu, Wuyang Li, Cheng Wang, Hui Chen, Yixuan Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01691">When 3D Partial Points Meets SAM: Tooth Point Cloud Segmentation with Sparse Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tooth point cloud segmentation is a fundamental task in many orthodontic applications. Current research mainly focuses on fully supervised learning which demands expensive and tedious manual point-wise annotation. Although recent weakly-supervised alternatives are proposed to use weak labels for 3D segmentation and achieve promising results, they tend to fail when the labels are extremely sparse. Inspired by the powerful promptable segmentation capability of the Segment Anything Model (SAM), we propose a framework named SAMTooth that leverages such capacity to complement the extremely sparse supervision. To automatically generate appropriate point prompts for SAM, we propose a novel Confidence-aware Prompt Generation strategy, where coarse category predictions are aggregated with confidence-aware filtering. Furthermore, to fully exploit the structural and shape clues in SAM's outputs for assisting the 3D feature learning, we advance a Mask-guided Representation Learning that re-projects the generated tooth masks of SAM into 3D space and constrains these points of different teeth to possess distinguished representations. To demonstrate the effectiveness of the framework, we conduct experiments on the public dataset and surprisingly find with only 0.1\% annotations (one point per tooth), our method can surpass recent weakly supervised methods by a large margin, and the performance is even comparable to the recent fully-supervised methods, showcasing the significant potential of applying SAM to 3D perception tasks with sparse labels. Code is available at https://github.com/CUHK-AIM-Group/SAMTooth.
<div id='section'>Paperid: <span id='pid'>125, <a href='https://arxiv.org/pdf/2409.01472.pdf' target='_blank'>https://arxiv.org/pdf/2409.01472.pdf</a></span>   <span><a href='https://github.com/xuanrui-work/WSSSByRec' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuanrui Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01472">Semantic Segmentation from Image Labels by Reconstruction from Structured Decomposition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised image segmentation (WSSS) from image tags remains challenging due to its under-constraint nature. Most mainstream work focus on the extraction of class activation map (CAM) and imposing various additional regularization. Contrary to the mainstream, we propose to frame WSSS as a problem of reconstruction from decomposition of the image using its mask, under which most regularization are embedded implicitly within the framework of the new problem. Our approach has demonstrated promising results on initial experiments, and shown robustness against the problem of background ambiguity. Our code is available at \url{https://github.com/xuanrui-work/WSSSByRec}.
<div id='section'>Paperid: <span id='pid'>126, <a href='https://arxiv.org/pdf/2408.17143.pdf' target='_blank'>https://arxiv.org/pdf/2408.17143.pdf</a></span>   <span><a href='https://github.com/n-kubiak/RenDetNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikolina Kubiak, Elliot Wortman, Armin Mustafa, Graeme Phillipson, Stephen Jolly, Simon Hadfield
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.17143">RenDetNet: Weakly-supervised Shadow Detection with Shadow Caster Verification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing shadow detection models struggle to differentiate dark image areas from shadows. In this paper, we tackle this issue by verifying that all detected shadows are real, i.e. they have paired shadow casters. We perform this step in a physically-accurate manner by differentiably re-rendering the scene and observing the changes stemming from carving out estimated shadow casters. Thanks to this approach, the RenDetNet proposed in this paper is the first learning-based shadow detection model whose supervisory signals can be computed in a self-supervised manner. The developed system compares favourably against recent models trained on our data. As part of this publication, we release our code on github.
<div id='section'>Paperid: <span id='pid'>127, <a href='https://arxiv.org/pdf/2408.16661.pdf' target='_blank'>https://arxiv.org/pdf/2408.16661.pdf</a></span>   <span><a href='https://github.com/farnooshar/EigenClusterVIS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Farnoosh Arefi, Amir M. Mansourian, Shohreh Kasaei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.16661">Improving Weakly-supervised Video Instance Segmentation by Leveraging Spatio-temporal Consistency</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The performance of Video Instance Segmentation (VIS) methods has improved significantly with the advent of transformer networks. However, these networks often face challenges in training due to the high annotation cost. To address this, unsupervised and weakly-supervised methods have been developed to reduce the dependency on annotations. This work introduces a novel weakly-supervised method called Eigen-Cluster VIS that, without requiring any mask annotations, achieves competitive accuracy compared to other VIS approaches. This method is based on two key innovations: a Temporal Eigenvalue Loss (TEL) and a clip-level Quality Cluster Coefficient (QCC). The TEL ensures temporal coherence by leveraging the eigenvalues of the Laplacian matrix derived from graph adjacency matrices. By minimizing the mean absolute error between the eigenvalues of adjacent frames, this loss function promotes smooth transitions and stable segmentation boundaries over time, reducing temporal discontinuities and improving overall segmentation quality. The QCC employs the K-means method to ensure the quality of spatio-temporal clusters without relying on ground truth masks. Using the Davies-Bouldin score, the QCC provides an unsupervised measure of feature discrimination, allowing the model to self-evaluate and adapt to varying object distributions, enhancing robustness during the testing phase. These enhancements are computationally efficient and straightforward, offering significant performance gains without additional annotated data. The proposed Eigen-Cluster VIS method is evaluated on the YouTube-Video Instance Segmentation (YouTube-VIS) 2019/2021 and Occluded Video Instance Segmentation (OVIS) datasets, demonstrating that it effectively narrows the performance gap between the fully-supervised and weakly-supervised VIS approaches. The code is available on https://github.com/farnooshar/EigenClusterVIS
<div id='section'>Paperid: <span id='pid'>128, <a href='https://arxiv.org/pdf/2408.16451.pdf' target='_blank'>https://arxiv.org/pdf/2408.16451.pdf</a></span>   <span><a href='https://github.com/yc-zh/WSVM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongcun Zhang, Jiajun Xu, Yina He, Shaozi Li, Zhiming Luo, Huangwei Lei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.16451">Weakly Supervised Object Detection for Automatic Tooth-marked Tongue Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tongue diagnosis in Traditional Chinese Medicine (TCM) is a crucial diagnostic method that can reflect an individual's health status. Traditional methods for identifying tooth-marked tongues are subjective and inconsistent because they rely on practitioner experience. We propose a novel fully automated Weakly Supervised method using Vision transformer and Multiple instance learning WSVM for tongue extraction and tooth-marked tongue recognition. Our approach first accurately detects and extracts the tongue region from clinical images, removing any irrelevant background information. Then, we implement an end-to-end weakly supervised object detection method. We utilize Vision Transformer (ViT) to process tongue images in patches and employ multiple instance loss to identify tooth-marked regions with only image-level annotations. WSVM achieves high accuracy in tooth-marked tongue classification, and visualization experiments demonstrate its effectiveness in pinpointing these regions. This automated approach enhances the objectivity and accuracy of tooth-marked tongue diagnosis. It provides significant clinical value by assisting TCM practitioners in making precise diagnoses and treatment recommendations. Code is available at https://github.com/yc-zh/WSVM.
<div id='section'>Paperid: <span id='pid'>129, <a href='https://arxiv.org/pdf/2408.14130.pdf' target='_blank'>https://arxiv.org/pdf/2408.14130.pdf</a></span>   <span><a href='https://github.com/stainlessnight/LLP-LargeBags' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shunsuke Kubo, Shinnosuke Matsuo, Daiki Suehiro, Kazuhiro Terada, Hiroaki Ito, Akihiko Yoshizawa, Ryoma Bise
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.14130">Theoretical Proportion Label Perturbation for Learning from Label Proportions in Large Bags</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning from label proportions (LLP) is a kind of weakly supervised learning that trains an instance-level classifier from label proportions of bags, which consist of sets of instances without using instance labels. A challenge in LLP arises when the number of instances in a bag (bag size) is numerous, making the traditional LLP methods difficult due to GPU memory limitations. This study aims to develop an LLP method capable of learning from bags with large sizes. In our method, smaller bags (mini-bags) are generated by sampling instances from large-sized bags (original bags), and these mini-bags are used in place of the original bags. However, the proportion of a mini-bag is unknown and differs from that of the original bag, leading to overfitting. To address this issue, we propose a perturbation method for the proportion labels of sampled mini-bags to mitigate overfitting to noisy label proportions. This perturbation is added based on the multivariate hypergeometric distribution, which is statistically modeled. Additionally, loss weighting is implemented to reduce the negative impact of proportions sampled from the tail of the distribution. Experimental results demonstrate that the proportion label perturbation and loss weighting achieve classification accuracy comparable to that obtained without sampling. Our codes are available at https://github.com/stainlessnight/LLP-LargeBags.
<div id='section'>Paperid: <span id='pid'>130, <a href='https://arxiv.org/pdf/2408.12489.pdf' target='_blank'>https://arxiv.org/pdf/2408.12489.pdf</a></span>   <span><a href='https://github.com/wbkit/Scribbles4All' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wolfgang Boettcher, Lukas Hoyer, Ozan Unal, Jan Eric Lenssen, Bernt Schiele
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.12489">Scribbles for All: Benchmarking Scribble Supervised Segmentation Across Datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we introduce Scribbles for All, a label and training data generation algorithm for semantic segmentation trained on scribble labels. Training or fine-tuning semantic segmentation models with weak supervision has become an important topic recently and was subject to significant advances in model quality. In this setting, scribbles are a promising label type to achieve high quality segmentation results while requiring a much lower annotation effort than usual pixel-wise dense semantic segmentation annotations. The main limitation of scribbles as source for weak supervision is the lack of challenging datasets for scribble segmentation, which hinders the development of novel methods and conclusive evaluations. To overcome this limitation, Scribbles for All provides scribble labels for several popular segmentation datasets and provides an algorithm to automatically generate scribble labels for any dataset with dense annotations, paving the way for new insights and model advancements in the field of weakly supervised segmentation. In addition to providing datasets and algorithm, we evaluate state-of-the-art segmentation models on our datasets and show that models trained with our synthetic labels perform competitively with respect to models trained on manual labels. Thus, our datasets enable state-of-the-art research into methods for scribble-labeled semantic segmentation. The datasets, scribble generation algorithm, and baselines are publicly available at https://github.com/wbkit/Scribbles4All
<div id='section'>Paperid: <span id='pid'>131, <a href='https://arxiv.org/pdf/2408.11505.pdf' target='_blank'>https://arxiv.org/pdf/2408.11505.pdf</a></span>   <span><a href='https://github.com/Hanminghao/MSCPT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Minghao Han, Linhao Qu, Dingkang Yang, Xukun Zhang, Xiaoying Wang, Lihua Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.11505">MSCPT: Few-shot Whole Slide Image Classification with Multi-scale and Context-focused Prompt Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple instance learning (MIL) has become a standard paradigm for the weakly supervised classification of whole slide images (WSIs). However, this paradigm relies on using a large number of labeled WSIs for training. The lack of training data and the presence of rare diseases pose significant challenges for these methods. Prompt tuning combined with pre-trained Vision-Language models (VLMs) is an effective solution to the Few-shot Weakly Supervised WSI Classification (FSWC) task. Nevertheless, applying prompt tuning methods designed for natural images to WSIs presents three significant challenges: 1) These methods fail to fully leverage the prior knowledge from the VLM's text modality; 2) They overlook the essential multi-scale and contextual information in WSIs, leading to suboptimal results; and 3) They lack exploration of instance aggregation methods. To address these problems, we propose a Multi-Scale and Context-focused Prompt Tuning (MSCPT) method for FSWC task. Specifically, MSCPT employs the frozen large language model to generate pathological visual language prior knowledge at multiple scales, guiding hierarchical prompt tuning. Additionally, we design a graph prompt tuning module to learn essential contextual information within WSI, and finally, a non-parametric cross-guided instance aggregation module has been introduced to derive the WSI-level features. Extensive experiments, visualizations, and interpretability analyses were conducted on five datasets and three downstream tasks using three VLMs, demonstrating the strong performance of our MSCPT. All codes have been made publicly accessible at https://github.com/Hanminghao/MSCPT.
<div id='section'>Paperid: <span id='pid'>132, <a href='https://arxiv.org/pdf/2408.10031.pdf' target='_blank'>https://arxiv.org/pdf/2408.10031.pdf</a></span>   <span><a href='https://github.com/covisionlab/dynamic-label-injection.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Emanuele Caruso, Francesco Pelosin, Alessandro Simoni, Marco Boschetti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.10031">Dynamic Label Injection for Imbalanced Industrial Defect Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we propose a simple yet effective method to tackle the problem of imbalanced multi-class semantic segmentation in deep learning systems. One of the key properties for a good training set is the balancing among the classes. When the input distribution is heavily imbalanced in the number of instances, the learning process could be hindered or difficult to carry on. To this end, we propose a Dynamic Label Injection (DLI) algorithm to impose a uniform distribution in the input batch. Our algorithm computes the current batch defect distribution and re-balances it by transferring defects using a combination of Poisson-based seamless image cloning and cut-paste techniques. A thorough experimental section on the Magnetic Tiles dataset shows better results of DLI compared to other balancing loss approaches also in the challenging weakly-supervised setup. The code is available at https://github.com/covisionlab/dynamic-label-injection.git
<div id='section'>Paperid: <span id='pid'>133, <a href='https://arxiv.org/pdf/2408.09411.pdf' target='_blank'>https://arxiv.org/pdf/2408.09411.pdf</a></span>   <span><a href='https://github.com/WltyBY/LNQ2023_training_code.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Litingyu Wang, Yijie Qu, Xiangde Luo, Wenjun Liao, Shichuan Zhang, Guotai Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09411">Weakly Supervised Lymph Nodes Segmentation Based on Partial Instance Annotations with Pre-trained Dual-branch Network and Pseudo Label Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Assessing the presence of potentially malignant lymph nodes aids in estimating cancer progression, and identifying surrounding benign lymph nodes can assist in determining potential metastatic pathways for cancer. For quantitative analysis, automatic segmentation of lymph nodes is crucial. However, due to the labor-intensive and time-consuming manual annotation process required for a large number of lymph nodes, it is more practical to annotate only a subset of the lymph node instances to reduce annotation costs. In this study, we propose a pre-trained Dual-Branch network with Dynamically Mixed Pseudo label (DBDMP) to learn from partial instance annotations for lymph nodes segmentation. To obtain reliable pseudo labels for lymph nodes that are not annotated, we employ a dual-decoder network to generate different outputs that are then dynamically mixed. We integrate the original weak partial annotations with the mixed pseudo labels to supervise the network. To further leverage the extensive amount of unannotated voxels, we apply a self-supervised pre-training strategy to enhance the model's feature extraction capability. Experiments on the mediastinal Lymph Node Quantification (LNQ) dataset demonstrate that our method, compared to directly learning from partial instance annotations, significantly improves the Dice Similarity Coefficient (DSC) from 11.04% to 54.10% and reduces the Average Symmetric Surface Distance (ASSD) from 20.83 $mm$ to 8.72 $mm$. The code is available at https://github.com/WltyBY/LNQ2023_training_code.git
<div id='section'>Paperid: <span id='pid'>134, <a href='https://arxiv.org/pdf/2408.09085.pdf' target='_blank'>https://arxiv.org/pdf/2408.09085.pdf</a></span>   <span><a href='https://xiaoaoran.github.io/projects/MM-SAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aoran Xiao, Weihao Xuan, Heli Qi, Yun Xing, Naoto Yokoya, Shijian Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09085">Segment Anything with Multiple Modalities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robust and accurate segmentation of scenes has become one core functionality in various visual recognition and navigation tasks. This has inspired the recent development of Segment Anything Model (SAM), a foundation model for general mask segmentation. However, SAM is largely tailored for single-modal RGB images, limiting its applicability to multi-modal data captured with widely-adopted sensor suites, such as LiDAR plus RGB, depth plus RGB, thermal plus RGB, etc. We develop MM-SAM, an extension and expansion of SAM that supports cross-modal and multi-modal processing for robust and enhanced segmentation with different sensor suites. MM-SAM features two key designs, namely, unsupervised cross-modal transfer and weakly-supervised multi-modal fusion, enabling label-efficient and parameter-efficient adaptation toward various sensor modalities. It addresses three main challenges: 1) adaptation toward diverse non-RGB sensors for single-modal processing, 2) synergistic processing of multi-modal data via sensor fusion, and 3) mask-free training for different downstream tasks. Extensive experiments show that MM-SAM consistently outperforms SAM by large margins, demonstrating its effectiveness and robustness across various sensors and data modalities.
<div id='section'>Paperid: <span id='pid'>135, <a href='https://arxiv.org/pdf/2408.07079.pdf' target='_blank'>https://arxiv.org/pdf/2408.07079.pdf</a></span>   <span><a href='https://github.com/EIDOSLAB/AnatCL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Carlo Alberto Barbano, Matteo Brunello, Benoit Dufumier, Marco Grangetto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.07079">Anatomical Foundation Models for Brain MRIs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep Learning (DL) in neuroimaging has become increasingly relevant for detecting neurological conditions and neurodegenerative disorders. One of the most predominant biomarkers in neuroimaging is represented by brain age, which has been shown to be a good indicator for different conditions, such as Alzheimer's Disease. Using brain age for weakly supervised pre-training of DL models in transfer learning settings has also recently shown promising results, especially when dealing with data scarcity of different conditions. On the other hand, anatomical information of brain MRIs (e.g. cortical thickness) can provide important information for learning good representations that can be transferred to many downstream tasks. In this work, we propose AnatCL, an anatomical foundation model for brain MRIs that i.) leverages anatomical information in a weakly contrastive learning approach, and ii.) achieves state-of-the-art performances across many different downstream tasks. To validate our approach we consider 12 different downstream tasks for the diagnosis of different conditions such as Alzheimer's Disease, autism spectrum disorder, and schizophrenia. Furthermore, we also target the prediction of 10 different clinical assessment scores using structural MRI data. Our findings show that incorporating anatomical information during pre-training leads to more robust and generalizable representations. Pre-trained models can be found at: https://github.com/EIDOSLAB/AnatCL.
<div id='section'>Paperid: <span id='pid'>136, <a href='https://arxiv.org/pdf/2408.05955.pdf' target='_blank'>https://arxiv.org/pdf/2408.05955.pdf</a></span>   <span><a href='https://github.com/sejong-rcv/PVLR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Geuntaek Lim, Hyunwoo Kim, Joonsoo Kim, Yukyung Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.05955">Probabilistic Vision-Language Representation for Weakly Supervised Temporal Action Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised temporal action localization (WTAL) aims to detect action instances in untrimmed videos using only video-level annotations. Since many existing works optimize WTAL models based on action classification labels, they encounter the task discrepancy problem (i.e., localization-by-classification). To tackle this issue, recent studies have attempted to utilize action category names as auxiliary semantic knowledge through vision-language pre-training (VLP). However, there are still areas where existing research falls short. Previous approaches primarily focused on leveraging textual information from language models but overlooked the alignment of dynamic human action and VLP knowledge in a joint space. Furthermore, the deterministic representation employed in previous studies struggles to capture fine-grained human motions. To address these problems, we propose a novel framework that aligns human action knowledge and VLP knowledge in a probabilistic embedding space. Moreover, we propose intra- and inter-distribution contrastive learning to enhance the probabilistic embedding space based on statistical similarities. Extensive experiments and ablation studies reveal that our method significantly outperforms all previous state-of-the-art methods. Code is available at https://github.com/sejong-rcv/PVLR.
<div id='section'>Paperid: <span id='pid'>137, <a href='https://arxiv.org/pdf/2408.05215.pdf' target='_blank'>https://arxiv.org/pdf/2408.05215.pdf</a></span>   <span><a href='https://github.com/nec-research/PICPS-ML4Sci' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Makoto Takamoto, Viktor Zaverkin, Mathias Niepert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.05215">Physics-Informed Weakly Supervised Learning for Interatomic Potentials</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning plays an increasingly important role in computational chemistry and materials science, complementing computationally intensive ab initio and first-principles methods. Despite their utility, machine-learning models often lack generalization capability and robustness during atomistic simulations, yielding unphysical energy and force predictions that hinder their real-world applications. We address this challenge by introducing a physics-informed, weakly supervised approach for training machine-learned interatomic potentials (MLIPs). We introduce two novel loss functions, extrapolating the potential energy via a Taylor expansion and using the concept of conservative forces. Our approach improves the accuracy of MLIPs applied to training tasks with sparse training data sets and reduces the need for pre-training computationally demanding models with large data sets. Particularly, we perform extensive experiments demonstrating reduced energy and force errors -- often lower by a factor of two -- for various baseline models and benchmark data sets. Moreover, we demonstrate improved robustness during MD simulations of the MLIP models trained with the proposed weakly supervised loss. Finally, our approach improves the fine-tuning of foundation models on sparse, highly accurate ab initio data. An implementation of our method and scripts for executing experiments are available at https://github.com/nec-research/PICPS-ML4Sci.
<div id='section'>Paperid: <span id='pid'>138, <a href='https://arxiv.org/pdf/2408.01191.pdf' target='_blank'>https://arxiv.org/pdf/2408.01191.pdf</a></span>   <span><a href='https://github.com/xrt11/tumor-segmentation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruitao Xie, Limai Jiang, Xiaoxi He, Yi Pan, Yunpeng Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.01191">A Weakly Supervised and Globally Explainable Learning Framework for Brain Tumor Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine-based brain tumor segmentation can help doctors make better diagnoses. However, the complex structure of brain tumors and expensive pixel-level annotations present challenges for automatic tumor segmentation. In this paper, we propose a counterfactual generation framework that not only achieves exceptional brain tumor segmentation performance without the need for pixel-level annotations, but also provides explainability. Our framework effectively separates class-related features from class-unrelated features of the samples, and generate new samples that preserve identity features while altering class attributes by embedding different class-related features. We perform topological data analysis on the extracted class-related features and obtain a globally explainable manifold, and for each abnormal sample to be segmented, a meaningful normal sample could be effectively generated with the guidance of the rule-based paths designed within the manifold for comparison for identifying the tumor regions. We evaluate our proposed method on two datasets, which demonstrates superior performance of brain tumor segmentation. The code is available at https://github.com/xrt11/tumor-segmentation.
<div id='section'>Paperid: <span id='pid'>139, <a href='https://arxiv.org/pdf/2408.01162.pdf' target='_blank'>https://arxiv.org/pdf/2408.01162.pdf</a></span>   <span><a href='https://github.com/bryanwong17/PreMix' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bryan Wong, Mun Yong Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.01162">PreMix: Label-Efficient Multiple Instance Learning via Non-Contrastive Pre-training and Feature Mixing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple instance learning (MIL) has emerged as a powerful framework for weakly supervised whole slide image (WSI) classification, enabling slide-level predictions without requiring detailed patch-level annotations. Despite its success, a critical limitation of current MIL methods lies in the underutilization of pre-training for the MIL aggregator. Most existing approaches initialize the aggregator randomly and train it from scratch, making performance highly sensitive to the quantity of labeled WSIs and ignoring the abundance of unlabeled WSIs commonly available in clinical settings. To address this, we propose PreMix, a novel framework that leverages a non-contrastive pre-training method, Barlow Twins, augmented with the Slide Mixing approach to generate additional positive pairs and enhance feature learning, particularly under limited labeled WSI conditions. Fine-tuning with Mixup and Manifold Mixup further enhances robustness by effectively handling the diverse sizes of gigapixel WSIs. Experimental results demonstrate that integrating PreMix as a plug-in module into HIPT yields an average F1 improvement of 4.7% over the baseline HIPT across various WSI training sizes and datasets. These findings underscore its potential to advance WSI classification with limited labeled data and its applicability to real-world histopathology practices. The code is available at https://github.com/bryanwong17/PreMix
<div id='section'>Paperid: <span id='pid'>140, <a href='https://arxiv.org/pdf/2407.21604.pdf' target='_blank'>https://arxiv.org/pdf/2407.21604.pdf</a></span>   <span><a href='https://github.com/kimjongwoo-cell/MicroMIL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jongwoo Kim, Bryan Wong, Huazhu Fu, Willmer Rafell QuiÃ±ones, Youngsin Ko, Mun Yong Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.21604">MicroMIL: Graph-Based Multiple Instance Learning for Context-Aware Diagnosis with Microscopic Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cancer diagnosis has greatly benefited from the integration of whole-slide images (WSIs) with multiple instance learning (MIL), enabling high-resolution analysis of tissue morphology. Graph-based MIL (GNN-MIL) approaches have emerged as powerful solutions for capturing contextual information in WSIs, thereby improving diagnostic accuracy. However, WSIs require significant computational and infrastructural resources, limiting accessibility in resource-constrained settings. Conventional light microscopes offer a cost-effective alternative, but applying GNN-MIL to such data is challenging due to extensive redundant images and missing spatial coordinates, which hinder contextual learning. To address these issues, we introduce MicroMIL, the first weakly-supervised MIL framework specifically designed for images acquired from conventional light microscopes. MicroMIL leverages a representative image extractor (RIE) that employs deep cluster embedding (DCE) and hard Gumbel-Softmax to dynamically reduce redundancy and select representative images. These images serve as graph nodes, with edges computed via cosine similarity, eliminating the need for spatial coordinates while preserving contextual information. Extensive experiments on a real-world colon cancer dataset and the BreakHis dataset demonstrate that MicroMIL achieves state-of-the-art performance, improving both diagnostic accuracy and robustness to redundancy. The code is available at https://github.com/kimjongwoo-cell/MicroMIL
<div id='section'>Paperid: <span id='pid'>141, <a href='https://arxiv.org/pdf/2407.19192.pdf' target='_blank'>https://arxiv.org/pdf/2407.19192.pdf</a></span>   <span><a href='https://github.com/wangbing1416/HAMI-M3D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bing Wang, Shengsheng Wang, Changchun Li, Renchu Guan, Ximing Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.19192">Harmfully Manipulated Images Matter in Multimodal Misinformation Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nowadays, misinformation is widely spreading over various social media platforms and causes extremely negative impacts on society. To combat this issue, automatically identifying misinformation, especially those containing multimodal content, has attracted growing attention from the academic and industrial communities, and induced an active research topic named Multimodal Misinformation Detection (MMD). Typically, existing MMD methods capture the semantic correlation and inconsistency between multiple modalities, but neglect some potential clues in multimodal content. Recent studies suggest that manipulated traces of the images in articles are non-trivial clues for detecting misinformation. Meanwhile, we find that the underlying intentions behind the manipulation, e.g., harmful and harmless, also matter in MMD. Accordingly, in this work, we propose to detect misinformation by learning manipulation features that indicate whether the image has been manipulated, as well as intention features regarding the harmful and harmless intentions of the manipulation. Unfortunately, the manipulation and intention labels that make these features discriminative are unknown. To overcome the problem, we propose two weakly supervised signals as alternatives by introducing additional datasets on image manipulation detection and formulating two classification tasks as positive and unlabeled learning problems. Based on these ideas, we propose a novel MMD method, namely Harmfully Manipulated Images Matter in MMD (HAMI-M3D). Extensive experiments across three benchmark datasets can demonstrate that HAMI-M3D can consistently improve the performance of any MMD baselines.
<div id='section'>Paperid: <span id='pid'>142, <a href='https://arxiv.org/pdf/2407.17197.pdf' target='_blank'>https://arxiv.org/pdf/2407.17197.pdf</a></span>   <span><a href='https://github.com/CEA-LIST/ALPI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Saad Lahlali, Nicolas Granger, HervÃ© Le Borgne, Quoc-Cuong Pham
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.17197">ALPI: Auto-Labeller with Proxy Injection for 3D Object Detection using 2D Labels Only</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D object detection plays a crucial role in various applications such as autonomous vehicles, robotics and augmented reality. However, training 3D detectors requires a costly precise annotation, which is a hindrance to scaling annotation to large datasets. To address this challenge, we propose a weakly supervised 3D annotator that relies solely on 2D bounding box annotations from images, along with size priors. One major problem is that supervising a 3D detection model using only 2D boxes is not reliable due to ambiguities between different 3D poses and their identical 2D projection. We introduce a simple yet effective and generic solution: we build 3D proxy objects with annotations by construction and add them to the training dataset. Our method requires only size priors to adapt to new classes. To better align 2D supervision with 3D detection, our method ensures depth invariance with a novel expression of the 2D losses. Finally, to detect more challenging instances, our annotator follows an offline pseudo-labelling scheme which gradually improves its 3D pseudo-labels. Extensive experiments on the KITTI dataset demonstrate that our method not only performs on-par or above previous works on the Car category, but also achieves performance close to fully supervised methods on more challenging classes. We further demonstrate the effectiveness and robustness of our method by being the first to experiment on the more challenging nuScenes dataset. We additionally propose a setting where weak labels are obtained from a 2D detector pre-trained on MS-COCO instead of human annotations. The code is available at https://github.com/CEA-LIST/ALPI
<div id='section'>Paperid: <span id='pid'>143, <a href='https://arxiv.org/pdf/2407.15036.pdf' target='_blank'>https://arxiv.org/pdf/2407.15036.pdf</a></span>   <span><a href='https://github.com/libeibeics/AsyCo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Beibei Li, Yiyuan Zheng, Beihong Jin, Tao Xiang, Haobo Wang, Lei Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.15036">AsyCo: An Asymmetric Dual-task Co-training Model for Partial-label Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partial-Label Learning (PLL) is a typical problem of weakly supervised learning, where each training instance is annotated with a set of candidate labels. Self-training PLL models achieve state-of-the-art performance but suffer from error accumulation problem caused by mistakenly disambiguated instances. Although co-training can alleviate this issue by training two networks simultaneously and allowing them to interact with each other, most existing co-training methods train two structurally identical networks with the same task, i.e., are symmetric, rendering it insufficient for them to correct each other due to their similar limitations. Therefore, in this paper, we propose an asymmetric dual-task co-training PLL model called AsyCo, which forces its two networks, i.e., a disambiguation network and an auxiliary network, to learn from different views explicitly by optimizing distinct tasks. Specifically, the disambiguation network is trained with self-training PLL task to learn label confidence, while the auxiliary network is trained in a supervised learning paradigm to learn from the noisy pairwise similarity labels that are constructed according to the learned label confidence. Finally, the error accumulation problem is mitigated via information distillation and confidence refinement. Extensive experiments on both uniform and instance-dependent partially labeled datasets demonstrate the effectiveness of AsyCo. The code is available at https://github.com/libeibeics/AsyCo.
<div id='section'>Paperid: <span id='pid'>144, <a href='https://arxiv.org/pdf/2407.13748.pdf' target='_blank'>https://arxiv.org/pdf/2407.13748.pdf</a></span>   <span><a href='https://github.com/gwenzhang/GGA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guowen Zhang, Junsong Fan, Liyi Chen, Zhaoxiang Zhang, Zhen Lei, Lei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.13748">General Geometry-aware Weakly Supervised 3D Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D object detection is an indispensable component for scene understanding. However, the annotation of large-scale 3D datasets requires significant human effort. To tackle this problem, many methods adopt weakly supervised 3D object detection that estimates 3D boxes by leveraging 2D boxes and scene/class-specific priors. However, these approaches generally depend on sophisticated manual priors, which is hard to generalize to novel categories and scenes. In this paper, we are motivated to propose a general approach, which can be easily adapted to new scenes and/or classes. A unified framework is developed for learning 3D object detectors from RGB images and associated 2D boxes. In specific, we propose three general components: prior injection module to obtain general object geometric priors from LLM model, 2D space projection constraint to minimize the discrepancy between the boundaries of projected 3D boxes and their corresponding 2D boxes on the image plane, and 3D space geometry constraint to build a Point-to-Box alignment loss to further refine the pose of estimated 3D boxes. Experiments on KITTI and SUN-RGBD datasets demonstrate that our method yields surprisingly high-quality 3D bounding boxes with only 2D annotation. The source code is available at https://github.com/gwenzhang/GGA.
<div id='section'>Paperid: <span id='pid'>145, <a href='https://arxiv.org/pdf/2407.11486.pdf' target='_blank'>https://arxiv.org/pdf/2407.11486.pdf</a></span>   <span><a href='https://github.com/CVIU-CSU/TCT-InfoNCE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jialong Huang, Gaojie Li, Shichao Kan, Jianfeng Liu, Yixiong Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.11486">An efficient framework based on large foundation model for cervical cytopathology whole slide image screening</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current cervical cytopathology whole slide image (WSI) screening primarily relies on detection-based approaches, which are limited in performance due to the expense and time-consuming annotation process. Multiple Instance Learning (MIL), a weakly supervised approach that relies solely on bag-level labels, can effectively alleviate these challenges. Nonetheless, MIL commonly employs frozen pretrained models or self-supervised learning for feature extraction, which suffers from low efficacy or inefficiency. In this paper, we propose an efficient framework for cervical cytopathology WSI classification using only WSI-level labels through unsupervised and weakly supervised learning. Given the sparse and dispersed nature of abnormal cells within cytopathological WSIs, we propose a strategy that leverages the pretrained foundation model to filter the top$k$ high-risk patches. Subsequently, we suggest parameter-efficient fine-tuning (PEFT) of a large foundation model using contrastive learning on the filtered patches to enhance its representation ability for task-specific signals. By training only the added linear adapters, we enhance the learning of patch-level features with substantially reduced time and memory consumption. Experiments conducted on the CSD and FNAC 2019 datasets demonstrate that the proposed method enhances the performance of various MIL methods and achieves state-of-the-art (SOTA) performance. The code and trained models are publicly available at https://github.com/CVIU-CSU/TCT-InfoNCE.
<div id='section'>Paperid: <span id='pid'>146, <a href='https://arxiv.org/pdf/2407.11216.pdf' target='_blank'>https://arxiv.org/pdf/2407.11216.pdf</a></span>   <span><a href='https://github.com/Chohoonhee/EV-WSSS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hoonhee Cho, Sung-Hoon Yoon, Hyeokjun Kweon, Kuk-Jin Yoon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.11216">Finding Meaning in Points: Weakly Supervised Semantic Segmentation for Event Cameras</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Event cameras excel in capturing high-contrast scenes and dynamic objects, offering a significant advantage over traditional frame-based cameras. Despite active research into leveraging event cameras for semantic segmentation, generating pixel-wise dense semantic maps for such challenging scenarios remains labor-intensive. As a remedy, we present EV-WSSS: a novel weakly supervised approach for event-based semantic segmentation that utilizes sparse point annotations. To fully leverage the temporal characteristics of event data, the proposed framework performs asymmetric dual-student learning between 1) the original forward event data and 2) the longer reversed event data, which contain complementary information from the past and the future, respectively. Besides, to mitigate the challenges posed by sparse supervision, we propose feature-level contrastive learning based on class-wise prototypes, carefully aggregated at both spatial region and sample levels. Additionally, we further excavate the potential of our dual-student learning model by exchanging prototypes between the two learning paths, thereby harnessing their complementary strengths. With extensive experiments on various datasets, including DSEC Night-Point with sparse point annotations newly provided by this paper, the proposed method achieves substantial segmentation results even without relying on pixel-level dense ground truths. The code and dataset are available at https://github.com/Chohoonhee/EV-WSSS.
<div id='section'>Paperid: <span id='pid'>147, <a href='https://arxiv.org/pdf/2407.09786.pdf' target='_blank'>https://arxiv.org/pdf/2407.09786.pdf</a></span>   <span><a href='https://github.com/ltwu6/malspc' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lintai Wu, Xianjing Cheng, Yong Xu, Huanqiang Zeng, Junhui Hou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.09786">Unsupervised 3D Point Cloud Completion via Multi-view Adversarial Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In real-world scenarios, scanned point clouds are often incomplete due to occlusion issues. The tasks of self-supervised and weakly-supervised point cloud completion involve reconstructing missing regions of these incomplete objects without the supervision of complete ground truth. Current methods either rely on multiple views of partial observations for supervision or overlook the intrinsic geometric similarity that can be identified and utilized from the given partial point clouds. In this paper, we propose MAL-UPC, a framework that effectively leverages both region-level and category-specific geometric similarities to complete missing structures. Our MAL-UPC does not require any 3D complete supervision and only necessitates single-view partial observations in the training set. Specifically, we first introduce a Pattern Retrieval Network to retrieve similar position and curvature patterns between the partial input and the predicted shape, then leverage these similarities to densify and refine the reconstructed results. Additionally, we render the reconstructed complete shape into multi-view depth maps and design an adversarial learning module to learn the geometry of the target shape from category-specific single-view depth images of the partial point clouds in the training set. To achieve anisotropic rendering, we design a density-aware radius estimation algorithm to improve the quality of the rendered images. Our MAL-UPC outperforms current state-of-the-art self-supervised methods and even some unpaired approaches. We will make the source code publicly available at https://github.com/ltwu6/malspc
<div id='section'>Paperid: <span id='pid'>148, <a href='https://arxiv.org/pdf/2407.07406.pdf' target='_blank'>https://arxiv.org/pdf/2407.07406.pdf</a></span>   <span><a href='https://github.com/med-air/GazeMedSeg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Zhong, Chenhui Tang, Yumeng Yang, Ruoxi Qi, Kang Zhou, Yuqi Gong, Pheng Ann Heng, Janet H. Hsiao, Qi Dou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.07406">Weakly-supervised Medical Image Segmentation with Gaze Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Eye gaze that reveals human observational patterns has increasingly been incorporated into solutions for vision tasks. Despite recent explorations on leveraging gaze to aid deep networks, few studies exploit gaze as an efficient annotation approach for medical image segmentation which typically entails heavy annotating costs. In this paper, we propose to collect dense weak supervision for medical image segmentation with a gaze annotation scheme. To train with gaze, we propose a multi-level framework that trains multiple networks from discriminative human attention, simulated with a set of pseudo-masks derived by applying hierarchical thresholds on gaze heatmaps. Furthermore, to mitigate gaze noise, a cross-level consistency is exploited to regularize overfitting noisy labels, steering models toward clean patterns learned by peer networks. The proposed method is validated on two public medical datasets of polyp and prostate segmentation tasks. We contribute a high-quality gaze dataset entitled GazeMedSeg as an extension to the popular medical segmentation datasets. To the best of our knowledge, this is the first gaze dataset for medical image segmentation. Our experiments demonstrate that gaze annotation outperforms previous label-efficient annotation schemes in terms of both performance and annotation time. Our collected gaze data and code are available at: https://github.com/med-air/GazeMedSeg.
<div id='section'>Paperid: <span id='pid'>149, <a href='https://arxiv.org/pdf/2407.05607.pdf' target='_blank'>https://arxiv.org/pdf/2407.05607.pdf</a></span>   <span><a href='https://github.com/dzungdoan6/WSTTA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Anh-Dzung Doan, Bach Long Nguyen, Terry Lim, Madhuka Jayawardhana, Surabhi Gupta, Christophe Guettier, Ian Reid, Markus Wagner, Tat-Jun Chin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05607">Weakly Supervised Test-Time Domain Adaptation for Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prior to deployment, an object detector is trained on a dataset compiled from a previous data collection campaign. However, the environment in which the object detector is deployed will invariably evolve, particularly in outdoor settings where changes in lighting, weather and seasons will significantly affect the appearance of the scene and target objects. It is almost impossible for all potential scenarios that the object detector may come across to be present in a finite training dataset. This necessitates continuous updates to the object detector to maintain satisfactory performance. Test-time domain adaptation techniques enable machine learning models to self-adapt based on the distributions of the testing data. However, existing methods mainly focus on fully automated adaptation, which makes sense for applications such as self-driving cars. Despite the prevalence of fully automated approaches, in some applications such as surveillance, there is usually a human operator overseeing the system's operation. We propose to involve the operator in test-time domain adaptation to raise the performance of object detection beyond what is achievable by fully automated adaptation. To reduce manual effort, the proposed method only requires the operator to provide weak labels, which are then used to guide the adaptation process. Furthermore, the proposed method can be performed in a streaming setting, where each online sample is observed only once. We show that the proposed method outperforms existing works, demonstrating a great benefit of human-in-the-loop test-time domain adaptation. Our code is publicly available at https://github.com/dzungdoan6/WSTTA
<div id='section'>Paperid: <span id='pid'>150, <a href='https://arxiv.org/pdf/2407.03575.pdf' target='_blank'>https://arxiv.org/pdf/2407.03575.pdf</a></span>   <span><a href='https://github.com/ChongQingNoSubway/DGR-MIL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhui Zhu, Xiwen Chen, Peijie Qiu, Aristeidis Sotiras, Abolfazl Razi, Yalin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.03575">DGR-MIL: Exploring Diverse Global Representation in Multiple Instance Learning for Whole Slide Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple instance learning (MIL) stands as a powerful approach in weakly supervised learning, regularly employed in histological whole slide image (WSI) classification for detecting tumorous lesions. However, existing mainstream MIL methods focus on modeling correlation between instances while overlooking the inherent diversity among instances. However, few MIL methods have aimed at diversity modeling, which empirically show inferior performance but with a high computational cost. To bridge this gap, we propose a novel MIL aggregation method based on diverse global representation (DGR-MIL), by modeling diversity among instances through a set of global vectors that serve as a summary of all instances. First, we turn the instance correlation into the similarity between instance embeddings and the predefined global vectors through a cross-attention mechanism. This stems from the fact that similar instance embeddings typically would result in a higher correlation with a certain global vector. Second, we propose two mechanisms to enforce the diversity among the global vectors to be more descriptive of the entire bag: (i) positive instance alignment and (ii) a novel, efficient, and theoretically guaranteed diversification learning paradigm. Specifically, the positive instance alignment module encourages the global vectors to align with the center of positive instances (e.g., instances containing tumors in WSI). To further diversify the global representations, we propose a novel diversification learning paradigm leveraging the determinantal point process. The proposed model outperforms the state-of-the-art MIL aggregation models by a substantial margin on the CAMELYON-16 and the TCGA-lung cancer datasets. The code is available at \url{https://github.com/ChongQingNoSubway/DGR-MIL}.
<div id='section'>Paperid: <span id='pid'>151, <a href='https://arxiv.org/pdf/2407.03009.pdf' target='_blank'>https://arxiv.org/pdf/2407.03009.pdf</a></span>   <span><a href='https://github.com/Kainmueller-Lab/TW-autoencoder' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyan Yu, Jannik Franzen, Wojciech Samek, Marina M. -C. HÃ¶hne, Dagmar Kainmueller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.03009">Model Guidance via Explanations Turns Image Classifiers into Segmentation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Heatmaps generated on inputs of image classification networks via explainable AI methods like Grad-CAM and LRP have been observed to resemble segmentations of input images in many cases. Consequently, heatmaps have also been leveraged for achieving weakly supervised segmentation with image-level supervision. On the other hand, losses can be imposed on differentiable heatmaps, which has been shown to serve for (1)~improving heatmaps to be more human-interpretable, (2)~regularization of networks towards better generalization, (3)~training diverse ensembles of networks, and (4)~for explicitly ignoring confounding input features. Due to the latter use case, the paradigm of imposing losses on heatmaps is often referred to as "Right for the right reasons". We unify these two lines of research by investigating semi-supervised segmentation as a novel use case for the Right for the Right Reasons paradigm. First, we show formal parallels between differentiable heatmap architectures and standard encoder-decoder architectures for image segmentation. Second, we show that such differentiable heatmap architectures yield competitive results when trained with standard segmentation losses. Third, we show that such architectures allow for training with weak supervision in the form of image-level labels and small numbers of pixel-level labels, outperforming comparable encoder-decoder models. Code is available at \url{https://github.com/Kainmueller-Lab/TW-autoencoder}.
<div id='section'>Paperid: <span id='pid'>152, <a href='https://arxiv.org/pdf/2407.02768.pdf' target='_blank'>https://arxiv.org/pdf/2407.02768.pdf</a></span>   <span><a href='https://github.com/NUST-Machine-Intelligence-Laboratory/KTSE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Chen, XiRuo Jiang, Gensheng Pei, Zeren Sun, Yucheng Wang, Yazhou Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.02768">Knowledge Transfer with Simulated Inter-Image Erasing for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Though adversarial erasing has prevailed in weakly supervised semantic segmentation to help activate integral object regions, existing approaches still suffer from the dilemma of under-activation and over-expansion due to the difficulty in determining when to stop erasing. In this paper, we propose a \textbf{K}nowledge \textbf{T}ransfer with \textbf{S}imulated Inter-Image \textbf{E}rasing (KTSE) approach for weakly supervised semantic segmentation to alleviate the above problem. In contrast to existing erasing-based methods that remove the discriminative part for more object discovery, we propose a simulated inter-image erasing scenario to weaken the original activation by introducing extra object information. Then, object knowledge is transferred from the anchor image to the consequent less activated localization map to strengthen network localization ability. Considering the adopted bidirectional alignment will also weaken the anchor image activation if appropriate constraints are missing, we propose a self-supervised regularization module to maintain the reliable activation in discriminative regions and improve the inter-class object boundary recognition for complex images with multiple categories of objects. In addition, we resort to intra-image erasing and propose a multi-granularity alignment module to gently enlarge the object activation to boost the object knowledge transfer. Extensive experiments and ablation studies on PASCAL VOC 2012 and COCO datasets demonstrate the superiority of our proposed approach. Source codes and models are available at https://github.com/NUST-Machine-Intelligence-Laboratory/KTSE.
<div id='section'>Paperid: <span id='pid'>153, <a href='https://arxiv.org/pdf/2407.00614.pdf' target='_blank'>https://arxiv.org/pdf/2407.00614.pdf</a></span>   <span><a href='https://github.com/yangfan293/GAAF-DEX' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/yangfan293/GAAF-DEX' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Yang, Wenrui Chen, Kailun Yang, Haoran Lin, Dongsheng Luo, Conghui Tang, Zhiyong Li, Yaonan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.00614">Learning Granularity-Aware Affordances from Human-Object Interaction for Tool-Based Functional Dexterous Grasping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To enable robots to use tools, the initial step is teaching robots to employ dexterous gestures for touching specific areas precisely where tasks are performed. Affordance features of objects serve as a bridge in the functional interaction between agents and objects. However, leveraging these affordance cues to help robots achieve functional tool grasping remains unresolved. To address this, we propose a granularity-aware affordance feature extraction method for locating functional affordance areas and predicting dexterous coarse gestures. We study the intrinsic mechanisms of human tool use. On one hand, we use fine-grained affordance features of object-functional finger contact areas to locate functional affordance regions. On the other hand, we use highly activated coarse-grained affordance features in hand-object interaction regions to predict grasp gestures. Additionally, we introduce a model-based post-processing module that transforms affordance localization and gesture prediction into executable robotic actions. This forms GAAF-Dex, a complete framework that learns Granularity-Aware Affordances from human-object interaction to enable tool-based functional grasping with dexterous hands. Unlike fully-supervised methods that require extensive data annotation, we employ a weakly supervised approach to extract relevant cues from exocentric (Exo) images of hand-object interactions to supervise feature extraction in egocentric (Ego) images. To support this approach, we have constructed a small-scale dataset, Functional Affordance Hand-object Interaction Dataset (FAH), which includes nearly 6K images of functional hand-object interaction Exo images and Ego images. Extensive experiments on the dataset demonstrate that our method outperforms state-of-the-art methods. The source code and the established dataset are available at https://github.com/yangfan293/GAAF-DEX.
<div id='section'>Paperid: <span id='pid'>154, <a href='https://arxiv.org/pdf/2406.19364.pdf' target='_blank'>https://arxiv.org/pdf/2406.19364.pdf</a></span>   <span><a href='https://github.com/xyx1024/SimTxtSeg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxin Xie, Tao Zhou, Yi Zhou, Geng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.19364">SimTxtSeg: Weakly-Supervised Medical Image Segmentation with Simple Text Cues</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised medical image segmentation is a challenging task that aims to reduce the annotation cost while keep the segmentation performance. In this paper, we present a novel framework, SimTxtSeg, that leverages simple text cues to generate high-quality pseudo-labels and study the cross-modal fusion in training segmentation models, simultaneously. Our contribution consists of two key components: an effective Textual-to-Visual Cue Converter that produces visual prompts from text prompts on medical images, and a text-guided segmentation model with Text-Vision Hybrid Attention that fuses text and image features. We evaluate our framework on two medical image segmentation tasks: colonic polyp segmentation and MRI brain tumor segmentation, and achieve consistent state-of-the-art performance. Source code is available at: https://github.com/xyx1024/SimTxtSeg.
<div id='section'>Paperid: <span id='pid'>155, <a href='https://arxiv.org/pdf/2406.18815.pdf' target='_blank'>https://arxiv.org/pdf/2406.18815.pdf</a></span>   <span><a href='https://github.com/c0510gy/MissionGNN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sanggeon Yun, Ryozo Masukawa, Minhyoung Na, Mohsen Imani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.18815">MissionGNN: Hierarchical Multimodal GNN-based Weakly Supervised Video Anomaly Recognition with Mission-Specific Knowledge Graph Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the context of escalating safety concerns across various domains, the tasks of Video Anomaly Detection (VAD) and Video Anomaly Recognition (VAR) have emerged as critically important for applications in intelligent surveillance, evidence investigation, violence alerting, etc. These tasks, aimed at identifying and classifying deviations from normal behavior in video data, face significant challenges due to the rarity of anomalies which leads to extremely imbalanced data and the impracticality of extensive frame-level data annotation for supervised learning. This paper introduces a novel hierarchical graph neural network (GNN) based model MissionGNN that addresses these challenges by leveraging a state-of-the-art large language model and a comprehensive knowledge graph for efficient weakly supervised learning in VAR. Our approach circumvents the limitations of previous methods by avoiding heavy gradient computations on large multimodal models and enabling fully frame-level training without fixed video segmentation. Utilizing automated, mission-specific knowledge graph generation, our model provides a practical and efficient solution for real-time video analysis without the constraints of previous segmentation-based or multimodal approaches. Experimental validation on benchmark datasets demonstrates our model's performance in VAD and VAR, highlighting its potential to redefine the landscape of anomaly detection and recognition in video surveillance systems. The code is available here: https://github.com/c0510gy/MissionGNN.
<div id='section'>Paperid: <span id='pid'>156, <a href='https://arxiv.org/pdf/2406.18558.pdf' target='_blank'>https://arxiv.org/pdf/2406.18558.pdf</a></span>   <span><a href='https://github.com/wsis-seg/BAISeg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tengbo Wang, Yu Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.18558">BAISeg: Boundary Assisted Weakly Supervised Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>How to extract instance-level masks without instance-level supervision is the main challenge of weakly supervised instance segmentation (WSIS). Popular WSIS methods estimate a displacement field (DF) via learning inter-pixel relations and perform clustering to identify instances. However, the resulting instance centroids are inherently unstable and vary significantly across different clustering algorithms. In this paper, we propose Boundary-Assisted Instance Segmentation (BAISeg), which is a novel paradigm for WSIS that realizes instance segmentation with pixel-level annotations. BAISeg comprises an instance-aware boundary detection (IABD) branch and a semantic segmentation branch. The IABD branch identifies instances by predicting class-agnostic instance boundaries rather than instance centroids, therefore, it is different from previous DF-based approaches. In particular, we proposed the Cascade Fusion Module (CFM) and the Deep Mutual Attention (DMA) in the IABD branch to obtain rich contextual information and capture instance boundaries with weak responses. During the training phase, we employed Pixel-to-Pixel Contrast to enhance the discriminative capacity of the IABD branch. This further strengthens the continuity and closedness of the instance boundaries. Extensive experiments on PASCAL VOC 2012 and MS COCO demonstrate the effectiveness of our approach, and we achieve considerable performance with only pixel-level annotations. The code will be available at https://github.com/wsis-seg/BAISeg.
<div id='section'>Paperid: <span id='pid'>157, <a href='https://arxiv.org/pdf/2406.15805.pdf' target='_blank'>https://arxiv.org/pdf/2406.15805.pdf</a></span>   <span><a href='https://github.com/hzx-9894/MMA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoxin Hu, Keyan Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.15805">Smart Feature is What You Need</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Lack of shape guidance and label jitter caused by information deficiency of weak label are the main problems in 3D weakly-supervised object detection. Current weakly-supervised models often use heuristics or assumptions methods to infer information from weak labels without taking advantage of the inherent clues of weakly-supervised and fully-supervised methods, thus it is difficult to explore a method that combines data utilization efficiency and model accuracy. In an attempt to address these issues, we propose a novel plug-and-in point cloud feature representation network called Multi-scale Mixed Attention (MMA). MMA utilizes adjacency attention within neighborhoods and disparity attention at different density scales to build a feature representation network. The smart feature representation obtained from MMA has shape tendency and object existence area inference, which can constrain the region of the detection boxes, thereby alleviating the problems caused by the information default of weak labels. Extensive experiments show that in indoor weak label scenarios, the fully-supervised network can perform close to that of the weakly-supervised network merely through the improvement of point feature by MMA. At the same time, MMA can turn waste into treasure, reversing the label jitter problem that originally interfered with weakly-supervised detection into the source of data enhancement, strengthening the performance of existing weak supervision detection methods. Our code is available at https://github.com/hzx-9894/MMA.
<div id='section'>Paperid: <span id='pid'>158, <a href='https://arxiv.org/pdf/2406.14274.pdf' target='_blank'>https://arxiv.org/pdf/2406.14274.pdf</a></span>   <span><a href='https://github.com/mc-lan/SP-TCL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengcheng Lan, Min Meng, Jun Yu, Jigang Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.14274">Learning to Discover Knowledge: A Weakly-Supervised Partial Domain Adaptation Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain adaptation has shown appealing performance by leveraging knowledge from a source domain with rich annotations. However, for a specific target task, it is cumbersome to collect related and high-quality source domains. In real-world scenarios, large-scale datasets corrupted with noisy labels are easy to collect, stimulating a great demand for automatic recognition in a generalized setting, i.e., weakly-supervised partial domain adaptation (WS-PDA), which transfers a classifier from a large source domain with noises in labels to a small unlabeled target domain. As such, the key issues of WS-PDA are: 1) how to sufficiently discover the knowledge from the noisy labeled source domain and the unlabeled target domain, and 2) how to successfully adapt the knowledge across domains. In this paper, we propose a simple yet effective domain adaptation approach, termed as self-paced transfer classifier learning (SP-TCL), to address the above issues, which could be regarded as a well-performing baseline for several generalized domain adaptation tasks. The proposed model is established upon the self-paced learning scheme, seeking a preferable classifier for the target domain. Specifically, SP-TCL learns to discover faithful knowledge via a carefully designed prudent loss function and simultaneously adapts the learned knowledge to the target domain by iteratively excluding source examples from training under the self-paced fashion. Extensive evaluations on several benchmark datasets demonstrate that SP-TCL significantly outperforms state-of-the-art approaches on several generalized domain adaptation tasks.
<div id='section'>Paperid: <span id='pid'>159, <a href='https://arxiv.org/pdf/2406.13124.pdf' target='_blank'>https://arxiv.org/pdf/2406.13124.pdf</a></span>   <span><a href='https://github.com/amazon-science/learning-to-generate-answers-with-citations' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rami Aly, Zhiqiang Tang, Samson Tan, George Karypis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.13124">Learning to Generate Answers with Citations via Factual Consistency Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) frequently hallucinate, impeding their reliability in mission-critical situations. One approach to address this issue is to provide citations to relevant sources alongside generated content, enhancing the verifiability of generations. However, citing passages accurately in answers remains a substantial challenge. This paper proposes a weakly-supervised fine-tuning method leveraging factual consistency models (FCMs). Our approach alternates between generating texts with citations and supervised fine-tuning with FCM-filtered citation data. Focused learning is integrated into the objective, directing the fine-tuning process to emphasise the factual unit tokens, as measured by an FCM. Results on the ALCE few-shot citation benchmark with various instruction-tuned LLMs demonstrate superior performance compared to in-context learning, vanilla supervised fine-tuning, and state-of-the-art methods, with an average improvement of $34.1$, $15.5$, and $10.5$ citation F$_1$ points, respectively. Moreover, in a domain transfer setting we show that the obtained citation generation ability robustly transfers to unseen datasets. Notably, our citation improvements contribute to the lowest factual error rate across baselines.
<div id='section'>Paperid: <span id='pid'>160, <a href='https://arxiv.org/pdf/2406.11189.pdf' target='_blank'>https://arxiv.org/pdf/2406.11189.pdf</a></span>   <span><a href='https://github.com/zbf1991/WeCLIP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingfeng Zhang, Siyue Yu, Yunchao Wei, Yao Zhao, Jimin Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.11189">Frozen CLIP: A Strong Backbone for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation has witnessed great achievements with image-level labels. Several recent approaches use the CLIP model to generate pseudo labels for training an individual segmentation model, while there is no attempt to apply the CLIP model as the backbone to directly segment objects with image-level labels. In this paper, we propose WeCLIP, a CLIP-based single-stage pipeline, for weakly supervised semantic segmentation. Specifically, the frozen CLIP model is applied as the backbone for semantic feature extraction, and a new decoder is designed to interpret extracted semantic features for final prediction. Meanwhile, we utilize the above frozen backbone to generate pseudo labels for training the decoder. Such labels cannot be optimized during training. We then propose a refinement module (RFM) to rectify them dynamically. Our architecture enforces the proposed decoder and RFM to benefit from each other to boost the final performance. Extensive experiments show that our approach significantly outperforms other approaches with less training cost. Additionally, our WeCLIP also obtains promising results for fully supervised settings. The code is available at https://github.com/zbf1991/WeCLIP.
<div id='section'>Paperid: <span id='pid'>161, <a href='https://arxiv.org/pdf/2406.04280.pdf' target='_blank'>https://arxiv.org/pdf/2406.04280.pdf</a></span>   <span><a href='https://github.com/bifold-pathomics/xMIL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Julius Hense, Mina Jamshidi Idaji, Oliver Eberle, Thomas Schnake, Jonas Dippel, Laure Ciernik, Oliver Buchstab, Andreas Mock, Frederick Klauschen, Klaus-Robert MÃ¼ller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04280">xMIL: Insightful Explanations for Multiple Instance Learning in Histopathology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple instance learning (MIL) is an effective and widely used approach for weakly supervised machine learning. In histopathology, MIL models have achieved remarkable success in tasks like tumor detection, biomarker prediction, and outcome prognostication. However, MIL explanation methods are still lagging behind, as they are limited to small bag sizes or disregard instance interactions. We revisit MIL through the lens of explainable AI (XAI) and introduce xMIL, a refined framework with more general assumptions. We demonstrate how to obtain improved MIL explanations using layer-wise relevance propagation (LRP) and conduct extensive evaluation experiments on three toy settings and four real-world histopathology datasets. Our approach consistently outperforms previous explanation attempts with particularly improved faithfulness scores on challenging biomarker prediction tasks. Finally, we showcase how xMIL explanations enable pathologists to extract insights from MIL models, representing a significant advance for knowledge discovery and model debugging in digital histopathology. Codes are available at: https://github.com/bifold-pathomics/xMIL.
<div id='section'>Paperid: <span id='pid'>162, <a href='https://arxiv.org/pdf/2406.02822.pdf' target='_blank'>https://arxiv.org/pdf/2406.02822.pdf</a></span>   <span><a href='https://github.com/andreschreiber/W-RIZZ' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Andre Schreiber, Arun N. Sivakumar, Peter Du, Mateus V. Gasparino, Girish Chowdhary, Katherine Driggs-Campbell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.02822">W-RIZZ: A Weakly-Supervised Framework for Relative Traversability Estimation in Mobile Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Successful deployment of mobile robots in unstructured domains requires an understanding of the environment and terrain to avoid hazardous areas, getting stuck, and colliding with obstacles. Traversability estimation--which predicts where in the environment a robot can travel--is one prominent approach that tackles this problem. Existing geometric methods may ignore important semantic considerations, while semantic segmentation approaches involve a tedious labeling process. Recent self-supervised methods reduce labeling tedium, but require additional data or models and tend to struggle to explicitly label untraversable areas. To address these limitations, we introduce a weakly-supervised method for relative traversability estimation. Our method involves manually annotating the relative traversability of a small number of point pairs, which significantly reduces labeling effort compared to traditional segmentation-based methods and avoids the limitations of self-supervised methods. We further improve the performance of our method through a novel cross-image labeling strategy and loss function. We demonstrate the viability and performance of our method through deployment on a mobile robot in outdoor environments.
<div id='section'>Paperid: <span id='pid'>163, <a href='https://arxiv.org/pdf/2405.16628.pdf' target='_blank'>https://arxiv.org/pdf/2405.16628.pdf</a></span>   <span><a href='https://github.com/s-sd/spurl/tree/main/wss' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaheer U. Saeed, Shiqi Huang, JoÃ£o Ramalhinho, Iani J. M. B. Gayo, Nina MontaÃ±a-Brown, Ester Bonmati, Stephen P. Pereira, Brian Davidson, Dean C. Barratt, Matthew J. Clarkson, Yipeng Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.16628">Competing for pixels: a self-play algorithm for weakly-supervised segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised segmentation (WSS) methods, reliant on image-level labels indicating object presence, lack explicit correspondence between labels and regions of interest (ROIs), posing a significant challenge. Despite this, WSS methods have attracted attention due to their much lower annotation costs compared to fully-supervised segmentation. Leveraging reinforcement learning (RL) self-play, we propose a novel WSS method that gamifies image segmentation of a ROI. We formulate segmentation as a competition between two agents that compete to select ROI-containing patches until exhaustion of all such patches. The score at each time-step, used to compute the reward for agent training, represents likelihood of object presence within the selection, determined by an object presence detector pre-trained using only image-level binary classification labels of object presence. Additionally, we propose a game termination condition that can be called by either side upon exhaustion of all ROI-containing patches, followed by the selection of a final patch from each. Upon termination, the agent is incentivised if ROI-containing patches are exhausted or disincentivised if an ROI-containing patch is found by the competitor. This competitive setup ensures minimisation of over- or under-segmentation, a common problem with WSS methods. Extensive experimentation across four datasets demonstrates significant performance improvements over recent state-of-the-art methods. Code: https://github.com/s-sd/spurl/tree/main/wss
<div id='section'>Paperid: <span id='pid'>164, <a href='https://arxiv.org/pdf/2405.16038.pdf' target='_blank'>https://arxiv.org/pdf/2405.16038.pdf</a></span>   <span><a href='https://github.com/XueZ-phd/Efficient-RGB-T-Early-Fusion-Detection' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xue Zhang, Si-Yuan Cao, Fang Wang, Runmin Zhang, Zhe Wu, Xiaohan Zhang, Xiaokai Bai, Hui-Liang Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.16038">Rethinking Early-Fusion Strategies for Improved Multispectral Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most recent multispectral object detectors employ a two-branch structure to extract features from RGB and thermal images. While the two-branch structure achieves better performance than a single-branch structure, it overlooks inference efficiency. This conflict is increasingly aggressive, as recent works solely pursue higher performance rather than both performance and efficiency. In this paper, we address this issue by improving the performance of efficient single-branch structures. We revisit the reasons causing the performance gap between these structures. For the first time, we reveal the information interference problem in the naive early-fusion strategy adopted by previous single-branch structures. Besides, we find that the domain gap between multispectral images, and weak feature representation of the single-branch structure are also key obstacles for performance. Focusing on these three problems, we propose corresponding solutions, including a novel shape-priority early-fusion strategy, a weakly supervised learning method, and a core knowledge distillation technique. Experiments demonstrate that single-branch networks equipped with these three contributions achieve significant performance enhancements while retaining high efficiency. Our code will be available at \url{https://github.com/XueZ-phd/Efficient-RGB-T-Early-Fusion-Detection}.
<div id='section'>Paperid: <span id='pid'>165, <a href='https://arxiv.org/pdf/2405.15961.pdf' target='_blank'>https://arxiv.org/pdf/2405.15961.pdf</a></span>   <span><a href='https://github.com/fpsluozi/SMD-SMOS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiran Luo, Joshua Feinglass, Tejas Gokhale, Kuan-Cheng Lee, Chitta Baral, Yezhou Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15961">Grounding Stylistic Domain Generalization with Quantitative Domain Shift Measures and Synthetic Scene Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG) is a challenging task in machine learning that requires a coherent ability to comprehend shifts across various domains through extraction of domain-invariant features. DG performance is typically evaluated by performing image classification in domains of various image styles. However, current methodology lacks quantitative understanding about shifts in stylistic domain, and relies on a vast amount of pre-training data, such as ImageNet1K, which are predominantly in photo-realistic style with weakly supervised class labels. Such a data-driven practice could potentially result in spurious correlation and inflated performance on DG benchmarks. In this paper, we introduce a new DG paradigm to address these risks. We first introduce two new quantitative measures ICV and IDD to describe domain shifts in terms of consistency of classes within one domain and similarity between two stylistic domains. We then present SuperMarioDomains (SMD), a novel synthetic multi-domain dataset sampled from video game scenes with more consistent classes and sufficient dissimilarity compared to ImageNet1K. We demonstrate our DG method SMOS. SMOS first uses SMD to train a precursor model, which is then used to ground the training on a DG benchmark. We observe that SMOS contributes to state-of-the-art performance across five DG benchmarks, gaining large improvements to performances on abstract domains along with on-par or slight improvements to those on photo-realistic domains. Our qualitative analysis suggests that these improvements can be attributed to reduced distributional divergence between originally distant domains. Our data are available at https://github.com/fpsluozi/SMD-SMOS .
<div id='section'>Paperid: <span id='pid'>166, <a href='https://arxiv.org/pdf/2405.15275.pdf' target='_blank'>https://arxiv.org/pdf/2405.15275.pdf</a></span>   <span><a href='https://github.com/Biomedical-Data-Analysis-Laboratory/GradeMIL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Saul Fuster, Umay Kiraz, Trygve EftestÃ¸l, Emiel A. M. Janssen, Kjersti Engan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15275">NMGrad: Advancing Histopathological Bladder Cancer Grading with Weakly Supervised Deep Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The most prevalent form of bladder cancer is urothelial carcinoma, characterized by a high recurrence rate and substantial lifetime treatment costs for patients. Grading is a prime factor for patient risk stratification, although it suffers from inconsistencies and variations among pathologists. Moreover, absence of annotations in medical imaging difficults training deep learning models. To address these challenges, we introduce a pipeline designed for bladder cancer grading using histological slides. First, it extracts urothelium tissue tiles at different magnification levels, employing a convolutional neural network for processing for feature extraction. Then, it engages in the slide-level prediction process. It employs a nested multiple instance learning approach with attention to predict the grade. To distinguish different levels of malignancy within specific regions of the slide, we include the origins of the tiles in our analysis. The attention scores at region level is shown to correlate with verified high-grade regions, giving some explainability to the model. Clinical evaluations demonstrate that our model consistently outperforms previous state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>167, <a href='https://arxiv.org/pdf/2405.15264.pdf' target='_blank'>https://arxiv.org/pdf/2405.15264.pdf</a></span>   <span><a href='https://github.com/Biomedical-Data-Analysis-Laboratory/HistoPrognostics' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Saul Fuster, Farbod Khoraminia, Julio Silva-RodrÃ­guez, Umay Kiraz, Geert J. L. H. van Leenders, Trygve EftestÃ¸l, Valery Naranjo, Emiel A. M. Janssen, Tahlita C. M. Zuiverloon, Kjersti Engan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15264">Self-Contrastive Weakly Supervised Learning Framework for Prognostic Prediction Using Whole Slide Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a pioneering investigation into the application of deep learning techniques to analyze histopathological images for addressing the substantial challenge of automated prognostic prediction. Prognostic prediction poses a unique challenge as the ground truth labels are inherently weak, and the model must anticipate future events that are not directly observable in the image. To address this challenge, we propose a novel three-part framework comprising of a convolutional network based tissue segmentation algorithm for region of interest delineation, a contrastive learning module for feature extraction, and a nested multiple instance learning classification module. Our study explores the significance of various regions of interest within the histopathological slides and exploits diverse learning scenarios. The pipeline is initially validated on artificially generated data and a simpler diagnostic task. Transitioning to prognostic prediction, tasks become more challenging. Employing bladder cancer as use case, our best models yield an AUC of 0.721 and 0.678 for recurrence and treatment outcome prediction respectively.
<div id='section'>Paperid: <span id='pid'>168, <a href='https://arxiv.org/pdf/2405.15228.pdf' target='_blank'>https://arxiv.org/pdf/2405.15228.pdf</a></span>   <span><a href='https://github.com/Tranquilxu/TMP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongnian Li, Jinghao Xu, Peng Ying, Meng Wei, Xinzheng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15228">Learning from True-False Labels via Multi-modal Prompt Retrieving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pre-trained Vision-Language Models (VLMs) exhibit strong zero-shot classification abilities, demonstrating great potential for generating weakly supervised labels. Unfortunately, existing weakly supervised learning methods are short of ability in generating accurate labels via VLMs. In this paper, we propose a novel weakly supervised labeling setting, namely True-False Labels (TFLs) which can achieve high accuracy when generated by VLMs. The TFL indicates whether an instance belongs to the label, which is randomly and uniformly sampled from the candidate label set. Specifically, we theoretically derive a risk-consistent estimator to explore and utilize the conditional probability distribution information of TFLs. Besides, we propose a convolutional-based Multi-modal Prompt Retrieving (MRP) method to bridge the gap between the knowledge of VLMs and target learning tasks. Experimental results demonstrate the effectiveness of the proposed TFL setting and MRP learning method. The code to reproduce the experiments is at https://github.com/Tranquilxu/TMP.
<div id='section'>Paperid: <span id='pid'>169, <a href='https://arxiv.org/pdf/2405.14271.pdf' target='_blank'>https://arxiv.org/pdf/2405.14271.pdf</a></span>   <span><a href='https://github.com/Eaphan/OLIVINE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Zhang, Junhui Hou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14271">Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contrastive image-to-LiDAR knowledge transfer, commonly used for learning 3D representations with synchronized images and point clouds, often faces a self-conflict dilemma. This issue arises as contrastive losses unintentionally dissociate features of unmatched points and pixels that share semantic labels, compromising the integrity of learned representations. To overcome this, we harness Visual Foundation Models (VFMs), which have revolutionized the acquisition of pixel-level semantics, to enhance 3D representation learning. Specifically, we utilize off-the-shelf VFMs to generate semantic labels for weakly-supervised pixel-to-point contrastive distillation. Additionally, we employ von Mises-Fisher distributions to structure the feature space, ensuring semantic embeddings within the same class remain consistent across varying inputs. Furthermore, we adapt sampling probabilities of points to address imbalances in spatial distribution and category frequency, promoting comprehensive and balanced learning. Extensive experiments demonstrate that our approach mitigates the challenges posed by traditional methods and consistently surpasses existing image-to-LiDAR contrastive distillation methods in downstream tasks. The source code is available at https://github.com/Eaphan/OLIVINE.
<div id='section'>Paperid: <span id='pid'>170, <a href='https://arxiv.org/pdf/2405.06288.pdf' target='_blank'>https://arxiv.org/pdf/2405.06288.pdf</a></span>   <span><a href='https://github.com/Torpedo2648/PCLMix' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Lei, Haolun Luo, Lituan Wang, Zhenwei Zhang, Lei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06288">PCLMix: Weakly Supervised Medical Image Segmentation via Pixel-Level Contrastive Learning and Dynamic Mix Augmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In weakly supervised medical image segmentation, the absence of structural priors and the discreteness of class feature distribution present a challenge, i.e., how to accurately propagate supervision signals from local to global regions without excessively spreading them to other irrelevant regions? To address this, we propose a novel weakly supervised medical image segmentation framework named PCLMix, comprising dynamic mix augmentation, pixel-level contrastive learning, and consistency regularization strategies. Specifically, PCLMix is built upon a heterogeneous dual-decoder backbone, addressing the absence of structural priors through a strategy of dynamic mix augmentation during training. To handle the discrete distribution of class features, PCLMix incorporates pixel-level contrastive learning based on prediction uncertainty, effectively enhancing the model's ability to differentiate inter-class pixel differences and intra-class consistency. Furthermore, to reinforce segmentation consistency and robustness, PCLMix employs an auxiliary decoder for dual consistency regularization. In the inference phase, the auxiliary decoder will be dropped and no computation complexity is increased. Extensive experiments on the ACDC dataset demonstrate that PCLMix appropriately propagates local supervision signals to the global scale, further narrowing the gap between weakly supervised and fully supervised segmentation methods. Our code is available at https://github.com/Torpedo2648/PCLMix.
<div id='section'>Paperid: <span id='pid'>171, <a href='https://arxiv.org/pdf/2405.05130.pdf' target='_blank'>https://arxiv.org/pdf/2405.05130.pdf</a></span>   <span><a href='https://github.com/shengyangsun/MSBT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengyang Sun, Xiaojin Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.05130">Multi-scale Bottleneck Transformer for Weakly Supervised Multimodal Violence Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised multimodal violence detection aims to learn a violence detection model by leveraging multiple modalities such as RGB, optical flow, and audio, while only video-level annotations are available. In the pursuit of effective multimodal violence detection (MVD), information redundancy, modality imbalance, and modality asynchrony are identified as three key challenges. In this work, we propose a new weakly supervised MVD method that explicitly addresses these challenges. Specifically, we introduce a multi-scale bottleneck transformer (MSBT) based fusion module that employs a reduced number of bottleneck tokens to gradually condense information and fuse each pair of modalities and utilizes a bottleneck token-based weighting scheme to highlight more important fused features. Furthermore, we propose a temporal consistency contrast loss to semantically align pairwise fused features. Experiments on the largest-scale XD-Violence dataset demonstrate that the proposed method achieves state-of-the-art performance. Code is available at https://github.com/shengyangsun/MSBT.
<div id='section'>Paperid: <span id='pid'>172, <a href='https://arxiv.org/pdf/2405.04405.pdf' target='_blank'>https://arxiv.org/pdf/2405.04405.pdf</a></span>   <span><a href='https://github.com/liupei101/MIREL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pei Liu, Luping Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04405">Weakly-Supervised Residual Evidential Learning for Multi-Instance Uncertainty Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Uncertainty estimation (UE), as an effective means of quantifying predictive uncertainty, is crucial for safe and reliable decision-making, especially in high-risk scenarios. Existing UE schemes usually assume that there are completely-labeled samples to support fully-supervised learning. In practice, however, many UE tasks often have no sufficiently-labeled data to use, such as the Multiple Instance Learning (MIL) with only weak instance annotations. To bridge this gap, this paper, for the first time, addresses the weakly-supervised issue of Multi-Instance UE (MIUE) and proposes a new baseline scheme, Multi-Instance Residual Evidential Learning (MIREL). Particularly, at the fine-grained instance UE with only weak supervision, we derive a multi-instance residual operator through the Fundamental Theorem of Symmetric Functions. On this operator derivation, we further propose MIREL to jointly model the high-order predictive distribution at bag and instance levels for MIUE. Extensive experiments empirically demonstrate that our MIREL not only could often make existing MIL networks perform better in MIUE, but also could surpass representative UE methods by large margins, especially in instance-level UE tasks. Our source code is available at https://github.com/liupei101/MIREL.
<div id='section'>Paperid: <span id='pid'>173, <a href='https://arxiv.org/pdf/2405.03140.pdf' target='_blank'>https://arxiv.org/pdf/2405.03140.pdf</a></span>   <span><a href='https://github.com/xiwenc1/TimeMIL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiwen Chen, Peijie Qiu, Wenhui Zhu, Huayu Li, Hao Wang, Aristeidis Sotiras, Yalin Wang, Abolfazl Razi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.03140">TimeMIL: Advancing Multivariate Time Series Classification via a Time-aware Multiple Instance Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks, including transformers and convolutional neural networks, have significantly improved multivariate time series classification (MTSC). However, these methods often rely on supervised learning, which does not fully account for the sparsity and locality of patterns in time series data (e.g., diseases-related anomalous points in ECG). To address this challenge, we formally reformulate MTSC as a weakly supervised problem, introducing a novel multiple-instance learning (MIL) framework for better localization of patterns of interest and modeling time dependencies within time series. Our novel approach, TimeMIL, formulates the temporal correlation and ordering within a time-aware MIL pooling, leveraging a tokenized transformer with a specialized learnable wavelet positional token. The proposed method surpassed 26 recent state-of-the-art methods, underscoring the effectiveness of the weakly supervised TimeMIL in MTSC. The code will be available at https://github.com/xiwenc1/TimeMIL.
<div id='section'>Paperid: <span id='pid'>174, <a href='https://arxiv.org/pdf/2405.02114.pdf' target='_blank'>https://arxiv.org/pdf/2405.02114.pdf</a></span>   <span><a href='https://github.com/xzhouzeng/PRPose' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xianzhou Zeng, Hao Qin, Ming Kong, Luyuan Chen, Qiang Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.02114">Probablistic Restoration with Adaptive Noise Sampling for 3D Human Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The accuracy and robustness of 3D human pose estimation (HPE) are limited by 2D pose detection errors and 2D to 3D ill-posed challenges, which have drawn great attention to Multi-Hypothesis HPE research. Most existing MH-HPE methods are based on generative models, which are computationally expensive and difficult to train. In this study, we propose a Probabilistic Restoration 3D Human Pose Estimation framework (PRPose) that can be integrated with any lightweight single-hypothesis model. Specifically, PRPose employs a weakly supervised approach to fit the hidden probability distribution of the 2D-to-3D lifting process in the Single-Hypothesis HPE model and then reverse-map the distribution to the 2D pose input through an adaptive noise sampling strategy to generate reasonable multi-hypothesis samples effectively. Extensive experiments on 3D HPE benchmarks (Human3.6M and MPI-INF-3DHP) highlight the effectiveness and efficiency of PRPose. Code is available at: https://github.com/xzhouzeng/PRPose.
<div id='section'>Paperid: <span id='pid'>175, <a href='https://arxiv.org/pdf/2405.00239.pdf' target='_blank'>https://arxiv.org/pdf/2405.00239.pdf</a></span>   <span><a href='https://github.com/ahxmeds/IgCONDA-PET.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shadab Ahamed, Arman Rahmim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.00239">IgCONDA-PET: Weakly-Supervised PET Anomaly Detection using Implicitly-Guided Attention-Conditional Counterfactual Diffusion Modeling -- a Multi-Center, Multi-Cancer, and Multi-Tracer Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Minimizing the need for pixel-level annotated data to train PET lesion detection and segmentation networks is highly desired and can be transformative, given time and cost constraints associated with expert annotations. Current unsupervised or weakly-supervised anomaly detection methods rely on autoencoder or generative adversarial networks (GANs) trained only on healthy data. While these approaches reduce annotation dependency, GAN-based methods are notably more challenging to train than non-GAN alternatives (such as autoencoders) due to issues such as the simultaneous optimization of two competing networks, mode collapse, and training instability. In this paper, we present the weakly-supervised $\textbf{I}$mplicitly-$\textbf{g}$uided $\textbf{CO}$u$\textbf{N}$terfactual diffusion model for $\textbf{D}$etecting $\textbf{A}$nomalies in $\textbf{PET}$ images (IgCONDA-PET). The solution is developed and validated using PET scans from six retrospective cohorts consisting of a total of 2652 cases (multi-cancer, multi-tracer) containing both local and public datasets (spanning multiple centers). The training is conditioned on image class labels (healthy vs. unhealthy) via attention modules, and we employ implicit diffusion guidance. We perform counterfactual generation which facilitates "unhealthy-to-healthy" domain translation by generating a synthetic, healthy version of an unhealthy input image, enabling the detection of anomalies through the calculated differences. The performance of our method was compared against several other deep learning based weakly-supervised or unsupervised methods as well as traditional methods like 41% SUV$_\text{max}$ thresholding. We also highlight the importance of incorporating attention modules in our network for the detection of small anomalies. The code is publicly available at: https://github.com/ahxmeds/IgCONDA-PET.git.
<div id='section'>Paperid: <span id='pid'>176, <a href='https://arxiv.org/pdf/2404.17253.pdf' target='_blank'>https://arxiv.org/pdf/2404.17253.pdf</a></span>   <span><a href='https://github.com/EPITAResearchLab/pouliquen.24.icdar' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Glen Pouliquen, Guillaume Chiron, Joseph Chazalon, Thierry GÃ©raud, Ahmad Montaser Awal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.17253">Weakly Supervised Training for Hologram Verification in Identity Documents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a method to remotely verify the authenticity of Optically Variable Devices (OVDs), often referred to as ``holograms'', in identity documents. Our method processes video clips captured with smartphones under common lighting conditions, and is evaluated on two public datasets: MIDV-HOLO and MIDV-2020. Thanks to a weakly-supervised training, we optimize a feature extraction and decision pipeline which achieves a new leading performance on MIDV-HOLO, while maintaining a high recall on documents from MIDV-2020 used as attack samples. It is also the first method, to date, to effectively address the photo replacement attack task, and can be trained on either genuine samples, attack samples, or both for increased performance. By enabling to verify OVD shapes and dynamics with very little supervision, this work opens the way towards the use of massive amounts of unlabeled data to build robust remote identity document verification systems on commodity smartphones. Code is available at https://github.com/EPITAResearchLab/pouliquen.24.icdar
<div id='section'>Paperid: <span id='pid'>177, <a href='https://arxiv.org/pdf/2404.16536.pdf' target='_blank'>https://arxiv.org/pdf/2404.16536.pdf</a></span>   <span><a href='https://github.com/liguohao96/WSDF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guohao Li, Hongyu Yang, Di Huang, Yunhong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.16536">3D Face Modeling via Weakly-supervised Disentanglement Network joint Identity-consistency Prior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative 3D face models featuring disentangled controlling factors hold immense potential for diverse applications in computer vision and computer graphics. However, previous 3D face modeling methods face a challenge as they demand specific labels to effectively disentangle these factors. This becomes particularly problematic when integrating multiple 3D face datasets to improve the generalization of the model. Addressing this issue, this paper introduces a Weakly-Supervised Disentanglement Framework, denoted as WSDF, to facilitate the training of controllable 3D face models without an overly stringent labeling requirement. Adhering to the paradigm of Variational Autoencoders (VAEs), the proposed model achieves disentanglement of identity and expression controlling factors through a two-branch encoder equipped with dedicated identity-consistency prior. It then faithfully re-entangles these factors via a tensor-based combination mechanism. Notably, the introduction of the Neutral Bank allows precise acquisition of subject-specific information using only identity labels, thereby averting degeneration due to insufficient supervision. Additionally, the framework incorporates a label-free second-order loss function for the expression factor to regulate deformation space and eliminate extraneous information, resulting in enhanced disentanglement. Extensive experiments have been conducted to substantiate the superior performance of WSDF. Our code is available at https://github.com/liguohao96/WSDF.
<div id='section'>Paperid: <span id='pid'>178, <a href='https://arxiv.org/pdf/2404.15653.pdf' target='_blank'>https://arxiv.org/pdf/2404.15653.pdf</a></span>   <span><a href='https://github.com/apple/corenet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sachin Mehta, Maxwell Horton, Fartash Faghri, Mohammad Hossein Sekhavat, Mahyar Najibi, Mehrdad Farajtabar, Oncel Tuzel, Mohammad Rastegari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.15653">CatLIP: CLIP-level Visual Recognition Accuracy with 2.7x Faster Pre-training on Web-scale Image-Text Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contrastive learning has emerged as a transformative method for learning effective visual representations through the alignment of image and text embeddings. However, pairwise similarity computation in contrastive loss between image and text pairs poses computational challenges. This paper presents a novel weakly supervised pre-training of vision models on web-scale image-text data. The proposed method reframes pre-training on image-text data as a classification task. Consequently, it eliminates the need for pairwise similarity computations in contrastive loss, achieving a remarkable $2.7\times$ acceleration in training speed compared to contrastive learning on web-scale data. Through extensive experiments spanning diverse vision tasks, including detection and segmentation, we demonstrate that the proposed method maintains high representation quality. Our source code along with pre-trained model weights and training recipes is available at \url{https://github.com/apple/corenet}.
<div id='section'>Paperid: <span id='pid'>179, <a href='https://arxiv.org/pdf/2404.14956.pdf' target='_blank'>https://arxiv.org/pdf/2404.14956.pdf</a></span>   <span><a href='https://github.com/zhangye-zoe/DAWN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ye Zhang, Yifeng Wang, Zijie Fang, Hao Bian, Linghan Cai, Ziyue Wang, Yongbing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.14956">DAWN: Domain-Adaptive Weakly Supervised Nuclei Segmentation via Cross-Task Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised segmentation methods have gained significant attention due to their ability to reduce the reliance on costly pixel-level annotations during model training. However, the current weakly supervised nuclei segmentation approaches typically follow a two-stage pseudo-label generation and network training process. The performance of the nuclei segmentation heavily relies on the quality of the generated pseudo-labels, thereby limiting its effectiveness. This paper introduces a novel domain-adaptive weakly supervised nuclei segmentation framework using cross-task interaction strategies to overcome the challenge of pseudo-label generation. Specifically, we utilize weakly annotated data to train an auxiliary detection task, which assists the domain adaptation of the segmentation network. To enhance the efficiency of domain adaptation, we design a consistent feature constraint module integrating prior knowledge from the source domain. Furthermore, we develop pseudo-label optimization and interactive training methods to improve the domain transfer capability. To validate the effectiveness of our proposed method, we conduct extensive comparative and ablation experiments on six datasets. The results demonstrate the superiority of our approach over existing weakly supervised approaches. Remarkably, our method achieves comparable or even better performance than fully supervised methods. Our code will be released in https://github.com/zhangye-zoe/DAWN.
<div id='section'>Paperid: <span id='pid'>180, <a href='https://arxiv.org/pdf/2404.07202.pdf' target='_blank'>https://arxiv.org/pdf/2404.07202.pdf</a></span>   <span><a href='https://weihaox.github.io/UMBRAE' target='_blank'>  GitHub</a></span> <span><a href='https://weihaox.github.io/UMBRAE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weihao Xia, Raoul de Charette, Cengiz Ãztireli, Jing-Hao Xue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.07202">UMBRAE: Unified Multimodal Brain Decoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address prevailing challenges of the brain-powered research, departing from the observation that the literature hardly recover accurate spatial information and require subject-specific models. To address these challenges, we propose UMBRAE, a unified multimodal decoding of brain signals. First, to extract instance-level conceptual and spatial details from neural signals, we introduce an efficient universal brain encoder for multimodal-brain alignment and recover object descriptions at multiple levels of granularity from subsequent multimodal large language model (MLLM). Second, we introduce a cross-subject training strategy mapping subject-specific features to a common feature space. This allows a model to be trained on multiple subjects without extra resources, even yielding superior results compared to subject-specific models. Further, we demonstrate this supports weakly-supervised adaptation to new subjects, with only a fraction of the total training data. Experiments demonstrate that UMBRAE not only achieves superior results in the newly introduced tasks but also outperforms methods in well established tasks. To assess our method, we construct and share with the community a comprehensive brain understanding benchmark BrainHub. Our code and benchmark are available at https://weihaox.github.io/UMBRAE.
<div id='section'>Paperid: <span id='pid'>181, <a href='https://arxiv.org/pdf/2404.05362.pdf' target='_blank'>https://arxiv.org/pdf/2404.05362.pdf</a></span>   <span><a href='https://github.com/tueimage/MAD-MIL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hassan Keshvarikhojasteh, Josien Pluim, Mitko Veta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.05362">Multi-head Attention-based Deep Multiple Instance Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces MAD-MIL, a Multi-head Attention-based Deep Multiple Instance Learning model, designed for weakly supervised Whole Slide Images (WSIs) classification in digital pathology. Inspired by the multi-head attention mechanism of the Transformer, MAD-MIL simplifies model complexity while achieving competitive results against advanced models like CLAM and DS-MIL. Evaluated on the MNIST-BAGS and public datasets, including TUPAC16, TCGA BRCA, TCGA LUNG, and TCGA KIDNEY, MAD-MIL consistently outperforms ABMIL. This demonstrates enhanced information diversity, interpretability, and efficiency in slide representation. The model's effectiveness, coupled with fewer trainable parameters and lower computational complexity makes it a promising solution for automated pathology workflows. Our code is available at https://github.com/tueimage/MAD-MIL.
<div id='section'>Paperid: <span id='pid'>182, <a href='https://arxiv.org/pdf/2404.04998.pdf' target='_blank'>https://arxiv.org/pdf/2404.04998.pdf</a></span>   <span><a href='https://github.com/gimpong/AAAI21-WSDHQ' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinpeng Wang, Bin Chen, Qiang Zhang, Zaiqiao Meng, Shangsong Liang, Shu-Tao Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.04998">Weakly Supervised Deep Hyperspherical Quantization for Image Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep quantization methods have shown high efficiency on large-scale image retrieval. However, current models heavily rely on ground-truth information, hindering the application of quantization in label-hungry scenarios. A more realistic demand is to learn from inexhaustible uploaded images that are associated with informal tags provided by amateur users. Though such sketchy tags do not obviously reveal the labels, they actually contain useful semantic information for supervising deep quantization. To this end, we propose Weakly-Supervised Deep Hyperspherical Quantization (WSDHQ), which is the first work to learn deep quantization from weakly tagged images. Specifically, 1) we use word embeddings to represent the tags and enhance their semantic information based on a tag correlation graph. 2) To better preserve semantic information in quantization codes and reduce quantization error, we jointly learn semantics-preserving embeddings and supervised quantizer on hypersphere by employing a well-designed fusion layer and tailor-made loss functions. Extensive experiments show that WSDHQ can achieve state-of-art performance on weakly-supervised compact coding. Code is available at https://github.com/gimpong/AAAI21-WSDHQ.
<div id='section'>Paperid: <span id='pid'>183, <a href='https://arxiv.org/pdf/2404.01751.pdf' target='_blank'>https://arxiv.org/pdf/2404.01751.pdf</a></span>   <span><a href='https://github.com/enyac-group/T-VSL/tree/main' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tanvir Mahmud, Yapeng Tian, Diana Marculescu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01751">T-VSL: Text-Guided Visual Sound Source Localization in Mixtures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual sound source localization poses a significant challenge in identifying the semantic region of each sounding source within a video. Existing self-supervised and weakly supervised source localization methods struggle to accurately distinguish the semantic regions of each sounding object, particularly in multi-source mixtures. These methods often rely on audio-visual correspondence as guidance, which can lead to substantial performance drops in complex multi-source localization scenarios. The lack of access to individual source sounds in multi-source mixtures during training exacerbates the difficulty of learning effective audio-visual correspondence for localization. To address this limitation, in this paper, we propose incorporating the text modality as an intermediate feature guide using tri-modal joint embedding models (e.g., AudioCLIP) to disentangle the semantic audio-visual source correspondence in multi-source mixtures. Our framework, dubbed T-VSL, begins by predicting the class of sounding entities in mixtures. Subsequently, the textual representation of each sounding source is employed as guidance to disentangle fine-grained audio-visual source correspondence from multi-source mixtures, leveraging the tri-modal AudioCLIP embedding. This approach enables our framework to handle a flexible number of sources and exhibits promising zero-shot transferability to unseen classes during test time. Extensive experiments conducted on the MUSIC, VGGSound, and VGGSound-Instruments datasets demonstrate significant performance improvements over state-of-the-art methods. Code is released at https://github.com/enyac-group/T-VSL/tree/main
<div id='section'>Paperid: <span id='pid'>184, <a href='https://arxiv.org/pdf/2404.00380.pdf' target='_blank'>https://arxiv.org/pdf/2404.00380.pdf</a></span>   <span><a href='https://github.com/shjo-april/DHR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sanghyun Jo, Fei Pan, In-Jae Yu, Kyungsu Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00380">DHR: Dual Features-Driven Hierarchical Rebalancing in Inter- and Intra-Class Regions for Weakly-Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised semantic segmentation (WSS) ensures high-quality segmentation with limited data and excels when employed as input seed masks for large-scale vision models such as Segment Anything. However, WSS faces challenges related to minor classes since those are overlooked in images with adjacent multiple classes, a limitation originating from the overfitting of traditional expansion methods like Random Walk. We first address this by employing unsupervised and weakly-supervised feature maps instead of conventional methodologies, allowing for hierarchical mask enhancement. This method distinctly categorizes higher-level classes and subsequently separates their associated lower-level classes, ensuring all classes are correctly restored in the mask without losing minor ones. Our approach, validated through extensive experimentation, significantly improves WSS across five benchmarks (VOC: 79.8\%, COCO: 53.9\%, Context: 49.0\%, ADE: 32.9\%, Stuff: 37.4\%), reducing the gap with fully supervised methods by over 84\% on the VOC validation set. Code is available at https://github.com/shjo-april/DHR.
<div id='section'>Paperid: <span id='pid'>185, <a href='https://arxiv.org/pdf/2404.00149.pdf' target='_blank'>https://arxiv.org/pdf/2404.00149.pdf</a></span>   <span><a href='https://github.com/skmhrk1209/VSRD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihua Liu, Hiroki Sakuma, Masatoshi Okutomi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00149">VSRD: Instance-Aware Volumetric Silhouette Rendering for Weakly Supervised 3D Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monocular 3D object detection poses a significant challenge in 3D scene understanding due to its inherently ill-posed nature in monocular depth estimation. Existing methods heavily rely on supervised learning using abundant 3D labels, typically obtained through expensive and labor-intensive annotation on LiDAR point clouds. To tackle this problem, we propose a novel weakly supervised 3D object detection framework named VSRD (Volumetric Silhouette Rendering for Detection) to train 3D object detectors without any 3D supervision but only weak 2D supervision. VSRD consists of multi-view 3D auto-labeling and subsequent training of monocular 3D object detectors using the pseudo labels generated in the auto-labeling stage. In the auto-labeling stage, we represent the surface of each instance as a signed distance field (SDF) and render its silhouette as an instance mask through our proposed instance-aware volumetric silhouette rendering. To directly optimize the 3D bounding boxes through rendering, we decompose the SDF of each instance into the SDF of a cuboid and the residual distance field (RDF) that represents the residual from the cuboid. This mechanism enables us to optimize the 3D bounding boxes in an end-to-end manner by comparing the rendered instance masks with the ground truth instance masks. The optimized 3D bounding boxes serve as effective training data for 3D object detection. We conduct extensive experiments on the KITTI-360 dataset, demonstrating that our method outperforms the existing weakly supervised 3D object detection methods. The code is available at https://github.com/skmhrk1209/VSRD.
<div id='section'>Paperid: <span id='pid'>186, <a href='https://arxiv.org/pdf/2403.20253.pdf' target='_blank'>https://arxiv.org/pdf/2403.20253.pdf</a></span>   <span><a href='https://github.com/HealthX-Lab/MedCLIP-SAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Taha Koleilat, Hojat Asgariandehkordi, Hassan Rivaz, Yiming Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.20253">MedCLIP-SAM: Bridging Text and Image Towards Universal Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical image segmentation of anatomical structures and pathology is crucial in modern clinical diagnosis, disease study, and treatment planning. To date, great progress has been made in deep learning-based segmentation techniques, but most methods still lack data efficiency, generalizability, and interactability. Consequently, the development of new, precise segmentation methods that demand fewer labeled datasets is of utmost importance in medical image analysis. Recently, the emergence of foundation models, such as CLIP and Segment-Anything-Model (SAM), with comprehensive cross-domain representation opened the door for interactive and universal image segmentation. However, exploration of these models for data-efficient medical image segmentation is still limited, but is highly necessary. In this paper, we propose a novel framework, called MedCLIP-SAM that combines CLIP and SAM models to generate segmentation of clinical scans using text prompts in both zero-shot and weakly supervised settings. To achieve this, we employed a new Decoupled Hard Negative Noise Contrastive Estimation (DHN-NCE) loss to fine-tune the BiomedCLIP model and the recent gScoreCAM to generate prompts to obtain segmentation masks from SAM in a zero-shot setting. Additionally, we explored the use of zero-shot segmentation labels in a weakly supervised paradigm to improve the segmentation quality further. By extensively testing three diverse segmentation tasks and medical image modalities (breast tumor ultrasound, brain tumor MRI, and lung X-ray), our proposed framework has demonstrated excellent accuracy. Code is available at https://github.com/HealthX-Lab/MedCLIP-SAM.
<div id='section'>Paperid: <span id='pid'>187, <a href='https://arxiv.org/pdf/2403.20105.pdf' target='_blank'>https://arxiv.org/pdf/2403.20105.pdf</a></span>   <span><a href='https://bcorrad.github.io/freesegdiff/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Barbara Toniella Corradini, Mustafa Shukor, Paul Couairon, Guillaume Couairon, Franco Scarselli, Matthieu Cord
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.20105">FreeSeg-Diff: Training-Free Open-Vocabulary Segmentation with Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation models have exhibited unprecedented capabilities in tackling many domains and tasks. Models such as CLIP are currently widely used to bridge cross-modal representations, and text-to-image diffusion models are arguably the leading models in terms of realistic image generation. Image generative models are trained on massive datasets that provide them with powerful internal spatial representations. In this work, we explore the potential benefits of such representations, beyond image generation, in particular, for dense visual prediction tasks. We focus on the task of image segmentation, which is traditionally solved by training models on closed-vocabulary datasets, with pixel-level annotations. To avoid the annotation cost or training large diffusion models, we constraint our setup to be zero-shot and training-free. In a nutshell, our pipeline leverages different and relatively small-sized, open-source foundation models for zero-shot open-vocabulary segmentation. The pipeline is as follows: the image is passed to both a captioner model (i.e. BLIP) and a diffusion model (i.e., Stable Diffusion Model) to generate a text description and visual representation, respectively. The features are clustered and binarized to obtain class agnostic masks for each object. These masks are then mapped to a textual class, using the CLIP model to support open-vocabulary. Finally, we add a refinement step that allows to obtain a more precise segmentation mask. Our approach (dubbed FreeSeg-Diff), which does not rely on any training, outperforms many training-based approaches on both Pascal VOC and COCO datasets. In addition, we show very competitive results compared to the recent weakly-supervised segmentation approaches. We provide comprehensive experiments showing the superiority of diffusion model features compared to other pretrained models. Project page: https://bcorrad.github.io/freesegdiff/
<div id='section'>Paperid: <span id='pid'>188, <a href='https://arxiv.org/pdf/2403.15019.pdf' target='_blank'>https://arxiv.org/pdf/2403.15019.pdf</a></span>   <span><a href='https://github.com/peoplelu/BSNet' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/peoplelu/BSNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Lu, Jiacheng Deng, Tianzhu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15019">BSNet: Box-Supervised Simulation-assisted Mean Teacher for 3D Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D instance segmentation (3DIS) is a crucial task, but point-level annotations are tedious in fully supervised settings. Thus, using bounding boxes (bboxes) as annotations has shown great potential. The current mainstream approach is a two-step process, involving the generation of pseudo-labels from box annotations and the training of a 3DIS network with the pseudo-labels. However, due to the presence of intersections among bboxes, not every point has a determined instance label, especially in overlapping areas. To generate higher quality pseudo-labels and achieve more precise weakly supervised 3DIS results, we propose the Box-Supervised Simulation-assisted Mean Teacher for 3D Instance Segmentation (BSNet), which devises a novel pseudo-labeler called Simulation-assisted Transformer. The labeler consists of two main components. The first is Simulation-assisted Mean Teacher, which introduces Mean Teacher for the first time in this task and constructs simulated samples to assist the labeler in acquiring prior knowledge about overlapping areas. To better model local-global structure, we also propose Local-Global Aware Attention as the decoder for teacher and student labelers. Extensive experiments conducted on the ScanNetV2 and S3DIS datasets verify the superiority of our designs. Code is available at \href{https://github.com/peoplelu/BSNet}{https://github.com/peoplelu/BSNet}.
<div id='section'>Paperid: <span id='pid'>189, <a href='https://arxiv.org/pdf/2403.11672.pdf' target='_blank'>https://arxiv.org/pdf/2403.11672.pdf</a></span>   <span><a href='https://github.com/zhaohaoyu376/WI-LD2ND' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Zhao, Yuliang Gu, Zhou Zhao, Bo Du, Yongchao Xu, Rui Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11672">WIA-LD2ND: Wavelet-based Image Alignment for Self-supervised Low-Dose CT Denoising</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In clinical examinations and diagnoses, low-dose computed tomography (LDCT) is crucial for minimizing health risks compared with normal-dose computed tomography (NDCT). However, reducing the radiation dose compromises the signal-to-noise ratio, leading to degraded quality of CT images. To address this, we analyze LDCT denoising task based on experimental results from the frequency perspective, and then introduce a novel self-supervised CT image denoising method called WIA-LD2ND, only using NDCT data. The proposed WIA-LD2ND comprises two modules: Wavelet-based Image Alignment (WIA) and Frequency-Aware Multi-scale Loss (FAM). First, WIA is introduced to align NDCT with LDCT by mainly adding noise to the high-frequency components, which is the main difference between LDCT and NDCT. Second, to better capture high-frequency components and detailed information, Frequency-Aware Multi-scale Loss (FAM) is proposed by effectively utilizing multi-scale feature space. Extensive experiments on two public LDCT denoising datasets demonstrate that our WIA-LD2ND, only uses NDCT, outperforms existing several state-of-the-art weakly-supervised and self-supervised methods. Source code is available at https://github.com/zhaohaoyu376/WI-LD2ND.
<div id='section'>Paperid: <span id='pid'>190, <a href='https://arxiv.org/pdf/2403.11184.pdf' target='_blank'>https://arxiv.org/pdf/2403.11184.pdf</a></span>   <span><a href='https://github.com/Wu0409/DuPL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanchen Wu, Xichen Ye, Kequan Yang, Jide Li, Xiaoqiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11184">DuPL: Dual Student with Trustworthy Progressive Learning for Robust Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, One-stage Weakly Supervised Semantic Segmentation (WSSS) with image-level labels has gained increasing interest due to simplification over its cumbersome multi-stage counterpart. Limited by the inherent ambiguity of Class Activation Map (CAM), we observe that one-stage pipelines often encounter confirmation bias caused by incorrect CAM pseudo-labels, impairing their final segmentation performance. Although recent works discard many unreliable pseudo-labels to implicitly alleviate this issue, they fail to exploit sufficient supervision for their models. To this end, we propose a dual student framework with trustworthy progressive learning (DuPL). Specifically, we propose a dual student network with a discrepancy loss to yield diverse CAMs for each sub-net. The two sub-nets generate supervision for each other, mitigating the confirmation bias caused by learning their own incorrect pseudo-labels. In this process, we progressively introduce more trustworthy pseudo-labels to be involved in the supervision through dynamic threshold adjustment with an adaptive noise filtering strategy. Moreover, we believe that every pixel, even discarded from supervision due to its unreliability, is important for WSSS. Thus, we develop consistency regularization on these discarded regions, providing supervision of every pixel. Experiment results demonstrate the superiority of the proposed DuPL over the recent state-of-the-art alternatives on PASCAL VOC 2012 and MS COCO datasets. Code is available at https://github.com/Wu0409/DuPL.
<div id='section'>Paperid: <span id='pid'>191, <a href='https://arxiv.org/pdf/2403.07630.pdf' target='_blank'>https://arxiv.org/pdf/2403.07630.pdf</a></span>   <span><a href='https://github.com/Barrett-python/CPAL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Feilong Tang, Zhongxing Xu, Zhaojun Qu, Wei Feng, Xingjian Jiang, Zongyuan Ge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07630">Hunting Attributes: Context Prototype-Aware Learning for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent weakly supervised semantic segmentation (WSSS) methods strive to incorporate contextual knowledge to improve the completeness of class activation maps (CAM). In this work, we argue that the knowledge bias between instances and contexts affects the capability of the prototype to sufficiently understand instance semantics. Inspired by prototype learning theory, we propose leveraging prototype awareness to capture diverse and fine-grained feature attributes of instances. The hypothesis is that contextual prototypes might erroneously activate similar and frequently co-occurring object categories due to this knowledge bias. Therefore, we propose to enhance the prototype representation ability by mitigating the bias to better capture spatial coverage in semantic object regions. With this goal, we present a Context Prototype-Aware Learning (CPAL) strategy, which leverages semantic context to enrich instance comprehension. The core of this method is to accurately capture intra-class variations in object features through context-aware prototypes, facilitating the adaptation to the semantic attributes of various instances. We design feature distribution alignment to optimize prototype awareness, aligning instance feature distributions with dense features. In addition, a unified training framework is proposed to combine label-guided classification supervision and prototypes-guided self-supervision. Experimental results on PASCAL VOC 2012 and MS COCO 2014 show that CPAL significantly improves off-the-shelf methods and achieves state-of-the-art performance. The project is available at https://github.com/Barrett-python/CPAL.
<div id='section'>Paperid: <span id='pid'>192, <a href='https://arxiv.org/pdf/2403.06676.pdf' target='_blank'>https://arxiv.org/pdf/2403.06676.pdf</a></span>   <span><a href='https://github.com/snskysk/CAM-Back-Again' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/snskysk/CAM-Back-Again' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shunsuke Yasuki, Masato Taki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.06676">CAM Back Again: Large Kernel CNNs from a Weakly Supervised Object Localization Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, convolutional neural networks (CNNs) with large size kernels have attracted much attention in the computer vision field, following the success of the Vision Transformers. Large kernel CNNs have been reported to perform well in downstream vision tasks as well as in classification performance. The reason for the high-performance of large kernel CNNs in downstream tasks has been attributed to the large effective receptive field (ERF) produced by large size kernels, but this view has not been fully tested. We therefore revisit the performance of large kernel CNNs in downstream task, focusing on the weakly supervised object localization (WSOL) task. WSOL, a difficult downstream task that is not fully supervised, provides a new angle to explore the capabilities of the large kernel CNNs. Our study compares the modern large kernel CNNs ConvNeXt, RepLKNet, and SLaK to test the validity of the naive expectation that ERF size is important for improving downstream task performance. Our analysis of the factors contributing to high performance provides a different perspective, in which the main factor is feature map improvement. Furthermore, we find that modern CNNs are robust to the CAM problems of local regions of objects being activated, which has long been discussed in WSOL. CAM is the most classic WSOL method, but because of the above-mentioned problems, it is often used as a baseline method for comparison. However, experiments on the CUB-200-2011 dataset show that simply combining a large kernel CNN, CAM, and simple data augmentation methods can achieve performance (90.99% MaxBoxAcc) comparable to the latest WSOL method, which is CNN-based and requires special training or complex post-processing. The code is available at https://github.com/snskysk/CAM-Back-Again.
<div id='section'>Paperid: <span id='pid'>193, <a href='https://arxiv.org/pdf/2403.06567.pdf' target='_blank'>https://arxiv.org/pdf/2403.06567.pdf</a></span>   <span><a href='https://github.com/MIC-DKFZ/foundation-models-for-cbmir' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Stefan Denner, David Zimmerer, Dimitrios Bounias, Markus Bujotzek, Shuhan Xiao, Raphael Stock, Lisa Kausch, Philipp Schader, Tobias Penzkofer, Paul F. JÃ¤ger, Klaus Maier-Hein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.06567">Leveraging Foundation Models for Content-Based Image Retrieval in Radiology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Content-based image retrieval (CBIR) has the potential to significantly improve diagnostic aid and medical research in radiology. However, current CBIR systems face limitations due to their specialization to certain pathologies, limiting their utility. On the other hand, several vision foundation models have been shown to produce general-purpose visual features. Therefore, in this work, we propose using vision foundation models as powerful and versatile off-the-shelf feature extractors for content-based image retrieval. Our contributions include: (1) benchmarking a diverse set of vision foundation models on an extensive dataset comprising 1.6 million 2D radiological images across four modalities and 161 pathologies; (2) identifying weakly-supervised models, particularly BiomedCLIP, as highly effective, achieving a achieving a P@1 of up to 0.594 (P@3: 0.590, P@5: 0.588, P@10: 0.583), comparable to specialized CBIR systems but without additional training; (3) conducting an in-depth analysis of the impact of index size on retrieval performance; (4) evaluating the quality of embedding spaces generated by different models; and (5) investigating specific challenges associated with retrieving anatomical versus pathological structures. Despite these challenges, our research underscores the vast potential of foundation models for CBIR in radiology, proposing a shift towards versatile, general-purpose medical image retrieval systems that do not require specific tuning. Our code, dataset splits and embeddings are publicly available under https://github.com/MIC-DKFZ/foundation-models-for-cbmir.
<div id='section'>Paperid: <span id='pid'>194, <a href='https://arxiv.org/pdf/2403.06154.pdf' target='_blank'>https://arxiv.org/pdf/2403.06154.pdf</a></span>   <span><a href='https://github.com/pipixin321/GlanceVAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huaxin Zhang, Xiang Wang, Xiaohao Xu, Xiaonan Huang, Chuchu Han, Yuehuan Wang, Changxin Gao, Shanjun Zhang, Nong Sang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.06154">GlanceVAD: Exploring Glance Supervision for Label-efficient Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, video anomaly detection has been extensively investigated in both unsupervised and weakly supervised settings to alleviate costly temporal labeling. Despite significant progress, these methods still suffer from unsatisfactory results such as numerous false alarms, primarily due to the absence of precise temporal anomaly annotation. In this paper, we present a novel labeling paradigm, termed "glance annotation", to achieve a better balance between anomaly detection accuracy and annotation cost. Specifically, glance annotation is a random frame within each abnormal event, which can be easily accessed and is cost-effective. To assess its effectiveness, we manually annotate the glance annotations for two standard video anomaly detection datasets: UCF-Crime and XD-Violence. Additionally, we propose a customized GlanceVAD method, that leverages gaussian kernels as the basic unit to compose the temporal anomaly distribution, enabling the learning of diverse and robust anomaly representations from the glance annotations. Through comprehensive analysis and experiments, we verify that the proposed labeling paradigm can achieve an excellent trade-off between annotation cost and model performance. Extensive experimental results also demonstrate the effectiveness of our GlanceVAD approach, which significantly outperforms existing advanced unsupervised and weakly supervised methods. Code and annotations will be publicly available at https://github.com/pipixin321/GlanceVAD.
<div id='section'>Paperid: <span id='pid'>195, <a href='https://arxiv.org/pdf/2403.05796.pdf' target='_blank'>https://arxiv.org/pdf/2403.05796.pdf</a></span>   <span><a href='https://github.com/BinghaoLu/KD-MSI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Binghao Lu, Caiwen Ding, Jinbo Bi, Dongjin Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05796">Weakly Supervised Change Detection via Knowledge Distillation and Multiscale Sigmoid Inference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Change detection, which aims to detect spatial changes from a pair of multi-temporal images due to natural or man-made causes, has been widely applied in remote sensing, disaster management, urban management, etc. Most existing change detection approaches, however, are fully supervised and require labor-intensive pixel-level labels. To address this, we develop a novel weakly supervised change detection technique via Knowledge Distillation and Multiscale Sigmoid Inference (KD-MSI) that leverages image-level labels. In our approach, the Class Activation Maps (CAM) are utilized not only to derive a change probability map but also to serve as a foundation for the knowledge distillation process. This is done through a joint training strategy of the teacher and student networks, enabling the student network to highlight potential change areas more accurately than teacher network based on image-level labels. Moreover, we designed a Multiscale Sigmoid Inference (MSI) module as a post processing step to further refine the change probability map from the trained student network. Empirical results on three public datasets, i.e., WHU-CD, DSIFN-CD, and LEVIR-CD, demonstrate that our proposed technique, with its integrated training strategy, significantly outperforms the state-of-the-art.
<div id='section'>Paperid: <span id='pid'>196, <a href='https://arxiv.org/pdf/2403.02818.pdf' target='_blank'>https://arxiv.org/pdf/2403.02818.pdf</a></span>   <span><a href='https://github.com/gaocq/SS3D2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenqiang Gao, Chuandong Liu, Jun Shu, Fangcen Liu, Jiang Liu, Luyu Yang, Xinbo Gao, Deyu Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.02818">Are Dense Labels Always Necessary for 3D Object Detection from Point Cloud?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current state-of-the-art (SOTA) 3D object detection methods often require a large amount of 3D bounding box annotations for training. However, collecting such large-scale densely-supervised datasets is notoriously costly. To reduce the cumbersome data annotation process, we propose a novel sparsely-annotated framework, in which we just annotate one 3D object per scene. Such a sparse annotation strategy could significantly reduce the heavy annotation burden, while inexact and incomplete sparse supervision may severely deteriorate the detection performance. To address this issue, we develop the SS3D++ method that alternatively improves 3D detector training and confident fully-annotated scene generation in a unified learning scheme. Using sparse annotations as seeds, we progressively generate confident fully-annotated scenes based on designing a missing-annotated instance mining module and reliable background mining module. Our proposed method produces competitive results when compared with SOTA weakly-supervised methods using the same or even more annotation costs. Besides, compared with SOTA fully-supervised methods, we achieve on-par or even better performance on the KITTI dataset with about 5x less annotation cost, and 90% of their performance on the Waymo dataset with about 15x less annotation cost. The additional unlabeled training scenes could further boost the performance. The code will be available at https://github.com/gaocq/SS3D2.
<div id='section'>Paperid: <span id='pid'>197, <a href='https://arxiv.org/pdf/2403.02566.pdf' target='_blank'>https://arxiv.org/pdf/2403.02566.pdf</a></span>   <span><a href='https://github.com/runminjiang/PW4MedSeg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Runmin Jiang, Zhaoxin Fan, Junhao Wu, Lenghan Zhu, Xin Huang, Tianyang Wang, Heng Huang, Min Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.02566">Enhancing Weakly Supervised 3D Medical Image Segmentation through Probabilistic-aware Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D medical image segmentation is a challenging task with crucial implications for disease diagnosis and treatment planning. Recent advances in deep learning have significantly enhanced fully supervised medical image segmentation. However, this approach heavily relies on labor-intensive and time-consuming fully annotated ground-truth labels, particularly for 3D volumes. To overcome this limitation, we propose a novel probabilistic-aware weakly supervised learning pipeline, specifically designed for 3D medical imaging. Our pipeline integrates three innovative components: a Probability-based Pseudo Label Generation technique for synthesizing dense segmentation masks from sparse annotations, a Probabilistic Multi-head Self-Attention network for robust feature extraction within our Probabilistic Transformer Network, and a Probability-informed Segmentation Loss Function to enhance training with annotation confidence. Demonstrating significant advances, our approach not only rivals the performance of fully supervised methods but also surpasses existing weakly supervised methods in CT and MRI datasets, achieving up to 18.1% improvement in Dice scores for certain organs. The code is available at https://github.com/runminjiang/PW4MedSeg.
<div id='section'>Paperid: <span id='pid'>198, <a href='https://arxiv.org/pdf/2403.01169.pdf' target='_blank'>https://arxiv.org/pdf/2403.01169.pdf</a></span>   <span><a href='https://github.com/shiwoaz/lap' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenchen Tao, Xiaohao Peng, Chong Wang, Jiafei Wu, Puning Zhao, Jun Wang, Jiangbo Qian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.01169">Learn Suspected Anomalies from Event Prompts for Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most models for weakly supervised video anomaly detection (WS-VAD) rely on multiple instance learning, aiming to distinguish normal and abnormal snippets without specifying the type of anomaly. However, the ambiguous nature of anomaly definitions across contexts may introduce inaccuracy in discriminating abnormal and normal events. To show the model what is anomalous, a novel framework is proposed to guide the learning of suspected anomalies from event prompts. Given a textual prompt dictionary of potential anomaly events and the captions generated from anomaly videos, the semantic anomaly similarity between them could be calculated to identify the suspected events for each video snippet. It enables a new multi-prompt learning process to constrain the visual-semantic features across all videos, as well as provides a new way to label pseudo anomalies for self-training. To demonstrate its effectiveness, comprehensive experiments and detailed ablation studies are conducted on four datasets, namely XD-Violence, UCF-Crime, TAD, and ShanghaiTech. Our proposed model outperforms most state-of-the-art methods in terms of AP or AUC (86.5\%, \hl{90.4}\%, 94.4\%, and 97.4\%). Furthermore, it shows promising performance in open-set and cross-dataset cases. The data, code, and models can be found at: \url{https://github.com/shiwoaz/lap}.
<div id='section'>Paperid: <span id='pid'>199, <a href='https://arxiv.org/pdf/2402.18467.pdf' target='_blank'>https://arxiv.org/pdf/2402.18467.pdf</a></span>   <span><a href='https://github.com/zwyang6/SeCo.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiwei Yang, Kexue Fu, Minghong Duan, Linhao Qu, Shuo Wang, Zhijian Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.18467">Separate and Conquer: Decoupling Co-occurrence via Decomposition and Representation for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation (WSSS) with image-level labels aims to achieve segmentation tasks without dense annotations. However, attributed to the frequent coupling of co-occurring objects and the limited supervision from image-level labels, the challenging co-occurrence problem is widely present and leads to false activation of objects in WSSS. In this work, we devise a 'Separate and Conquer' scheme SeCo to tackle this issue from dimensions of image space and feature space. In the image space, we propose to 'separate' the co-occurring objects with image decomposition by subdividing images into patches. Importantly, we assign each patch a category tag from Class Activation Maps (CAMs), which spatially helps remove the co-context bias and guide the subsequent representation. In the feature space, we propose to 'conquer' the false activation by enhancing semantic representation with multi-granularity knowledge contrast. To this end, a dual-teacher-single-student architecture is designed and tag-guided contrast is conducted, which guarantee the correctness of knowledge and further facilitate the discrepancy among co-contexts. We streamline the multi-staged WSSS pipeline end-to-end and tackle this issue without external supervision. Extensive experiments are conducted, validating the efficiency of our method and the superiority over previous single-staged and even multi-staged competitors on PASCAL VOC and MS COCO. Code is available at https://github.com/zwyang6/SeCo.git.
<div id='section'>Paperid: <span id='pid'>200, <a href='https://arxiv.org/pdf/2402.17891.pdf' target='_blank'>https://arxiv.org/pdf/2402.17891.pdf</a></span>   <span><a href='https://github.com/youshyee/CoSA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Yang, Hossein Rahmani, Sue Black, Bryan M. Williams
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17891">Weakly Supervised Co-training with Swapping Assignments for Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Class activation maps (CAMs) are commonly employed in weakly supervised semantic segmentation (WSSS) to produce pseudo-labels. Due to incomplete or excessive class activation, existing studies often resort to offline CAM refinement, introducing additional stages or proposing offline modules. This can cause optimization difficulties for single-stage methods and limit generalizability. In this study, we aim to reduce the observed CAM inconsistency and error to mitigate reliance on refinement processes. We propose an end-to-end WSSS model incorporating guided CAMs, wherein our segmentation model is trained while concurrently optimizing CAMs online. Our method, Co-training with Swapping Assignments (CoSA), leverages a dual-stream framework, where one sub-network learns from the swapped assignments generated by the other. We introduce three techniques: i) soft perplexity-based regularization to penalize uncertain regions; ii) a threshold-searching approach to dynamically revise the confidence threshold; and iii) contrastive separation to address the coexistence problem. CoSA demonstrates exceptional performance, achieving mIoU of 76.2\% and 51.0\% on VOC and COCO validation datasets, respectively, surpassing existing baselines by a substantial margin. Notably, CoSA is the first single-stage approach to outperform all existing multi-stage methods including those with additional supervision. Code is avilable at \url{https://github.com/youshyee/CoSA}.
<div id='section'>Paperid: <span id='pid'>201, <a href='https://arxiv.org/pdf/2402.17555.pdf' target='_blank'>https://arxiv.org/pdf/2402.17555.pdf</a></span>   <span><a href='https://github.com/Zxl19990529/Class-driven-Scribble-Promotion-Network' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinliang Zhang, Lei Zhu, Hangzhou He, Lujia Jin, Yanye Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17555">Scribble Hides Class: Promoting Scribble-Based Weakly-Supervised Semantic Segmentation with Its Class Label</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scribble-based weakly-supervised semantic segmentation using sparse scribble supervision is gaining traction as it reduces annotation costs when compared to fully annotated alternatives. Existing methods primarily generate pseudo-labels by diffusing labeled pixels to unlabeled ones with local cues for supervision. However, this diffusion process fails to exploit global semantics and class-specific cues, which are important for semantic segmentation. In this study, we propose a class-driven scribble promotion network, which utilizes both scribble annotations and pseudo-labels informed by image-level classes and global semantics for supervision. Directly adopting pseudo-labels might misguide the segmentation model, thus we design a localization rectification module to correct foreground representations in the feature space. To further combine the advantages of both supervisions, we also introduce a distance entropy loss for uncertainty reduction, which adapts per-pixel confidence weights according to the reliable region determined by the scribble and pseudo-label's boundary. Experiments on the ScribbleSup dataset with different qualities of scribble annotations outperform all the previous methods, demonstrating the superiority and robustness of our method.The code is available at https://github.com/Zxl19990529/Class-driven-Scribble-Promotion-Network.
<div id='section'>Paperid: <span id='pid'>202, <a href='https://arxiv.org/pdf/2402.16001.pdf' target='_blank'>https://arxiv.org/pdf/2402.16001.pdf</a></span>   <span><a href='https://github.com/yu-ni1989/ANLC-Former' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huan Ni, Yubin Zhao, Haiyan Guan, Cheng Jiang, Yongshi Jie, Xing Wang, Yiyang Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.16001">Cross-Resolution Land Cover Classification Using Outdated Products and Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale high-resolution land cover classification is a prerequisite for constructing Earth system models and addressing ecological and resource issues. Advancements in satellite sensor technology have led to an improvement in spatial resolution and wider coverage areas. Nevertheless, the lack of high-resolution labeled data is still a challenge, hindering the largescale application of land cover classification methods. In this paper, we propose a Transformerbased weakly supervised method for cross-resolution land cover classification using outdated data. First, to capture long-range dependencies without missing the fine-grained details of objects, we propose a U-Net-like Transformer based on a reverse difference mechanism (RDM) using dynamic sparse attention. Second, we propose an anti-noise loss calculation (ANLC) module based on optimal transport (OT). Anti-noise loss calculation identifies confident areas (CA) and vague areas (VA) based on the OT matrix, which relieves the impact of noises in outdated land cover products. By introducing a weakly supervised loss with weights and employing unsupervised loss, the RDM-based U-Net-like Transformer was trained. Remote sensing images with 1 m resolution and the corresponding ground-truths of six states in the United States were employed to validate the performance of the proposed method. The experiments utilized outdated land cover products with 30 m resolution from 2013 as training labels, and produced land cover maps with 1 m resolution from 2017. The results show the superiority of the proposed method compared to state-of-the-art methods. The code is available at https://github.com/yu-ni1989/ANLC-Former.
<div id='section'>Paperid: <span id='pid'>203, <a href='https://arxiv.org/pdf/2402.14812.pdf' target='_blank'>https://arxiv.org/pdf/2402.14812.pdf</a></span>   <span><a href='https://github.com/hustvl/WeakSAM' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/hustvl/WeakSAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lianghui Zhu, Junwei Zhou, Yan Liu, Xin Hao, Wenyu Liu, Xinggang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.14812">WeakSAM: Segment Anything Meets Weakly-supervised Instance-level Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised visual recognition using inexact supervision is a critical yet challenging learning problem. It significantly reduces human labeling costs and traditionally relies on multi-instance learning and pseudo-labeling. This paper introduces WeakSAM and solves the weakly-supervised object detection (WSOD) and segmentation by utilizing the pre-learned world knowledge contained in a vision foundation model, i.e., the Segment Anything Model (SAM). WeakSAM addresses two critical limitations in traditional WSOD retraining, i.e., pseudo ground truth (PGT) incompleteness and noisy PGT instances, through adaptive PGT generation and Region of Interest (RoI) drop regularization. It also addresses the SAM's problems of requiring prompts and category unawareness for automatic object detection and segmentation. Our results indicate that WeakSAM significantly surpasses previous state-of-the-art methods in WSOD and WSIS benchmarks with large margins, i.e. average improvements of 7.4% and 8.5%, respectively. The code is available at \url{https://github.com/hustvl/WeakSAM}.
<div id='section'>Paperid: <span id='pid'>204, <a href='https://arxiv.org/pdf/2402.12208.pdf' target='_blank'>https://arxiv.org/pdf/2402.12208.pdf</a></span>   <span><a href='https://github.com/jishengpeng/languagecodec' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengpeng Ji, Minghui Fang, Jialong Zuo, Ziyue Jiang, Dingdong Wang, Hanting Wang, Hai Huang, Zhou Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.12208">Language-Codec: Bridging Discrete Codec Representations and Speech Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, large language models have achieved significant success in generative tasks related to speech, audio, music, and other signal domains. A crucial element of these models is the discrete acoustic codecs, which serve as an intermediate representation replacing the mel-spectrogram. However, there exist several gaps between discrete codecs and downstream speech language models. Specifically, 1) Due to the reconstruction paradigm of the Codec model and the structure of residual vector quantization, the initial channel of the codebooks contains excessive information, making it challenging to directly generate acoustic tokens from weakly supervised signals such as text in downstream tasks. 2) numerous codebooks increases the burden on downstream speech language models. Consequently, leveraging the characteristics of speech language models, we propose Language-Codec. In the Language-Codec, we introduce a Masked Channel Residual Vector Quantization (MCRVQ) mechanism along with improved fourier transform structures and attention blocks, refined discriminator design to address the aforementioned gaps. We compare our method with competing audio compression algorithms and observe significant outperformance across extensive evaluations. Furthermore, we also validate the efficiency of the Language-Codec on downstream speech language models. The source code and pre-trained models can be accessed at https://github.com/jishengpeng/languagecodec .
<div id='section'>Paperid: <span id='pid'>205, <a href='https://arxiv.org/pdf/2402.12128.pdf' target='_blank'>https://arxiv.org/pdf/2402.12128.pdf</a></span>   <span><a href='https://github.com/gzq17/Weakly-Supervised-by-MIP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhanqiang Guo, Zimeng Tan, Jianjiang Feng, Jie Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.12128">3D Vascular Segmentation Supervised by 2D Annotation of Maximum Intensity Projection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vascular structure segmentation plays a crucial role in medical analysis and clinical applications. The practical adoption of fully supervised segmentation models is impeded by the intricacy and time-consuming nature of annotating vessels in the 3D space. This has spurred the exploration of weakly-supervised approaches that reduce reliance on expensive segmentation annotations. Despite this, existing weakly supervised methods employed in organ segmentation, which encompass points, bounding boxes, or graffiti, have exhibited suboptimal performance when handling sparse vascular structure. To alleviate this issue, we employ maximum intensity projection (MIP) to decrease the dimensionality of 3D volume to 2D image for efficient annotation, and the 2D labels are utilized to provide guidance and oversight for training 3D vessel segmentation model. Initially, we generate pseudo-labels for 3D blood vessels using the annotations of 2D projections. Subsequently, taking into account the acquisition method of the 2D labels, we introduce a weakly-supervised network that fuses 2D-3D deep features via MIP to further improve segmentation performance. Furthermore, we integrate confidence learning and uncertainty estimation to refine the generated pseudo-labels, followed by fine-tuning the segmentation network. Our method is validated on five datasets (including cerebral vessel, aorta and coronary artery), demonstrating highly competitive performance in segmenting vessels and the potential to significantly reduce the time and effort required for vessel annotation. Our code is available at: https://github.com/gzq17/Weakly-Supervised-by-MIP.
<div id='section'>Paperid: <span id='pid'>206, <a href='https://arxiv.org/pdf/2402.11985.pdf' target='_blank'>https://arxiv.org/pdf/2402.11985.pdf</a></span>   <span><a href='https://github.com/philip-mueller/wsrpn' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Philip MÃ¼ller, Felix Meissen, Georgios Kaissis, Daniel Rueckert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.11985">Weakly Supervised Object Detection in Chest X-Rays with Differentiable ROI Proposal Networks and Soft ROI Pooling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised object detection (WSup-OD) increases the usefulness and interpretability of image classification algorithms without requiring additional supervision. The successes of multiple instance learning in this task for natural images, however, do not translate well to medical images due to the very different characteristics of their objects (i.e. pathologies). In this work, we propose Weakly Supervised ROI Proposal Networks (WSRPN), a new method for generating bounding box proposals on the fly using a specialized region of interest-attention (ROI-attention) module. WSRPN integrates well with classic backbone-head classification algorithms and is end-to-end trainable with only image-label supervision. We experimentally demonstrate that our new method outperforms existing methods in the challenging task of disease localization in chest X-ray images. Code: https://github.com/philip-mueller/wsrpn
<div id='section'>Paperid: <span id='pid'>207, <a href='https://arxiv.org/pdf/2402.09811.pdf' target='_blank'>https://arxiv.org/pdf/2402.09811.pdf</a></span>   <span><a href='https://github.com/IITB-LEAP-OCR/TEXTRON' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dhruv Kudale, Badri Vishal Kasuba, Venkatapathy Subramanian, Parag Chaudhuri, Ganesh Ramakrishnan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.09811">TEXTRON: Weakly Supervised Multilingual Text Detection through Data Programming</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Several recent deep learning (DL) based techniques perform considerably well on image-based multilingual text detection. However, their performance relies heavily on the availability and quality of training data. There are numerous types of page-level document images consisting of information in several modalities, languages, fonts, and layouts. This makes text detection a challenging problem in the field of computer vision (CV), especially for low-resource or handwritten languages. Furthermore, there is a scarcity of word-level labeled data for text detection, especially for multilingual settings and Indian scripts that incorporate both printed and handwritten text. Conventionally, Indian script text detection requires training a DL model on plenty of labeled data, but to the best of our knowledge, no relevant datasets are available. Manual annotation of such data requires a lot of time, effort, and expertise. In order to solve this problem, we propose TEXTRON, a Data Programming-based approach, where users can plug various text detection methods into a weak supervision-based learning framework. One can view this approach to multilingual text detection as an ensemble of different CV-based techniques and DL approaches. TEXTRON can leverage the predictions of DL models pre-trained on a significant amount of language data in conjunction with CV-based methods to improve text detection in other languages. We demonstrate that TEXTRON can improve the detection performance for documents written in Indian languages, despite the absence of corresponding labeled data. Further, through extensive experimentation, we show improvement brought about by our approach over the current State-of-the-art (SOTA) models, especially for handwritten Devanagari text. Code and dataset has been made available at https://github.com/IITB-LEAP-OCR/TEXTRON
<div id='section'>Paperid: <span id='pid'>208, <a href='https://arxiv.org/pdf/2402.07633.pdf' target='_blank'>https://arxiv.org/pdf/2402.07633.pdf</a></span>   <span><a href='https://github.com/ZechengLi19/CIM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zecheng Li, Zening Zeng, Yuqi Liang, Jin-Gang Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.07633">Complete Instances Mining for Weakly Supervised Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised instance segmentation (WSIS) using only image-level labels is a challenging task due to the difficulty of aligning coarse annotations with the finer task. However, with the advancement of deep neural networks (DNNs), WSIS has garnered significant attention. Following a proposal-based paradigm, we encounter a redundant segmentation problem resulting from a single instance being represented by multiple proposals. For example, we feed a picture of a dog and proposals into the network and expect to output only one proposal containing a dog, but the network outputs multiple proposals. To address this problem, we propose a novel approach for WSIS that focuses on the online refinement of complete instances through the use of MaskIoU heads to predict the integrity scores of proposals and a Complete Instances Mining (CIM) strategy to explicitly model the redundant segmentation problem and generate refined pseudo labels. Our approach allows the network to become aware of multiple instances and complete instances, and we further improve its robustness through the incorporation of an Anti-noise strategy. Empirical evaluations on the PASCAL VOC 2012 and MS COCO datasets demonstrate that our method achieves state-of-the-art performance with a notable margin. Our implementation will be made available at https://github.com/ZechengLi19/CIM.
<div id='section'>Paperid: <span id='pid'>209, <a href='https://arxiv.org/pdf/2402.04563.pdf' target='_blank'>https://arxiv.org/pdf/2402.04563.pdf</a></span>   <span><a href='https://github.com/LeemSaebom/Attention-Guided-CAM-Visual-Explanations-of-Vision-Transformer-Guided-by-Self-Attention.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Saebom Leem, Hyunseok Seo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04563">Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision Transformer(ViT) is one of the most widely used models in the computer vision field with its great performance on various tasks. In order to fully utilize the ViT-based architecture in various applications, proper visualization methods with a decent localization performance are necessary, but these methods employed in CNN-based models are still not available in ViT due to its unique structure. In this work, we propose an attention-guided visualization method applied to ViT that provides a high-level semantic explanation for its decision. Our method selectively aggregates the gradients directly propagated from the classification output to each self-attention, collecting the contribution of image features extracted from each location of the input image. These gradients are additionally guided by the normalized self-attention scores, which are the pairwise patch correlation scores. They are used to supplement the gradients on the patch-level context information efficiently detected by the self-attention mechanism. This approach of our method provides elaborate high-level semantic explanations with great localization performance only with the class labels. As a result, our method outperforms the previous leading explainability methods of ViT in the weakly-supervised localization task and presents great capability in capturing the full instances of the target class object. Meanwhile, our method provides a visualization that faithfully explains the model, which is demonstrated in the perturbation comparison test.
<div id='section'>Paperid: <span id='pid'>210, <a href='https://arxiv.org/pdf/2402.01155.pdf' target='_blank'>https://arxiv.org/pdf/2402.01155.pdf</a></span>   <span><a href='https://github.com/Sohanpatnaik106/CABINET_QA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sohan Patnaik, Heril Changwal, Milan Aggarwal, Sumit Bhatia, Yaman Kumar, Balaji Krishnamurthy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.01155">CABINET: Content Relevance based Noise Reduction for Table Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Table understanding capability of Large Language Models (LLMs) has been extensively studied through the task of question-answering (QA) over tables. Typically, only a small part of the whole table is relevant to derive the answer for a given question. The irrelevant parts act as noise and are distracting information, resulting in sub-optimal performance due to the vulnerability of LLMs to noise. To mitigate this, we propose CABINET (Content RelevAnce-Based NoIse ReductioN for TablE QuesTion-Answering) - a framework to enable LLMs to focus on relevant tabular data by suppressing extraneous information. CABINET comprises an Unsupervised Relevance Scorer (URS), trained differentially with the QA LLM, that weighs the table content based on its relevance to the input question before feeding it to the question-answering LLM (QA LLM). To further aid the relevance scorer, CABINET employs a weakly supervised module that generates a parsing statement describing the criteria of rows and columns relevant to the question and highlights the content of corresponding table cells. CABINET significantly outperforms various tabular LLM baselines, as well as GPT3-based in-context learning methods, is more robust to noise, maintains outperformance on tables of varying sizes, and establishes new SoTA performance on WikiTQ, FeTaQA, and WikiSQL datasets. We release our code and datasets at https://github.com/Sohanpatnaik106/CABINET_QA.
<div id='section'>Paperid: <span id='pid'>211, <a href='https://arxiv.org/pdf/2401.17828.pdf' target='_blank'>https://arxiv.org/pdf/2401.17828.pdf</a></span>   <span><a href='https://github.com/RozhanAhmadi/SWTformer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rozhan Ahmadi, Shohreh Kasaei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.17828">Leveraging Swin Transformer for Local-to-Global Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, weakly supervised semantic segmentation using image-level labels as supervision has received significant attention in the field of computer vision. Most existing methods have addressed the challenges arising from the lack of spatial information in these labels by focusing on facilitating supervised learning through the generation of pseudo-labels from class activation maps (CAMs). Due to the localized pattern detection of CNNs, CAMs often emphasize only the most discriminative parts of an object, making it challenging to accurately distinguish foreground objects from each other and the background. Recent studies have shown that Vision Transformer (ViT) features, due to their global view, are more effective in capturing the scene layout than CNNs. However, the use of hierarchical ViTs has not been extensively explored in this field. This work explores the use of Swin Transformer by proposing "SWTformer" to enhance the accuracy of the initial seed CAMs by bringing local and global views together. SWTformer-V1 generates class probabilities and CAMs using only the patch tokens as features. SWTformer-V2 incorporates a multi-scale feature fusion mechanism to extract additional information and utilizes a background-aware mechanism to generate more accurate localization maps with improved cross-object discrimination. Based on experiments on the PascalVOC 2012 dataset, SWTformer-V1 achieves a 0.98% mAP higher localization accuracy, outperforming state-of-the-art models. It also yields comparable performance by 0.82% mIoU on average higher than other methods in generating initial localization maps, depending only on the classification network. SWTformer-V2 further improves the accuracy of the generated seed CAMs by 5.32% mIoU, further proving the effectiveness of the local-to-global view provided by the Swin transformer. Code available at: https://github.com/RozhanAhmadi/SWTformer
<div id='section'>Paperid: <span id='pid'>212, <a href='https://arxiv.org/pdf/2401.14442.pdf' target='_blank'>https://arxiv.org/pdf/2401.14442.pdf</a></span>   <span><a href='https://github.com/AstraZeneca/SelfPAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Talip Ucar, Aubin Ramon, Dino Oglic, Rebecca Croasdale-Wood, Tom Diethe, Pietro Sormanni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.14442">Improving Antibody Humanness Prediction using Patent Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We investigate the potential of patent data for improving the antibody humanness prediction using a multi-stage, multi-loss training process. Humanness serves as a proxy for the immunogenic response to antibody therapeutics, one of the major causes of attrition in drug discovery and a challenging obstacle for their use in clinical settings. We pose the initial learning stage as a weakly-supervised contrastive-learning problem, where each antibody sequence is associated with possibly multiple identifiers of function and the objective is to learn an encoder that groups them according to their patented properties. We then freeze a part of the contrastive encoder and continue training it on the patent data using the cross-entropy loss to predict the humanness score of a given antibody sequence. We illustrate the utility of the patent data and our approach by performing inference on three different immunogenicity datasets, unseen during training. Our empirical results demonstrate that the learned model consistently outperforms the alternative baselines and establishes new state-of-the-art on five out of six inference tasks, irrespective of the used metric.
<div id='section'>Paperid: <span id='pid'>213, <a href='https://arxiv.org/pdf/2401.11791.pdf' target='_blank'>https://arxiv.org/pdf/2401.11791.pdf</a></span>   <span><a href='https://projectdisr.github.io/semples/' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/NVlabs/SemPLeS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ci-Siang Lin, Chien-Yi Wang, Yu-Chiang Frank Wang, Min-Hung Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11791">Semantic Prompt Learning for Weakly-Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-Supervised Semantic Segmentation (WSSS) aims to train segmentation models using image data with only image-level supervision. Since precise pixel-level annotations are not accessible, existing methods typically focus on producing pseudo masks for training segmentation models by refining CAM-like heatmaps. However, the produced heatmaps may capture only the discriminative image regions of object categories or the associated co-occurring backgrounds. To address the issues, we propose a Semantic Prompt Learning for WSSS (SemPLeS) framework, which learns to effectively prompt the CLIP latent space to enhance the semantic alignment between the segmented regions and the target object categories. More specifically, we propose Contrastive Prompt Learning and Prompt-guided Semantic Refinement to learn the prompts that adequately describe and suppress the co-occurring backgrounds associated with each object category. In this way, SemPLeS can perform better semantic alignment between object regions and class labels, resulting in desired pseudo masks for training segmentation models. The proposed SemPLeS framework achieves competitive performance on standard WSSS benchmarks, PASCAL VOC 2012 and MS COCO 2014, and shows compatibility with other WSSS methods. Code: https://github.com/NVlabs/SemPLeS.
<div id='section'>Paperid: <span id='pid'>214, <a href='https://arxiv.org/pdf/2401.11719.pdf' target='_blank'>https://arxiv.org/pdf/2401.11719.pdf</a></span>   <span><a href='https://github.com/Barrett-python/SFC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinqiao Zhao, Feilong Tang, Xiaoyang Wang, Jimin Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11719">SFC: Shared Feature Calibration in Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image-level weakly supervised semantic segmentation has received increasing attention due to its low annotation cost. Existing methods mainly rely on Class Activation Mapping (CAM) to obtain pseudo-labels for training semantic segmentation models. In this work, we are the first to demonstrate that long-tailed distribution in training data can cause the CAM calculated through classifier weights over-activated for head classes and under-activated for tail classes due to the shared features among head- and tail- classes. This degrades pseudo-label quality and further influences final semantic segmentation performance. To address this issue, we propose a Shared Feature Calibration (SFC) method for CAM generation. Specifically, we leverage the class prototypes that carry positive shared features and propose a Multi-Scaled Distribution-Weighted (MSDW) consistency loss for narrowing the gap between the CAMs generated through classifier weights and class prototypes during training. The MSDW loss counterbalances over-activation and under-activation by calibrating the shared features in head-/tail-class classifier weights. Experimental results show that our SFC significantly improves CAM boundaries and achieves new state-of-the-art performances. The project is available at https://github.com/Barrett-python/SFC.
<div id='section'>Paperid: <span id='pid'>215, <a href='https://arxiv.org/pdf/2401.11115.pdf' target='_blank'>https://arxiv.org/pdf/2401.11115.pdf</a></span>   <span><a href='https://nhathoang2002.github.io/MotionMix-page/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nhat M. Hoang, Kehong Gong, Chuan Guo, Michael Bi Mi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11115">MotionMix: Weakly-Supervised Diffusion for Controllable Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Controllable generation of 3D human motions becomes an important topic as the world embraces digital transformation. Existing works, though making promising progress with the advent of diffusion models, heavily rely on meticulously captured and annotated (e.g., text) high-quality motion corpus, a resource-intensive endeavor in the real world. This motivates our proposed MotionMix, a simple yet effective weakly-supervised diffusion model that leverages both noisy and unannotated motion sequences. Specifically, we separate the denoising objectives of a diffusion model into two stages: obtaining conditional rough motion approximations in the initial $T-T^*$ steps by learning the noisy annotated motions, followed by the unconditional refinement of these preliminary motions during the last $T^*$ steps using unannotated motions. Notably, though learning from two sources of imperfect data, our model does not compromise motion generation quality compared to fully supervised approaches that access gold data. Extensive experiments on several benchmarks demonstrate that our MotionMix, as a versatile framework, consistently achieves state-of-the-art performances on text-to-motion, action-to-motion, and music-to-dance tasks. Project page: https://nhathoang2002.github.io/MotionMix-page/
<div id='section'>Paperid: <span id='pid'>216, <a href='https://arxiv.org/pdf/2401.09883.pdf' target='_blank'>https://arxiv.org/pdf/2401.09883.pdf</a></span>   <span><a href='https://github.com/CVI-SZU/QA-CLIMS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Songhe Deng, Wei Zhuo, Jinheng Xie, Linlin Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.09883">Question-Answer Cross Language Image Matching for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Class Activation Map (CAM) has emerged as a popular tool for weakly supervised semantic segmentation (WSSS), allowing the localization of object regions in an image using only image-level labels. However, existing CAM methods suffer from under-activation of target object regions and false-activation of background regions due to the fact that a lack of detailed supervision can hinder the model's ability to understand the image as a whole. In this paper, we propose a novel Question-Answer Cross-Language-Image Matching framework for WSSS (QA-CLIMS), leveraging the vision-language foundation model to maximize the text-based understanding of images and guide the generation of activation maps. First, a series of carefully designed questions are posed to the VQA (Visual Question Answering) model with Question-Answer Prompt Engineering (QAPE) to generate a corpus of both foreground target objects and backgrounds that are adaptive to query images. We then employ contrastive learning in a Region Image Text Contrastive (RITC) network to compare the obtained foreground and background regions with the generated corpus. Our approach exploits the rich textual information from the open vocabulary as additional supervision, enabling the model to generate high-quality CAMs with a more complete object region and reduce false-activation of background regions. We conduct extensive analysis to validate the proposed method and show that our approach performs state-of-the-art on both PASCAL VOC 2012 and MS COCO datasets. Code is available at: https://github.com/CVI-SZU/QA-CLIMS
<div id='section'>Paperid: <span id='pid'>217, <a href='https://arxiv.org/pdf/2401.07437.pdf' target='_blank'>https://arxiv.org/pdf/2401.07437.pdf</a></span>   <span><a href='https://github.com/hust-linyi/bonus' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Lin, Zeyu Wang, Dong Zhang, Kwang-Ting Cheng, Hao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.07437">BoNuS: Boundary Mining for Nuclei Segmentation with Partial Point Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nuclei segmentation is a fundamental prerequisite in the digital pathology workflow. The development of automated methods for nuclei segmentation enables quantitative analysis of the wide existence and large variances in nuclei morphometry in histopathology images. However, manual annotation of tens of thousands of nuclei is tedious and time-consuming, which requires significant amount of human effort and domain-specific expertise. To alleviate this problem, in this paper, we propose a weakly-supervised nuclei segmentation method that only requires partial point labels of nuclei. Specifically, we propose a novel boundary mining framework for nuclei segmentation, named BoNuS, which simultaneously learns nuclei interior and boundary information from the point labels. To achieve this goal, we propose a novel boundary mining loss, which guides the model to learn the boundary information by exploring the pairwise pixel affinity in a multiple-instance learning manner. Then, we consider a more challenging problem, i.e., partial point label, where we propose a nuclei detection module with curriculum learning to detect the missing nuclei with prior morphological knowledge. The proposed method is validated on three public datasets, MoNuSeg, CPM, and CoNIC datasets. Experimental results demonstrate the superior performance of our method to the state-of-the-art weakly-supervised nuclei segmentation methods. Code: https://github.com/hust-linyi/bonus.
<div id='section'>Paperid: <span id='pid'>218, <a href='https://arxiv.org/pdf/2401.02584.pdf' target='_blank'>https://arxiv.org/pdf/2401.02584.pdf</a></span>   <span><a href='https://github.com/wsntxxn/TextToAudioGrounding' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuenan Xu, Ziyang Ma, Mengyue Wu, Kai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.02584">Towards Weakly Supervised Text-to-Audio Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-audio grounding (TAG) task aims to predict the onsets and offsets of sound events described by natural language. This task can facilitate applications such as multimodal information retrieval. This paper focuses on weakly-supervised text-to-audio grounding (WSTAG), where frame-level annotations of sound events are unavailable, and only the caption of a whole audio clip can be utilized for training. WSTAG is superior to strongly-supervised approaches in its scalability to large audio-text datasets. Two WSTAG frameworks are studied in this paper: sentence-level and phrase-level. First, we analyze the limitations of mean pooling used in the previous WSTAG approach and investigate the effects of different pooling strategies. We then propose phrase-level WSTAG to use matching labels between audio clips and phrases for training. Advanced negative sampling strategies and self-supervision are proposed to enhance the accuracy of the weak labels and provide pseudo strong labels. Experimental results show that our system significantly outperforms the previous WSTAG SOTA. Finally, we conduct extensive experiments to analyze the effects of several factors on phrase-level WSTAG. The code and model is available at https://github.com/wsntxxn/TextToAudioGrounding.
<div id='section'>Paperid: <span id='pid'>219, <a href='https://arxiv.org/pdf/2312.16578.pdf' target='_blank'>https://arxiv.org/pdf/2312.16578.pdf</a></span>   <span><a href='https://github.com/Sunny599/AAAI24-3DWSSG-MMA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiawei Li, Qingyuan Xu, Jing Zhang, Tianyi Zhang, Qian Yu, Lu Sheng, Dong Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.16578">Multi-modality Affinity Inference for Weakly Supervised 3D Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D point cloud semantic segmentation has a wide range of applications. Recently, weakly supervised point cloud segmentation methods have been proposed, aiming to alleviate the expensive and laborious manual annotation process by leveraging scene-level labels. However, these methods have not effectively exploited the rich geometric information (such as shape and scale) and appearance information (such as color and texture) present in RGB-D scans. Furthermore, current approaches fail to fully leverage the point affinity that can be inferred from the feature extraction network, which is crucial for learning from weak scene-level labels. Additionally, previous work overlooks the detrimental effects of the long-tailed distribution of point cloud data in weakly supervised 3D semantic segmentation. To this end, this paper proposes a simple yet effective scene-level weakly supervised point cloud segmentation method with a newly introduced multi-modality point affinity inference module. The point affinity proposed in this paper is characterized by features from multiple modalities (e.g., point cloud and RGB), and is further refined by normalizing the classifier weights to alleviate the detrimental effects of long-tailed distribution without the need of the prior of category distribution. Extensive experiments on the ScanNet and S3DIS benchmarks verify the effectiveness of our proposed method, which outperforms the state-of-the-art by ~4% to ~6% mIoU. Codes are released at https://github.com/Sunny599/AAAI24-3DWSSG-MMA.
<div id='section'>Paperid: <span id='pid'>220, <a href='https://arxiv.org/pdf/2312.16388.pdf' target='_blank'>https://arxiv.org/pdf/2312.16388.pdf</a></span>   <span><a href='https://github.com/sunoh-kim/pps' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sunoh Kim, Jungchan Cho, Joonsang Yu, YoungJoon Yoo, Jin Young Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.16388">Gaussian Mixture Proposals with Pull-Push Learning Scheme to Capture Diverse Events for Weakly Supervised Temporal Video Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the weakly supervised temporal video grounding study, previous methods use predetermined single Gaussian proposals which lack the ability to express diverse events described by the sentence query. To enhance the expression ability of a proposal, we propose a Gaussian mixture proposal (GMP) that can depict arbitrary shapes by learning importance, centroid, and range of every Gaussian in the mixture. In learning GMP, each Gaussian is not trained in a feature space but is implemented over a temporal location. Thus the conventional feature-based learning for Gaussian mixture model is not valid for our case. In our special setting, to learn moderately coupled Gaussian mixture capturing diverse events, we newly propose a pull-push learning scheme using pulling and pushing losses, each of which plays an opposite role to the other. The effects of components in our scheme are verified in-depth with extensive ablation studies and the overall scheme achieves state-of-the-art performance. Our code is available at https://github.com/sunoh-kim/pps.
<div id='section'>Paperid: <span id='pid'>221, <a href='https://arxiv.org/pdf/2312.14138.pdf' target='_blank'>https://arxiv.org/pdf/2312.14138.pdf</a></span>   <span><a href='https://github.com/Qinying-Liu/CASE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qinying Liu, Zilei Wang, Shenghai Rong, Junjie Li, Yixin Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.14138">Revisiting Foreground and Background Separation in Weakly-supervised Temporal Action Localization: A Clustering-based Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised temporal action localization aims to localize action instances in videos with only video-level action labels. Existing methods mainly embrace a localization-by-classification pipeline that optimizes the snippet-level prediction with a video classification loss. However, this formulation suffers from the discrepancy between classification and detection, resulting in inaccurate separation of foreground and background (F\&B) snippets. To alleviate this problem, we propose to explore the underlying structure among the snippets by resorting to unsupervised snippet clustering, rather than heavily relying on the video classification loss. Specifically, we propose a novel clustering-based F\&B separation algorithm. It comprises two core components: a snippet clustering component that groups the snippets into multiple latent clusters and a cluster classification component that further classifies the cluster as foreground or background. As there are no ground-truth labels to train these two components, we introduce a unified self-labeling mechanism based on optimal transport to produce high-quality pseudo-labels that match several plausible prior distributions. This ensures that the cluster assignments of the snippets can be accurately associated with their F\&B labels, thereby boosting the F\&B separation. We evaluate our method on three benchmarks: THUMOS14, ActivityNet v1.2 and v1.3. Our method achieves promising performance on all three benchmarks while being significantly more lightweight than previous methods. Code is available at https://github.com/Qinying-Liu/CASE
<div id='section'>Paperid: <span id='pid'>222, <a href='https://arxiv.org/pdf/2312.13646.pdf' target='_blank'>https://arxiv.org/pdf/2312.13646.pdf</a></span>   <span><a href='https://github.com/k0u-id/CARB' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongseob Kim, Seungho Lee, Junsuk Choe, Hyunjung Shim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.13646">Weakly Supervised Semantic Segmentation for Driving Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>State-of-the-art techniques in weakly-supervised semantic segmentation (WSSS) using image-level labels exhibit severe performance degradation on driving scene datasets such as Cityscapes. To address this challenge, we develop a new WSSS framework tailored to driving scene datasets. Based on extensive analysis of dataset characteristics, we employ Contrastive Language-Image Pre-training (CLIP) as our baseline to obtain pseudo-masks. However, CLIP introduces two key challenges: (1) pseudo-masks from CLIP lack in representing small object classes, and (2) these masks contain notable noise. We propose solutions for each issue as follows. (1) We devise Global-Local View Training that seamlessly incorporates small-scale patches during model training, thereby enhancing the model's capability to handle small-sized yet critical objects in driving scenes (e.g., traffic light). (2) We introduce Consistency-Aware Region Balancing (CARB), a novel technique that discerns reliable and noisy regions through evaluating the consistency between CLIP masks and segmentation predictions. It prioritizes reliable pixels over noisy pixels via adaptive loss weighting. Notably, the proposed method achieves 51.8\% mIoU on the Cityscapes test dataset, showcasing its potential as a strong WSSS baseline on driving scene datasets. Experimental results on CamVid and WildDash2 demonstrate the effectiveness of our method across diverse datasets, even with small-scale datasets or visually challenging conditions. The code is available at https://github.com/k0u-id/CARB.
<div id='section'>Paperid: <span id='pid'>223, <a href='https://arxiv.org/pdf/2312.12828.pdf' target='_blank'>https://arxiv.org/pdf/2312.12828.pdf</a></span>   <span><a href='https://github.com/linyq2117/TagCLIP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqi Lin, Minghao Chen, Kaipeng Zhang, Hengjia Li, Mingming Li, Zheng Yang, Dongqin Lv, Binbin Lin, Haifeng Liu, Deng Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.12828">TagCLIP: A Local-to-Global Framework to Enhance Open-Vocabulary Multi-Label Classification of CLIP Without Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contrastive Language-Image Pre-training (CLIP) has demonstrated impressive capabilities in open-vocabulary classification. The class token in the image encoder is trained to capture the global features to distinguish different text descriptions supervised by contrastive loss, making it highly effective for single-label classification. However, it shows poor performance on multi-label datasets because the global feature tends to be dominated by the most prominent class and the contrastive nature of softmax operation aggravates it. In this study, we observe that the multi-label classification results heavily rely on discriminative local features but are overlooked by CLIP. As a result, we dissect the preservation of patch-wise spatial information in CLIP and proposed a local-to-global framework to obtain image tags. It comprises three steps: (1) patch-level classification to obtain coarse scores; (2) dual-masking attention refinement (DMAR) module to refine the coarse scores; (3) class-wise reidentification (CWR) module to remedy predictions from a global perspective. This framework is solely based on frozen CLIP and significantly enhances its multi-label classification performance on various benchmarks without dataset-specific training. Besides, to comprehensively assess the quality and practicality of generated tags, we extend their application to the downstream task, i.e., weakly supervised semantic segmentation (WSSS) with generated tags as image-level pseudo labels. Experiments demonstrate that this classify-then-segment paradigm dramatically outperforms other annotation-free segmentation methods and validates the effectiveness of generated tags. Our code is available at https://github.com/linyq2117/TagCLIP.
<div id='section'>Paperid: <span id='pid'>224, <a href='https://arxiv.org/pdf/2312.08916.pdf' target='_blank'>https://arxiv.org/pdf/2312.08916.pdf</a></span>   <span><a href='https://github.com/Jessie459/feature-self-reinforcement' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingxuan He, Lechao Cheng, Chaowei Fang, Zunlei Feng, Tingting Mu, Mingli Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.08916">Progressive Feature Self-reinforcement for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Compared to conventional semantic segmentation with pixel-level supervision, Weakly Supervised Semantic Segmentation (WSSS) with image-level labels poses the challenge that it always focuses on the most discriminative regions, resulting in a disparity between fully supervised conditions. A typical manifestation is the diminished precision on the object boundaries, leading to a deteriorated accuracy of WSSS. To alleviate this issue, we propose to adaptively partition the image content into deterministic regions (e.g., confident foreground and background) and uncertain regions (e.g., object boundaries and misclassified categories) for separate processing. For uncertain cues, we employ an activation-based masking strategy and seek to recover the local information with self-distilled knowledge. We further assume that the unmasked confident regions should be robust enough to preserve the global semantics. Building upon this, we introduce a complementary self-enhancement method that constrains the semantic consistency between these confident regions and an augmented image with the same class labels. Extensive experiments conducted on PASCAL VOC 2012 and MS COCO 2014 demonstrate that our proposed single-stage approach for WSSS not only outperforms state-of-the-art benchmarks remarkably but also surpasses multi-stage methodologies that trade complexity for accuracy. The code can be found at \url{https://github.com/Jessie459/feature-self-reinforcement}.
<div id='section'>Paperid: <span id='pid'>225, <a href='https://arxiv.org/pdf/2312.07530.pdf' target='_blank'>https://arxiv.org/pdf/2312.07530.pdf</a></span>   <span><a href='https://github.com/kuanchihhuang/VG-W3D' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/kuanchihhuang/VG-W3D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kuan-Chih Huang, Yi-Hsuan Tsai, Ming-Hsuan Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.07530">Weakly Supervised 3D Object Detection via Multi-Level Visual Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised 3D object detection aims to learn a 3D detector with lower annotation cost, e.g., 2D labels. Unlike prior work which still relies on few accurate 3D annotations, we propose a framework to study how to leverage constraints between 2D and 3D domains without requiring any 3D labels. Specifically, we employ visual data from three perspectives to establish connections between 2D and 3D domains. First, we design a feature-level constraint to align LiDAR and image features based on object-aware regions. Second, the output-level constraint is developed to enforce the overlap between 2D and projected 3D box estimations. Finally, the training-level constraint is utilized by producing accurate and consistent 3D pseudo-labels that align with the visual data. We conduct extensive experiments on the KITTI dataset to validate the effectiveness of the proposed three constraints. Without using any 3D labels, our method achieves favorable performance against state-of-the-art approaches and is competitive with the method that uses 500-frame 3D annotations. Code will be made publicly available at https://github.com/kuanchihhuang/VG-W3D.
<div id='section'>Paperid: <span id='pid'>226, <a href='https://arxiv.org/pdf/2312.07374.pdf' target='_blank'>https://arxiv.org/pdf/2312.07374.pdf</a></span>   <span><a href='https://lwpyh.github.io/GenSAM/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jian Hu, Jiayi Lin, Weitong Cai, Shaogang Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.07374">Relax Image-Specific Prompt Requirement in SAM: A Single Generic Prompt for Segmenting Camouflaged Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Camouflaged object detection (COD) approaches heavily rely on pixel-level annotated datasets. Weakly-supervised COD (WSCOD) approaches use sparse annotations like scribbles or points to reduce annotation effort, but this can lead to decreased accuracy. The Segment Anything Model (SAM) shows remarkable segmentation ability with sparse prompts like points. However, manual prompt is not always feasible, as it may not be accessible in real-world application. Additionally, it only provides localization information instead of semantic one, which can intrinsically cause ambiguity in interpreting the targets. In this work, we aim to eliminate the need for manual prompt. The key idea is to employ Cross-modal Chains of Thought Prompting (CCTP) to reason visual prompts using the semantic information given by a generic text prompt. To that end, we introduce a test-time adaptation per-instance mechanism called Generalizable SAM (GenSAM) to automatically enerate and optimize visual prompts the generic task prompt for WSCOD. In particular, CCTP maps a single generic text prompt onto image-specific consensus foreground and background heatmaps using vision-language models, acquiring reliable visual prompts. Moreover, to test-time adapt the visual prompts, we further propose Progressive Mask Generation (PMG) to iteratively reweight the input image, guiding the model to focus on the targets in a coarse-to-fine manner. Crucially, all network parameters are fixed, avoiding the need for additional training. Experiments demonstrate the superiority of GenSAM. Experiments on three benchmarks demonstrate that GenSAM outperforms point supervision approaches and achieves comparable results to scribble supervision ones, solely relying on general task descriptions as prompts. our codes is in: https://lwpyh.github.io/GenSAM/.
<div id='section'>Paperid: <span id='pid'>227, <a href='https://arxiv.org/pdf/2312.07169.pdf' target='_blank'>https://arxiv.org/pdf/2312.07169.pdf</a></span>   <span><a href='https://github.com/AKASH2907/semi-sup-active-learning' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/AKASH2907/semi-sup-active-learning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ayush Singh, Aayush J Rana, Akash Kumar, Shruti Vyas, Yogesh Singh Rawat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.07169">Semi-supervised Active Learning for Video Action Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we focus on label efficient learning for video action detection. We develop a novel semi-supervised active learning approach which utilizes both labeled as well as unlabeled data along with informative sample selection for action detection. Video action detection requires spatio-temporal localization along with classification, which poses several challenges for both active learning informative sample selection as well as semi-supervised learning pseudo label generation. First, we propose NoiseAug, a simple augmentation strategy which effectively selects informative samples for video action detection. Next, we propose fft-attention, a novel technique based on high-pass filtering which enables effective utilization of pseudo label for SSL in video action detection by emphasizing on relevant activity region within a video. We evaluate the proposed approach on three different benchmark datasets, UCF-101-24, JHMDB-21, and Youtube-VOS. First, we demonstrate its effectiveness on video action detection where the proposed approach outperforms prior works in semi-supervised and weakly-supervised learning along with several baseline approaches in both UCF101-24 and JHMDB-21. Next, we also show its effectiveness on Youtube-VOS for video object segmentation demonstrating its generalization capability for other dense prediction tasks in videos. The code and models is publicly available at: \url{https://github.com/AKASH2907/semi-sup-active-learning}.
<div id='section'>Paperid: <span id='pid'>228, <a href='https://arxiv.org/pdf/2312.06988.pdf' target='_blank'>https://arxiv.org/pdf/2312.06988.pdf</a></span>   <span><a href='https://github.com/jiangxb98/mwsis-plugin' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangfeng Jiang, Jun Liu, Yuzhi Wu, Wenlong Liao, Tao He, Pai Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.06988">MWSIS: Multimodal Weakly Supervised Instance Segmentation with 2D Box Annotations for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Instance segmentation is a fundamental research in computer vision, especially in autonomous driving. However, manual mask annotation for instance segmentation is quite time-consuming and costly. To address this problem, some prior works attempt to apply weakly supervised manner by exploring 2D or 3D boxes. However, no one has ever successfully segmented 2D and 3D instances simultaneously by only using 2D box annotations, which could further reduce the annotation cost by an order of magnitude. Thus, we propose a novel framework called Multimodal Weakly Supervised Instance Segmentation (MWSIS), which incorporates various fine-grained label generation and correction modules for both 2D and 3D modalities to improve the quality of pseudo labels, along with a new multimodal cross-supervision approach, named Consistency Sparse Cross-modal Supervision (CSCS), to reduce the inconsistency of multimodal predictions by response distillation. Particularly, transferring the 3D backbone to downstream tasks not only improves the performance of the 3D detectors, but also outperforms fully supervised instance segmentation with only 5% fully supervised annotations. On the Waymo dataset, the proposed framework demonstrates significant improvements over the baseline, especially achieving 2.59% mAP and 12.75% mAP increases for 2D and 3D instance segmentation tasks, respectively. The code is available at https://github.com/jiangxb98/mwsis-plugin.
<div id='section'>Paperid: <span id='pid'>229, <a href='https://arxiv.org/pdf/2312.06049.pdf' target='_blank'>https://arxiv.org/pdf/2312.06049.pdf</a></span>   <span><a href='https://github.com/guotengg/SSPNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jifeng Shen, Teng Guo, Xin Zuo, Heng Fan, Wankou Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.06049">SSPNet: Scale and Spatial Priors Guided Generalizable and Interpretable Pedestrian Attribute Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Global feature based Pedestrian Attribute Recognition (PAR) models are often poorly localized when using Grad-CAM for attribute response analysis, which has a significant impact on the interpretability, generalizability and performance. Previous researches have attempted to improve generalization and interpretation through meticulous model design, yet they often have neglected or underutilized effective prior information crucial for PAR. To this end, a novel Scale and Spatial Priors Guided Network (SSPNet) is proposed for PAR, which is mainly composed of the Adaptive Feature Scale Selection (AFSS) and Prior Location Extraction (PLE) modules. The AFSS module learns to provide reasonable scale prior information for different attribute groups, allowing the model to focus on different levels of feature maps with varying semantic granularity. The PLE module reveals potential attribute spatial prior information, which avoids unnecessary attention on irrelevant areas and lowers the risk of model over-fitting. More specifically, the scale prior in AFSS is adaptively learned from different layers of feature pyramid with maximum accuracy, while the spatial priors in PLE can be revealed from part feature with different granularity (such as image blocks, human pose keypoint and sparse sampling points). Besides, a novel IoU based attribute localization metric is proposed for Weakly-supervised Pedestrian Attribute Localization (WPAL) based on the improved Grad-CAM for attribute response mask. The experimental results on the intra-dataset and cross-dataset evaluations demonstrate the effectiveness of our proposed method in terms of mean accuracy (mA). Furthermore, it also achieves superior performance on the PCS dataset for attribute localization in terms of IoU. Code will be released at https://github.com/guotengg/SSPNet.
<div id='section'>Paperid: <span id='pid'>230, <a href='https://arxiv.org/pdf/2312.05923.pdf' target='_blank'>https://arxiv.org/pdf/2312.05923.pdf</a></span>   <span><a href='https://github.com/streamer-AP/CGNet' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/streamer-AP/CGNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyan Liu, Guorong Li, Yuankai Qi, Ziheng Yan, Zhenjun Han, Anton van den Hengel, Ming-Hsuan Yang, Qingming Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.05923">Weakly Supervised Video Individual CountingWeakly Supervised Video Individual Counting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Individual Counting (VIC) aims to predict the number of unique individuals in a single video. % Existing methods learn representations based on trajectory labels for individuals, which are annotation-expensive. % To provide a more realistic reflection of the underlying practical challenge, we introduce a weakly supervised VIC task, wherein trajectory labels are not provided. Instead, two types of labels are provided to indicate traffic entering the field of view (inflow) and leaving the field view (outflow). % We also propose the first solution as a baseline that formulates the task as a weakly supervised contrastive learning problem under group-level matching. In doing so, we devise an end-to-end trainable soft contrastive loss to drive the network to distinguish inflow, outflow, and the remaining. % To facilitate future study in this direction, we generate annotations from the existing VIC datasets SenseCrowd and CroHD and also build a new dataset, UAVVIC. % Extensive results show that our baseline weakly supervised method outperforms supervised methods, and thus, little information is lost in the transition to the more practically relevant weakly supervised task. The code and trained model will be public at \href{https://github.com/streamer-AP/CGNet}{CGNet}
<div id='section'>Paperid: <span id='pid'>231, <a href='https://arxiv.org/pdf/2312.05490.pdf' target='_blank'>https://arxiv.org/pdf/2312.05490.pdf</a></span>   <span><a href='https://github.com/RenaoYan/PMIL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Renao Yan, Qiehe Sun, Cheng Jin, Yiqing Liu, Yonghong He, Tian Guan, Hao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.05490">Shapley Values-enabled Progressive Pseudo Bag Augmentation for Whole Slide Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In computational pathology, whole-slide image (WSI) classification presents a formidable challenge due to its gigapixel resolution and limited fine-grained annotations. Multiple-instance learning (MIL) offers a weakly supervised solution, yet refining instance-level information from bag-level labels remains challenging. While most of the conventional MIL methods use attention scores to estimate instance importance scores (IIS) which contribute to the prediction of the slide labels, these often lead to skewed attention distributions and inaccuracies in identifying crucial instances. To address these issues, we propose a new approach inspired by cooperative game theory: employing Shapley values to assess each instance's contribution, thereby improving IIS estimation. The computation of the Shapley value is then accelerated using attention, meanwhile retaining the enhanced instance identification and prioritization. We further introduce a framework for the progressive assignment of pseudo bags based on estimated IIS, encouraging more balanced attention distributions in MIL models. Our extensive experiments on CAMELYON-16, BRACS, TCGA-LUNG, and TCGA-BRCA datasets show our method's superiority over existing state-of-the-art approaches, offering enhanced interpretability and class-wise insights. Our source code is available at https://github.com/RenaoYan/PMIL.
<div id='section'>Paperid: <span id='pid'>232, <a href='https://arxiv.org/pdf/2312.04554.pdf' target='_blank'>https://arxiv.org/pdf/2312.04554.pdf</a></span>   <span><a href='https://catherine-r-he.github.io/SelfEQ/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruozhen He, Paola Cascante-Bonilla, Ziyan Yang, Alexander C. Berg, Vicente Ordonez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.04554">Improved Visual Grounding through Self-Consistent Explanations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-language models trained to match images with text can be combined with visual explanation methods to point to the locations of specific objects in an image. Our work shows that the localization --"grounding"-- abilities of these models can be further improved by finetuning for self-consistent visual explanations. We propose a strategy for augmenting existing text-image datasets with paraphrases using a large language model, and SelfEQ, a weakly-supervised strategy on visual explanation maps for paraphrases that encourages self-consistency. Specifically, for an input textual phrase, we attempt to generate a paraphrase and finetune the model so that the phrase and paraphrase map to the same region in the image. We posit that this both expands the vocabulary that the model is able to handle, and improves the quality of the object locations highlighted by gradient-based visual explanation methods (e.g. GradCAM). We demonstrate that SelfEQ improves performance on Flickr30k, ReferIt, and RefCOCO+ over a strong baseline method and several prior works. Particularly, comparing to other methods that do not use any type of box annotations, we obtain 84.07% on Flickr30k (an absolute improvement of 4.69%), 67.40% on ReferIt (an absolute improvement of 7.68%), and 75.10%, 55.49% on RefCOCO+ test sets A and B respectively (an absolute improvement of 3.74% on average).
<div id='section'>Paperid: <span id='pid'>233, <a href='https://arxiv.org/pdf/2312.03585.pdf' target='_blank'>https://arxiv.org/pdf/2312.03585.pdf</a></span>   <span><a href='https://github.com/HAL-42/FMA-WSSS.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaobo Yang, Xiaojin Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.03585">Foundation Model Assisted Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work aims to leverage pre-trained foundation models, such as contrastive language-image pre-training (CLIP) and segment anything model (SAM), to address weakly supervised semantic segmentation (WSSS) using image-level labels. To this end, we propose a coarse-to-fine framework based on CLIP and SAM for generating high-quality segmentation seeds. Specifically, we construct an image classification task and a seed segmentation task, which are jointly performed by CLIP with frozen weights and two sets of learnable task-specific prompts. A SAM-based seeding (SAMS) module is designed and applied to each task to produce either coarse or fine seed maps. Moreover, we design a multi-label contrastive loss supervised by image-level labels and a CAM activation loss supervised by the generated coarse seed map. These losses are used to learn the prompts, which are the only parts need to be learned in our framework. Once the prompts are learned, we input each image along with the learned segmentation-specific prompts into CLIP and the SAMS module to produce high-quality segmentation seeds. These seeds serve as pseudo labels to train an off-the-shelf segmentation network like other two-stage WSSS methods. Experiments show that our method achieves the state-of-the-art performance on PASCAL VOC 2012 and competitive results on MS COCO 2014. Code is available at https://github.com/HAL-42/FMA-WSSS.git.
<div id='section'>Paperid: <span id='pid'>234, <a href='https://arxiv.org/pdf/2312.02366.pdf' target='_blank'>https://arxiv.org/pdf/2312.02366.pdf</a></span>   <span><a href='https://github.com/MohammedSB/DINOv2ForRadiology' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammed Baharoon, Waseem Qureshi, Jiahong Ouyang, Yanwu Xu, Abdulrhman Aljouie, Wei Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02366">Evaluating General Purpose Vision Foundation Models for Medical Image Analysis: An Experimental Study of DINOv2 on Radiology Benchmarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of deep learning systems into healthcare has been hindered by the resource-intensive process of data annotation and the inability of these systems to generalize to different data distributions. Foundation models, which are models pre-trained on large datasets, have emerged as a solution to reduce reliance on annotated data and enhance model generalizability and robustness. DINOv2 is an open-source foundation model pre-trained with self-supervised learning on 142 million curated natural images that exhibits promising capabilities across various vision tasks. Nevertheless, a critical question remains unanswered regarding DINOv2's adaptability to radiological imaging, and whether its features are sufficiently general to benefit radiology image analysis. Therefore, this study comprehensively evaluates the performance DINOv2 for radiology, conducting over 200 evaluations across diverse modalities (X-ray, CT, and MRI). To measure the effectiveness and generalizability of DINOv2's feature representations, we analyze the model across medical image analysis tasks including disease classification and organ segmentation on both 2D and 3D images, and under different settings like kNN, few-shot learning, linear-probing, end-to-end fine-tuning, and parameter-efficient fine-tuning. Comparative analyses with established supervised, self-supervised, and weakly-supervised models reveal DINOv2's superior performance and cross-task generalizability. The findings contribute insights to potential avenues for optimizing pre-training strategies for medical imaging and enhancing the broader understanding of DINOv2's role in bridging the gap between natural and radiological image analysis. Our code is available at https://github.com/MohammedSB/DINOv2ForRadiology
<div id='section'>Paperid: <span id='pid'>235, <a href='https://arxiv.org/pdf/2312.02317.pdf' target='_blank'>https://arxiv.org/pdf/2312.02317.pdf</a></span>   <span><a href='https://github.com/ruijie-wang-uzh/GNN2R' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruijie Wang, Luca Rossetto, Michael Cochez, Abraham Bernstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02317">GNN2R: Weakly-Supervised Rationale-Providing Question Answering over Knowledge Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most current methods for multi-hop question answering (QA) over knowledge graphs (KGs) only provide final conclusive answers without explanations, such as a set of KG entities that is difficult for normal users to review and comprehend. This issue severely limits the application of KG-based QA in real-world scenarios. However, it is non-trivial to solve due to two challenges: First, annotations of reasoning chains of multi-hop questions, which could serve as supervision for explanation generation, are usually lacking. Second, it is difficult to maintain high efficiency when explicit KG triples need to be retrieved to generate explanations. In this paper, we propose a novel Graph Neural Network-based Two-Step Reasoning model (GNN2R) to solve this issue. GNN2R can provide both final answers and reasoning subgraphs as a rationale behind final answers efficiently with only weak supervision that is available through question-final answer pairs. We extensively evaluated GNN2R with detailed analyses in experiments. The results demonstrate that, in terms of effectiveness, efficiency, and quality of generated explanations, GNN2R outperforms existing state-of-the-art methods that are applicable to this task. Our code and pre-trained models are available at https://github.com/ruijie-wang-uzh/GNN2R.
<div id='section'>Paperid: <span id='pid'>236, <a href='https://arxiv.org/pdf/2312.02208.pdf' target='_blank'>https://arxiv.org/pdf/2312.02208.pdf</a></span>   <span><a href='https://github.com/KangchengLiu' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kangcheng Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02208">A Data-efficient Framework for Robotics Large-scale LiDAR Scene Parsing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing state-of-the-art 3D point clouds understanding methods only perform well in a fully supervised manner. To the best of our knowledge, there exists no unified framework which simultaneously solves the downstream high-level understanding tasks, especially when labels are extremely limited. This work presents a general and simple framework to tackle point clouds understanding when labels are limited. We propose a novel unsupervised region expansion based clustering method for generating clusters. More importantly, we innovatively propose to learn to merge the over-divided clusters based on the local low-level geometric property similarities and the learned high-level feature similarities supervised by weak labels. Hence, the true weak labels guide pseudo labels merging taking both geometric and semantic feature correlations into consideration. Finally, the self-supervised reconstruction and data augmentation optimization modules are proposed to guide the propagation of labels among semantically similar points within a scene. Experimental Results demonstrate that our framework has the best performance among the three most important weakly supervised point clouds understanding tasks including semantic segmentation, instance segmentation, and object detection even when limited points are labeled, under the data-efficient settings for the large-scale 3D semantic scene parsing. The developed techniques have postentials to be applied to downstream tasks for better representations in robotic manipulation and robotic autonomous navigation. Codes and models are publicly available at: https://github.com/KangchengLiu.
<div id='section'>Paperid: <span id='pid'>237, <a href='https://arxiv.org/pdf/2312.01764.pdf' target='_blank'>https://arxiv.org/pdf/2312.01764.pdf</a></span>   <span><a href='https://github.com/ArielZc/DE-Net' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Zhang, Guorong Li, Yuankai Qi, Hanhua Ye, Laiyun Qing, Ming-Hsuan Yang, Qingming Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.01764">Dynamic Erasing Network Based on Multi-Scale Temporal Features for Weakly Supervised Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The goal of weakly supervised video anomaly detection is to learn a detection model using only video-level labeled data. However, prior studies typically divide videos into fixed-length segments without considering the complexity or duration of anomalies. Moreover, these studies usually just detect the most abnormal segments, potentially overlooking the completeness of anomalies. To address these limitations, we propose a Dynamic Erasing Network (DE-Net) for weakly supervised video anomaly detection, which learns multi-scale temporal features. Specifically, to handle duration variations of abnormal events, we first propose a multi-scale temporal modeling module, capable of extracting features from segments of varying lengths and capturing both local and global visual information across different temporal scales. Then, we design a dynamic erasing strategy, which dynamically assesses the completeness of the detected anomalies and erases prominent abnormal segments in order to encourage the model to discover gentle abnormal segments in a video. The proposed method obtains favorable performance compared to several state-of-the-art approaches on three datasets: XD-Violence, TAD, and UCF-Crime. Code will be made available at https://github.com/ArielZc/DE-Net.
<div id='section'>Paperid: <span id='pid'>238, <a href='https://arxiv.org/pdf/2311.18610.pdf' target='_blank'>https://arxiv.org/pdf/2311.18610.pdf</a></span>   <span><a href='https://daoyig.github.io/DiffCAD/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Daoyi Gao, DÃ¡vid Rozenberszki, Stefan Leutenegger, Angela Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.18610">DiffCAD: Weakly-Supervised Probabilistic CAD Model Retrieval and Alignment from an RGB Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Perceiving 3D structures from RGB images based on CAD model primitives can enable an effective, efficient 3D object-based representation of scenes. However, current approaches rely on supervision from expensive annotations of CAD models associated with real images, and encounter challenges due to the inherent ambiguities in the task -- both in depth-scale ambiguity in monocular perception, as well as inexact matches of CAD database models to real observations. We thus propose DiffCAD, the first weakly-supervised probabilistic approach to CAD retrieval and alignment from an RGB image. We formulate this as a conditional generative task, leveraging diffusion to learn implicit probabilistic models capturing the shape, pose, and scale of CAD objects in an image. This enables multi-hypothesis generation of different plausible CAD reconstructions, requiring only a few hypotheses to characterize ambiguities in depth/scale and inexact shape matches. Our approach is trained only on synthetic data, leveraging monocular depth and mask estimates to enable robust zero-shot adaptation to various real target domains. Despite being trained solely on synthetic data, our multi-hypothesis approach can even surpass the supervised state-of-the-art on the Scan2CAD dataset by 5.9% with 8 hypotheses.
<div id='section'>Paperid: <span id='pid'>239, <a href='https://arxiv.org/pdf/2311.17532.pdf' target='_blank'>https://arxiv.org/pdf/2311.17532.pdf</a></span>   <span><a href='https://xingqunqi-lab.github.io/Emo-Transition-Gesture/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingqun Qi, Jiahao Pan, Peng Li, Ruibin Yuan, Xiaowei Chi, Mengfei Li, Wenhan Luo, Wei Xue, Shanghang Zhang, Qifeng Liu, Yike Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17532">Weakly-Supervised Emotion Transition Learning for Diverse 3D Co-speech Gesture Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating vivid and emotional 3D co-speech gestures is crucial for virtual avatar animation in human-machine interaction applications. While the existing methods enable generating the gestures to follow a single emotion label, they overlook that long gesture sequence modeling with emotion transition is more practical in real scenes. In addition, the lack of large-scale available datasets with emotional transition speech and corresponding 3D human gestures also limits the addressing of this task. To fulfill this goal, we first incorporate the ChatGPT-4 and an audio inpainting approach to construct the high-fidelity emotion transition human speeches. Considering obtaining the realistic 3D pose annotations corresponding to the dynamically inpainted emotion transition audio is extremely difficult, we propose a novel weakly supervised training strategy to encourage authority gesture transitions. Specifically, to enhance the coordination of transition gestures w.r.t different emotional ones, we model the temporal association representation between two different emotional gesture sequences as style guidance and infuse it into the transition generation. We further devise an emotion mixture mechanism that provides weak supervision based on a learnable mixed emotion label for transition gestures. Last, we present a keyframe sampler to supply effective initial posture cues in long sequences, enabling us to generate diverse gestures. Extensive experiments demonstrate that our method outperforms the state-of-the-art models constructed by adapting single emotion-conditioned counterparts on our newly defined emotion transition task and datasets. Our code and dataset will be released on the project page: https://xingqunqi-lab.github.io/Emo-Transition-Gesture/.
<div id='section'>Paperid: <span id='pid'>240, <a href='https://arxiv.org/pdf/2311.16720.pdf' target='_blank'>https://arxiv.org/pdf/2311.16720.pdf</a></span>   <span><a href='https://github.com/Alibaba-NLP/RankingGPT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Longhui Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, Min Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.16720">A Two-Stage Adaptation of Large Language Models for Text Ranking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text ranking is a critical task in information retrieval. Recent advances in pre-trained language models (PLMs), especially large language models (LLMs), present new opportunities for applying them to text ranking. While supervised fine-tuning (SFT) with ranking data has been widely explored to better align PLMs with text ranking goals, previous studies have focused primarily on encoder-only and encoder-decoder PLMs. Research on leveraging decoder-only LLMs for text ranking remains scarce. An exception to this is RankLLaMA, which uses direct SFT to explore LLaMA's potential for text ranking. In this work, we propose a two-stage progressive paradigm to better adapt LLMs to text ranking. First, we conduct continual pre-training (CPT) of LLMs on a large weakly-supervised corpus. Second, we perform SFT, and propose an improved optimization strategy building upon RankLLaMA. Our experimental results on multiple benchmarks show that our approach outperforms previous methods in both in-domain and out-domain scenarios.
<div id='section'>Paperid: <span id='pid'>241, <a href='https://arxiv.org/pdf/2311.15367.pdf' target='_blank'>https://arxiv.org/pdf/2311.15367.pdf</a></span>   <span><a href='https://github.com/cool-xuan/BN-WVAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixuan Zhou, Yi Qu, Xing Xu, Fumin Shen, Jingkuan Song, Hengtao Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.15367">BatchNorm-based Weakly Supervised Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In weakly supervised video anomaly detection (WVAD), where only video-level labels indicating the presence or absence of abnormal events are available, the primary challenge arises from the inherent ambiguity in temporal annotations of abnormal occurrences. Inspired by the statistical insight that temporal features of abnormal events often exhibit outlier characteristics, we propose a novel method, BN-WVAD, which incorporates BatchNorm into WVAD. In the proposed BN-WVAD, we leverage the Divergence of Feature from Mean vector (DFM) of BatchNorm as a reliable abnormality criterion to discern potential abnormal snippets in abnormal videos. The proposed DFM criterion is also discriminative for anomaly recognition and more resilient to label noise, serving as the additional anomaly score to amend the prediction of the anomaly classifier that is susceptible to noisy labels. Moreover, a batch-level selection strategy is devised to filter more abnormal snippets in videos where more abnormal events occur. The proposed BN-WVAD model demonstrates state-of-the-art performance on UCF-Crime with an AUC of 87.24%, and XD-Violence, where AP reaches up to 84.93%. Our code implementation is accessible at https://github.com/cool-xuan/BN-WVAD.
<div id='section'>Paperid: <span id='pid'>242, <a href='https://arxiv.org/pdf/2311.14758.pdf' target='_blank'>https://arxiv.org/pdf/2311.14758.pdf</a></span>   <span><a href='https://github.com/yuyi1005/point2rbox-mmrotate' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Yu, Xue Yang, Qingyun Li, Feipeng Da, Jifeng Dai, Yu Qiao, Junchi Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.14758">Point2RBox: Combine Knowledge from Synthetic Visual Patterns for End-to-end Oriented Object Detection with Single Point Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapidly increasing demand for oriented object detection (OOD), recent research involving weakly-supervised detectors for learning rotated box (RBox) from the horizontal box (HBox) has attracted more and more attention. In this paper, we explore a more challenging yet label-efficient setting, namely single point-supervised OOD, and present our approach called Point2RBox. Specifically, we propose to leverage two principles: 1) Synthetic pattern knowledge combination: By sampling around each labeled point on the image, we spread the object feature to synthetic visual patterns with known boxes to provide the knowledge for box regression. 2) Transform self-supervision: With a transformed input image (e.g. scaled/rotated), the output RBoxes are trained to follow the same transformation so that the network can perceive the relative size/rotation between objects. The detector is further enhanced by a few devised techniques to cope with peripheral issues, e.g. the anchor/layer assignment as the size of the object is not available in our point supervision setting. To our best knowledge, Point2RBox is the first end-to-end solution for point-supervised OOD. In particular, our method uses a lightweight paradigm, yet it achieves a competitive performance among point-supervised alternatives, 41.05%/27.62%/80.01% on DOTA/DIOR/HRSC datasets.
<div id='section'>Paperid: <span id='pid'>243, <a href='https://arxiv.org/pdf/2311.13258.pdf' target='_blank'>https://arxiv.org/pdf/2311.13258.pdf</a></span>   <span><a href='https://github.com/Yangyi-Chen/vi-struct' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yangyi Chen, Xingyao Wang, Manling Li, Derek Hoiem, Heng Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.13258">ViStruct: Visual Structural Knowledge Extraction via Curriculum Guided Code-Vision Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>State-of-the-art vision-language models (VLMs) still have limited performance in structural knowledge extraction, such as relations between objects. In this work, we present ViStruct, a training framework to learn VLMs for effective visual structural knowledge extraction. Two novel designs are incorporated. First, we propose to leverage the inherent structure of programming language to depict visual structural information. This approach enables explicit and consistent representation of visual structural information of multiple granularities, such as concepts, relations, and events, in a well-organized structured format. Second, we introduce curriculum-based learning for VLMs to progressively comprehend visual structures, from fundamental visual concepts to intricate event structures. Our intuition is that lower-level knowledge may contribute to complex visual structure understanding. Furthermore, we compile and release a collection of datasets tailored for visual structural knowledge extraction. We adopt a weakly-supervised approach to directly generate visual event structures from captions for ViStruct training, capitalizing on abundant image-caption pairs from the web. In experiments, we evaluate ViStruct on visual structure prediction tasks, demonstrating its effectiveness in improving the understanding of visual structures. The code is public at \url{https://github.com/Yangyi-Chen/vi-struct}.
<div id='section'>Paperid: <span id='pid'>244, <a href='https://arxiv.org/pdf/2311.11273.pdf' target='_blank'>https://arxiv.org/pdf/2311.11273.pdf</a></span>   <span><a href='https://github.com/luckybird1994/MMCPF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lv Tang, Peng-Tao Jiang, Zhihao Shen, Hao Zhang, Jinwei Chen, Bo Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.11273">Chain of Visual Perception: Harnessing Multimodal Large Language Models for Zero-shot Camouflaged Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce a novel multimodal camo-perceptive framework (MMCPF) aimed at handling zero-shot Camouflaged Object Detection (COD) by leveraging the powerful capabilities of Multimodal Large Language Models (MLLMs). Recognizing the inherent limitations of current COD methodologies, which predominantly rely on supervised learning models demanding extensive and accurately annotated datasets, resulting in weak generalization, our research proposes a zero-shot MMCPF that circumvents these challenges. Although MLLMs hold significant potential for broad applications, their effectiveness in COD is hindered and they would make misinterpretations of camouflaged objects. To address this challenge, we further propose a strategic enhancement called the Chain of Visual Perception (CoVP), which significantly improves the perceptual capabilities of MLLMs in camouflaged scenes by leveraging both linguistic and visual cues more effectively. We validate the effectiveness of MMCPF on five widely used COD datasets, containing CAMO, COD10K, NC4K, MoCA-Mask and OVCamo. Experiments show that MMCPF can outperform all existing state-of-the-art zero-shot COD methods, and achieve competitive performance compared to weakly-supervised and fully-supervised methods, which demonstrates the potential of MMCPF. The Github link of this paper is \url{https://github.com/luckybird1994/MMCPF}.
<div id='section'>Paperid: <span id='pid'>245, <a href='https://arxiv.org/pdf/2311.11176.pdf' target='_blank'>https://arxiv.org/pdf/2311.11176.pdf</a></span>   <span><a href='https://github.com/YueXin18/MorSeg-CAM-SAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Yue, Xiaoling Liu, Qing Zhao, Jianqiang Li, Changwei Song, Suqin Liu, Zhikai Yang, Guanghui Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.11176">Morphology-Enhanced CAM-Guided SAM for weakly supervised Breast Lesion Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ultrasound imaging plays a critical role in the early detection of breast cancer. Accurate identification and segmentation of lesions are essential steps in clinical practice, requiring methods to assist physicians in lesion segmentation. However, ultrasound lesion segmentation models based on supervised learning require extensive manual labeling, which is both time-consuming and labor-intensive. In this study, we present a novel framework for weakly supervised lesion segmentation in early breast ultrasound images. Our method uses morphological enhancement and class activation map (CAM)-guided localization. Finally, we employ the Segment Anything Model (SAM), a computer vision foundation model, for detailed segmentation. This approach does not require pixel-level annotation, thereby reducing the cost of data annotation. The performance of our method is comparable to supervised learning methods that require manual annotations, achieving a Dice score of 74.39% and outperforming comparative supervised models in terms of Hausdorff distance in the BUSI dataset. These results demonstrate that our framework effectively integrates weakly supervised learning with SAM, providing a promising solution for breast cancer image analysis. The code for this study is available at: https://github.com/YueXin18/MorSeg-CAM-SAM.
<div id='section'>Paperid: <span id='pid'>246, <a href='https://arxiv.org/pdf/2311.09726.pdf' target='_blank'>https://arxiv.org/pdf/2311.09726.pdf</a></span>   <span><a href='https://github.com/guanyuezhen/MS-Former' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenglai Li, Chang Tang, Xinwang Liu, Changdong Li, Xianju Li, Wei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.09726">MS-Former: Memory-Supported Transformer for Weakly Supervised Change Detection with Patch-Level Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fully supervised change detection methods have achieved significant advancements in performance, yet they depend severely on acquiring costly pixel-level labels. Considering that the patch-level annotations also contain abundant information corresponding to both changed and unchanged objects in bi-temporal images, an intuitive solution is to segment the changes with patch-level annotations. How to capture the semantic variations associated with the changed and unchanged regions from the patch-level annotations to obtain promising change results is the critical challenge for the weakly supervised change detection task. In this paper, we propose a memory-supported transformer (MS-Former), a novel framework consisting of a bi-directional attention block (BAB) and a patch-level supervision scheme (PSS) tailored for weakly supervised change detection with patch-level annotations. More specifically, the BAM captures contexts associated with the changed and unchanged regions from the temporal difference features to construct informative prototypes stored in the memory bank. On the other hand, the BAM extracts useful information from the prototypes as supplementary contexts to enhance the temporal difference features, thereby better distinguishing changed and unchanged regions. After that, the PSS guides the network learning valuable knowledge from the patch-level annotations, thus further elevating the performance. Experimental results on three benchmark datasets demonstrate the effectiveness of our proposed method in the change detection task. The demo code for our work will be publicly available at \url{https://github.com/guanyuezhen/MS-Former}.
<div id='section'>Paperid: <span id='pid'>247, <a href='https://arxiv.org/pdf/2311.08080.pdf' target='_blank'>https://arxiv.org/pdf/2311.08080.pdf</a></span>   <span><a href='https://github.com/ckm3/Deep-LC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaiming Cui, D. J. Armstrong, Fabo Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.08080">Identifying Light-curve Signals with a Deep Learning Based Object Detection Algorithm. II. A General Light Curve Classification Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vast amounts of astronomical photometric data are generated from various projects, requiring significant effort to identify variable stars and other object classes. In light of this, a general, widely applicable classification framework would simplify the process of designing specific classifiers for various astronomical objects. We present a novel deep learning framework for classifying light curves using a weakly supervised object detection model. Our framework identifies the optimal windows for both light curves and power spectra automatically, and zooms in on their corresponding data. This allows for automatic feature extraction from both time and frequency domains, enabling our model to handle data across different scales and sampling intervals. We train our model on data sets obtained from Kepler, TESS, and Zwicky Transient Facility multiband observations of variable stars and transients. We achieve an accuracy of 87% for combined variables and transient events, which is comparable to the performance of previous feature-based models. Our trained model can be utilized directly for other missions, such as the All-sky Automated Survey for Supernovae, without requiring any retraining or fine-tuning. To address known issues with miscalibrated predictive probabilities, we apply conformal prediction to generate robust predictive sets that guarantee true-label coverage with a given probability. Additionally, we incorporate various anomaly detection algorithms to empower our model with the ability to identify out-of-distribution objects. Our framework is implemented in the Deep-LC toolkit, which is an open-source Python package hosted on Github (https://github.com/ckm3/Deep-LC) and PyPI.
<div id='section'>Paperid: <span id='pid'>248, <a href='https://arxiv.org/pdf/2311.05155.pdf' target='_blank'>https://arxiv.org/pdf/2311.05155.pdf</a></span>   <span><a href='https://github.com/koustavagoswami/Weakly_supervised-Cognate_Detection' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Koustava Goswami, Priya Rani, Theodorus Fransen, John P. McCrae
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.05155">Weakly-supervised Deep Cognate Detection Framework for Low-Resourced Languages Using Morphological Knowledge of Closely-Related Languages</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Exploiting cognates for transfer learning in under-resourced languages is an exciting opportunity for language understanding tasks, including unsupervised machine translation, named entity recognition and information retrieval. Previous approaches mainly focused on supervised cognate detection tasks based on orthographic, phonetic or state-of-the-art contextual language models, which under-perform for most under-resourced languages. This paper proposes a novel language-agnostic weakly-supervised deep cognate detection framework for under-resourced languages using morphological knowledge from closely related languages. We train an encoder to gain morphological knowledge of a language and transfer the knowledge to perform unsupervised and weakly-supervised cognate detection tasks with and without the pivot language for the closely-related languages. While unsupervised, it overcomes the need for hand-crafted annotation of cognates. We performed experiments on different published cognate detection datasets across language families and observed not only significant improvement over the state-of-the-art but also our method outperformed the state-of-the-art supervised and unsupervised methods. Our model can be extended to a wide range of languages from any language family as it overcomes the requirement of the annotation of the cognate pairs for training. The code and dataset building scripts can be found at https://github.com/koustavagoswami/Weakly_supervised-Cognate_Detection
<div id='section'>Paperid: <span id='pid'>249, <a href='https://arxiv.org/pdf/2311.00180.pdf' target='_blank'>https://arxiv.org/pdf/2311.00180.pdf</a></span>   <span><a href='https://github.com/brown-palm/ObjectPrompt' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ce Zhang, Changcheng Fu, Shijie Wang, Nakul Agarwal, Kwonjoon Lee, Chiho Choi, Chen Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.00180">Object-centric Video Representation for Long-term Action Anticipation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper focuses on building object-centric representations for long-term action anticipation in videos. Our key motivation is that objects provide important cues to recognize and predict human-object interactions, especially when the predictions are longer term, as an observed "background" object could be used by the human actor in the future. We observe that existing object-based video recognition frameworks either assume the existence of in-domain supervised object detectors or follow a fully weakly-supervised pipeline to infer object locations from action labels. We propose to build object-centric video representations by leveraging visual-language pretrained models. This is achieved by "object prompts", an approach to extract task-specific object-centric representations from general-purpose pretrained models without finetuning. To recognize and predict human-object interactions, we use a Transformer-based neural architecture which allows the "retrieval" of relevant objects for action anticipation at various time scales. We conduct extensive evaluations on the Ego4D, 50Salads, and EGTEA Gaze+ benchmarks. Both quantitative and qualitative results confirm the effectiveness of our proposed method.
<div id='section'>Paperid: <span id='pid'>250, <a href='https://arxiv.org/pdf/2311.00048.pdf' target='_blank'>https://arxiv.org/pdf/2311.00048.pdf</a></span>   <span><a href='https://github.com/sotiraslab/SCMIL.git' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/sotiraslab/SCMIL.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Peijie Qiu, Pan Xiao, Wenhui Zhu, Yalin Wang, Aristeidis Sotiras
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.00048">SC-MIL: Sparsely Coded Multiple Instance Learning for Whole Slide Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple Instance Learning (MIL) has been widely used in weakly supervised whole slide image (WSI) classification. Typical MIL methods include a feature embedding part, which embeds the instances into features via a pre-trained feature extractor, and an MIL aggregator that combines instance embeddings into predictions. Most efforts have typically focused on improving these parts. This involves refining the feature embeddings through self-supervised pre-training as well as modeling the correlations between instances separately.
  In this paper, we proposed a sparsely coding MIL (SC-MIL) method that addresses those two aspects at the same time by leveraging sparse dictionary learning. The sparse dictionary learning captures the similarities of instances by expressing them as sparse linear combinations of atoms in an over-complete dictionary. In addition, imposing sparsity improves instance feature embeddings by suppressing irrelevant instances while retaining the most relevant ones. To make the conventional sparse coding algorithm compatible with deep learning, we unrolled it into a sparsely coded module leveraging deep unrolling. The proposed SC module can be incorporated into any existing MIL framework in a plug-and-play manner with an acceptable computational cost. The experimental results on multiple datasets demonstrated that the proposed SC module could substantially boost the performance of state-of-the-art MIL methods. The codes are available at \href{https://github.com/sotiraslab/SCMIL.git}{https://github.com/sotiraslab/SCMIL.git}.
<div id='section'>Paperid: <span id='pid'>251, <a href='https://arxiv.org/pdf/2310.20706.pdf' target='_blank'>https://arxiv.org/pdf/2310.20706.pdf</a></span>   <span><a href='https://github.com/mustansarfiaz/DDAM-PS' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/mustansarfiaz/DDAM-PS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammed Khaleed Almansoori, Mustansar Fiaz, Hisham Cholakkal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.20706">DDAM-PS: Diligent Domain Adaptive Mixer for Person Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Person search (PS) is a challenging computer vision problem where the objective is to achieve joint optimization for pedestrian detection and re-identification (ReID). Although previous advancements have shown promising performance in the field under fully and weakly supervised learning fashion, there exists a major gap in investigating the domain adaptation ability of PS models. In this paper, we propose a diligent domain adaptive mixer (DDAM) for person search (DDAP-PS) framework that aims to bridge a gap to improve knowledge transfer from the labeled source domain to the unlabeled target domain. Specifically, we introduce a novel DDAM module that generates moderate mixed-domain representations by combining source and target domain representations. The proposed DDAM module encourages domain mixing to minimize the distance between the two extreme domains, thereby enhancing the ReID task. To achieve this, we introduce two bridge losses and a disparity loss. The objective of the two bridge losses is to guide the moderate mixed-domain representations to maintain an appropriate distance from both the source and target domain representations. The disparity loss aims to prevent the moderate mixed-domain representations from being biased towards either the source or target domains, thereby avoiding overfitting. Furthermore, we address the conflict between the two subtasks, localization and ReID, during domain adaptation. To handle this cross-task conflict, we forcefully decouple the norm-aware embedding, which aids in better learning of the moderate mixed-domain representation. We conduct experiments to validate the effectiveness of our proposed method. Our approach demonstrates favorable performance on the challenging PRW and CUHK-SYSU datasets. Our source code is publicly available at \url{https://github.com/mustansarfiaz/DDAM-PS}.
<div id='section'>Paperid: <span id='pid'>252, <a href='https://arxiv.org/pdf/2310.13479.pdf' target='_blank'>https://arxiv.org/pdf/2310.13479.pdf</a></span>   <span><a href='https://github.com/fgirbal/segment-select-correct' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Francisco Eiras, Kemal Oksuz, Adel Bibi, Philip H. S. Torr, Puneet K. Dokania
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.13479">Segment, Select, Correct: A Framework for Weakly-Supervised Referring Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring Image Segmentation (RIS) - the problem of identifying objects in images through natural language sentences - is a challenging task currently mostly solved through supervised learning. However, while collecting referred annotation masks is a time-consuming process, the few existing weakly-supervised and zero-shot approaches fall significantly short in performance compared to fully-supervised learning ones. To bridge the performance gap without mask annotations, we propose a novel weakly-supervised framework that tackles RIS by decomposing it into three steps: obtaining instance masks for the object mentioned in the referencing instruction (segment), using zero-shot learning to select a potentially correct mask for the given instruction (select), and bootstrapping a model which allows for fixing the mistakes of zero-shot selection (correct). In our experiments, using only the first two steps (zero-shot segment and select) outperforms other zero-shot baselines by as much as 16.5%, while our full method improves upon this much stronger baseline and sets the new state-of-the-art for weakly-supervised RIS, reducing the gap between the weakly-supervised and fully-supervised methods in some cases from around 33% to as little as 7%. Code is available at https://github.com/fgirbal/segment-select-correct.
<div id='section'>Paperid: <span id='pid'>253, <a href='https://arxiv.org/pdf/2310.12817.pdf' target='_blank'>https://arxiv.org/pdf/2310.12817.pdf</a></span>   <span><a href='https://jimmy15923.github.io/mit_web/' target='_blank'>  GitHub</a></span> <span><a href='https://jimmy15923.github.io/mit_web/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng-Kun Yang, Min-Hung Chen, Yung-Yu Chuang, Yen-Yu Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.12817">2D-3D Interlaced Transformer for Point Cloud Segmentation with Scene-Level Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a Multimodal Interlaced Transformer (MIT) that jointly considers 2D and 3D data for weakly supervised point cloud segmentation. Research studies have shown that 2D and 3D features are complementary for point cloud segmentation. However, existing methods require extra 2D annotations to achieve 2D-3D information fusion. Considering the high annotation cost of point clouds, effective 2D and 3D feature fusion based on weakly supervised learning is in great demand. To this end, we propose a transformer model with two encoders and one decoder for weakly supervised point cloud segmentation using only scene-level class tags. Specifically, the two encoders compute the self-attended features for 3D point clouds and 2D multi-view images, respectively. The decoder implements interlaced 2D-3D cross-attention and carries out implicit 2D and 3D feature fusion. We alternately switch the roles of queries and key-value pairs in the decoder layers. It turns out that the 2D and 3D features are iteratively enriched by each other. Experiments show that it performs favorably against existing weakly supervised point cloud segmentation methods by a large margin on the S3DIS and ScanNet benchmarks. The project page will be available at https://jimmy15923.github.io/mit_web/.
<div id='section'>Paperid: <span id='pid'>254, <a href='https://arxiv.org/pdf/2310.11275.pdf' target='_blank'>https://arxiv.org/pdf/2310.11275.pdf</a></span>   <span><a href='https://github.com/hpi-dhc/xmen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Florian Borchert, Ignacio Llorca, Roland Roller, Bert Arnrich, Matthieu-P. Schapranow
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.11275">xMEN: A Modular Toolkit for Cross-Lingual Medical Entity Normalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Objective: To improve performance of medical entity normalization across many languages, especially when fewer language resources are available compared to English.
  Materials and Methods: We introduce xMEN, a modular system for cross-lingual medical entity normalization, which performs well in both low- and high-resource scenarios. When synonyms in the target language are scarce for a given terminology, we leverage English aliases via cross-lingual candidate generation. For candidate ranking, we incorporate a trainable cross-encoder model if annotations for the target task are available. We also evaluate cross-encoders trained in a weakly supervised manner based on machine-translated datasets from a high resource domain. Our system is publicly available as an extensible Python toolkit.
  Results: xMEN improves the state-of-the-art performance across a wide range of multilingual benchmark datasets. Weakly supervised cross-encoders are effective when no training data is available for the target task. Through the compatibility of xMEN with the BigBIO framework, it can be easily used with existing and prospective datasets.
  Discussion: Our experiments show the importance of balancing the output of general-purpose candidate generators with subsequent trainable re-rankers, which we achieve through a rank regularization term in the loss function of the cross-encoder. However, error analysis reveals that multi-word expressions and other complex entities are still challenging.
  Conclusion: xMEN exhibits strong performance for medical entity normalization in multiple languages, even when no labeled data and few terminology aliases for the target language are available. Its configuration system and evaluation modules enable reproducible benchmarks. Models and code are available online at the following URL: https://github.com/hpi-dhc/xmen
<div id='section'>Paperid: <span id='pid'>255, <a href='https://arxiv.org/pdf/2310.10533.pdf' target='_blank'>https://arxiv.org/pdf/2310.10533.pdf</a></span>   <span><a href='https://github.com/CircleRadon/APro' target='_blank'>  GitHub</a></span> <span><a href='https://LiWentomng.github.io/apro/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wentong Li, Yuqian Yuan, Song Wang, Wenyu Liu, Dongqi Tang, Jian Liu, Jianke Zhu, Lei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.10533">Label-efficient Segmentation via Affinity Propagation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised segmentation with label-efficient sparse annotations has attracted increasing research attention to reduce the cost of laborious pixel-wise labeling process, while the pairwise affinity modeling techniques play an essential role in this task. Most of the existing approaches focus on using the local appearance kernel to model the neighboring pairwise potentials. However, such a local operation fails to capture the long-range dependencies and ignores the topology of objects. In this work, we formulate the affinity modeling as an affinity propagation process, and propose a local and a global pairwise affinity terms to generate accurate soft pseudo labels. An efficient algorithm is also developed to reduce significantly the computational cost. The proposed approach can be conveniently plugged into existing segmentation networks. Experiments on three typical label-efficient segmentation tasks, i.e. box-supervised instance segmentation, point/scribble-supervised semantic segmentation and CLIP-guided semantic segmentation, demonstrate the superior performance of the proposed approach.
<div id='section'>Paperid: <span id='pid'>256, <a href='https://arxiv.org/pdf/2310.09278.pdf' target='_blank'>https://arxiv.org/pdf/2310.09278.pdf</a></span>   <span><a href='https://github.com/intelligolabs/Detaux' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Geri Skenderi, Luigi Capogrosso, Andrea Toaiari, Matteo Denitto, Franco Fummi, Simone Melzi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.09278">Disentangled Latent Spaces Facilitate Data-Driven Auxiliary Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Auxiliary tasks facilitate learning in situations where data is scarce or the principal task of interest is extremely complex. This idea is primarily inspired by the improved generalization capability induced by solving multiple tasks simultaneously, which leads to a more robust shared representation. Nevertheless, finding optimal auxiliary tasks is a crucial problem that often requires hand-crafted solutions or expensive meta-learning approaches. In this paper, we propose a novel framework, dubbed Detaux, whereby a weakly supervised disentanglement procedure is used to discover a new unrelated auxiliary classification task, which allows us to go from a Single-Task Learning (STL) to a Multi-Task Learning (MTL) problem. The disentanglement procedure works at the representation level, isolating the variation related to the principal task into an isolated subspace and additionally producing an arbitrary number of orthogonal subspaces, each of which encourages high separability among projections. We generate the auxiliary classification task through a clustering procedure on the most disentangled subspace, obtaining a discrete set of labels. Subsequently, the original data, the labels associated with the principal task, and the newly discovered ones can be fed into any MTL framework. Experimental validation on both synthetic and real data, along with various ablation studies, demonstrates promising results, revealing the potential in what has been, so far, an unexplored connection between learning disentangled representations and MTL. The source code is available at https://github.com/intelligolabs/Detaux.
<div id='section'>Paperid: <span id='pid'>257, <a href='https://arxiv.org/pdf/2310.05541.pdf' target='_blank'>https://arxiv.org/pdf/2310.05541.pdf</a></span>   <span><a href='https://ai4ce.github.io/CoVPR/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Li, Zonglin Lyu, Mingxuan Lu, Chao Chen, Michael Milford, Chen Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.05541">Collaborative Visual Place Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual place recognition (VPR) capabilities enable autonomous robots to navigate complex environments by discovering the environment's topology based on visual input. Most research efforts focus on enhancing the accuracy and robustness of single-robot VPR but often encounter issues such as occlusion due to individual viewpoints. Despite a number of research on multi-robot metric-based localization, there is a notable gap in research concerning more robust and efficient place-based localization with a multi-robot system. This work proposes collaborative VPR, where multiple robots share abstracted visual features to enhance place recognition capabilities. We also introduce a novel collaborative VPR framework based on similarity-regularized information fusion, reducing irrelevant noise while harnessing valuable data from collaborators. This framework seamlessly integrates with well-established single-robot VPR techniques and supports end-to-end training with a weakly-supervised contrastive loss. We conduct experiments in urban, rural, and indoor scenes, achieving a notable improvement over single-agent VPR in urban environments (~12\%), along with consistent enhancements in rural (~3\%) and indoor (~1\%) scenarios. Our work presents a promising solution to the pressing challenges of VPR, representing a substantial step towards safe and robust autonomous systems.
<div id='section'>Paperid: <span id='pid'>258, <a href='https://arxiv.org/pdf/2310.05184.pdf' target='_blank'>https://arxiv.org/pdf/2310.05184.pdf</a></span>   <span><a href='https://github.com/Lu-Feng/AANet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Feng Lu, Lijun Zhang, Shuting Dong, Baifan Chen, Chun Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.05184">AANet: Aggregation and Alignment Network with Semi-hard Positive Sample Mining for Hierarchical Place Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual place recognition (VPR) is one of the research hotspots in robotics, which uses visual information to locate robots. Recently, the hierarchical two-stage VPR methods have become popular in this field due to the trade-off between accuracy and efficiency. These methods retrieve the top-k candidate images using the global features in the first stage, then re-rank the candidates by matching the local features in the second stage. However, they usually require additional algorithms (e.g. RANSAC) for geometric consistency verification in re-ranking, which is time-consuming. Here we propose a Dynamically Aligning Local Features (DALF) algorithm to align the local features under spatial constraints. It is significantly more efficient than the methods that need geometric consistency verification. We present a unified network capable of extracting global features for retrieving candidates via an aggregation module and aligning local features for re-ranking via the DALF alignment module. We call this network AANet. Meanwhile, many works use the simplest positive samples in triplet for weakly supervised training, which limits the ability of the network to recognize harder positive pairs. To address this issue, we propose a Semi-hard Positive Sample Mining (ShPSM) strategy to select appropriate hard positive images for training more robust VPR networks. Extensive experiments on four benchmark VPR datasets show that the proposed AANet can outperform several state-of-the-art methods with less time consumption. The code is released at https://github.com/Lu-Feng/AANet.
<div id='section'>Paperid: <span id='pid'>259, <a href='https://arxiv.org/pdf/2310.03821.pdf' target='_blank'>https://arxiv.org/pdf/2310.03821.pdf</a></span>   <span><a href='https://github.com/jacky121298/WLST' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tsung-Lin Tsou, Tsung-Han Wu, Winston H. Hsu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.03821">WLST: Weak Labels Guided Self-training for Weakly-supervised Domain Adaptation on 3D Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of domain adaptation (DA) on 3D object detection, most of the work is dedicated to unsupervised domain adaptation (UDA). Yet, without any target annotations, the performance gap between the UDA approaches and the fully-supervised approach is still noticeable, which is impractical for real-world applications. On the other hand, weakly-supervised domain adaptation (WDA) is an underexplored yet practical task that only requires few labeling effort on the target domain. To improve the DA performance in a cost-effective way, we propose a general weak labels guided self-training framework, WLST, designed for WDA on 3D object detection. By incorporating autolabeler, which can generate 3D pseudo labels from 2D bounding boxes, into the existing self-training pipeline, our method is able to generate more robust and consistent pseudo labels that would benefit the training process on the target domain. Extensive experiments demonstrate the effectiveness, robustness, and detector-agnosticism of our WLST framework. Notably, it outperforms previous state-of-the-art methods on all evaluation tasks.
<div id='section'>Paperid: <span id='pid'>260, <a href='https://arxiv.org/pdf/2310.03739.pdf' target='_blank'>https://arxiv.org/pdf/2310.03739.pdf</a></span>   <span><a href='https://align-prop.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, Katerina Fragkiadaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.03739">Aligning Text-to-Image Diffusion Models with Reward Backpropagation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-image diffusion models have recently emerged at the forefront of image generation, powered by very large-scale unsupervised or weakly supervised text-to-image training datasets. Due to their unsupervised training, controlling their behavior in downstream tasks, such as maximizing human-perceived image quality, image-text alignment, or ethical image generation, is difficult. Recent works finetune diffusion models to downstream reward functions using vanilla reinforcement learning, notorious for the high variance of the gradient estimators. In this paper, we propose AlignProp, a method that aligns diffusion models to downstream reward functions using end-to-end backpropagation of the reward gradient through the denoising process. While naive implementation of such backpropagation would require prohibitive memory resources for storing the partial derivatives of modern text-to-image models, AlignProp finetunes low-rank adapter weight modules and uses gradient checkpointing, to render its memory usage viable. We test AlignProp in finetuning diffusion models to various objectives, such as image-text semantic alignment, aesthetics, compressibility and controllability of the number of objects present, as well as their combinations. We show AlignProp achieves higher rewards in fewer training steps than alternatives, while being conceptually simpler, making it a straightforward choice for optimizing diffusion models for differentiable reward functions of interest. Code and Visualization results are available at https://align-prop.github.io/.
<div id='section'>Paperid: <span id='pid'>261, <a href='https://arxiv.org/pdf/2310.01924.pdf' target='_blank'>https://arxiv.org/pdf/2310.01924.pdf</a></span>   <span><a href='https://github.com/Sanofi-Public/DDS-RoFormerMIL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Etienne Pochet, Rami Maroun, Roger Trullo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.01924">RoFormer for Position Aware Multiple Instance Learning in Whole Slide Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Whole slide image (WSI) classification is a critical task in computational pathology. However, the gigapixel-size of such images remains a major challenge for the current state of deep-learning. Current methods rely on multiple-instance learning (MIL) models with frozen feature extractors. Given the the high number of instances in each image, MIL methods have long assumed independence and permutation-invariance of patches, disregarding the tissue structure and correlation between patches. Recent works started studying this correlation between instances but the computational workload of such a high number of tokens remained a limiting factor. In particular, relative position of patches remains unaddressed. We propose to apply a straightforward encoding module, namely a RoFormer layer , relying on memory-efficient exact self-attention and relative positional encoding. This module can perform full self-attention with relative position encoding on patches of large and arbitrary shaped WSIs, solving the need for correlation between instances and spatial modeling of tissues. We demonstrate that our method outperforms state-of-the-art MIL models on three commonly used public datasets (TCGA-NSCLC, BRACS and Camelyon16)) on weakly supervised classification tasks. Code is available at https://github.com/Sanofi-Public/DDS-RoFormerMIL
<div id='section'>Paperid: <span id='pid'>262, <a href='https://arxiv.org/pdf/2310.00454.pdf' target='_blank'>https://arxiv.org/pdf/2310.00454.pdf</a></span>   <span><a href='https://github.com/fadamsyah/SimLVSeg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fadillah Maani, Asim Ukaye, Nada Saadi, Numan Saeed, Mohammad Yaqub
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.00454">SimLVSeg: Simplifying Left Ventricular Segmentation in 2D+Time Echocardiograms with Self- and Weakly-Supervised Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Echocardiography has become an indispensable clinical imaging modality for general heart health assessment. From calculating biomarkers such as ejection fraction to the probability of a patient's heart failure, accurate segmentation of the heart structures allows doctors to assess the heart's condition and devise treatments with greater precision and accuracy. However, achieving accurate and reliable left ventricle segmentation is time-consuming and challenging due to different reasons. Hence, clinicians often rely on segmenting the left ventricular (LV) in two specific echocardiogram frames to make a diagnosis. This limited coverage in manual LV segmentation poses a challenge for developing automatic LV segmentation with high temporal consistency, as the resulting dataset is typically annotated sparsely. In response to this challenge, this work introduces SimLVSeg, a novel paradigm that enables video-based networks for consistent LV segmentation from sparsely annotated echocardiogram videos. SimLVSeg consists of self-supervised pre-training with temporal masking, followed by weakly supervised learning tailored for LV segmentation from sparse annotations. We demonstrate how SimLVSeg outperforms the state-of-the-art solutions by achieving a 93.32% (95%CI 93.21-93.43%) dice score on the largest 2D+time echocardiography dataset (EchoNet-Dynamic) while being more efficient. SimLVSeg is compatible with two types of video segmentation networks: 2D super image and 3D segmentation. To show the effectiveness of our approach, we provide extensive ablation studies, including pre-training settings and various deep learning backbones. We further conduct an out-of-distribution test to showcase SimLVSeg's generalizability on unseen distribution (CAMUS dataset). The code is publicly available at https://github.com/fadamsyah/SimLVSeg.
<div id='section'>Paperid: <span id='pid'>263, <a href='https://arxiv.org/pdf/2310.00377.pdf' target='_blank'>https://arxiv.org/pdf/2310.00377.pdf</a></span>   <span><a href='https://github.com/GauravBh1010tt/DPViT.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gaurav Bhatt, Deepayan Das, Leonid Sigal, Vineeth N Balasubramanian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.00377">Mitigating the Effect of Incidental Correlations on Part-based Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intelligent systems possess a crucial characteristic of breaking complicated problems into smaller reusable components or parts and adjusting to new tasks using these part representations. However, current part-learners encounter difficulties in dealing with incidental correlations resulting from the limited observations of objects that may appear only in specific arrangements or with specific backgrounds. These incidental correlations may have a detrimental impact on the generalization and interpretability of learned part representations. This study asserts that part-based representations could be more interpretable and generalize better with limited data, employing two innovative regularization methods. The first regularization separates foreground and background information's generative process via a unique mixture-of-parts formulation. Structural constraints are imposed on the parts using a weakly-supervised loss, guaranteeing that the mixture-of-parts for foreground and background entails soft, object-agnostic masks. The second regularization assumes the form of a distillation loss, ensuring the invariance of the learned parts to the incidental background correlations. Furthermore, we incorporate sparse and orthogonal constraints to facilitate learning high-quality part representations. By reducing the impact of incidental background correlations on the learned parts, we exhibit state-of-the-art (SoTA) performance on few-shot learning tasks on benchmark datasets, including MiniImagenet, TieredImageNet, and FC100. We also demonstrate that the part-based representations acquired through our approach generalize better than existing techniques, even under domain shifts of the background and common data corruption on the ImageNet-9 dataset. The implementation is available on GitHub: https://github.com/GauravBh1010tt/DPViT.git
<div id='section'>Paperid: <span id='pid'>264, <a href='https://arxiv.org/pdf/2310.00108.pdf' target='_blank'>https://arxiv.org/pdf/2310.00108.pdf</a></span>   <span><a href='https://github.com/ruoxi-jia-group/CLIP-MIA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Myeongseob Ko, Ming Jin, Chenguang Wang, Ruoxi Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.00108">Practical Membership Inference Attacks Against Large-Scale Multi-Modal Models: A Pilot Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Membership inference attacks (MIAs) aim to infer whether a data point has been used to train a machine learning model. These attacks can be employed to identify potential privacy vulnerabilities and detect unauthorized use of personal data. While MIAs have been traditionally studied for simple classification models, recent advancements in multi-modal pre-training, such as CLIP, have demonstrated remarkable zero-shot performance across a range of computer vision tasks. However, the sheer scale of data and models presents significant computational challenges for performing the attacks.
  This paper takes a first step towards developing practical MIAs against large-scale multi-modal models. We introduce a simple baseline strategy by thresholding the cosine similarity between text and image features of a target point and propose further enhancing the baseline by aggregating cosine similarity across transformations of the target. We also present a new weakly supervised attack method that leverages ground-truth non-members (e.g., obtained by using the publication date of a target model and the timestamps of the open data) to further enhance the attack. Our evaluation shows that CLIP models are susceptible to our attack strategies, with our simple baseline achieving over $75\%$ membership identification accuracy. Furthermore, our enhanced attacks outperform the baseline across multiple models and datasets, with the weakly supervised attack demonstrating an average-case performance improvement of $17\%$ and being at least $7$X more effective at low false-positive rates. These findings highlight the importance of protecting the privacy of multi-modal foundational models, which were previously assumed to be less susceptible to MIAs due to less overfitting. Our code is available at https://github.com/ruoxi-jia-group/CLIP-MIA.
<div id='section'>Paperid: <span id='pid'>265, <a href='https://arxiv.org/pdf/2309.16992.pdf' target='_blank'>https://arxiv.org/pdf/2309.16992.pdf</a></span>   <span><a href='https://github.com/vignywang/SAMFeat' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingqian Wu, Rongtao Xu, Zach Wood-Doughty, Changwei Wang, Shibiao Xu, Edmund Y. Lam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.16992">Segment Anything Model is a Good Teacher for Local Feature Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Local feature detection and description play an important role in many computer vision tasks, which are designed to detect and describe keypoints in "any scene" and "any downstream task". Data-driven local feature learning methods need to rely on pixel-level correspondence for training, which is challenging to acquire at scale, thus hindering further improvements in performance. In this paper, we propose SAMFeat to introduce SAM (segment anything model), a fundamental model trained on 11 million images, as a teacher to guide local feature learning and thus inspire higher performance on limited datasets. To do so, first, we construct an auxiliary task of Attention-weighted Semantic Relation Distillation (ASRD), which distillates feature relations with category-agnostic semantic information learned by the SAM encoder into a local feature learning network, to improve local feature description using semantic discrimination. Second, we develop a technique called Weakly Supervised Contrastive Learning Based on Semantic Grouping (WSC), which utilizes semantic groupings derived from SAM as weakly supervised signals, to optimize the metric space of local descriptors. Third, we design an Edge Attention Guidance (EAG) to further improve the accuracy of local feature detection and description by prompting the network to pay more attention to the edge region guided by SAM. SAMFeat's performance on various tasks such as image matching on HPatches, and long-term visual localization on Aachen Day-Night showcases its superiority over previous local features. The release code is available at https://github.com/vignywang/SAMFeat.
<div id='section'>Paperid: <span id='pid'>266, <a href='https://arxiv.org/pdf/2309.16701.pdf' target='_blank'>https://arxiv.org/pdf/2309.16701.pdf</a></span>   <span><a href='https://github.com/yny0506/Massive-Videos-Moment-Retrieval' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nakyeong Yang, Minsung Kim, Seunghyun Yoon, Joongbo Shin, Kyomin Jung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.16701">MVMR: A New Framework for Evaluating Faithfulness of Video Moment Retrieval against Multiple Distractors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the explosion of multimedia content, video moment retrieval (VMR), which aims to detect a video moment that matches a given text query from a video, has been studied intensively as a critical problem. However, the existing VMR framework evaluates video moment retrieval performance, assuming that a video is given, which may not reveal whether the models exhibit overconfidence in the falsely given video. In this paper, we propose the MVMR (Massive Videos Moment Retrieval for Faithfulness Evaluation) task that aims to retrieve video moments within a massive video set, including multiple distractors, to evaluate the faithfulness of VMR models. For this task, we suggest an automated massive video pool construction framework to categorize negative (distractors) and positive (false-negative) video sets using textual and visual semantic distance verification methods. We extend existing VMR datasets using these methods and newly construct three practical MVMR datasets. To solve the task, we further propose a strong informative sample-weighted learning method, CroCs, which employs two contrastive learning mechanisms: (1) weakly-supervised potential negative learning and (2) cross-directional hard-negative learning. Experimental results on the MVMR datasets reveal that existing VMR models are easily distracted by the misinformation (distractors), whereas our model shows significantly robust performance, demonstrating that CroCs is essential to distinguishing positive moments against distractors. Our code and datasets are publicly available: https://github.com/yny0506/Massive-Videos-Moment-Retrieval.
<div id='section'>Paperid: <span id='pid'>267, <a href='https://arxiv.org/pdf/2309.16064.pdf' target='_blank'>https://arxiv.org/pdf/2309.16064.pdf</a></span>   <span><a href='https://github.com/recursionpharma/maes_microscopy' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Oren Kraus, Kian Kenyon-Dean, Saber Saberian, Maryam Fallah, Peter McLean, Jess Leung, Vasudev Sharma, Ayla Khan, Jia Balakrishnan, Safiye Celik, Maciej Sypetkowski, Chi Vicky Cheng, Kristen Morse, Maureen Makes, Ben Mabey, Berton Earnshaw
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.16064">Masked Autoencoders are Scalable Learners of Cellular Morphology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inferring biological relationships from cellular phenotypes in high-content microscopy screens provides significant opportunity and challenge in biological research. Prior results have shown that deep vision models can capture biological signal better than hand-crafted features. This work explores how self-supervised deep learning approaches scale when training larger models on larger microscopy datasets. Our results show that both CNN- and ViT-based masked autoencoders significantly outperform weakly supervised baselines. At the high-end of our scale, a ViT-L/8 trained on over 3.5-billion unique crops sampled from 93-million microscopy images achieves relative improvements as high as 28% over our best weakly supervised baseline at inferring known biological relationships curated from public databases. Relevant code and select models released with this work can be found at: https://github.com/recursionpharma/maes_microscopy.
<div id='section'>Paperid: <span id='pid'>268, <a href='https://arxiv.org/pdf/2309.15796.pdf' target='_blank'>https://arxiv.org/pdf/2309.15796.pdf</a></span>   <span><a href='https://github.com/k2-fsa/icefall' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongji Gao, Hainan Xu, Desh Raj, Leibny Paola Garcia Perera, Daniel Povey, Sanjeev Khudanpur
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.15796">Learning from Flawed Data: Weakly Supervised Automatic Speech Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training automatic speech recognition (ASR) systems requires large amounts of well-curated paired data. However, human annotators usually perform "non-verbatim" transcription, which can result in poorly trained models. In this paper, we propose Omni-temporal Classification (OTC), a novel training criterion that explicitly incorporates label uncertainties originating from such weak supervision. This allows the model to effectively learn speech-text alignments while accommodating errors present in the training transcripts. OTC extends the conventional CTC objective for imperfect transcripts by leveraging weighted finite state transducers. Through experiments conducted on the LibriSpeech and LibriVox datasets, we demonstrate that training ASR models with OTC avoids performance degradation even with transcripts containing up to 70% errors, a scenario where CTC models fail completely. Our implementation is available at https://github.com/k2-fsa/icefall.
<div id='section'>Paperid: <span id='pid'>269, <a href='https://arxiv.org/pdf/2309.13404.pdf' target='_blank'>https://arxiv.org/pdf/2309.13404.pdf</a></span>   <span><a href='https://github.com/Breezewrf/WS-YOLO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rongfeng Wei, Jinlin Wu, Xuexue Bai, Ming Feng, Zhen Lei, Hongbin Liu, Zhen Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.13404">Weakly Supervised YOLO Network for Surgical Instrument Localization in Endoscopic Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In minimally invasive surgery, surgical instrument localization is a crucial task for endoscopic videos, which enables various applications for improving surgical outcomes. However, annotating the instrument localization in endoscopic videos is tedious and labor-intensive. In contrast, obtaining the category information is easy and efficient in real-world applications. To fully utilize the category information and address the localization problem, we propose a weakly supervised localization framework named WS-YOLO for surgical instruments. By leveraging the instrument category information as the weak supervision, our WS-YOLO framework adopts an unsupervised multi-round training strategy for the localization capability training. We validate our WS-YOLO framework on the Endoscopic Vision Challenge 2023 dataset, which achieves remarkable performance in the weakly supervised surgical instrument localization. The source code is available at https://github.com/Breezewrf/WS-YOLO.
<div id='section'>Paperid: <span id='pid'>270, <a href='https://arxiv.org/pdf/2309.12943.pdf' target='_blank'>https://arxiv.org/pdf/2309.12943.pdf</a></span>   <span><a href='https://github.com/wpy1999/BAS-Extension' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Zhai, Pingyu Wu, Kai Zhu, Yang Cao, Feng Wu, Zheng-Jun Zha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.12943">Background Activation Suppression for Weakly Supervised Object Localization and Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised object localization and semantic segmentation aim to localize objects using only image-level labels. Recently, a new paradigm has emerged by generating a foreground prediction map (FPM) to achieve pixel-level localization. While existing FPM-based methods use cross-entropy to evaluate the foreground prediction map and to guide the learning of the generator, this paper presents two astonishing experimental observations on the object localization learning process: For a trained network, as the foreground mask expands, 1) the cross-entropy converges to zero when the foreground mask covers only part of the object region. 2) The activation value continuously increases until the foreground mask expands to the object boundary. Therefore, to achieve a more effective localization performance, we argue for the usage of activation value to learn more object regions. In this paper, we propose a Background Activation Suppression (BAS) method. Specifically, an Activation Map Constraint (AMC) module is designed to facilitate the learning of generator by suppressing the background activation value. Meanwhile, by using foreground region guidance and area constraint, BAS can learn the whole region of the object. In the inference phase, we consider the prediction maps of different categories together to obtain the final localization results. Extensive experiments show that BAS achieves significant and consistent improvement over the baseline methods on the CUB-200-2011 and ILSVRC datasets. In addition, our method also achieves state-of-the-art weakly supervised semantic segmentation performance on the PASCAL VOC 2012 and MS COCO 2014 datasets. Code and models are available at https://github.com/wpy1999/BAS-Extension.
<div id='section'>Paperid: <span id='pid'>271, <a href='https://arxiv.org/pdf/2309.12382.pdf' target='_blank'>https://arxiv.org/pdf/2309.12382.pdf</a></span>   <span><a href='https://github.com/naver-ai/scob' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Daehee Kim, Yoonsik Kim, DongHyun Kim, Yumin Lim, Geewook Kim, Taeho Kil
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.12382">SCOB: Universal Text Understanding via Character-wise Supervised Contrastive Learning with Online Text Rendering for Bridging Domain Gap</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inspired by the great success of language model (LM)-based pre-training, recent studies in visual document understanding have explored LM-based pre-training methods for modeling text within document images. Among them, pre-training that reads all text from an image has shown promise, but often exhibits instability and even fails when applied to broader domains, such as those involving both visual documents and scene text images. This is a substantial limitation for real-world scenarios, where the processing of text image inputs in diverse domains is essential. In this paper, we investigate effective pre-training tasks in the broader domains and also propose a novel pre-training method called SCOB that leverages character-wise supervised contrastive learning with online text rendering to effectively pre-train document and scene text domains by bridging the domain gap. Moreover, SCOB enables weakly supervised learning, significantly reducing annotation costs. Extensive benchmarks demonstrate that SCOB generally improves vanilla pre-training methods and achieves comparable performance to state-of-the-art methods. Our findings suggest that SCOB can be served generally and effectively for read-type pre-training methods. The code will be available at https://github.com/naver-ai/scob.
<div id='section'>Paperid: <span id='pid'>272, <a href='https://arxiv.org/pdf/2309.11966.pdf' target='_blank'>https://arxiv.org/pdf/2309.11966.pdf</a></span>   <span><a href='https://florise.github.io/neural_labeling_web/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Floris Erich, Naoya Chiba, Yusuke Yoshiyasu, Noriaki Ando, Ryo Hanai, Yukiyasu Domae
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.11966">NeuralLabeling: A versatile toolset for labeling vision datasets using Neural Radiance Fields</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present NeuralLabeling, a labeling approach and toolset for annotating 3D scenes using either bounding boxes or meshes and generating segmentation masks, affordance maps, 2D bounding boxes, 3D bounding boxes, 6DOF object poses, depth maps, and object meshes. NeuralLabeling uses Neural Radiance Fields (NeRF) as a renderer, allowing labeling to be performed using 3D spatial tools while incorporating geometric clues such as occlusions, relying only on images captured from multiple viewpoints as input. To demonstrate the applicability of NeuralLabeling to a practical problem in robotics, we added ground truth depth maps to 30000 frames of transparent object RGB and noisy depth maps of glasses placed in a dishwasher captured using an RGBD sensor, yielding the Dishwasher30k dataset. We show that training a simple deep neural network with supervision using the annotated depth maps yields a higher reconstruction performance than training with the previously applied weakly supervised approach. We also show how instance segmentation and depth completion datasets generated using NeuralLabeling can be incorporated into a robot application for grasping transparent objects placed in a dishwasher with an accuracy of 83.3%, compared to 16.3% without depth completion.
<div id='section'>Paperid: <span id='pid'>273, <a href='https://arxiv.org/pdf/2309.10650.pdf' target='_blank'>https://arxiv.org/pdf/2309.10650.pdf</a></span>   <span><a href='https://github.com/AmayaGS/MUSTANG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Amaya Gallagher-Syed, Luca Rossi, Felice Rivellese, Costantino Pitzalis, Myles Lewis, Michael Barnes, Gregory Slabaugh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.10650">MUSTANG: Multi-Stain Self-Attention Graph Multiple Instance Learning Pipeline for Histopathology Whole Slide Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Whole Slide Images (WSIs) present a challenging computer vision task due to their gigapixel size and presence of numerous artefacts. Yet they are a valuable resource for patient diagnosis and stratification, often representing the gold standard for diagnostic tasks. Real-world clinical datasets tend to come as sets of heterogeneous WSIs with labels present at the patient-level, with poor to no annotations. Weakly supervised attention-based multiple instance learning approaches have been developed in recent years to address these challenges, but can fail to resolve both long and short-range dependencies. Here we propose an end-to-end multi-stain self-attention graph (MUSTANG) multiple instance learning pipeline, which is designed to solve a weakly-supervised gigapixel multi-image classification task, where the label is assigned at the patient-level, but no slide-level labels or region annotations are available. The pipeline uses a self-attention based approach by restricting the operations to a highly sparse k-Nearest Neighbour Graph of embedded WSI patches based on the Euclidean distance. We show this approach achieves a state-of-the-art F1-score/AUC of 0.89/0.92, outperforming the widely used CLAM model. Our approach is highly modular and can easily be modified to suit different clinical datasets, as it only requires a patient-level label without annotations and accepts WSI sets of different sizes, as the graphs can be of varying sizes and structures. The source code can be found at https://github.com/AmayaGS/MUSTANG.
<div id='section'>Paperid: <span id='pid'>274, <a href='https://arxiv.org/pdf/2309.09599.pdf' target='_blank'>https://arxiv.org/pdf/2309.09599.pdf</a></span>   <span><a href='https://github.com/paathelb/MEDL-U' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Helbert Paat, Qing Lian, Weilong Yao, Tong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.09599">MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential Deep Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advancements in deep learning-based 3D object detection necessitate the availability of large-scale datasets. However, this requirement introduces the challenge of manual annotation, which is often both burdensome and time-consuming. To tackle this issue, the literature has seen the emergence of several weakly supervised frameworks for 3D object detection which can automatically generate pseudo labels for unlabeled data. Nevertheless, these generated pseudo labels contain noise and are not as accurate as those labeled by humans. In this paper, we present the first approach that addresses the inherent ambiguities present in pseudo labels by introducing an Evidential Deep Learning (EDL) based uncertainty estimation framework. Specifically, we propose MEDL-U, an EDL framework based on MTrans, which not only generates pseudo labels but also quantifies the associated uncertainties. However, applying EDL to 3D object detection presents three primary challenges: (1) relatively lower pseudolabel quality in comparison to other autolabelers; (2) excessively high evidential uncertainty estimates; and (3) lack of clear interpretability and effective utilization of uncertainties for downstream tasks. We tackle these issues through the introduction of an uncertainty-aware IoU-based loss, an evidence-aware multi-task loss function, and the implementation of a post-processing stage for uncertainty refinement. Our experimental results demonstrate that probabilistic detectors trained using the outputs of MEDL-U surpass deterministic detectors trained using outputs from previous 3D annotators on the KITTI val set for all difficulty levels. Moreover, MEDL-U achieves state-of-the-art results on the KITTI official test set compared to existing 3D automatic annotators.
<div id='section'>Paperid: <span id='pid'>275, <a href='https://arxiv.org/pdf/2309.07914.pdf' target='_blank'>https://arxiv.org/pdf/2309.07914.pdf</a></span>   <span><a href='https://github.com/seqam-lab/ALWOD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuting Wang, Velibor Ilic, Jiatong Li, Branislav Kisacanin, Vladimir Pavlovic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.07914">ALWOD: Active Learning for Weakly-Supervised Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object detection (OD), a crucial vision task, remains challenged by the lack of large training datasets with precise object localization labels. In this work, we propose ALWOD, a new framework that addresses this problem by fusing active learning (AL) with weakly and semi-supervised object detection paradigms. Because the performance of AL critically depends on the model initialization, we propose a new auxiliary image generator strategy that utilizes an extremely small labeled set, coupled with a large weakly tagged set of images, as a warm-start for AL. We then propose a new AL acquisition function, another critical factor in AL success, that leverages the student-teacher OD pair disagreement and uncertainty to effectively propose the most informative images to annotate. Finally, to complete the AL loop, we introduce a new labeling task delegated to human annotators, based on selection and correction of model-proposed detections, which is both rapid and effective in labeling the informative images. We demonstrate, across several challenging benchmarks, that ALWOD significantly narrows the gap between the ODs trained on few partially labeled but strategically selected image instances and those that rely on the fully-labeled data. Our code is publicly available on https://github.com/seqam-lab/ALWOD.
<div id='section'>Paperid: <span id='pid'>276, <a href='https://arxiv.org/pdf/2309.06924.pdf' target='_blank'>https://arxiv.org/pdf/2309.06924.pdf</a></span>   <span><a href='https://github.com/zhaodongsun/contrast-phys' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaodong Sun, Xiaobai Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.06924">Contrast-Phys+: Unsupervised and Weakly-supervised Video-based Remote Physiological Measurement via Spatiotemporal Contrast</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video-based remote physiological measurement utilizes facial videos to measure the blood volume change signal, which is also called remote photoplethysmography (rPPG). Supervised methods for rPPG measurements have been shown to achieve good performance. However, the drawback of these methods is that they require facial videos with ground truth (GT) physiological signals, which are often costly and difficult to obtain. In this paper, we propose Contrast-Phys+, a method that can be trained in both unsupervised and weakly-supervised settings. We employ a 3DCNN model to generate multiple spatiotemporal rPPG signals and incorporate prior knowledge of rPPG into a contrastive loss function. We further incorporate the GT signals into contrastive learning to adapt to partial or misaligned labels. The contrastive loss encourages rPPG/GT signals from the same video to be grouped together, while pushing those from different videos apart. We evaluate our methods on five publicly available datasets that include both RGB and Near-infrared videos. Contrast-Phys+ outperforms the state-of-the-art supervised methods, even when using partially available or misaligned GT signals, or no labels at all. Additionally, we highlight the advantages of our methods in terms of computational efficiency, noise robustness, and generalization. Our code is available at https://github.com/zhaodongsun/contrast-phys.
<div id='section'>Paperid: <span id='pid'>277, <a href='https://arxiv.org/pdf/2309.05528.pdf' target='_blank'>https://arxiv.org/pdf/2309.05528.pdf</a></span>   <span><a href='https://github.com/loic-lb/OOD_MIL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>LoÃ¯c Le Bescond, Maria Vakalopoulou, Stergios Christodoulidis, Fabrice AndrÃ©, Hugues Talbot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.05528">On the detection of Out-Of-Distribution samples in Multiple Instance Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The deployment of machine learning solutions in real-world scenarios often involves addressing the challenge of out-of-distribution (OOD) detection. While significant efforts have been devoted to OOD detection in classical supervised settings, the context of weakly supervised learning, particularly the Multiple Instance Learning (MIL) framework, remains under-explored. In this study, we tackle this challenge by adapting post-hoc OOD detection methods to the MIL setting while introducing a novel benchmark specifically designed to assess OOD detection performance in weakly supervised scenarios. Across extensive experiments based on diverse public datasets, KNN emerges as the best-performing method overall. However, it exhibits significant shortcomings on some datasets, emphasizing the complexity of this under-explored and challenging topic. Our findings shed light on the complex nature of OOD detection under the MIL framework, emphasizing the importance of developing novel, robust, and reliable methods that can generalize effectively in a weakly supervised context. The code for the paper is available here: https://github.com/loic-lb/OOD_MIL.
<div id='section'>Paperid: <span id='pid'>278, <a href='https://arxiv.org/pdf/2309.05490.pdf' target='_blank'>https://arxiv.org/pdf/2309.05490.pdf</a></span>   <span><a href='https://github.com/santiago2205/LSSQPS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Santiago Rivier, Carlos Hinojosa, Silvio Giancola, Bernard Ghanem
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.05490">Learning Semantic Segmentation with Query Points Supervision on Aerial Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semantic segmentation is crucial in remote sensing, where high-resolution satellite images are segmented into meaningful regions. Recent advancements in deep learning have significantly improved satellite image segmentation. However, most of these methods are typically trained in fully supervised settings that require high-quality pixel-level annotations, which are expensive and time-consuming to obtain. In this work, we present a weakly supervised learning algorithm to train semantic segmentation algorithms that only rely on query point annotations instead of full mask labels. Our proposed approach performs accurate semantic segmentation and improves efficiency by significantly reducing the cost and time required for manual annotation. Specifically, we generate superpixels and extend the query point labels into those superpixels that group similar meaningful semantics. Then, we train semantic segmentation models supervised with images partially labeled with the superpixel pseudo-labels. We benchmark our weakly supervised training approach on an aerial image dataset and different semantic segmentation architectures, showing that we can reach competitive performance compared to fully supervised training while reducing the annotation effort. The code of our proposed approach is publicly available at: https://github.com/santiago2205/LSSQPS.
<div id='section'>Paperid: <span id='pid'>279, <a href='https://arxiv.org/pdf/2309.05261.pdf' target='_blank'>https://arxiv.org/pdf/2309.05261.pdf</a></span>   <span><a href='https://gbc-iitd.github.io/wsod-gbc' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Soumen Basu, Ashish Papanai, Mayank Gupta, Pankaj Gupta, Chetan Arora
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.05261">Gall Bladder Cancer Detection from US Images with Only Image Level Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automated detection of Gallbladder Cancer (GBC) from Ultrasound (US) images is an important problem, which has drawn increased interest from researchers. However, most of these works use difficult-to-acquire information such as bounding box annotations or additional US videos. In this paper, we focus on GBC detection using only image-level labels. Such annotation is usually available based on the diagnostic report of a patient, and do not require additional annotation effort from the physicians. However, our analysis reveals that it is difficult to train a standard image classification model for GBC detection. This is due to the low inter-class variance (a malignant region usually occupies only a small portion of a US image), high intra-class variance (due to the US sensor capturing a 2D slice of a 3D object leading to large viewpoint variations), and low training data availability. We posit that even when we have only the image level label, still formulating the problem as object detection (with bounding box output) helps a deep neural network (DNN) model focus on the relevant region of interest. Since no bounding box annotations is available for training, we pose the problem as weakly supervised object detection (WSOD). Motivated by the recent success of transformer models in object detection, we train one such model, DETR, using multi-instance-learning (MIL) with self-supervised instance selection to suit the WSOD task. Our proposed method demonstrates an improvement of AP and detection sensitivity over the SOTA transformer-based and CNN-based WSOD methods. Project page is at https://gbc-iitd.github.io/wsod-gbc
<div id='section'>Paperid: <span id='pid'>280, <a href='https://arxiv.org/pdf/2309.03874.pdf' target='_blank'>https://arxiv.org/pdf/2309.03874.pdf</a></span>   <span><a href='https://github.com/eyalgomel/box-based-refinement' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Eyal Gomel, Tal Shaharabany, Lior Wolf
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.03874">Box-based Refinement for Weakly Supervised and Unsupervised Localization Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>It has been established that training a box-based detector network can enhance the localization performance of weakly supervised and unsupervised methods. Moreover, we extend this understanding by demonstrating that these detectors can be utilized to improve the original network, paving the way for further advancements. To accomplish this, we train the detectors on top of the network output instead of the image data and apply suitable loss backpropagation. Our findings reveal a significant improvement in phrase grounding for the ``what is where by looking'' task, as well as various methods of unsupervised object discovery. Our code is available at https://github.com/eyalgomel/box-based-refinement.
<div id='section'>Paperid: <span id='pid'>281, <a href='https://arxiv.org/pdf/2309.01246.pdf' target='_blank'>https://arxiv.org/pdf/2309.01246.pdf</a></span>   <span><a href='https://github.com/yhZhai/WSCL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanhao Zhai, Tianyu Luan, David Doermann, Junsong Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.01246">Towards Generic Image Manipulation Detection with Weakly-Supervised Self-Consistency Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As advanced image manipulation techniques emerge, detecting the manipulation becomes increasingly important. Despite the success of recent learning-based approaches for image manipulation detection, they typically require expensive pixel-level annotations to train, while exhibiting degraded performance when testing on images that are differently manipulated compared with training images. To address these limitations, we propose weakly-supervised image manipulation detection, such that only binary image-level labels (authentic or tampered with) are required for training purpose. Such a weakly-supervised setting can leverage more training images and has the potential to adapt quickly to new manipulation techniques. To improve the generalization ability, we propose weakly-supervised self-consistency learning (WSCL) to leverage the weakly annotated images. Specifically, two consistency properties are learned: multi-source consistency (MSC) and inter-patch consistency (IPC). MSC exploits different content-agnostic information and enables cross-source learning via an online pseudo label generation and refinement process. IPC performs global pair-wise patch-patch relationship reasoning to discover a complete region of manipulation. Extensive experiments validate that our WSCL, even though is weakly supervised, exhibits competitive performance compared with fully-supervised counterpart under both in-distribution and out-of-distribution evaluations, as well as reasonable manipulation localization ability.
<div id='section'>Paperid: <span id='pid'>282, <a href='https://arxiv.org/pdf/2308.16819.pdf' target='_blank'>https://arxiv.org/pdf/2308.16819.pdf</a></span>   <span><a href='https://github.com/fraunhoferhhi/BTSeg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Johannes KÃ¼nzel, Anna Hilsmann, Peter Eisert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.16819">BTSeg: Barlow Twins Regularization for Domain Adaptation in Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce BTSeg (Barlow Twins regularized Segmentation), an innovative, semi-supervised training approach enhancing semantic segmentation models in order to effectively tackle adverse weather conditions without requiring additional labeled training data. Images captured at similar locations but under varying adverse conditions are regarded as manifold representation of the same scene, thereby enabling the model to conceptualize its understanding of the environment. BTSeg shows cutting-edge performance for the new challenging ACG benchmark and sets a new state-of-the-art for weakly-supervised domain adaptation for the ACDC dataset. To support further research, we have made our code publicly available at https://github.com/fraunhoferhhi/BTSeg .
<div id='section'>Paperid: <span id='pid'>283, <a href='https://arxiv.org/pdf/2308.16777.pdf' target='_blank'>https://arxiv.org/pdf/2308.16777.pdf</a></span>   <span><a href='https://github.com/kodenii/Ref-Diff' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Minheng Ni, Yabo Zhang, Kailai Feng, Xiaoming Li, Yiwen Guo, Wangmeng Zuo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.16777">Ref-Diff: Zero-shot Referring Image Segmentation with Generative Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Zero-shot referring image segmentation is a challenging task because it aims to find an instance segmentation mask based on the given referring descriptions, without training on this type of paired data. Current zero-shot methods mainly focus on using pre-trained discriminative models (e.g., CLIP). However, we have observed that generative models (e.g., Stable Diffusion) have potentially understood the relationships between various visual elements and text descriptions, which are rarely investigated in this task. In this work, we introduce a novel Referring Diffusional segmentor (Ref-Diff) for this task, which leverages the fine-grained multi-modal information from generative models. We demonstrate that without a proposal generator, a generative model alone can achieve comparable performance to existing SOTA weakly-supervised models. When we combine both generative and discriminative models, our Ref-Diff outperforms these competing methods by a significant margin. This indicates that generative models are also beneficial for this task and can complement discriminative models for better referring segmentation. Our code is publicly available at https://github.com/kodenii/Ref-Diff.
<div id='section'>Paperid: <span id='pid'>284, <a href='https://arxiv.org/pdf/2308.15512.pdf' target='_blank'>https://arxiv.org/pdf/2308.15512.pdf</a></span>   <span><a href='https://southflame.github.io/sag/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongwon Kim, Namyup Kim, Cuiling Lan, Suha Kwak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.15512">Shatter and Gather: Learning Referring Image Segmentation with Text Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring image segmentation, the task of segmenting any arbitrary entities described in free-form texts, opens up a variety of vision applications. However, manual labeling of training data for this task is prohibitively costly, leading to lack of labeled data for training. We address this issue by a weakly supervised learning approach using text descriptions of training images as the only source of supervision. To this end, we first present a new model that discovers semantic entities in input image and then combines such entities relevant to text query to predict the mask of the referent. We also present a new loss function that allows the model to be trained without any further supervision. Our method was evaluated on four public benchmarks for referring image segmentation, where it clearly outperformed the existing method for the same task and recent open-vocabulary segmentation models on all the benchmarks.
<div id='section'>Paperid: <span id='pid'>285, <a href='https://arxiv.org/pdf/2308.15216.pdf' target='_blank'>https://arxiv.org/pdf/2308.15216.pdf</a></span>   <span><a href='https://github.com/cilix-ai/on-the-fly-guidance' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuelin Xin, Yicheng Chen, Shengxiang Ji, Kun Han, Xiaohui Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.15216">On-the-Fly Guidance Training for Medical Image Registration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study introduces a novel On-the-Fly Guidance (OFG) training framework for enhancing existing learning-based image registration models, addressing the limitations of weakly-supervised and unsupervised methods. Weakly-supervised methods struggle due to the scarcity of labeled data, and unsupervised methods directly depend on image similarity metrics for accuracy. Our method proposes a supervised fashion for training registration models, without the need for any labeled data. OFG generates pseudo-ground truth during training by refining deformation predictions with a differentiable optimizer, enabling direct supervised learning. OFG optimizes deformation predictions efficiently, improving the performance of registration models without sacrificing inference speed. Our method is tested across several benchmark datasets and leading models, it significantly enhanced performance, providing a plug-and-play solution for training learning-based registration models. Code available at: https://github.com/cilix-ai/on-the-fly-guidance
<div id='section'>Paperid: <span id='pid'>286, <a href='https://arxiv.org/pdf/2308.14575.pdf' target='_blank'>https://arxiv.org/pdf/2308.14575.pdf</a></span>   <span><a href='https://github.com/fawnliu/TRIS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fang Liu, Yuhao Liu, Yuqiu Kong, Ke Xu, Lihe Zhang, Baocai Yin, Gerhard Hancke, Rynson Lau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.14575">Referring Image Segmentation Using Text Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing Referring Image Segmentation (RIS) methods typically require expensive pixel-level or box-level annotations for supervision. In this paper, we observe that the referring texts used in RIS already provide sufficient information to localize the target object. Hence, we propose a novel weakly-supervised RIS framework to formulate the target localization problem as a classification process to differentiate between positive and negative text expressions. While the referring text expressions for an image are used as positive expressions, the referring text expressions from other images can be used as negative expressions for this image. Our framework has three main novelties. First, we propose a bilateral prompt method to facilitate the classification process, by harmonizing the domain discrepancy between visual and linguistic features. Second, we propose a calibration method to reduce noisy background information and improve the correctness of the response maps for target object localization. Third, we propose a positive response map selection strategy to generate high-quality pseudo-labels from the enhanced response maps, for training a segmentation network for RIS inference. For evaluation, we propose a new metric to measure localization accuracy. Experiments on four benchmarks show that our framework achieves promising performances to existing fully-supervised RIS methods while outperforming state-of-the-art weakly-supervised methods adapted from related areas. Code is available at https://github.com/fawnliu/TRIS.
<div id='section'>Paperid: <span id='pid'>287, <a href='https://arxiv.org/pdf/2308.13218.pdf' target='_blank'>https://arxiv.org/pdf/2308.13218.pdf</a></span>   <span><a href='https://github.com/yangbang18/MultiCapCLIP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bang Yang, Fenglin Liu, Xian Wu, Yaowei Wang, Xu Sun, Yuexian Zou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.13218">MultiCapCLIP: Auto-Encoding Prompts for Zero-Shot Multilingual Visual Captioning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Supervised visual captioning models typically require a large scale of images or videos paired with descriptions in a specific language (i.e., the vision-caption pairs) for training. However, collecting and labeling large-scale datasets is time-consuming and expensive for many scenarios and languages. Therefore, sufficient labeled pairs are usually not available. To deal with the label shortage problem, we present a simple yet effective zero-shot approach MultiCapCLIP that can generate visual captions for different scenarios and languages without any labeled vision-caption pairs of downstream datasets. In the training stage, MultiCapCLIP only requires text data for input. Then it conducts two main steps: 1) retrieving concept prompts that preserve the corresponding domain knowledge of new scenarios; 2) auto-encoding the prompts to learn writing styles to output captions in a desired language. In the testing stage, MultiCapCLIP instead takes visual data as input directly to retrieve the concept prompts to generate the final visual descriptions. The extensive experiments on image and video captioning across four benchmarks and four languages (i.e., English, Chinese, German, and French) confirm the effectiveness of our approach. Compared with state-of-the-art zero-shot and weakly-supervised methods, our method achieves 4.8% and 21.5% absolute improvements in terms of BLEU@4 and CIDEr metrics. Our code is available at https://github.com/yangbang18/MultiCapCLIP.
<div id='section'>Paperid: <span id='pid'>288, <a href='https://arxiv.org/pdf/2308.11681.pdf' target='_blank'>https://arxiv.org/pdf/2308.11681.pdf</a></span>   <span><a href='https://github.com/nwpu-zxr/VadCLIP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Wu, Xuerong Zhou, Guansong Pang, Lingru Zhou, Qingsen Yan, Peng Wang, Yanning Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.11681">VadCLIP: Adapting Vision-Language Models for Weakly Supervised Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent contrastive language-image pre-training (CLIP) model has shown great success in a wide range of image-level tasks, revealing remarkable ability for learning powerful visual representations with rich semantics. An open and worthwhile problem is efficiently adapting such a strong model to the video domain and designing a robust video anomaly detector. In this work, we propose VadCLIP, a new paradigm for weakly supervised video anomaly detection (WSVAD) by leveraging the frozen CLIP model directly without any pre-training and fine-tuning process. Unlike current works that directly feed extracted features into the weakly supervised classifier for frame-level binary classification, VadCLIP makes full use of fine-grained associations between vision and language on the strength of CLIP and involves dual branch. One branch simply utilizes visual features for coarse-grained binary classification, while the other fully leverages the fine-grained language-image alignment. With the benefit of dual branch, VadCLIP achieves both coarse-grained and fine-grained video anomaly detection by transferring pre-trained knowledge from CLIP to WSVAD task. We conduct extensive experiments on two commonly-used benchmarks, demonstrating that VadCLIP achieves the best performance on both coarse-grained and fine-grained WSVAD, surpassing the state-of-the-art methods by a large margin. Specifically, VadCLIP achieves 84.51% AP and 88.02% AUC on XD-Violence and UCF-Crime, respectively. Code and features are released at https://github.com/nwpu-zxr/VadCLIP.
<div id='section'>Paperid: <span id='pid'>289, <a href='https://arxiv.org/pdf/2308.11166.pdf' target='_blank'>https://arxiv.org/pdf/2308.11166.pdf</a></span>   <span><a href='https://github.com/SmiletoE/HPAL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zongyi Xu, Bo Yuan, Shanshan Zhao, Qianni Zhang, Xinbo Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.11166">Hierarchical Point-based Active Learning for Semi-supervised Point Cloud Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Impressive performance on point cloud semantic segmentation has been achieved by fully-supervised methods with large amounts of labelled data. As it is labour-intensive to acquire large-scale point cloud data with point-wise labels, many attempts have been made to explore learning 3D point cloud segmentation with limited annotations. Active learning is one of the effective strategies to achieve this purpose but is still under-explored. The most recent methods of this kind measure the uncertainty of each pre-divided region for manual labelling but they suffer from redundant information and require additional efforts for region division. This paper aims at addressing this issue by developing a hierarchical point-based active learning strategy. Specifically, we measure the uncertainty for each point by a hierarchical minimum margin uncertainty module which considers the contextual information at multiple levels. Then, a feature-distance suppression strategy is designed to select important and representative points for manual labelling. Besides, to better exploit the unlabelled data, we build a semi-supervised segmentation framework based on our active strategy. Extensive experiments on the S3DIS and ScanNetV2 datasets demonstrate that the proposed framework achieves 96.5% and 100% performance of fully-supervised baseline with only 0.07% and 0.1% training data, respectively, outperforming the state-of-the-art weakly-supervised and active learning methods. The code will be available at https://github.com/SmiletoE/HPAL.
<div id='section'>Paperid: <span id='pid'>290, <a href='https://arxiv.org/pdf/2308.11072.pdf' target='_blank'>https://arxiv.org/pdf/2308.11072.pdf</a></span>   <span><a href='https://joefioresi718.github.io/TeD-SPAD_webpage/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Joseph Fioresi, Ishan Rajendrakumar Dave, Mubarak Shah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.11072">TeD-SPAD: Temporal Distinctiveness for Self-supervised Privacy-preservation for video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video anomaly detection (VAD) without human monitoring is a complex computer vision task that can have a positive impact on society if implemented successfully. While recent advances have made significant progress in solving this task, most existing approaches overlook a critical real-world concern: privacy. With the increasing popularity of artificial intelligence technologies, it becomes crucial to implement proper AI ethics into their development. Privacy leakage in VAD allows models to pick up and amplify unnecessary biases related to people's personal information, which may lead to undesirable decision making. In this paper, we propose TeD-SPAD, a privacy-aware video anomaly detection framework that destroys visual private information in a self-supervised manner. In particular, we propose the use of a temporally-distinct triplet loss to promote temporally discriminative features, which complements current weakly-supervised VAD methods. Using TeD-SPAD, we achieve a positive trade-off between privacy protection and utility anomaly detection performance on three popular weakly supervised VAD datasets: UCF-Crime, XD-Violence, and ShanghaiTech. Our proposed anonymization model reduces private attribute prediction by 32.25% while only reducing frame-level ROC AUC on the UCF-Crime anomaly detection dataset by 3.69%. Project Page: https://joefioresi718.github.io/TeD-SPAD_webpage/
<div id='section'>Paperid: <span id='pid'>291, <a href='https://arxiv.org/pdf/2308.10112.pdf' target='_blank'>https://arxiv.org/pdf/2308.10112.pdf</a></span>   <span><a href='https://github.com/ChongQingNoSubway/PDL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhui Zhu, Peijie Qiu, Xiwen Chen, Oana M. Dumitrascu, Yalin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.10112">PDL: Regularizing Multiple Instance Learning with Progressive Dropout Layers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple instance learning (MIL) was a weakly supervised learning approach that sought to assign binary class labels to collections of instances known as bags. However, due to their weak supervision nature, the MIL methods were susceptible to overfitting and required assistance in developing comprehensive representations of target instances. While regularization typically effectively combated overfitting, its integration with the MIL model has been frequently overlooked in prior studies. Meanwhile, current regularization methods for MIL have shown limitations in their capacity to uncover a diverse array of representations. In this study, we delve into the realm of regularization within the MIL model, presenting a novel approach in the form of a Progressive Dropout Layer (PDL). We aim to not only address overfitting but also empower the MIL model in uncovering intricate and impactful feature representations. The proposed method was orthogonal to existing MIL methods and could be easily integrated into them to boost performance. Our extensive evaluation across a range of MIL benchmark datasets demonstrated that the incorporation of the PDL into multiple MIL methods not only elevated their classification performance but also augmented their potential for weakly-supervised feature localizations.
<div id='section'>Paperid: <span id='pid'>292, <a href='https://arxiv.org/pdf/2308.05991.pdf' target='_blank'>https://arxiv.org/pdf/2308.05991.pdf</a></span>   <span><a href='https://github.com/Yinyf0804/WSOD-CBL/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufei Yin, Jiajun Deng, Wengang Zhou, Li Li, Houqiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.05991">Cyclic-Bootstrap Labeling for Weakly Supervised Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent progress in weakly supervised object detection is featured by a combination of multiple instance detection networks (MIDN) and ordinal online refinement. However, with only image-level annotation, MIDN inevitably assigns high scores to some unexpected region proposals when generating pseudo labels. These inaccurate high-scoring region proposals will mislead the training of subsequent refinement modules and thus hamper the detection performance. In this work, we explore how to ameliorate the quality of pseudo-labeling in MIDN. Formally, we devise Cyclic-Bootstrap Labeling (CBL), a novel weakly supervised object detection pipeline, which optimizes MIDN with rank information from a reliable teacher network. Specifically, we obtain this teacher network by introducing a weighted exponential moving average strategy to take advantage of various refinement modules. A novel class-specific ranking distillation algorithm is proposed to leverage the output of weighted ensembled teacher network for distilling MIDN with rank information. As a result, MIDN is guided to assign higher scores to accurate proposals among their neighboring ones, thus benefiting the subsequent pseudo labeling. Extensive experiments on the prevalent PASCAL VOC 2007 \& 2012 and COCO datasets demonstrate the superior performance of our CBL framework. Code will be available at https://github.com/Yinyf0804/WSOD-CBL/.
<div id='section'>Paperid: <span id='pid'>293, <a href='https://arxiv.org/pdf/2308.05648.pdf' target='_blank'>https://arxiv.org/pdf/2308.05648.pdf</a></span>   <span><a href='https://github.com/sLdZ0306/CCR' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/sLdZ0306/CCR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zezhong Lv, Bing Su, Ji-Rong Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.05648">Counterfactual Cross-modality Reasoning for Weakly Supervised Video Moment Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video moment localization aims to retrieve the target segment of an untrimmed video according to the natural language query. Weakly supervised methods gains attention recently, as the precise temporal location of the target segment is not always available. However, one of the greatest challenges encountered by the weakly supervised method is implied in the mismatch between the video and language induced by the coarse temporal annotations. To refine the vision-language alignment, recent works contrast the cross-modality similarities driven by reconstructing masked queries between positive and negative video proposals. However, the reconstruction may be influenced by the latent spurious correlation between the unmasked and the masked parts, which distorts the restoring process and further degrades the efficacy of contrastive learning since the masked words are not completely reconstructed from the cross-modality knowledge. In this paper, we discover and mitigate this spurious correlation through a novel proposed counterfactual cross-modality reasoning method. Specifically, we first formulate query reconstruction as an aggregated causal effect of cross-modality and query knowledge. Then by introducing counterfactual cross-modality knowledge into this aggregation, the spurious impact of the unmasked part contributing to the reconstruction is explicitly modeled. Finally, by suppressing the unimodal effect of masked query, we can rectify the reconstructions of video proposals to perform reasonable contrastive learning. Extensive experimental evaluations demonstrate the effectiveness of our proposed method. The code is available at \href{https://github.com/sLdZ0306/CCR}{https://github.com/sLdZ0306/CCR}.
<div id='section'>Paperid: <span id='pid'>294, <a href='https://arxiv.org/pdf/2308.05166.pdf' target='_blank'>https://arxiv.org/pdf/2308.05166.pdf</a></span>   <span><a href='https://github.com/Nikhel1/Gal-CAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikhel Gupta, Zeeshan Hayder, Ray P. Norris, Minh Huynh, Lars Petersson, X. Rosalind Wang, Heinz Andernach, BÃ¤rbel S. Koribalski, Miranda Yew, Evan J. Crawford
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.05166">Deep Learning for Morphological Identification of Extended Radio Galaxies using Weak Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The present work discusses the use of a weakly-supervised deep learning algorithm that reduces the cost of labelling pixel-level masks for complex radio galaxies with multiple components. The algorithm is trained on weak class-level labels of radio galaxies to get class activation maps (CAMs). The CAMs are further refined using an inter-pixel relations network (IRNet) to get instance segmentation masks over radio galaxies and the positions of their infrared hosts. We use data from the Australian Square Kilometre Array Pathfinder (ASKAP) telescope, specifically the Evolutionary Map of the Universe (EMU) Pilot Survey, which covered a sky area of 270 square degrees with an RMS sensitivity of 25-35 $Î¼$Jy/beam. We demonstrate that weakly-supervised deep learning algorithms can achieve high accuracy in predicting pixel-level information, including masks for the extended radio emission encapsulating all galaxy components and the positions of the infrared host galaxies. We evaluate the performance of our method using mean Average Precision (mAP) across multiple classes at a standard intersection over union (IoU) threshold of 0.5. We show that the model achieves a mAP$_{50}$ of 67.5\% and 76.8\% for radio masks and infrared host positions, respectively. The network architecture can be found at the following link: https://github.com/Nikhel1/Gal-CAM
<div id='section'>Paperid: <span id='pid'>295, <a href='https://arxiv.org/pdf/2308.04321.pdf' target='_blank'>https://arxiv.org/pdf/2308.04321.pdf</a></span>   <span><a href='https://github.com/OpenNLPLab/ACR_WSSS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weixuan Sun, Yanhao Zhang, Zhen Qin, Zheyuan Liu, Lin Cheng, Fanyi Wang, Yiran Zhong, Nick Barnes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.04321">All-pairs Consistency Learning for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we propose a new transformer-based regularization to better localize objects for Weakly supervised semantic segmentation (WSSS). In image-level WSSS, Class Activation Map (CAM) is adopted to generate object localization as pseudo segmentation labels. To address the partial activation issue of the CAMs, consistency regularization is employed to maintain activation intensity invariance across various image augmentations. However, such methods ignore pair-wise relations among regions within each CAM, which capture context and should also be invariant across image views. To this end, we propose a new all-pairs consistency regularization (ACR). Given a pair of augmented views, our approach regularizes the activation intensities between a pair of augmented views, while also ensuring that the affinity across regions within each view remains consistent. We adopt vision transformers as the self-attention mechanism naturally embeds pair-wise affinity. This enables us to simply regularize the distance between the attention matrices of augmented image pairs. Additionally, we introduce a novel class-wise localization method that leverages the gradients of the class token. Our method can be seamlessly integrated into existing WSSS methods using transformers without modifying the architectures. We evaluate our method on PASCAL VOC and MS COCO datasets. Our method produces noticeably better class localization maps (67.3% mIoU on PASCAL VOC train), resulting in superior WSSS performances.
<div id='section'>Paperid: <span id='pid'>296, <a href='https://arxiv.org/pdf/2308.04197.pdf' target='_blank'>https://arxiv.org/pdf/2308.04197.pdf</a></span>   <span><a href='https://github.com/solicucu/D3G' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanjun Li, Xiujun Shu, Sunan He, Ruizhi Qiao, Wei Wen, Taian Guo, Bei Gan, Xing Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.04197">D3G: Exploring Gaussian Prior for Temporal Sentence Grounding with Glance Annotation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Temporal sentence grounding (TSG) aims to locate a specific moment from an untrimmed video with a given natural language query. Recently, weakly supervised methods still have a large performance gap compared to fully supervised ones, while the latter requires laborious timestamp annotations. In this study, we aim to reduce the annotation cost yet keep competitive performance for TSG task compared to fully supervised ones. To achieve this goal, we investigate a recently proposed glance-supervised temporal sentence grounding task, which requires only single frame annotation (referred to as glance annotation) for each query. Under this setup, we propose a Dynamic Gaussian prior based Grounding framework with Glance annotation (D3G), which consists of a Semantic Alignment Group Contrastive Learning module (SA-GCL) and a Dynamic Gaussian prior Adjustment module (DGA). Specifically, SA-GCL samples reliable positive moments from a 2D temporal map via jointly leveraging Gaussian prior and semantic consistency, which contributes to aligning the positive sentence-moment pairs in the joint embedding space. Moreover, to alleviate the annotation bias resulting from glance annotation and model complex queries consisting of multiple events, we propose the DGA module, which adjusts the distribution dynamically to approximate the ground truth of target moments. Extensive experiments on three challenging benchmarks verify the effectiveness of the proposed D3G. It outperforms the state-of-the-art weakly supervised methods by a large margin and narrows the performance gap compared to fully supervised methods. Code is available at https://github.com/solicucu/D3G.
<div id='section'>Paperid: <span id='pid'>297, <a href='https://arxiv.org/pdf/2308.01779.pdf' target='_blank'>https://arxiv.org/pdf/2308.01779.pdf</a></span>   <span><a href='https://github.com/LiWentomng/Point2Mask' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wentong Li, Yuqian Yuan, Song Wang, Jianke Zhu, Jianshu Li, Jian Liu, Lei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.01779">Point2Mask: Point-supervised Panoptic Segmentation via Optimal Transport</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised image segmentation has recently attracted increasing research attentions, aiming to avoid the expensive pixel-wise labeling. In this paper, we present an effective method, namely Point2Mask, to achieve high-quality panoptic prediction using only a single random point annotation per target for training. Specifically, we formulate the panoptic pseudo-mask generation as an Optimal Transport (OT) problem, where each ground-truth (gt) point label and pixel sample are defined as the label supplier and consumer, respectively. The transportation cost is calculated by the introduced task-oriented maps, which focus on the category-wise and instance-wise differences among the various thing and stuff targets. Furthermore, a centroid-based scheme is proposed to set the accurate unit number for each gt point supplier. Hence, the pseudo-mask generation is converted into finding the optimal transport plan at a globally minimal transportation cost, which can be solved via the Sinkhorn-Knopp Iteration. Experimental results on Pascal VOC and COCO demonstrate the promising performance of our proposed Point2Mask approach to point-supervised panoptic segmentation. Source code is available at: https://github.com/LiWentomng/Point2Mask.
<div id='section'>Paperid: <span id='pid'>298, <a href='https://arxiv.org/pdf/2307.16415.pdf' target='_blank'>https://arxiv.org/pdf/2307.16415.pdf</a></span>   <span><a href='https://github.com/XiaojunTang22/ICCV2023-DDGNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaojun Tang, Junsong Fan, Chuanchen Luo, Zhaoxiang Zhang, Man Zhang, Zongyuan Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.16415">DDG-Net: Discriminability-Driven Graph Network for Weakly-supervised Temporal Action Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised temporal action localization (WTAL) is a practical yet challenging task. Due to large-scale datasets, most existing methods use a network pretrained in other datasets to extract features, which are not suitable enough for WTAL. To address this problem, researchers design several modules for feature enhancement, which improve the performance of the localization module, especially modeling the temporal relationship between snippets. However, all of them neglect the adverse effects of ambiguous information, which would reduce the discriminability of others. Considering this phenomenon, we propose Discriminability-Driven Graph Network (DDG-Net), which explicitly models ambiguous snippets and discriminative snippets with well-designed connections, preventing the transmission of ambiguous information and enhancing the discriminability of snippet-level representations. Additionally, we propose feature consistency loss to prevent the assimilation of features and drive the graph convolution network to generate more discriminative representations. Extensive experiments on THUMOS14 and ActivityNet1.2 benchmarks demonstrate the effectiveness of DDG-Net, establishing new state-of-the-art results on both datasets. Source code is available at \url{https://github.com/XiaojunTang22/ICCV2023-DDGNet}.
<div id='section'>Paperid: <span id='pid'>299, <a href='https://arxiv.org/pdf/2307.13459.pdf' target='_blank'>https://arxiv.org/pdf/2307.13459.pdf</a></span>   <span><a href='https://jinnan-chen.github.io/ws3dpt/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinnan Chen, Chen Li, Gim Hee Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.13459">Weakly-supervised 3D Pose Transfer with Keypoints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The main challenges of 3D pose transfer are: 1) Lack of paired training data with different characters performing the same pose; 2) Disentangling pose and shape information from the target mesh; 3) Difficulty in applying to meshes with different topologies. We thus propose a novel weakly-supervised keypoint-based framework to overcome these difficulties. Specifically, we use a topology-agnostic keypoint detector with inverse kinematics to compute transformations between the source and target meshes. Our method only requires supervision on the keypoints, can be applied to meshes with different topologies and is shape-invariant for the target which allows extraction of pose-only information from the target meshes without transferring shape information. We further design a cycle reconstruction to perform self-supervised pose transfer without the need for ground truth deformed mesh with the same pose and shape as the target and source, respectively. We evaluate our approach on benchmark human and animal datasets, where we achieve superior performance compared to the state-of-the-art unsupervised approaches and even comparable performance with the fully supervised approaches. We test on the more challenging Mixamo dataset to verify our approach's ability in handling meshes with different topologies and complex clothes. Cross-dataset evaluation further shows the strong generalization ability of our approach.
<div id='section'>Paperid: <span id='pid'>300, <a href='https://arxiv.org/pdf/2307.13251.pdf' target='_blank'>https://arxiv.org/pdf/2307.13251.pdf</a></span>   <span><a href='https://github.com/VinAIResearch/GaPro' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tuan Duc Ngo, Binh-Son Hua, Khoi Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.13251">GaPro: Box-Supervised 3D Point Cloud Instance Segmentation Using Gaussian Processes as Pseudo Labelers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Instance segmentation on 3D point clouds (3DIS) is a longstanding challenge in computer vision, where state-of-the-art methods are mainly based on full supervision. As annotating ground truth dense instance masks is tedious and expensive, solving 3DIS with weak supervision has become more practical. In this paper, we propose GaPro, a new instance segmentation for 3D point clouds using axis-aligned 3D bounding box supervision. Our two-step approach involves generating pseudo labels from box annotations and training a 3DIS network with the resulting labels. Additionally, we employ the self-training strategy to improve the performance of our method further. We devise an effective Gaussian Process to generate pseudo instance masks from the bounding boxes and resolve ambiguities when they overlap, resulting in pseudo instance masks with their uncertainty values. Our experiments show that GaPro outperforms previous weakly supervised 3D instance segmentation methods and has competitive performance compared to state-of-the-art fully supervised ones. Furthermore, we demonstrate the robustness of our approach, where we can adapt various state-of-the-art fully supervised methods to the weak supervision task by using our pseudo labels for training. The source code and trained models are available at https://github.com/VinAIResearch/GaPro.
<div id='section'>Paperid: <span id='pid'>301, <a href='https://arxiv.org/pdf/2307.10912.pdf' target='_blank'>https://arxiv.org/pdf/2307.10912.pdf</a></span>   <span><a href='https://github.com/weijun88/WeakPolyp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun Wei, Yiwen Hu, Shuguang Cui, S. Kevin Zhou, Zhen Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.10912">WeakPolyp: You Only Look Bounding Box for Polyp Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Limited by expensive pixel-level labels, polyp segmentation models are plagued by data shortage and suffer from impaired generalization. In contrast, polyp bounding box annotations are much cheaper and more accessible. Thus, to reduce labeling cost, we propose to learn a weakly supervised polyp segmentation model (i.e., WeakPolyp) completely based on bounding box annotations. However, coarse bounding boxes contain too much noise. To avoid interference, we introduce the mask-to-box (M2B) transformation. By supervising the outer box mask of the prediction instead of the prediction itself, M2B greatly mitigates the mismatch between the coarse label and the precise prediction. But, M2B only provides sparse supervision, leading to non-unique predictions. Therefore, we further propose a scale consistency (SC) loss for dense supervision. By explicitly aligning predictions across the same image at different scales, the SC loss largely reduces the variation of predictions. Note that our WeakPolyp is a plug-and-play model, which can be easily ported to other appealing backbones. Besides, the proposed modules are only used during training, bringing no computation cost to inference. Extensive experiments demonstrate the effectiveness of our proposed WeakPolyp, which surprisingly achieves a comparable performance with a fully supervised model, requiring no mask annotations at all.
<div id='section'>Paperid: <span id='pid'>302, <a href='https://arxiv.org/pdf/2307.10853.pdf' target='_blank'>https://arxiv.org/pdf/2307.10853.pdf</a></span>   <span><a href='https://github.com/zhenghuizhao/TransWCD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenghui Zhao, Lixiang Ru, Chen Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.10853">Exploring Effective Priors and Efficient Models for Weakly-Supervised Change Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised change detection (WSCD) aims to detect pixel-level changes with only image-level annotations. Owing to its label efficiency, WSCD is drawing increasing attention recently. However, current WSCD methods often encounter the challenge of change missing and fabricating, i.e., the inconsistency between image-level annotations and pixel-level predictions. Specifically, change missing refer to the situation that the WSCD model fails to predict any changed pixels, even though the image-level label indicates changed, and vice versa for change fabricating. To address this challenge, in this work, we leverage global-scale and local-scale priors in WSCD and propose two components: a Dilated Prior (DP) decoder and a Label Gated (LG) constraint. The DP decoder decodes samples with the changed image-level label, skips samples with the unchanged label, and replaces them with an all-unchanged pixel-level label. The LG constraint is derived from the correspondence between changed representations and image-level labels, penalizing the model when it mispredicts the change status. Additionally, we develop TransWCD, a simple yet powerful transformer-based model, showcasing the potential of weakly-supervised learning in change detection. By integrating the DP decoder and LG constraint into TransWCD, we form TransWCD-DL. Our proposed TransWCD and TransWCD-DL achieve significant +6.33% and +9.55% F1 score improvements over the state-of-the-art methods on the WHU-CD dataset, respectively. Some performance metrics even exceed several fully-supervised change detection (FSCD) competitors. Code will be available at https://github.com/zhenghuizhao/TransWCD.
<div id='section'>Paperid: <span id='pid'>303, <a href='https://arxiv.org/pdf/2307.09756.pdf' target='_blank'>https://arxiv.org/pdf/2307.09756.pdf</a></span>   <span><a href='https://github.com/callsys/GenPromp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuzhong Zhao, Qixiang Ye, Weijia Wu, Chunhua Shen, Fang Wan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.09756">Generative Prompt Model for Weakly Supervised Object Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised object localization (WSOL) remains challenging when learning object localization models from image category labels. Conventional methods that discriminatively train activation models ignore representative yet less discriminative object parts. In this study, we propose a generative prompt model (GenPromp), defining the first generative pipeline to localize less discriminative object parts by formulating WSOL as a conditional image denoising procedure. During training, GenPromp converts image category labels to learnable prompt embeddings which are fed to a generative model to conditionally recover the input image with noise and learn representative embeddings. During inference, enPromp combines the representative embeddings with discriminative embeddings (queried from an off-the-shelf vision-language model) for both representative and discriminative capacity. The combined embeddings are finally used to generate multi-scale high-quality attention maps, which facilitate localizing full object extent. Experiments on CUB-200-2011 and ILSVRC show that GenPromp respectively outperforms the best discriminative models by 5.2% and 5.6% (Top-1 Loc), setting a solid baseline for WSOL with the generative model. Code is available at https://github.com/callsys/GenPromp.
<div id='section'>Paperid: <span id='pid'>304, <a href='https://arxiv.org/pdf/2307.04617.pdf' target='_blank'>https://arxiv.org/pdf/2307.04617.pdf</a></span>   <span><a href='https://github.com/Guerbet-AI/wsp-contrastive' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Emma Sarfati, Alexandre BÃ´ne, Marc-Michel RohÃ©, Pietro Gori, Isabelle Bloch
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.04617">Weakly-supervised positional contrastive learning: application to cirrhosis classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large medical imaging datasets can be cheaply and quickly annotated with low-confidence, weak labels (e.g., radiological scores). Access to high-confidence labels, such as histology-based diagnoses, is rare and costly. Pretraining strategies, like contrastive learning (CL) methods, can leverage unlabeled or weakly-annotated datasets. These methods typically require large batch sizes, which poses a difficulty in the case of large 3D images at full resolution, due to limited GPU memory. Nevertheless, volumetric positional information about the spatial context of each 2D slice can be very important for some medical applications. In this work, we propose an efficient weakly-supervised positional (WSP) contrastive learning strategy where we integrate both the spatial context of each 2D slice and a weak label via a generic kernel-based loss function. We illustrate our method on cirrhosis prediction using a large volume of weakly-labeled images, namely radiological low-confidence annotations, and small strongly-labeled (i.e., high-confidence) datasets. The proposed model improves the classification AUC by 5% with respect to a baseline model on our internal dataset, and by 26% on the public LIHC dataset from the Cancer Genome Atlas. The code is available at: https://github.com/Guerbet-AI/wsp-contrastive.
<div id='section'>Paperid: <span id='pid'>305, <a href='https://arxiv.org/pdf/2307.03376.pdf' target='_blank'>https://arxiv.org/pdf/2307.03376.pdf</a></span>   <span><a href='https://github.com/npucvr/WSCUOD.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunqiu Lv, Jing Zhang, Nick Barnes, Yuchao Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.03376">Weakly-supervised Contrastive Learning for Unsupervised Object Discovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsupervised object discovery (UOD) refers to the task of discriminating the whole region of objects from the background within a scene without relying on labeled datasets, which benefits the task of bounding-box-level localization and pixel-level segmentation. This task is promising due to its ability to discover objects in a generic manner. We roughly categorise existing techniques into two main directions, namely the generative solutions based on image resynthesis, and the clustering methods based on self-supervised models. We have observed that the former heavily relies on the quality of image reconstruction, while the latter shows limitations in effectively modeling semantic correlations. To directly target at object discovery, we focus on the latter approach and propose a novel solution by incorporating weakly-supervised contrastive learning (WCL) to enhance semantic information exploration. We design a semantic-guided self-supervised learning model to extract high-level semantic features from images, which is achieved by fine-tuning the feature encoder of a self-supervised model, namely DINO, via WCL. Subsequently, we introduce Principal Component Analysis (PCA) to localize object regions. The principal projection direction, corresponding to the maximal eigenvalue, serves as an indicator of the object region(s). Extensive experiments on benchmark unsupervised object discovery datasets demonstrate the effectiveness of our proposed solution. The source code and experimental results are publicly available via our project page at https://github.com/npucvr/WSCUOD.git.
<div id='section'>Paperid: <span id='pid'>306, <a href='https://arxiv.org/pdf/2307.02249.pdf' target='_blank'>https://arxiv.org/pdf/2307.02249.pdf</a></span>   <span><a href='https://github.com/miccaiif/INS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Linhao Qu, Yingfan Ma, Xiaoyuan Luo, Manning Wang, Zhijian Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.02249">Rethinking Multiple Instance Learning for Whole Slide Image Classification: A Good Instance Classifier is All You Need</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised whole slide image classification is usually formulated as a multiple instance learning (MIL) problem, where each slide is treated as a bag, and the patches cut out of it are treated as instances. Existing methods either train an instance classifier through pseudo-labeling or aggregate instance features into a bag feature through attention mechanisms and then train a bag classifier, where the attention scores can be used for instance-level classification. However, the pseudo instance labels constructed by the former usually contain a lot of noise, and the attention scores constructed by the latter are not accurate enough, both of which affect their performance. In this paper, we propose an instance-level MIL framework based on contrastive learning and prototype learning to effectively accomplish both instance classification and bag classification tasks. To this end, we propose an instance-level weakly supervised contrastive learning algorithm for the first time under the MIL setting to effectively learn instance feature representation. We also propose an accurate pseudo label generation method through prototype learning. We then develop a joint training strategy for weakly supervised contrastive learning, prototype learning, and instance classifier training. Extensive experiments and visualizations on four datasets demonstrate the powerful performance of our method. Codes are available at https://github.com/miccaiif/INS.
<div id='section'>Paperid: <span id='pid'>307, <a href='https://arxiv.org/pdf/2307.00097.pdf' target='_blank'>https://arxiv.org/pdf/2307.00097.pdf</a></span>   <span><a href='https://github.com/rB080/WSS_POLE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Balamurali Murugesan, Rukhshanda Hussain, Rajarshi Bhattacharya, Ismail Ben Ayed, Jose Dolz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.00097">Prompting classes: Exploring the Power of Prompt Class Learning in Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, CLIP-based approaches have exhibited remarkable performance on generalization and few-shot learning tasks, fueled by the power of contrastive language-vision pre-training. In particular, prompt tuning has emerged as an effective strategy to adapt the pre-trained language-vision models to downstream tasks by employing task-related textual tokens. Motivated by this progress, in this work we question whether other fundamental problems, such as weakly supervised semantic segmentation (WSSS), can benefit from prompt tuning. Our findings reveal two interesting observations that shed light on the impact of prompt tuning on WSSS. First, modifying only the class token of the text prompt results in a greater impact on the Class Activation Map (CAM), compared to arguably more complex strategies that optimize the context. And second, the class token associated with the image ground truth does not necessarily correspond to the category that yields the best CAM. Motivated by these observations, we introduce a novel approach based on a PrOmpt cLass lEarning (POLE) strategy. Through extensive experiments we demonstrate that our simple, yet efficient approach achieves SOTA performance in a well-known WSSS benchmark. These results highlight not only the benefits of language-vision models in WSSS but also the potential of prompt learning for this problem. The code is available at https://github.com/rB080/WSS_POLE.
<div id='section'>Paperid: <span id='pid'>308, <a href='https://arxiv.org/pdf/2306.17373.pdf' target='_blank'>https://arxiv.org/pdf/2306.17373.pdf</a></span>   <span><a href='https://github.com/szc19990412/HVTSurv' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuchen Shao, Yang Chen, Hao Bian, Jian Zhang, Guojun Liu, Yongbing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.17373">HVTSurv: Hierarchical Vision Transformer for Patient-Level Survival Prediction from Whole Slide Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Survival prediction based on whole slide images (WSIs) is a challenging task for patient-level multiple instance learning (MIL). Due to the vast amount of data for a patient (one or multiple gigapixels WSIs) and the irregularly shaped property of WSI, it is difficult to fully explore spatial, contextual, and hierarchical interaction in the patient-level bag. Many studies adopt random sampling pre-processing strategy and WSI-level aggregation models, which inevitably lose critical prognostic information in the patient-level bag. In this work, we propose a hierarchical vision Transformer framework named HVTSurv, which can encode the local-level relative spatial information, strengthen WSI-level context-aware communication, and establish patient-level hierarchical interaction. Firstly, we design a feature pre-processing strategy, including feature rearrangement and random window masking. Then, we devise three layers to progressively obtain patient-level representation, including a local-level interaction layer adopting Manhattan distance, a WSI-level interaction layer employing spatial shuffle, and a patient-level interaction layer using attention pooling. Moreover, the design of hierarchical network helps the model become more computationally efficient. Finally, we validate HVTSurv with 3,104 patients and 3,752 WSIs across 6 cancer types from The Cancer Genome Atlas (TCGA). The average C-Index is 2.50-11.30% higher than all the prior weakly supervised methods over 6 TCGA datasets. Ablation study and attention visualization further verify the superiority of the proposed HVTSurv. Implementation is available at: https://github.com/szc19990412/HVTSurv.
<div id='section'>Paperid: <span id='pid'>309, <a href='https://arxiv.org/pdf/2306.15880.pdf' target='_blank'>https://arxiv.org/pdf/2306.15880.pdf</a></span>   <span><a href='https://github.com/jianzongwu/Awesome-Open-Vocabulary' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/jianzongwu/Awesome-Open-Vocabulary' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianzong Wu, Xiangtai Li, Shilin Xu, Haobo Yuan, Henghui Ding, Yibo Yang, Xia Li, Jiangning Zhang, Yunhai Tong, Xudong Jiang, Bernard Ghanem, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.15880">Towards Open Vocabulary Learning: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of visual scene understanding, deep neural networks have made impressive advancements in various core tasks like segmentation, tracking, and detection. However, most approaches operate on the close-set assumption, meaning that the model can only identify pre-defined categories that are present in the training set. Recently, open vocabulary settings were proposed due to the rapid progress of vision language pre-training. These new approaches seek to locate and recognize categories beyond the annotated label space. The open vocabulary approach is more general, practical, and effective compared to weakly supervised and zero-shot settings. This paper provides a thorough review of open vocabulary learning, summarizing and analyzing recent developments in the field. In particular, we begin by comparing it to related concepts such as zero-shot learning, open-set recognition, and out-of-distribution detection. Then, we review several closely related tasks in the case of segmentation and detection, including long-tail problems, few-shot, and zero-shot settings. For the method survey, we first present the basic knowledge of detection and segmentation in close-set as the preliminary knowledge. Next, we examine various scenarios in which open vocabulary learning is used, identifying common design elements and core ideas. Then, we compare the recent detection and segmentation approaches in commonly used datasets and benchmarks. Finally, we conclude with insights, issues, and discussions regarding future research directions. To our knowledge, this is the first comprehensive literature review of open vocabulary learning. We keep tracing related works at https://github.com/jianzongwu/Awesome-Open-Vocabulary.
<div id='section'>Paperid: <span id='pid'>310, <a href='https://arxiv.org/pdf/2306.14451.pdf' target='_blank'>https://arxiv.org/pdf/2306.14451.pdf</a></span>   <span><a href='https://github.com/yujiangpu20/PEL4VAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujiang Pu, Xiaoyu Wu, Lulu Yang, Shengjin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.14451">Learning Prompt-Enhanced Context Features for Weakly-Supervised Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video anomaly detection under weak supervision presents significant challenges, particularly due to the lack of frame-level annotations during training. While prior research has utilized graph convolution networks and self-attention mechanisms alongside multiple instance learning (MIL)-based classification loss to model temporal relations and learn discriminative features, these methods often employ multi-branch architectures to capture local and global dependencies separately, resulting in increased parameters and computational costs. Moreover, the coarse-grained interclass separability provided by the binary constraint of MIL-based loss neglects the fine-grained discriminability within anomalous classes. In response, this paper introduces a weakly supervised anomaly detection framework that focuses on efficient context modeling and enhanced semantic discriminability. We present a Temporal Context Aggregation (TCA) module that captures comprehensive contextual information by reusing the similarity matrix and implementing adaptive fusion. Additionally, we propose a Prompt-Enhanced Learning (PEL) module that integrates semantic priors using knowledge-based prompts to boost the discriminative capacity of context features while ensuring separability between anomaly sub-classes. Extensive experiments validate the effectiveness of our method's components, demonstrating competitive performance with reduced parameters and computational effort on three challenging benchmarks: UCF-Crime, XD-Violence, and ShanghaiTech datasets. Notably, our approach significantly improves the detection accuracy of certain anomaly sub-classes, underscoring its practical value and efficacy. Our code is available at: https://github.com/yujiangpu20/PEL4VAD.
<div id='section'>Paperid: <span id='pid'>311, <a href='https://arxiv.org/pdf/2306.14003.pdf' target='_blank'>https://arxiv.org/pdf/2306.14003.pdf</a></span>   <span><a href='https://github.com/yuzhimanhua/FUTEX' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Zhang, Bowen Jin, Xiusi Chen, Yanzhen Shen, Yunyi Zhang, Yu Meng, Jiawei Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.14003">Weakly Supervised Multi-Label Classification of Full-Text Scientific Papers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Instead of relying on human-annotated training samples to build a classifier, weakly supervised scientific paper classification aims to classify papers only using category descriptions (e.g., category names, category-indicative keywords). Existing studies on weakly supervised paper classification are less concerned with two challenges: (1) Papers should be classified into not only coarse-grained research topics but also fine-grained themes, and potentially into multiple themes, given a large and fine-grained label space; and (2) full text should be utilized to complement the paper title and abstract for classification. Moreover, instead of viewing the entire paper as a long linear sequence, one should exploit the structural information such as citation links across papers and the hierarchy of sections and paragraphs in each paper. To tackle these challenges, in this study, we propose FUTEX, a framework that uses the cross-paper network structure and the in-paper hierarchy structure to classify full-text scientific papers under weak supervision. A network-aware contrastive fine-tuning module and a hierarchy-aware aggregation module are designed to leverage the two types of structural signals, respectively. Experiments on two benchmark datasets demonstrate that FUTEX significantly outperforms competitive baselines and is on par with fully supervised classifiers that use 1,000 to 60,000 ground-truth training samples.
<div id='section'>Paperid: <span id='pid'>312, <a href='https://arxiv.org/pdf/2306.13301.pdf' target='_blank'>https://arxiv.org/pdf/2306.13301.pdf</a></span>   <span><a href='https://github.com/zhizhongchai/ORF-Net' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/zhizhongchai/ORF-Net/tree/main' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhizhong Chai, Luyang Luo, Huangjing Lin, Pheng-Ann Heng, Hao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.13301">Deep Omni-supervised Learning for Rib Fracture Detection from Chest Radiology Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning (DL)-based rib fracture detection has shown promise of playing an important role in preventing mortality and improving patient outcome. Normally, developing DL-based object detection models requires a huge amount of bounding box annotation. However, annotating medical data is time-consuming and expertise-demanding, making obtaining a large amount of fine-grained annotations extremely infeasible. This poses a pressing need {for} developing label-efficient detection models to alleviate radiologists' labeling burden. To tackle this challenge, the literature on object detection has witnessed an increase of weakly-supervised and semi-supervised approaches, yet still lacks a unified framework that leverages various forms of fully-labeled, weakly-labeled, and unlabeled data. In this paper, we present a novel omni-supervised object detection network, ORF-Netv2, to leverage as much available supervision as possible. Specifically, a multi-branch omni-supervised detection head is introduced with each branch trained with a specific type of supervision. A co-training-based dynamic label assignment strategy is then proposed to enable flexible and robust learning from the weakly-labeled and unlabeled data. Extensive evaluation was conducted for the proposed framework with three rib fracture datasets on both chest CT and X-ray. By leveraging all forms of supervision, ORF-Netv2 achieves mAPs of 34.7, 44.7, and 19.4 on the three datasets, respectively, surpassing the baseline detector which uses only box annotations by mAP gains of 3.8, 4.8, and 5.0, respectively. Furthermore, ORF-Netv2 consistently outperforms other competitive label-efficient methods over various scenarios, showing a promising framework for label-efficient fracture detection. The code is available at: https://github.com/zhizhongchai/ORF-Net.
<div id='section'>Paperid: <span id='pid'>313, <a href='https://arxiv.org/pdf/2306.12153.pdf' target='_blank'>https://arxiv.org/pdf/2306.12153.pdf</a></span>   <span><a href='https://github.com/lseventeen/DIAS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wentao Liu, Tong Tian, Lemeng Wang, Weijin Xu, Lei Li, Haoyuan Li, Wenyi Zhao, Siyu Tian, Xipeng Pan, Huihua Yang, Feng Gao, Yiming Deng, Xin Yang, Ruisheng Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.12153">DIAS: A Dataset and Benchmark for Intracranial Artery Segmentation in DSA sequences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The automated segmentation of Intracranial Arteries (IA) in Digital Subtraction Angiography (DSA) plays a crucial role in the quantification of vascular morphology, significantly contributing to computer-assisted stroke research and clinical practice. Current research primarily focuses on the segmentation of single-frame DSA using proprietary datasets. However, these methods face challenges due to the inherent limitation of single-frame DSA, which only partially displays vascular contrast, thereby hindering accurate vascular structure representation. In this work, we introduce DIAS, a dataset specifically developed for IA segmentation in DSA sequences. We establish a comprehensive benchmark for evaluating DIAS, covering full, weak, and semi-supervised segmentation methods. Specifically, we propose the vessel sequence segmentation network, in which the sequence feature extraction module effectively captures spatiotemporal representations of intravascular contrast, achieving intracranial artery segmentation in 2D+Time DSA sequences. For weakly-supervised IA segmentation, we propose a novel scribble learning-based image segmentation framework, which, under the guidance of scribble labels, employs cross pseudo-supervision and consistency regularization to improve the performance of the segmentation network. Furthermore, we introduce the random patch-based self-training framework, aimed at alleviating the performance constraints encountered in IA segmentation due to the limited availability of annotated DSA data. Our extensive experiments on the DIAS dataset demonstrate the effectiveness of these methods as potential baselines for future research and clinical applications. The dataset and code are publicly available at https://doi.org/10.5281/zenodo.11396520 and https://github.com/lseventeen/DIAS.
<div id='section'>Paperid: <span id='pid'>314, <a href='https://arxiv.org/pdf/2306.07193.pdf' target='_blank'>https://arxiv.org/pdf/2306.07193.pdf</a></span>   <span><a href='https://github.com/ritaranx/wander' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/ritaranx/wander' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ran Xu, Yue Yu, Joyce C. Ho, Carl Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.07193">Weakly-Supervised Scientific Document Classification via Retrieval-Augmented Multi-Stage Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scientific document classification is a critical task for a wide range of applications, but the cost of obtaining massive amounts of human-labeled data can be prohibitive. To address this challenge, we propose a weakly-supervised approach for scientific document classification using label names only. In scientific domains, label names often include domain-specific concepts that may not appear in the document corpus, making it difficult to match labels and documents precisely. To tackle this issue, we propose WANDER, which leverages dense retrieval to perform matching in the embedding space to capture the semantics of label names. We further design the label name expansion module to enrich the label name representations. Lastly, a self-training step is used to refine the predictions. The experiments on three datasets show that WANDER outperforms the best baseline by 11.9% on average. Our code will be published at https://github.com/ritaranx/wander.
<div id='section'>Paperid: <span id='pid'>315, <a href='https://arxiv.org/pdf/2306.05350.pdf' target='_blank'>https://arxiv.org/pdf/2306.05350.pdf</a></span>   <span><a href='https://github.com/usc-sail/peft-ser' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tiantian Feng, Shrikanth Narayanan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.05350">PEFT-SER: On the Use of Parameter Efficient Transfer Learning Approaches For Speech Emotion Recognition Using Pre-trained Speech Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many recent studies have focused on fine-tuning pre-trained models for speech emotion recognition (SER), resulting in promising performance compared to traditional methods that rely largely on low-level, knowledge-inspired acoustic features. These pre-trained speech models learn general-purpose speech representations using self-supervised or weakly-supervised learning objectives from large-scale datasets. Despite the significant advances made in SER through the use of pre-trained architecture, fine-tuning these large pre-trained models for different datasets requires saving copies of entire weight parameters, rendering them impractical to deploy in real-world settings. As an alternative, this work explores parameter-efficient fine-tuning (PEFT) approaches for adapting pre-trained speech models for emotion recognition. Specifically, we evaluate the efficacy of adapter tuning, embedding prompt tuning, and LoRa (Low-rank approximation) on four popular SER testbeds. Our results reveal that LoRa achieves the best fine-tuning performance in emotion recognition while enhancing fairness and requiring only a minimal extra amount of weight parameters. Furthermore, our findings offer novel insights into future research directions in SER, distinct from existing approaches focusing on directly fine-tuning the model architecture. Our code is publicly available under: https://github.com/usc-sail/peft-ser.
<div id='section'>Paperid: <span id='pid'>316, <a href='https://arxiv.org/pdf/2306.05029.pdf' target='_blank'>https://arxiv.org/pdf/2306.05029.pdf</a></span>   <span><a href='https://github.com/hustvl/MMIL-Transformer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruijie Zhang, Qiaozhe Zhang, Yingzhuang Liu, Hao Xin, Yan Liu, Xinggang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.05029">Multi-level Multiple Instance Learning with Transformer for Whole Slide Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Whole slide image (WSI) refers to a type of high-resolution scanned tissue image, which is extensively employed in computer-assisted diagnosis (CAD). The extremely high resolution and limited availability of region-level annotations make employing deep learning methods for WSI-based digital diagnosis challenging. Recently integrating multiple instance learning (MIL) and Transformer for WSI analysis shows very promising results. However, designing effective Transformers for this weakly-supervised high-resolution image analysis is an underexplored yet important problem. In this paper, we propose a Multi-level MIL (MMIL) scheme by introducing a hierarchical structure to MIL, which enables efficient handling of MIL tasks involving a large number of instances. Based on MMIL, we instantiated MMIL-Transformer, an efficient Transformer model with windowed exact self-attention for large-scale MIL tasks. To validate its effectiveness, we conducted a set of experiments on WSI classification tasks, where MMIL-Transformer demonstrate superior performance compared to existing state-of-the-art methods, i.e., 96.80% test AUC and 97.67% test accuracy on the CAMELYON16 dataset, 99.04% test AUC and 94.37% test accuracy on the TCGA-NSCLC dataset, respectively. All code and pre-trained models are available at: https://github.com/hustvl/MMIL-Transformer
<div id='section'>Paperid: <span id='pid'>317, <a href='https://arxiv.org/pdf/2306.03630.pdf' target='_blank'>https://arxiv.org/pdf/2306.03630.pdf</a></span>   <span><a href='https://github.com/baneitixiaomai/MIRV' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aixuan Li, Yuxin Mao, Jing Zhang, Yuchao Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.03630">Mutual Information Regularization for Weakly-supervised RGB-D Salient Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present a weakly-supervised RGB-D salient object detection model via scribble supervision. Specifically, as a multimodal learning task, we focus on effective multimodal representation learning via inter-modal mutual information regularization. In particular, following the principle of disentangled representation learning, we introduce a mutual information upper bound with a mutual information minimization regularizer to encourage the disentangled representation of each modality for salient object detection. Based on our multimodal representation learning framework, we introduce an asymmetric feature extractor for our multimodal data, which is proven more effective than the conventional symmetric backbone setting. We also introduce multimodal variational auto-encoder as stochastic prediction refinement techniques, which takes pseudo labels from the first training stage as supervision and generates refined prediction. Experimental results on benchmark RGB-D salient object detection datasets verify both effectiveness of our explicit multimodal disentangled representation learning method and the stochastic prediction refinement strategy, achieving comparable performance with the state-of-the-art fully supervised models. Our code and data are available at: https://github.com/baneitixiaomai/MIRV.
<div id='section'>Paperid: <span id='pid'>318, <a href='https://arxiv.org/pdf/2306.02691.pdf' target='_blank'>https://arxiv.org/pdf/2306.02691.pdf</a></span>   <span><a href='https://github.com/wuyongjianCODE/Cyclic' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Zhou, Yongjian Wu, Zihua Wang, Bingzheng Wei, Maode Lai, Jianzhong Shou, Yubo Fan, Yan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.02691">Cyclic Learning: Bridging Image-level Labels and Nuclei Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nuclei instance segmentation on histopathology images is of great clinical value for disease analysis. Generally, fully-supervised algorithms for this task require pixel-wise manual annotations, which is especially time-consuming and laborious for the high nuclei density. To alleviate the annotation burden, we seek to solve the problem through image-level weakly supervised learning, which is underexplored for nuclei instance segmentation. Compared with most existing methods using other weak annotations (scribble, point, etc.) for nuclei instance segmentation, our method is more labor-saving. The obstacle to using image-level annotations in nuclei instance segmentation is the lack of adequate location information, leading to severe nuclei omission or overlaps. In this paper, we propose a novel image-level weakly supervised method, called cyclic learning, to solve this problem. Cyclic learning comprises a front-end classification task and a back-end semi-supervised instance segmentation task to benefit from multi-task learning (MTL). We utilize a deep learning classifier with interpretability as the front-end to convert image-level labels to sets of high-confidence pseudo masks and establish a semi-supervised architecture as the back-end to conduct nuclei instance segmentation under the supervision of these pseudo masks. Most importantly, cyclic learning is designed to circularly share knowledge between the front-end classifier and the back-end semi-supervised part, which allows the whole system to fully extract the underlying information from image-level labels and converge to a better optimum. Experiments on three datasets demonstrate the good generality of our method, which outperforms other image-level weakly supervised methods for nuclei instance segmentation, and achieves comparable performance to fully-supervised methods.
<div id='section'>Paperid: <span id='pid'>319, <a href='https://arxiv.org/pdf/2306.00451.pdf' target='_blank'>https://arxiv.org/pdf/2306.00451.pdf</a></span>   <span><a href='https://github.com/lofrienger/S2ME' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>An Wang, Mengya Xu, Yang Zhang, Mobarakol Islam, Hongliang Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.00451">S$^2$ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-supervised Polyp Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fully-supervised polyp segmentation has accomplished significant triumphs over the years in advancing the early diagnosis of colorectal cancer. However, label-efficient solutions from weak supervision like scribbles are rarely explored yet primarily meaningful and demanding in medical practice due to the expensiveness and scarcity of densely-annotated polyp data. Besides, various deployment issues, including data shifts and corruption, put forward further requests for model generalization and robustness. To address these concerns, we design a framework of Spatial-Spectral Dual-branch Mutual Teaching and Entropy-guided Pseudo Label Ensemble Learning (S$^2$ME). Concretely, for the first time in weakly-supervised medical image segmentation, we promote the dual-branch co-teaching framework by leveraging the intrinsic complementarity of features extracted from the spatial and spectral domains and encouraging cross-space consistency through collaborative optimization. Furthermore, to produce reliable mixed pseudo labels, which enhance the effectiveness of ensemble learning, we introduce a novel adaptive pixel-wise fusion technique based on the entropy guidance from the spatial and spectral branches. Our strategy efficiently mitigates the deleterious effects of uncertainty and noise present in pseudo labels and surpasses previous alternatives in terms of efficacy. Ultimately, we formulate a holistic optimization objective to learn from the hybrid supervision of scribbles and pseudo labels. Extensive experiments and evaluation on four public datasets demonstrate the superiority of our method regarding in-distribution accuracy, out-of-distribution generalization, and robustness, highlighting its promising clinical significance. Our code is available at https://github.com/lofrienger/S2ME.
<div id='section'>Paperid: <span id='pid'>320, <a href='https://arxiv.org/pdf/2305.19867.pdf' target='_blank'>https://arxiv.org/pdf/2305.19867.pdf</a></span>   <span><a href='https://github.com/hasan1292/mDDPM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hasan Iqbal, Umar Khalid, Jing Hua, Chen Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.19867">Unsupervised Anomaly Detection in Medical Images Using Masked Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>It can be challenging to identify brain MRI anomalies using supervised deep-learning techniques due to anatomical heterogeneity and the requirement for pixel-level labeling. Unsupervised anomaly detection approaches provide an alternative solution by relying only on sample-level labels of healthy brains to generate a desired representation to identify abnormalities at the pixel level. Although, generative models are crucial for generating such anatomically consistent representations of healthy brains, accurately generating the intricate anatomy of the human brain remains a challenge. In this study, we present a method called masked-DDPM (mDPPM), which introduces masking-based regularization to reframe the generation task of diffusion models. Specifically, we introduce Masked Image Modeling (MIM) and Masked Frequency Modeling (MFM) in our self-supervised approach that enables models to learn visual representations from unlabeled data. To the best of our knowledge, this is the first attempt to apply MFM in DPPM models for medical applications. We evaluate our approach on datasets containing tumors and numerous sclerosis lesions and exhibit the superior performance of our unsupervised method as compared to the existing fully/weakly supervised baselines. Code is available at https://github.com/hasan1292/mDDPM.
<div id='section'>Paperid: <span id='pid'>321, <a href='https://arxiv.org/pdf/2305.19812.pdf' target='_blank'>https://arxiv.org/pdf/2305.19812.pdf</a></span>   <span><a href='https://github.com/xiaoaoran/3D_label_efficient_learning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aoran Xiao, Xiaoqin Zhang, Ling Shao, Shijian Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.19812">A Survey of Label-Efficient Deep Learning for 3D Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the past decade, deep neural networks have achieved significant progress in point cloud learning. However, collecting large-scale precisely-annotated training data is extremely laborious and expensive, which hinders the scalability of existing point cloud datasets and poses a bottleneck for efficient exploration of point cloud data in various tasks and applications. Label-efficient learning offers a promising solution by enabling effective deep network training with much-reduced annotation efforts. This paper presents the first comprehensive survey of label-efficient learning of point clouds. We address three critical questions in this emerging research field: i) the importance and urgency of label-efficient learning in point cloud processing, ii) the subfields it encompasses, and iii) the progress achieved in this area. To achieve this, we propose a taxonomy that organizes label-efficient learning methods based on the data prerequisites provided by different types of labels. We categorize four typical label-efficient learning approaches that significantly reduce point cloud annotation efforts: data augmentation, domain transfer learning, weakly-supervised learning, and pretrained foundation models. For each approach, we outline the problem setup and provide an extensive literature review that showcases relevant progress and challenges. Finally, we share insights into current research challenges and potential future directions. A project associated with this survey has been built at https://github.com/xiaoaoran/3D_label_efficient_learning.
<div id='section'>Paperid: <span id='pid'>322, <a href='https://arxiv.org/pdf/2305.17861.pdf' target='_blank'>https://arxiv.org/pdf/2305.17861.pdf</a></span>   <span><a href='https://github.com/RenHuan1999/CVPR2023_P-MIL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huan Ren, Wenfei Yang, Tianzhu Zhang, Yongdong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.17861">Proposal-Based Multiple Instance Learning for Weakly-Supervised Temporal Action Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised temporal action localization aims to localize and recognize actions in untrimmed videos with only video-level category labels during training. Without instance-level annotations, most existing methods follow the Segment-based Multiple Instance Learning (S-MIL) framework, where the predictions of segments are supervised by the labels of videos. However, the objective for acquiring segment-level scores during training is not consistent with the target for acquiring proposal-level scores during testing, leading to suboptimal results. To deal with this problem, we propose a novel Proposal-based Multiple Instance Learning (P-MIL) framework that directly classifies the candidate proposals in both the training and testing stages, which includes three key designs: 1) a surrounding contrastive feature extraction module to suppress the discriminative short proposals by considering the surrounding contrastive information, 2) a proposal completeness evaluation module to inhibit the low-quality proposals with the guidance of the completeness pseudo labels, and 3) an instance-level rank consistency loss to achieve robust detection by leveraging the complementarity of RGB and FLOW modalities. Extensive experimental results on two challenging benchmarks including THUMOS14 and ActivityNet demonstrate the superior performance of our method.
<div id='section'>Paperid: <span id='pid'>323, <a href='https://arxiv.org/pdf/2305.17343.pdf' target='_blank'>https://arxiv.org/pdf/2305.17343.pdf</a></span>   <span><a href='https://github.com/Franklin905/VALOR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yung-Hsuan Lai, Yen-Chun Chen, Yu-Chiang Frank Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.17343">Modality-Independent Teachers Meet Weakly-Supervised Audio-Visual Event Parser</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-visual learning has been a major pillar of multi-modal machine learning, where the community mostly focused on its modality-aligned setting, i.e., the audio and visual modality are both assumed to signal the prediction target. With the Look, Listen, and Parse dataset (LLP), we investigate the under-explored unaligned setting, where the goal is to recognize audio and visual events in a video with only weak labels observed. Such weak video-level labels only tell what events happen without knowing the modality they are perceived (audio, visual, or both). To enhance learning in this challenging setting, we incorporate large-scale contrastively pre-trained models as the modality teachers. A simple, effective, and generic method, termed Visual-Audio Label Elaboration (VALOR), is innovated to harvest modality labels for the training events. Empirical studies show that the harvested labels significantly improve an attentional baseline by 8.0 in average F-score (Type@AV). Surprisingly, we found that modality-independent teachers outperform their modality-fused counterparts since they are noise-proof from the other potentially unaligned modality. Moreover, our best model achieves the new state-of-the-art on all metrics of LLP by a substantial margin (+5.4 F-score for Type@AV). VALOR is further generalized to Audio-Visual Event Localization and achieves the new state-of-the-art as well. Code is available at: https://github.com/Franklin905/VALOR.
<div id='section'>Paperid: <span id='pid'>324, <a href='https://arxiv.org/pdf/2305.17054.pdf' target='_blank'>https://arxiv.org/pdf/2305.17054.pdf</a></span>   <span><a href='https://github.com/miccai2023anony/RenalVesselSeg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Peidi Xu, Olga Sosnovtseva, Charlotte Mehlin SÃ¸rensen, Kenny Erleben, Sune Darkner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.17054">Extremely weakly-supervised blood vessel segmentation with physiologically based synthesis and domain adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate analysis and modeling of renal functions require a precise segmentation of the renal blood vessels. Micro-CT scans provide image data at higher resolutions, making more small vessels near the renal cortex visible. Although deep-learning-based methods have shown state-of-the-art performance in automatic blood vessel segmentations, they require a large amount of labeled training data. However, voxel-wise labeling in micro-CT scans is extremely time-consuming given the huge volume sizes. To mitigate the problem, we simulate synthetic renal vascular trees physiologically while generating corresponding scans of the simulated trees by training a generative model on unlabeled scans. This enables the generative model to learn the mapping implicitly without the need for explicit functions to emulate the image acquisition process. We further propose an additional segmentation branch over the generative model trained on the generated scans. We demonstrate that the model can directly segment blood vessels on real scans and validate our method on both 3D micro-CT scans of rat kidneys and a proof-of-concept experiment on 2D retinal images. Code and 3D results are available at https://github.com/miccai2023anony/RenalVesselSeg
<div id='section'>Paperid: <span id='pid'>325, <a href='https://arxiv.org/pdf/2305.15832.pdf' target='_blank'>https://arxiv.org/pdf/2305.15832.pdf</a></span>   <span><a href='https://github.com/LiyaoTang/ERDA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liyao Tang, Zhe Chen, Shanshan Zhao, Chaoyue Wang, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.15832">All Points Matter: Entropy-Regularized Distribution Alignment for Weakly-supervised 3D Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pseudo-labels are widely employed in weakly supervised 3D segmentation tasks where only sparse ground-truth labels are available for learning. Existing methods often rely on empirical label selection strategies, such as confidence thresholding, to generate beneficial pseudo-labels for model training. This approach may, however, hinder the comprehensive exploitation of unlabeled data points. We hypothesize that this selective usage arises from the noise in pseudo-labels generated on unlabeled data. The noise in pseudo-labels may result in significant discrepancies between pseudo-labels and model predictions, thus confusing and affecting the model training greatly. To address this issue, we propose a novel learning strategy to regularize the generated pseudo-labels and effectively narrow the gaps between pseudo-labels and model predictions. More specifically, our method introduces an Entropy Regularization loss and a Distribution Alignment loss for weakly supervised learning in 3D segmentation tasks, resulting in an ERDA learning strategy. Interestingly, by using KL distance to formulate the distribution alignment loss, it reduces to a deceptively simple cross-entropy-based loss which optimizes both the pseudo-label generation network and the 3D segmentation network simultaneously. Despite the simplicity, our method promisingly improves the performance. We validate the effectiveness through extensive experiments on various baselines and large-scale datasets. Results show that ERDA effectively enables the effective usage of all unlabeled data points for learning and achieves state-of-the-art performance under different settings. Remarkably, our method can outperform fully-supervised baselines using only 1% of true annotations. Code and model will be made publicly available at https://github.com/LiyaoTang/ERDA.
<div id='section'>Paperid: <span id='pid'>326, <a href='https://arxiv.org/pdf/2305.14691.pdf' target='_blank'>https://arxiv.org/pdf/2305.14691.pdf</a></span>   <span><a href='https://github.com/DongChen06/Label-efficient-in-Agriculture' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiajia Li, Dong Chen, Xinda Qi, Zhaojian Li, Yanbo Huang, Daniel Morris, Xiaobo Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.14691">Label-Efficient Learning in Agriculture: A Comprehensive Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The past decade has witnessed many great successes of machine learning (ML) and deep learning (DL) applications in agricultural systems, including weed control, plant disease diagnosis, agricultural robotics, and precision livestock management. Despite tremendous progresses, one downside of such ML/DL models is that they generally rely on large-scale labeled datasets for training, and the performance of such models is strongly influenced by the size and quality of available labeled data samples. In addition, collecting, processing, and labeling such large-scale datasets is extremely costly and time-consuming, partially due to the rising cost in human labor. Therefore, developing label-efficient ML/DL methods for agricultural applications has received significant interests among researchers and practitioners. In fact, there are more than 50 papers on developing and applying deep-learning-based label-efficient techniques to address various agricultural problems since 2016, which motivates the authors to provide a timely and comprehensive review of recent label-efficient ML/DL methods in agricultural applications. To this end, we first develop a principled taxonomy to organize these methods according to the degree of supervision, including weak supervision (i.e., active learning and semi-/weakly- supervised learning), and no supervision (i.e., un-/self- supervised learning), supplemented by representative state-of-the-art label-efficient ML/DL methods. In addition, a systematic review of various agricultural applications exploiting these label-efficient algorithms, such as precision agriculture, plant phenotyping, and postharvest quality assessment, is presented. Finally, we discuss the current problems and challenges, as well as future research directions. A well-classified paper list can be accessed at https://github.com/DongChen06/Label-efficient-in-Agriculture.
<div id='section'>Paperid: <span id='pid'>327, <a href='https://arxiv.org/pdf/2305.14093.pdf' target='_blank'>https://arxiv.org/pdf/2305.14093.pdf</a></span>   <span><a href='https://github.com/Kunhao-Liu/3D-OVS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kunhao Liu, Fangneng Zhan, Jiahui Zhang, Muyu Xu, Yingchen Yu, Abdulmotaleb El Saddik, Christian Theobalt, Eric Xing, Shijian Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.14093">Weakly Supervised 3D Open-vocabulary Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-vocabulary segmentation of 3D scenes is a fundamental function of human perception and thus a crucial objective in computer vision research. However, this task is heavily impeded by the lack of large-scale and diverse 3D open-vocabulary segmentation datasets for training robust and generalizable models. Distilling knowledge from pre-trained 2D open-vocabulary segmentation models helps but it compromises the open-vocabulary feature as the 2D models are mostly finetuned with close-vocabulary datasets. We tackle the challenges in 3D open-vocabulary segmentation by exploiting pre-trained foundation models CLIP and DINO in a weakly supervised manner. Specifically, given only the open-vocabulary text descriptions of the objects in a scene, we distill the open-vocabulary multimodal knowledge and object reasoning capability of CLIP and DINO into a neural radiance field (NeRF), which effectively lifts 2D features into view-consistent 3D segmentation. A notable aspect of our approach is that it does not require any manual segmentation annotations for either the foundation models or the distillation process. Extensive experiments show that our method even outperforms fully supervised models trained with segmentation annotations in certain scenes, suggesting that 3D open-vocabulary segmentation can be effectively learned from 2D images and text-image pairs. Code is available at \url{https://github.com/Kunhao-Liu/3D-OVS}.
<div id='section'>Paperid: <span id='pid'>328, <a href='https://arxiv.org/pdf/2305.12749.pdf' target='_blank'>https://arxiv.org/pdf/2305.12749.pdf</a></span>   <span><a href='https://github.com/ZihanWangKi/x-TC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihan Wang, Tianle Wang, Dheeraj Mekala, Jingbo Shang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.12749">A Benchmark on Extremely Weakly Supervised Text Classification: Reconcile Seed Matching and Prompting Approaches</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Etremely Weakly Supervised Text Classification (XWS-TC) refers to text classification based on minimal high-level human guidance, such as a few label-indicative seed words or classification instructions. There are two mainstream approaches for XWS-TC, however, never being rigorously compared: (1) training classifiers based on pseudo-labels generated by (softly) matching seed words (SEED) and (2) prompting (and calibrating) language models using classification instruction (and raw texts) to decode label words (PROMPT). This paper presents the first XWS-TC benchmark to compare the two approaches on fair grounds, where the datasets, supervisions, and hyperparameter choices are standardized across methods. Our benchmarking results suggest that (1) Both SEED and PROMPT approaches are competitive and there is no clear winner; (2) SEED is empirically more tolerant than PROMPT to human guidance (e.g., seed words, classification instructions, and label words) changes; (3) SEED is empirically more selective than PROMPT to the pre-trained language models; (4) Recent SEED and PROMPT methods have close connections and a clustering post-processing step based on raw in-domain texts is a strong performance booster to both. We hope this benchmark serves as a guideline in selecting XWS-TC methods in different scenarios and stimulate interest in developing guidance- and model-robust XWS-TC methods. We release the repo at https://github.com/ZihanWangKi/x-TC.
<div id='section'>Paperid: <span id='pid'>329, <a href='https://arxiv.org/pdf/2305.12223.pdf' target='_blank'>https://arxiv.org/pdf/2305.12223.pdf</a></span>   <span><a href='https://github.com/TencentARC/GVT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangzhi Wang, Yixiao Ge, Xiaohan Ding, Mohan Kankanhalli, Ying Shan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.12223">What Makes for Good Visual Tokenizers for Large Language Models?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We empirically investigate proper pre-training methods to build good visual tokenizers, making Large Language Models (LLMs) powerful Multimodal Large Language Models (MLLMs). In our benchmark, which is curated to evaluate MLLMs visual semantic understanding and fine-grained perception capabilities, we discussed different visual tokenizers pre-trained with dominant methods (i.e., DeiT, CLIP, MAE, DINO), and observe that: i) Fully/weakly supervised models capture more semantics than self-supervised models, but the gap is narrowed by scaling up the pre-training dataset. ii) Self-supervised models are better at fine-grained perception, where patch-level supervision is particularly effective. iii) Tuning the visual tokenizer leads to the loss of semantics obtained from large-scale pretraining, which is unfavorable with relatively small-scale instruction-tuning dataset. Given the findings, we reviewed methods that attempted to unify semantics and fine-grained visual understanding, e.g., patch-level feature distillation with semantically-rich targets. We obtain an intriguing insight mask-based strategies that were once all the rage may not be applicable for obtaining good visual tokenizers. Based on this critical observation, we obtain a new MLLM equipped with a tailored Good Visual Tokenizer (GVT), which exhibits strong visual comprehension capability at multiple scales. In particular, without introducing extra parameters and task-specific fine-tuning, GVT achieves superior performance on visual question answering, image captioning, and other fine-grained visual understanding tasks such as object counting and multi-class identification.
<div id='section'>Paperid: <span id='pid'>330, <a href='https://arxiv.org/pdf/2305.11229.pdf' target='_blank'>https://arxiv.org/pdf/2305.11229.pdf</a></span>   <span><a href='https://github.com/usc-sail/trust-ser' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tiantian Feng, Rajat Hebbar, Shrikanth Narayanan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.11229">TrustSER: On the Trustworthiness of Fine-tuning Pre-trained Speech Embeddings For Speech Emotion Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent studies have explored the use of pre-trained embeddings for speech emotion recognition (SER), achieving comparable performance to conventional methods that rely on low-level knowledge-inspired acoustic features. These embeddings are often generated from models trained on large-scale speech datasets using self-supervised or weakly-supervised learning objectives. Despite the significant advancements made in SER through the use of pre-trained embeddings, there is a limited understanding of the trustworthiness of these methods, including privacy breaches, unfair performance, vulnerability to adversarial attacks, and computational cost, all of which may hinder the real-world deployment of these systems. In response, we introduce TrustSER, a general framework designed to evaluate the trustworthiness of SER systems using deep learning methods, with a focus on privacy, safety, fairness, and sustainability, offering unique insights into future research in the field of SER. Our code is publicly available under: https://github.com/usc-sail/trust-ser.
<div id='section'>Paperid: <span id='pid'>331, <a href='https://arxiv.org/pdf/2305.10661.pdf' target='_blank'>https://arxiv.org/pdf/2305.10661.pdf</a></span>   <span><a href='https://github.com/yitongli123/ISC-TE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yitong Li, Chang Liu, Jie Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.10661">Scribble-Supervised Target Extraction Method Based on Inner Structure-Constraint for Remote Sensing Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised learning based on scribble annotations in target extraction of remote sensing images has drawn much interest due to scribbles' flexibility in denoting winding objects and low cost of manually labeling. However, scribbles are too sparse to identify object structure and detailed information, bringing great challenges in target localization and boundary description. To alleviate these problems, in this paper, we construct two inner structure-constraints, a deformation consistency loss and a trainable active contour loss, together with a scribble-constraint to supervise the optimization of the encoder-decoder network without introducing any auxiliary module or extra operation based on prior cues. Comprehensive experiments demonstrate our method's superiority over five state-of-the-art algorithms in this field. Source code is available at https://github.com/yitongli123/ISC-TE.
<div id='section'>Paperid: <span id='pid'>332, <a href='https://arxiv.org/pdf/2305.08685.pdf' target='_blank'>https://arxiv.org/pdf/2305.08685.pdf</a></span>   <span><a href='https://github.com/linhuixiao/CLIP-VG' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/linhuixiao/CLIP-VG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Linhui Xiao, Xiaoshan Yang, Fang Peng, Ming Yan, Yaowei Wang, Changsheng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.08685">CLIP-VG: Self-paced Curriculum Adapting of CLIP for Visual Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Grounding (VG) is a crucial topic in the field of vision and language, which involves locating a specific region described by expressions within an image. To reduce the reliance on manually labeled data, unsupervised visual grounding have been developed to locate regions using pseudo-labels. However, the performance of existing unsupervised methods is highly dependent on the quality of pseudo-labels and these methods always encounter issues with limited diversity. In order to utilize vision and language pre-trained models to address the grounding problem, and reasonably take advantage of pseudo-labels, we propose CLIP-VG, a novel method that can conduct self-paced curriculum adapting of CLIP with pseudo-language labels. We propose a simple yet efficient end-to-end network architecture to realize the transfer of CLIP to the visual grounding. Based on the CLIP-based architecture, we further propose single-source and multi-source curriculum adapting algorithms, which can progressively find more reliable pseudo-labels to learn an optimal model, thereby achieving a balance between reliability and diversity for the pseudo-language labels. Our method outperforms the current state-of-the-art unsupervised method by a significant margin on RefCOCO/+/g datasets in both single-source and multi-source scenarios, with improvements ranging from 6.78$\%$ to 10.67$\%$ and 11.39$\%$ to 14.87$\%$, respectively. The results even outperform existing weakly supervised visual grounding methods. Furthermore, our method is also competitive in fully supervised setting. The code and models are available at https://github.com/linhuixiao/CLIP-VG.
<div id='section'>Paperid: <span id='pid'>333, <a href='https://arxiv.org/pdf/2305.08491.pdf' target='_blank'>https://arxiv.org/pdf/2305.08491.pdf</a></span>   <span><a href='https://github.com/fwu11/MCC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fangwen Wu, Jingxuan He, Yufei Yin, Yanbin Hao, Gang Huang, Lechao Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.08491">Masked Collaborative Contrast for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study introduces an efficacious approach, Masked Collaborative Contrast (MCC), to highlight semantic regions in weakly supervised semantic segmentation. MCC adroitly draws inspiration from masked image modeling and contrastive learning to devise a novel framework that induces keys to contract toward semantic regions. Unlike prevalent techniques that directly eradicate patch regions in the input image when generating masks, we scrutinize the neighborhood relations of patch tokens by exploring masks considering keys on the affinity matrix. Moreover, we generate positive and negative samples in contrastive learning by utilizing the masked local output and contrasting it with the global output. Elaborate experiments on commonly employed datasets evidences that the proposed MCC mechanism effectively aligns global and local perspectives within the image, attaining impressive performance. The source code is available at \url{https://github.com/fwu11/MCC}.
<div id='section'>Paperid: <span id='pid'>334, <a href='https://arxiv.org/pdf/2305.08295.pdf' target='_blank'>https://arxiv.org/pdf/2305.08295.pdf</a></span>   <span><a href='https://github.com/ntucllab/CLImage\_Dataset' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hsiu-Hsuan Wang, Tan-Ha Mai, Nai-Xuan Ye, Wei-I Lin, Hsuan-Tien Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.08295">CLImage: Human-Annotated Datasets for Complementary-Label Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Complementary-label learning (CLL) is a weakly-supervised learning paradigm that aims to train a multi-class classifier using only complementary labels, which indicate classes to which an instance does not belong. Despite numerous algorithmic proposals for CLL, their practical applicability remains unverified for two reasons. Firstly, these algorithms often rely on assumptions about the generation of complementary labels, and it is not clear how far the assumptions are from reality. Secondly, their evaluation has been limited to synthetically labeled datasets. To gain insights into the real-world performance of CLL algorithms, we developed a protocol to collect complementary labels from human annotators. Our efforts resulted in the creation of four datasets: CLCIFAR10, CLCIFAR20, CLMicroImageNet10, and CLMicroImageNet20, derived from well-known classification datasets CIFAR10, CIFAR100, and TinyImageNet200. These datasets represent the very first real-world CLL datasets, namely CLImage, which are publicly available at: https://github.com/ntucllab/CLImage\_Dataset. Through extensive benchmark experiments, we discovered a notable decrease in performance when transitioning from synthetically labeled datasets to real-world datasets. We investigated the key factors contributing to the decrease with a thorough dataset-level ablation study. Our analyses highlight annotation noise as the most influential factor in the real-world datasets. In addition, we discover that the biased-nature of human-annotated complementary labels and the difficulty to validate with only complementary labels are two outstanding barriers to practical CLL. These findings suggest that the community focus more research efforts on developing CLL algorithms and validation schemes that are robust to noisy and biased complementary-label distributions.
<div id='section'>Paperid: <span id='pid'>335, <a href='https://arxiv.org/pdf/2305.05887.pdf' target='_blank'>https://arxiv.org/pdf/2305.05887.pdf</a></span>   <span><a href='https://github.com/HE-Lingfeng/ROI-Extraction' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingfeng He, Mengze Xu, Jie Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.05887">Weakly-supervised ROI extraction method based on contrastive learning for remote sensing images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>ROI extraction is an active but challenging task in remote sensing because of the complicated landform, the complex boundaries and the requirement of annotations. Weakly supervised learning (WSL) aims at learning a mapping from input image to pixel-wise prediction under image-wise labels, which can dramatically decrease the labor cost. However, due to the imprecision of labels, the accuracy and time consumption of WSL methods are relatively unsatisfactory. In this paper, we propose a two-step ROI extraction based on contractive learning. Firstly, we present to integrate multiscale Grad-CAM to obtain pseudo pixelwise annotations with well boundaries. Then, to reduce the compact of misjudgments in pseudo annotations, we construct a contrastive learning strategy to encourage the features inside ROI as close as possible and separate background features from foreground features. Comprehensive experiments demonstrate the superiority of our proposal. Code is available at https://github.com/HE-Lingfeng/ROI-Extraction
<div id='section'>Paperid: <span id='pid'>336, <a href='https://arxiv.org/pdf/2305.05841.pdf' target='_blank'>https://arxiv.org/pdf/2305.05841.pdf</a></span>   <span><a href='https://bupt-ai-cz.github.io/SMAF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guoqing Yang, Chuang Zhu, Yu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.05841">A Self-Training Framework Based on Multi-Scale Attention Fusion for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation (WSSS) based on image-level labels is challenging since it is hard to obtain complete semantic regions. To address this issue, we propose a self-training method that utilizes fused multi-scale class-aware attention maps. Our observation is that attention maps of different scales contain rich complementary information, especially for large and small objects. Therefore, we collect information from attention maps of different scales and obtain multi-scale attention maps. We then apply denoising and reactivation strategies to enhance the potential regions and reduce noisy areas. Finally, we use the refined attention maps to retrain the network. Experiments showthat our method enables the model to extract rich semantic information from multi-scale images and achieves 72.4% mIou scores on both the PASCAL VOC 2012 validation and test sets. The code is available at https://bupt-ai-cz.github.io/SMAF.
<div id='section'>Paperid: <span id='pid'>337, <a href='https://arxiv.org/pdf/2305.05803.pdf' target='_blank'>https://arxiv.org/pdf/2305.05803.pdf</a></span>   <span><a href='https://github.com/cskyl/SAM_WSSS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianle Chen, Zheda Mai, Ruiwen Li, Wei-lun Chao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.05803">Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation (WSSS) aims to bypass the need for laborious pixel-level annotation by using only image-level annotation. Most existing methods rely on Class Activation Maps (CAM) to derive pixel-level pseudo-labels and use them to train a fully supervised semantic segmentation model. Although these pseudo-labels are class-aware, indicating the coarse regions for particular classes, they are not object-aware and fail to delineate accurate object boundaries. To address this, we introduce a simple yet effective method harnessing the Segment Anything Model (SAM), a class-agnostic foundation model capable of producing fine-grained instance masks of objects, parts, and subparts. We use CAM pseudo-labels as cues to select and combine SAM masks, resulting in high-quality pseudo-labels that are both class-aware and object-aware. Our approach is highly versatile and can be easily integrated into existing WSSS methods without any modification. Despite its simplicity, our approach shows consistent gain over the state-of-the-art WSSS methods on both PASCAL VOC and MS-COCO datasets.
<div id='section'>Paperid: <span id='pid'>338, <a href='https://arxiv.org/pdf/2305.05154.pdf' target='_blank'>https://arxiv.org/pdf/2305.05154.pdf</a></span>   <span><a href='https://github.com/NUST-Machine-Intelligence-Laboratory/MDBA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Chen, Yazhou Yao, Jinhui Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.05154">Multi-Granularity Denoising and Bidirectional Alignment for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation (WSSS) models relying on class activation maps (CAMs) have achieved desirable performance comparing to the non-CAMs-based counterparts. However, to guarantee WSSS task feasible, we need to generate pseudo labels by expanding the seeds from CAMs which is complex and time-consuming, thus hindering the design of efficient end-to-end (single-stage) WSSS approaches. To tackle the above dilemma, we resort to the off-the-shelf and readily accessible saliency maps for directly obtaining pseudo labels given the image-level class labels. Nevertheless, the salient regions may contain noisy labels and cannot seamlessly fit the target objects, and saliency maps can only be approximated as pseudo labels for simple images containing single-class objects. As such, the achieved segmentation model with these simple images cannot generalize well to the complex images containing multi-class objects. To this end, we propose an end-to-end multi-granularity denoising and bidirectional alignment (MDBA) model, to alleviate the noisy label and multi-class generalization issues. Specifically, we propose the online noise filtering and progressive noise detection modules to tackle image-level and pixel-level noise, respectively. Moreover, a bidirectional alignment mechanism is proposed to reduce the data distribution gap at both input and output space with simple-to-complex image synthesis and complex-to-simple adversarial learning. MDBA can reach the mIoU of 69.5\% and 70.2\% on validation and test sets for the PASCAL VOC 2012 dataset. The source codes and models have been made available at \url{https://github.com/NUST-Machine-Intelligence-Laboratory/MDBA}.
<div id='section'>Paperid: <span id='pid'>339, <a href='https://arxiv.org/pdf/2305.00696.pdf' target='_blank'>https://arxiv.org/pdf/2305.00696.pdf</a></span>   <span><a href='https://github.com/LitaoYang-Jet/TPMIL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Litao Yang, Deval Mehta, Sidong Liu, Dwarikanath Mahapatra, Antonio Di Ieva, Zongyuan Ge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.00696">TPMIL: Trainable Prototype Enhanced Multiple Instance Learning for Whole Slide Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Digital pathology based on whole slide images (WSIs) plays a key role in cancer diagnosis and clinical practice. Due to the high resolution of the WSI and the unavailability of patch-level annotations, WSI classification is usually formulated as a weakly supervised problem, which relies on multiple instance learning (MIL) based on patches of a WSI. In this paper, we aim to learn an optimal patch-level feature space by integrating prototype learning with MIL. To this end, we develop a Trainable Prototype enhanced deep MIL (TPMIL) framework for weakly supervised WSI classification. In contrast to the conventional methods which rely on a certain number of selected patches for feature space refinement, we softly cluster all the instances by allocating them to their corresponding prototypes. Additionally, our method is able to reveal the correlations between different tumor subtypes through distances between corresponding trained prototypes. More importantly, TPMIL also enables to provide a more accurate interpretability based on the distance of the instances from the trained prototypes which serves as an alternative to the conventional attention score-based interpretability. We test our method on two WSI datasets and it achieves a new SOTA. GitHub repository: https://github.com/LitaoYang-Jet/TPMIL
<div id='section'>Paperid: <span id='pid'>340, <a href='https://arxiv.org/pdf/2305.00607.pdf' target='_blank'>https://arxiv.org/pdf/2305.00607.pdf</a></span>   <span><a href='https://github.com/lgzlIlIlI/Boosting-WTAL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guozhang Li, De Cheng, Xinpeng Ding, Nannan Wang, Xiaoyu Wang, Xinbo Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.00607">Boosting Weakly-Supervised Temporal Action Localization with Text Information</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the lack of temporal annotation, current Weakly-supervised Temporal Action Localization (WTAL) methods are generally stuck into over-complete or incomplete localization. In this paper, we aim to leverage the text information to boost WTAL from two aspects, i.e., (a) the discriminative objective to enlarge the inter-class difference, thus reducing the over-complete; (b) the generative objective to enhance the intra-class integrity, thus finding more complete temporal boundaries. For the discriminative objective, we propose a Text-Segment Mining (TSM) mechanism, which constructs a text description based on the action class label, and regards the text as the query to mine all class-related segments. Without the temporal annotation of actions, TSM compares the text query with the entire videos across the dataset to mine the best matching segments while ignoring irrelevant ones. Due to the shared sub-actions in different categories of videos, merely applying TSM is too strict to neglect the semantic-related segments, which results in incomplete localization. We further introduce a generative objective named Video-text Language Completion (VLC), which focuses on all semantic-related segments from videos to complete the text sentence. We achieve the state-of-the-art performance on THUMOS14 and ActivityNet1.3. Surprisingly, we also find our proposed method can be seamlessly applied to existing methods, and improve their performances with a clear margin. The code is available at https://github.com/lgzlIlIlI/Boosting-WTAL.
<div id='section'>Paperid: <span id='pid'>341, <a href='https://arxiv.org/pdf/2304.10671.pdf' target='_blank'>https://arxiv.org/pdf/2304.10671.pdf</a></span>   <span><a href='https://github.com/jiyuuchc/lacss' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ji Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.10671">Point-supervised Single-cell Segmentation via Collaborative Knowledge Sharing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite their superior performance, deep-learning methods often suffer from the disadvantage of needing large-scale well-annotated training data. In response, recent literature has seen a proliferation of efforts aimed at reducing the annotation burden. This paper focuses on a weakly-supervised training setting for single-cell segmentation models, where the only available training label is the rough locations of individual cells. The specific problem is of practical interest due to the widely available nuclei counter-stain data in biomedical literature, from which the cell locations can be derived programmatically. Of more general interest is a proposed self-learning method called collaborative knowledge sharing, which is related to but distinct from the more well-known consistency learning methods. This strategy achieves self-learning by sharing knowledge between a principal model and a very light-weight collaborator model. Importantly, the two models are entirely different in their architectures, capacities, and model outputs: In our case, the principal model approaches the segmentation problem from an object-detection perspective, whereas the collaborator model a sematic segmentation perspective. We assessed the effectiveness of this strategy by conducting experiments on LIVECell, a large single-cell segmentation dataset of bright-field images, and on A431 dataset, a fluorescence image dataset in which the location labels are generated automatically from nuclei counter-stain data. Implementing code is available at https://github.com/jiyuuchc/lacss
<div id='section'>Paperid: <span id='pid'>342, <a href='https://arxiv.org/pdf/2304.09059.pdf' target='_blank'>https://arxiv.org/pdf/2304.09059.pdf</a></span>   <span><a href='https://github.com/ChunyanWang1/ws-fcn' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunyan Wang, Dong Zhang, Liyan Zhang, Jinhui Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.09059">Coupling Global Context and Local Contents for Weakly-Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Thanks to the advantages of the friendly annotations and the satisfactory performance, Weakly-Supervised Semantic Segmentation (WSSS) approaches have been extensively studied. Recently, the single-stage WSSS was awakened to alleviate problems of the expensive computational costs and the complicated training procedures in multi-stage WSSS. However, results of such an immature model suffer from problems of background incompleteness and object incompleteness. We empirically find that they are caused by the insufficiency of the global object context and the lack of the local regional contents, respectively. Under these observations, we propose a single-stage WSSS model with only the image-level class label supervisions, termed as Weakly Supervised Feature Coupling Network (WS-FCN), which can capture the multi-scale context formed from the adjacent feature grids, and encode the fine-grained spatial information from the low-level features into the high-level ones. Specifically, a flexible context aggregation module is proposed to capture the global object context in different granular spaces. Besides, a semantically consistent feature fusion module is proposed in a bottom-up parameter-learnable fashion to aggregate the fine-grained local contents. Based on these two modules, WS-FCN lies in a self-supervised end-to-end training fashion. Extensive experimental results on the challenging PASCAL VOC 2012 and MS COCO 2014 demonstrate the effectiveness and efficiency of WS-FCN, which can achieve state-of-the-art results by 65.02\% and 64.22\% mIoU on PASCAL VOC 2012 val set and test set, 34.12\% mIoU on MS COCO 2014 val set, respectively. The code and weight have been released at:https://github.com/ChunyanWang1/ws-fcn.
<div id='section'>Paperid: <span id='pid'>343, <a href='https://arxiv.org/pdf/2304.08271.pdf' target='_blank'>https://arxiv.org/pdf/2304.08271.pdf</a></span>   <span><a href='https://github.com/ryylcc/OWSOL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinheng Xie, Zhaochuan Luo, Yuexiang Li, Haozhe Liu, Linlin Shen, Mike Zheng Shou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.08271">Open-World Weakly-Supervised Object Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While remarkable success has been achieved in weakly-supervised object localization (WSOL), current frameworks are not capable of locating objects of novel categories in open-world settings. To address this issue, we are the first to introduce a new weakly-supervised object localization task called OWSOL (Open-World Weakly-Supervised Object Localization). During training, all labeled data comes from known categories and, both known and novel categories exist in the unlabeled data. To handle such data, we propose a novel paradigm of contrastive representation co-learning using both labeled and unlabeled data to generate a complete G-CAM (Generalized Class Activation Map) for object localization, without the requirement of bounding box annotation. As no class label is available for the unlabelled data, we conduct clustering over the full training set and design a novel multiple semantic centroids-driven contrastive loss for representation learning. We re-organize two widely used datasets, i.e., ImageNet-1K and iNatLoc500, and propose OpenImages150 to serve as evaluation benchmarks for OWSOL. Extensive experiments demonstrate that the proposed method can surpass all baselines by a large margin. We believe that this work can shift the close-set localization towards the open-world setting and serve as a foundation for subsequent works. Code will be released at https://github.com/ryylcc/OWSOL.
<div id='section'>Paperid: <span id='pid'>344, <a href='https://arxiv.org/pdf/2304.05635.pdf' target='_blank'>https://arxiv.org/pdf/2304.05635.pdf</a></span>   <span><a href='https://github.com/llmir/FedICRA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Li Lin, Jiewei Wu, Yixiang Liu, Kenneth K. Y. Wong, Xiaoying Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.05635">Unifying and Personalizing Weakly-supervised Federated Medical Image Segmentation via Adaptive Representation and Aggregation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated learning (FL) enables multiple sites to collaboratively train powerful deep models without compromising data privacy and security. The statistical heterogeneity (e.g., non-IID data and domain shifts) is a primary obstacle in FL, impairing the generalization performance of the global model. Weakly supervised segmentation, which uses sparsely-grained (i.e., point-, bounding box-, scribble-, block-wise) supervision, is increasingly being paid attention to due to its great potential of reducing annotation costs. However, there may exist label heterogeneity, i.e., different annotation forms across sites. In this paper, we propose a novel personalized FL framework for medical image segmentation, named FedICRA, which uniformly leverages heterogeneous weak supervision via adaptIve Contrastive Representation and Aggregation. Concretely, to facilitate personalized modeling and to avoid confusion, a channel selection based site contrastive representation module is employed to adaptively cluster intra-site embeddings and separate inter-site ones. To effectively integrate the common knowledge from the global model with the unique knowledge from each local model, an adaptive aggregation module is applied for updating and initializing local models at the element level. Additionally, a weakly supervised objective function that leverages a multiscale tree energy loss and a gated CRF loss is employed to generate more precise pseudo-labels and further boost the segmentation performance. Through extensive experiments on two distinct medical image segmentation tasks of different modalities, the proposed FedICRA demonstrates overwhelming performance over other state-of-the-art personalized FL methods. Its performance even approaches that of fully supervised training on centralized data. Our code and data are available at https://github.com/llmir/FedICRA.
<div id='section'>Paperid: <span id='pid'>345, <a href='https://arxiv.org/pdf/2304.04902.pdf' target='_blank'>https://arxiv.org/pdf/2304.04902.pdf</a></span>   <span><a href='https://github.com/HealthX-Lab/HGI-SAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Amirhossein Rasoulian, Soorena Salari, Yiming Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.04902">Weakly Supervised Intracranial Hemorrhage Segmentation using Head-Wise Gradient-Infused Self-Attention Maps from a Swin Transformer in Categorical Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intracranial hemorrhage (ICH) is a life-threatening medical emergency that requires timely and accurate diagnosis for effective treatment and improved patient survival rates. While deep learning techniques have emerged as the leading approach for medical image analysis and processing, the most commonly employed supervised learning often requires large, high-quality annotated datasets that can be costly to obtain, particularly for pixel/voxel-wise image segmentation. To address this challenge and facilitate ICH treatment decisions, we introduce a novel weakly supervised method for ICH segmentation, utilizing a Swin transformer trained on an ICH classification task with categorical labels. Our approach leverages a hierarchical combination of head-wise gradient-infused self-attention maps to generate accurate image segmentation. Additionally, we conducted an exploratory study on different learning strategies and showed that binary ICH classification has a more positive impact on self-attention maps compared to full ICH subtyping. With a mean Dice score of 0.44, our technique achieved similar ICH segmentation performance as the popular U-Net and Swin-UNETR models with full supervision and outperformed a similar weakly supervised approach using GradCAM, demonstrating the excellent potential of the proposed framework in challenging medical image segmentation tasks. Our code is available at https://github.com/HealthX-Lab/HGI-SAM.
<div id='section'>Paperid: <span id='pid'>346, <a href='https://arxiv.org/pdf/2304.04442.pdf' target='_blank'>https://arxiv.org/pdf/2304.04442.pdf</a></span>   <span><a href='https://github.com/YeRen123455/SIRST-Single-Point-Supervision' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyang Li, Yingqian Wang, Longguang Wang, Fei Zhang, Ting Liu, Zaiping Lin, Wei An, Yulan Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.04442">Monte Carlo Linear Clustering with Single-Point Supervision is Enough for Infrared Small Target Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single-frame infrared small target (SIRST) detection aims at separating small targets from clutter backgrounds on infrared images. Recently, deep learning based methods have achieved promising performance on SIRST detection, but at the cost of a large amount of training data with expensive pixel-level annotations. To reduce the annotation burden, we propose the first method to achieve SIRST detection with single-point supervision. The core idea of this work is to recover the per-pixel mask of each target from the given single point label by using clustering approaches, which looks simple but is indeed challenging since targets are always insalient and accompanied with background clutters. To handle this issue, we introduce randomness to the clustering process by adding noise to the input images, and then obtain much more reliable pseudo masks by averaging the clustered results. Thanks to this "Monte Carlo" clustering approach, our method can accurately recover pseudo masks and thus turn arbitrary fully supervised SIRST detection networks into weakly supervised ones with only single point annotation. Experiments on four datasets demonstrate that our method can be applied to existing SIRST detection networks to achieve comparable performance with their fully supervised counterparts, which reveals that single-point supervision is strong enough for SIRST detection. Our code will be available at: https://github.com/YeRen123455/SIRST-Single-Point-Supervision.
<div id='section'>Paperid: <span id='pid'>347, <a href='https://arxiv.org/pdf/2304.04403.pdf' target='_blank'>https://arxiv.org/pdf/2304.04403.pdf</a></span>   <span><a href='https://github.com/open-mmlab/mmrotate' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Yu, Xue Yang, Qingyun Li, Yue Zhou, Gefan Zhang, Feipeng Da, Junchi Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.04403">H2RBox-v2: Incorporating Symmetry for Boosting Horizontal Box Supervised Oriented Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapidly increasing demand for oriented object detection, e.g. in autonomous driving and remote sensing, the recently proposed paradigm involving weakly-supervised detector H2RBox for learning rotated box (RBox) from the more readily-available horizontal box (HBox) has shown promise. This paper presents H2RBox-v2, to further bridge the gap between HBox-supervised and RBox-supervised oriented object detection. Specifically, we propose to leverage the reflection symmetry via flip and rotate consistencies, using a weakly-supervised network branch similar to H2RBox, together with a novel self-supervised branch that learns orientations from the symmetry inherent in visual objects. The detector is further stabilized and enhanced by practical techniques to cope with peripheral issues e.g. angular periodicity. To our best knowledge, H2RBox-v2 is the first symmetry-aware self-supervised paradigm for oriented object detection. In particular, our method shows less susceptibility to low-quality annotation and insufficient training data compared to H2RBox. Specifically, H2RBox-v2 achieves very close performance to a rotation annotation trained counterpart -- Rotated FCOS: 1) DOTA-v1.0/1.5/2.0: 72.31%/64.76%/50.33% vs. 72.44%/64.53%/51.77%; 2) HRSC: 89.66% vs. 88.99%; 3) FAIR1M: 42.27% vs. 41.25%.
<div id='section'>Paperid: <span id='pid'>348, <a href='https://arxiv.org/pdf/2304.02621.pdf' target='_blank'>https://arxiv.org/pdf/2304.02621.pdf</a></span>   <span><a href='https://github.com/arvijj/hfpl' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Arvi Jonnarth, Yushan Zhang, Michael Felsberg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.02621">High-fidelity Pseudo-labels for Boosting Weakly-Supervised Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image-level weakly-supervised semantic segmentation (WSSS) reduces the usually vast data annotation cost by surrogate segmentation masks during training. The typical approach involves training an image classification network using global average pooling (GAP) on convolutional feature maps. This enables the estimation of object locations based on class activation maps (CAMs), which identify the importance of image regions. The CAMs are then used to generate pseudo-labels, in the form of segmentation masks, to supervise a segmentation model in the absence of pixel-level ground truth. Our work is based on two techniques for improving CAMs; importance sampling, which is a substitute for GAP, and the feature similarity loss, which utilizes a heuristic that object contours almost always align with color edges in images. However, both are based on the multinomial posterior with softmax, and implicitly assume that classes are mutually exclusive, which turns out suboptimal in our experiments. Thus, we reformulate both techniques based on binomial posteriors of multiple independent binary problems. This has two benefits; their performance is improved and they become more general, resulting in an add-on method that can boost virtually any WSSS method. This is demonstrated on a wide variety of baselines on the PASCAL VOC dataset, improving the region similarity and contour quality of all implemented state-of-the-art methods. Experiments on the MS COCO dataset further show that our proposed add-on is well-suited for large-scale settings. Our code implementation is available at https://github.com/arvijj/hfpl.
<div id='section'>Paperid: <span id='pid'>349, <a href='https://arxiv.org/pdf/2304.01969.pdf' target='_blank'>https://arxiv.org/pdf/2304.01969.pdf</a></span>   <span><a href='https://github.com/pkargupta/MEGClass/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Priyanka Kargupta, Tanay Komarlu, Susik Yoon, Xuan Wang, Jiawei Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.01969">MEGClass: Extremely Weakly Supervised Text Classification via Mutually-Enhancing Text Granularities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text classification is essential for organizing unstructured text. Traditional methods rely on human annotations or, more recently, a set of class seed words for supervision, which can be costly, particularly for specialized or emerging domains. To address this, using class surface names alone as extremely weak supervision has been proposed. However, existing approaches treat different levels of text granularity (documents, sentences, or words) independently, disregarding inter-granularity class disagreements and the context identifiable exclusively through joint extraction. In order to tackle these issues, we introduce MEGClass, an extremely weakly-supervised text classification method that leverages Mutually-Enhancing Text Granularities. MEGClass utilizes coarse- and fine-grained context signals obtained by jointly considering a document's most class-indicative words and sentences. This approach enables the learning of a contextualized document representation that captures the most discriminative class indicators. By preserving the heterogeneity of potential classes, MEGClass can select the most informative class-indicative documents as iterative feedback to enhance the initial word-based class representations and ultimately fine-tune a pre-trained text classifier. Extensive experiments on seven benchmark datasets demonstrate that MEGClass outperforms other weakly and extremely weakly supervised methods.
<div id='section'>Paperid: <span id='pid'>350, <a href='https://arxiv.org/pdf/2304.01184.pdf' target='_blank'>https://arxiv.org/pdf/2304.01184.pdf</a></span>   <span><a href='https://github.com/hustvl/WeakTr' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lianghui Zhu, Yingyue Li, Jiemin Fang, Yan Liu, Hao Xin, Wenyu Liu, Xinggang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.01184">WeakTr: Exploring Plain Vision Transformer for Weakly-supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper explores the properties of the plain Vision Transformer (ViT) for Weakly-supervised Semantic Segmentation (WSSS). The class activation map (CAM) is of critical importance for understanding a classification network and launching WSSS. We observe that different attention heads of ViT focus on different image areas. Thus a novel weight-based method is proposed to end-to-end estimate the importance of attention heads, while the self-attention maps are adaptively fused for high-quality CAM results that tend to have more complete objects. Besides, we propose a ViT-based gradient clipping decoder for online retraining with the CAM results to complete the WSSS task. We name this plain Transformer-based Weakly-supervised learning framework WeakTr. It achieves the state-of-the-art WSSS performance on standard benchmarks, i.e., 78.4% mIoU on the val set of PASCAL VOC 2012 and 50.3% mIoU on the val set of COCO 2014. Code is available at https://github.com/hustvl/WeakTr.
<div id='section'>Paperid: <span id='pid'>351, <a href='https://arxiv.org/pdf/2303.17811.pdf' target='_blank'>https://arxiv.org/pdf/2303.17811.pdf</a></span>   <span><a href='https://github.com/Seonghoon-Yu/Zero-shot-RIS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seonghoon Yu, Paul Hongsuck Seo, Jeany Son
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.17811">Zero-shot Referring Image Segmentation with Global-Local Context Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring image segmentation (RIS) aims to find a segmentation mask given a referring expression grounded to a region of the input image. Collecting labelled datasets for this task, however, is notoriously costly and labor-intensive. To overcome this issue, we propose a simple yet effective zero-shot referring image segmentation method by leveraging the pre-trained cross-modal knowledge from CLIP. In order to obtain segmentation masks grounded to the input text, we propose a mask-guided visual encoder that captures global and local contextual information of an input image. By utilizing instance masks obtained from off-the-shelf mask proposal techniques, our method is able to segment fine-detailed Istance-level groundings. We also introduce a global-local text encoder where the global feature captures complex sentence-level semantics of the entire input expression while the local feature focuses on the target noun phrase extracted by a dependency parser. In our experiments, the proposed method outperforms several zero-shot baselines of the task and even the weakly supervised referring expression segmentation method with substantial margins. Our code is available at https://github.com/Seonghoon-Yu/Zero-shot-RIS.
<div id='section'>Paperid: <span id='pid'>352, <a href='https://arxiv.org/pdf/2303.17410.pdf' target='_blank'>https://arxiv.org/pdf/2303.17410.pdf</a></span>   <span><a href='https://github.com/deepplants/PC2M' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Simone Rossetti, Nico SamÃ, Fiora Pirri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.17410">Removing supervision in semantic segmentation with local-global matching and area balancing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Removing supervision in semantic segmentation is still tricky. Current approaches can deal with common categorical patterns yet resort to multi-stage architectures. We design a novel end-to-end model leveraging local-global patch matching to predict categories, good localization, area and shape of objects for semantic segmentation. The local-global matching is, in turn, compelled by optimal transport plans fulfilling area constraints nearing a solution for exact shape prediction. Our model attains state-of-the-art in Weakly Supervised Semantic Segmentation, only image-level labels, with 75% mIoU on PascalVOC2012 val set and 46% on MS-COCO2014 val set. Dropping the image-level labels and clustering self-supervised learned features to yield pseudo-multi-level labels, we obtain an unsupervised model for semantic segmentation. We also attain state-of-the-art on Unsupervised Semantic Segmentation with 43.6% mIoU on PascalVOC2012 val set and 19.4% on MS-COCO2014 val set. The code is available at https://github.com/deepplants/PC2M.
<div id='section'>Paperid: <span id='pid'>353, <a href='https://arxiv.org/pdf/2303.16891.pdf' target='_blank'>https://arxiv.org/pdf/2303.16891.pdf</a></span>   <span><a href='https://vibashan.github.io/ovis-web/' target='_blank'>  GitHub</a></span> <span><a href='https://vibashan.github.io/ovis-web/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Vibashan VS, Ning Yu, Chen Xing, Can Qin, Mingfei Gao, Juan Carlos Niebles, Vishal M. Patel, Ran Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.16891">Mask-free OVIS: Open-Vocabulary Instance Segmentation without Manual Mask Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing instance segmentation models learn task-specific information using manual mask annotations from base (training) categories. These mask annotations require tremendous human effort, limiting the scalability to annotate novel (new) categories. To alleviate this problem, Open-Vocabulary (OV) methods leverage large-scale image-caption pairs and vision-language models to learn novel categories. In summary, an OV method learns task-specific information using strong supervision from base annotations and novel category information using weak supervision from image-captions pairs. This difference between strong and weak supervision leads to overfitting on base categories, resulting in poor generalization towards novel categories. In this work, we overcome this issue by learning both base and novel categories from pseudo-mask annotations generated by the vision-language model in a weakly supervised manner using our proposed Mask-free OVIS pipeline. Our method automatically generates pseudo-mask annotations by leveraging the localization ability of a pre-trained vision-language model for objects present in image-caption pairs. The generated pseudo-mask annotations are then used to supervise an instance segmentation model, freeing the entire pipeline from any labour-expensive instance-level annotations and overfitting. Our extensive experiments show that our method trained with just pseudo-masks significantly improves the mAP scores on the MS-COCO dataset and OpenImages dataset compared to the recent state-of-the-art methods trained with manual masks. Codes and models are provided in https://vibashan.github.io/ovis-web/.
<div id='section'>Paperid: <span id='pid'>354, <a href='https://arxiv.org/pdf/2303.15904.pdf' target='_blank'>https://arxiv.org/pdf/2303.15904.pdf</a></span>   <span><a href='https://github.com/SysCV/MaskFreeVis;' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/SysCV/MaskFreeVis' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Ke, Martin Danelljan, Henghui Ding, Yu-Wing Tai, Chi-Keung Tang, Fisher Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.15904">Mask-Free Video Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent advancement in Video Instance Segmentation (VIS) has largely been driven by the use of deeper and increasingly data-hungry transformer-based models. However, video masks are tedious and expensive to annotate, limiting the scale and diversity of existing VIS datasets. In this work, we aim to remove the mask-annotation requirement. We propose MaskFreeVIS, achieving highly competitive VIS performance, while only using bounding box annotations for the object state. We leverage the rich temporal mask consistency constraints in videos by introducing the Temporal KNN-patch Loss (TK-Loss), providing strong mask supervision without any labels. Our TK-Loss finds one-to-many matches across frames, through an efficient patch-matching step followed by a K-nearest neighbor selection. A consistency loss is then enforced on the found matches. Our mask-free objective is simple to implement, has no trainable parameters, is computationally efficient, yet outperforms baselines employing, e.g., state-of-the-art optical flow to enforce temporal mask consistency. We validate MaskFreeVIS on the YouTube-VIS 2019/2021, OVIS and BDD100K MOTS benchmarks. The results clearly demonstrate the efficacy of our method by drastically narrowing the gap between fully and weakly-supervised VIS performance. Our code and trained models are available at https://github.com/SysCV/MaskFreeVis.
<div id='section'>Paperid: <span id='pid'>355, <a href='https://arxiv.org/pdf/2303.15149.pdf' target='_blank'>https://arxiv.org/pdf/2303.15149.pdf</a></span>   <span><a href='https://pinakinathc.github.io/sketch-detect' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pinaki Nath Chowdhury, Ayan Kumar Bhunia, Aneeshan Sain, Subhadeep Koley, Tao Xiang, Yi-Zhe Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.15149">What Can Human Sketches Do for Object Detection?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sketches are highly expressive, inherently capturing subjective and fine-grained visual cues. The exploration of such innate properties of human sketches has, however, been limited to that of image retrieval. In this paper, for the first time, we cultivate the expressiveness of sketches but for the fundamental vision task of object detection. The end result is a sketch-enabled object detection framework that detects based on what \textit{you} sketch -- \textit{that} ``zebra'' (e.g., one that is eating the grass) in a herd of zebras (instance-aware detection), and only the \textit{part} (e.g., ``head" of a ``zebra") that you desire (part-aware detection). We further dictate that our model works without (i) knowing which category to expect at testing (zero-shot) and (ii) not requiring additional bounding boxes (as per fully supervised) and class labels (as per weakly supervised). Instead of devising a model from the ground up, we show an intuitive synergy between foundation models (e.g., CLIP) and existing sketch models build for sketch-based image retrieval (SBIR), which can already elegantly solve the task -- CLIP to provide model generalisation, and SBIR to bridge the (sketch$\rightarrow$photo) gap. In particular, we first perform independent prompting on both sketch and photo branches of an SBIR model to build highly generalisable sketch and photo encoders on the back of the generalisation ability of CLIP. We then devise a training paradigm to adapt the learned encoders for object detection, such that the region embeddings of detected boxes are aligned with the sketch and photo embeddings from SBIR. Evaluating our framework on standard object detection datasets like PASCAL-VOC and MS-COCO outperforms both supervised (SOD) and weakly-supervised object detectors (WSOD) on zero-shot setups. Project Page: \url{https://pinakinathc.github.io/sketch-detect}
<div id='section'>Paperid: <span id='pid'>356, <a href='https://arxiv.org/pdf/2303.14727.pdf' target='_blank'>https://arxiv.org/pdf/2303.14727.pdf</a></span>   <span><a href='https://github.com/liuzhengzhe/One-Thing-One-Click' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengzhe Liu, Xiaojuan Qi, Chi-Wing Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.14727">You Only Need One Thing One Click: Self-Training for Weakly Supervised 3D Scene Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D scene understanding, e.g., point cloud semantic and instance segmentation, often requires large-scale annotated training data, but clearly, point-wise labels are too tedious to prepare. While some recent methods propose to train a 3D network with small percentages of point labels, we take the approach to an extreme and propose ``One Thing One Click,'' meaning that the annotator only needs to label one point per object. To leverage these extremely sparse labels in network training, we design a novel self-training approach, in which we iteratively conduct the training and label propagation, facilitated by a graph propagation module. Also, we adopt a relation network to generate the per-category prototype to enhance the pseudo label quality and guide the iterative training. Besides, our model can be compatible to 3D instance segmentation equipped with a point-clustering strategy. Experimental results on both ScanNet-v2 and S3DIS show that our self-training approach, with extremely-sparse annotations, outperforms all existing weakly supervised methods for 3D semantic and instance segmentation by a large margin, and our results are also comparable to those of the fully supervised counterparts. Codes and models are available at https://github.com/liuzhengzhe/One-Thing-One-Click.
<div id='section'>Paperid: <span id='pid'>357, <a href='https://arxiv.org/pdf/2303.13852.pdf' target='_blank'>https://arxiv.org/pdf/2303.13852.pdf</a></span>   <span><a href='https://renjiaoyi.github.io/relighting/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Renjiao Yi, Chenyang Zhu, Kai Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.13852">Weakly-supervised Single-view Image Relighting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a learning-based approach to relight a single image of Lambertian and low-frequency specular objects. Our method enables inserting objects from photographs into new scenes and relighting them under the new environment lighting, which is essential for AR applications. To relight the object, we solve both inverse rendering and re-rendering. To resolve the ill-posed inverse rendering, we propose a weakly-supervised method by a low-rank constraint. To facilitate the weakly-supervised training, we contribute Relit, a large-scale (750K images) dataset of videos with aligned objects under changing illuminations. For re-rendering, we propose a differentiable specular rendering layer to render low-frequency non-Lambertian materials under various illuminations of spherical harmonics. The whole pipeline is end-to-end and efficient, allowing for a mobile app implementation of AR object insertion. Extensive evaluations demonstrate that our method achieves state-of-the-art performance. Project page: https://renjiaoyi.github.io/relighting/.
<div id='section'>Paperid: <span id='pid'>358, <a href='https://arxiv.org/pdf/2303.13519.pdf' target='_blank'>https://arxiv.org/pdf/2303.13519.pdf</a></span>   <span><a href='https://medhini.github.io/task_structure' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Medhini Narasimhan, Licheng Yu, Sean Bell, Ning Zhang, Trevor Darrell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.13519">Learning and Verification of Task Structure in Instructional Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Given the enormous number of instructional videos available online, learning a diverse array of multi-step task models from videos is an appealing goal. We introduce a new pre-trained video model, VideoTaskformer, focused on representing the semantics and structure of instructional videos. We pre-train VideoTaskformer using a simple and effective objective: predicting weakly supervised textual labels for steps that are randomly masked out from an instructional video (masked step modeling). Compared to prior work which learns step representations locally, our approach involves learning them globally, leveraging video of the entire surrounding task as context. From these learned representations, we can verify if an unseen video correctly executes a given task, as well as forecast which steps are likely to be taken after a given step. We introduce two new benchmarks for detecting mistakes in instructional videos, to verify if there is an anomalous step and if steps are executed in the right order. We also introduce a long-term forecasting benchmark, where the goal is to predict long-range future steps from a given step. Our method outperforms previous baselines on these tasks, and we believe the tasks will be a valuable way for the community to measure the quality of step representations. Additionally, we evaluate VideoTaskformer on 3 existing benchmarks -- procedural activity recognition, step classification, and step forecasting -- and demonstrate on each that our method outperforms existing baselines and achieves new state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>359, <a href='https://arxiv.org/pdf/2303.13496.pdf' target='_blank'>https://arxiv.org/pdf/2303.13496.pdf</a></span>   <span><a href='https://github.com/facebookresearch/maws/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mannat Singh, Quentin Duval, Kalyan Vasudev Alwala, Haoqi Fan, Vaibhav Aggarwal, Aaron Adcock, Armand Joulin, Piotr DollÃ¡r, Christoph Feichtenhofer, Ross Girshick, Rohit Girdhar, Ishan Misra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.13496">The effectiveness of MAE pre-pretraining for billion-scale pretraining</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper revisits the standard pretrain-then-finetune paradigm used in computer vision for visual recognition tasks. Typically, state-of-the-art foundation models are pretrained using large scale (weakly) supervised datasets with billions of images. We introduce an additional pre-pretraining stage that is simple and uses the self-supervised MAE technique to initialize the model. While MAE has only been shown to scale with the size of models, we find that it scales with the size of the training dataset as well. Thus, our MAE-based pre-pretraining scales with both model and data size making it applicable for training foundation models. Pre-pretraining consistently improves both the model convergence and the downstream transfer performance across a range of model scales (millions to billions of parameters), and dataset sizes (millions to billions of images). We measure the effectiveness of pre-pretraining on 10 different visual recognition tasks spanning image classification, video recognition, object detection, low-shot classification and zero-shot recognition. Our largest model achieves new state-of-the-art results on iNaturalist-18 (91.7%), ImageNet-ReaL (91.1%), 1-shot ImageNet-1k (63.6%), and zero-shot transfer on Food-101 (96.2%). Our study reveals that model initialization plays a significant role, even for web-scale pretraining with billions of images, and our models are available publicly.
<div id='section'>Paperid: <span id='pid'>360, <a href='https://arxiv.org/pdf/2303.12370.pdf' target='_blank'>https://arxiv.org/pdf/2303.12370.pdf</a></span>   <span><a href='https://github.com/svip-lab/WeakSVR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sixun Dong, Huazhang Hu, Dongze Lian, Weixin Luo, Yicheng Qian, Shenghua Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.12370">Weakly Supervised Video Representation Learning with Unaligned Text for Sequential Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sequential video understanding, as an emerging video understanding task, has driven lots of researchers' attention because of its goal-oriented nature. This paper studies weakly supervised sequential video understanding where the accurate time-stamp level text-video alignment is not provided. We solve this task by borrowing ideas from CLIP. Specifically, we use a transformer to aggregate frame-level features for video representation and use a pre-trained text encoder to encode the texts corresponding to each action and the whole video, respectively. To model the correspondence between text and video, we propose a multiple granularity loss, where the video-paragraph contrastive loss enforces matching between the whole video and the complete script, and a fine-grained frame-sentence contrastive loss enforces the matching between each action and its description. As the frame-sentence correspondence is not available, we propose to use the fact that video actions happen sequentially in the temporal domain to generate pseudo frame-sentence correspondence and supervise the network training with the pseudo labels. Extensive experiments on video sequence verification and text-to-video matching show that our method outperforms baselines by a large margin, which validates the effectiveness of our proposed approach. Code is available at https://github.com/svip-lab/WeakSVR
<div id='section'>Paperid: <span id='pid'>361, <a href='https://arxiv.org/pdf/2303.12369.pdf' target='_blank'>https://arxiv.org/pdf/2303.12369.pdf</a></span>   <span><a href='https://github.com/ktr-hubrt/UMIL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hui Lv, Zhongqi Yue, Qianru Sun, Bin Luo, Zhen Cui, Hanwang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.12369">Unbiased Multiple Instance Learning for Weakly Supervised Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Video Anomaly Detection (WSVAD) is challenging because the binary anomaly label is only given on the video level, but the output requires snippet-level predictions. So, Multiple Instance Learning (MIL) is prevailing in WSVAD. However, MIL is notoriously known to suffer from many false alarms because the snippet-level detector is easily biased towards the abnormal snippets with simple context, confused by the normality with the same bias, and missing the anomaly with a different pattern. To this end, we propose a new MIL framework: Unbiased MIL (UMIL), to learn unbiased anomaly features that improve WSVAD. At each MIL training iteration, we use the current detector to divide the samples into two groups with different context biases: the most confident abnormal/normal snippets and the rest ambiguous ones. Then, by seeking the invariant features across the two sample groups, we can remove the variant context biases. Extensive experiments on benchmarks UCF-Crime and TAD demonstrate the effectiveness of our UMIL. Our code is provided at https://github.com/ktr-hubrt/UMIL.
<div id='section'>Paperid: <span id='pid'>362, <a href='https://arxiv.org/pdf/2303.12148.pdf' target='_blank'>https://arxiv.org/pdf/2303.12148.pdf</a></span>   <span><a href='https://github.com/Novestars/Neural-Pre-processing' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinzi He, Alan Wang, Mert R. Sabuncu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.12148">Neural Pre-Processing: A Learning Framework for End-to-end Brain MRI Pre-processing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Head MRI pre-processing involves converting raw images to an intensity-normalized, skull-stripped brain in a standard coordinate space. In this paper, we propose an end-to-end weakly supervised learning approach, called Neural Pre-processing (NPP), for solving all three sub-tasks simultaneously via a neural network, trained on a large dataset without individual sub-task supervision. Because the overall objective is highly under-constrained, we explicitly disentangle geometric-preserving intensity mapping (skull-stripping and intensity normalization) and spatial transformation (spatial normalization). Quantitative results show that our model outperforms state-of-the-art methods which tackle only a single sub-task. Our ablation experiments demonstrate the importance of the architecture design we chose for NPP. Furthermore, NPP affords the user the flexibility to control each of these tasks at inference time. The code and model are freely-available at \url{https://github.com/Novestars/Neural-Pre-processing}.
<div id='section'>Paperid: <span id='pid'>363, <a href='https://arxiv.org/pdf/2303.11668.pdf' target='_blank'>https://arxiv.org/pdf/2303.11668.pdf</a></span>   <span><a href='https://github.com/SIAnalytics/RS_AnomalyDetection.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongjin Jeon, Youngtack Oh, Doyoung Jeong, Hyunguk Choi, Junsik Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.11668">Focus or Not: A Baseline for Anomaly Event Detection On the Open Public Places with Satellite Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, monitoring the world wide area with satellite images has been emerged as an important issue.
  Site monitoring task can be divided into two independent tasks; 1) Change Detection and 2) Anomaly Event Detection.
  Unlike to change detection research is actively conducted based on the numerous datasets(\eg LEVIR-CD, WHU-CD, S2Looking, xView2 and etc...) to meet up the expectations of industries or governments, research on AI models for detecting anomaly events is passively and rarely conducted.
  In this paper, we introduce a novel satellite imagery dataset(AED-RS) for detecting anomaly events on the open public places.
  AED-RS Dataset contains satellite images of normal and abnormal situations of 8 open public places from all over the world.
  Each places are labeled with different criteria based on the difference of characteristics of each places.
  With this dataset, we introduce a baseline model for our dataset TB-FLOW, which can be trained in weakly-supervised manner and shows reasonable performance on the AED-RS Dataset compared with the other NF(Normalizing-Flow) based anomaly detection models. Our dataset and code will be publicly open in \url{https://github.com/SIAnalytics/RS_AnomalyDetection.git}.
<div id='section'>Paperid: <span id='pid'>364, <a href='https://arxiv.org/pdf/2303.10689.pdf' target='_blank'>https://arxiv.org/pdf/2303.10689.pdf</a></span>   <span><a href='https://github.com/ChunmengLiu1/MECPformer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunmeng Liu, Guangyao Li, Yao Shen, Ruiqi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.10689">MECPformer: Multi-estimations Complementary Patch with CNN-Transformers for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The initial seed based on the convolutional neural network (CNN) for weakly supervised semantic segmentation always highlights the most discriminative regions but fails to identify the global target information. Methods based on transformers have been proposed successively benefiting from the advantage of capturing long-range feature representations. However, we observe a flaw regardless of the gifts based on the transformer. Given a class, the initial seeds generated based on the transformer may invade regions belonging to other classes. Inspired by the mentioned issues, we devise a simple yet effective method with Multi-estimations Complementary Patch (MECP) strategy and Adaptive Conflict Module (ACM), dubbed MECPformer. Given an image, we manipulate it with the MECP strategy at different epochs, and the network mines and deeply fuses the semantic information at different levels. In addition, ACM adaptively removes conflicting pixels and exploits the network self-training capability to mine potential target information. Without bells and whistles, our MECPformer has reached new state-of-the-art 72.0% mIoU on the PASCAL VOC 2012 and 42.4% on MS COCO 2014 dataset. The code is available at https://github.com/ChunmengLiu1/MECPformer.
<div id='section'>Paperid: <span id='pid'>365, <a href='https://arxiv.org/pdf/2303.10438.pdf' target='_blank'>https://arxiv.org/pdf/2303.10438.pdf</a></span>   <span><a href='https://github.com/wpy1999/SAT' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/wpy1999/SAT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pingyu Wu, Wei Zhai, Yang Cao, Jiebo Luo, Zheng-Jun Zha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.10438">Spatial-Aware Token for Weakly Supervised Object Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised object localization (WSOL) is a challenging task aiming to localize objects with only image-level supervision. Recent works apply visual transformer to WSOL and achieve significant success by exploiting the long-range feature dependency in self-attention mechanism. However, existing transformer-based methods synthesize the classification feature maps as the localization map, which leads to optimization conflicts between classification and localization tasks. To address this problem, we propose to learn a task-specific spatial-aware token (SAT) to condition localization in a weakly supervised manner. Specifically, a spatial token is first introduced in the input space to aggregate representations for localization task. Then a spatial aware attention module is constructed, which allows spatial token to generate foreground probabilities of different patches by querying and to extract localization knowledge from the classification task. Besides, for the problem of sparse and unbalanced pixel-level supervision obtained from the image-level label, two spatial constraints, including batch area loss and normalization loss, are designed to compensate and enhance this supervision. Experiments show that the proposed SAT achieves state-of-the-art performance on both CUB-200 and ImageNet, with 98.45% and 73.13% GT-known Loc, respectively. Even under the extreme setting of using only 1 image per class from ImageNet for training, SAT already exceeds the SOTA method by 2.1% GT-known Loc. Code and models are available at https://github.com/wpy1999/SAT.
<div id='section'>Paperid: <span id='pid'>366, <a href='https://arxiv.org/pdf/2303.09665.pdf' target='_blank'>https://arxiv.org/pdf/2303.09665.pdf</a></span>   <span><a href='https://reagan1311.github.io/locate/,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gen Li, Varun Jampani, Deqing Sun, Laura Sevilla-Lara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.09665">LOCATE: Localize and Transfer Object Parts for Weakly Supervised Affordance Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans excel at acquiring knowledge through observation. For example, we can learn to use new tools by watching demonstrations. This skill is fundamental for intelligent systems to interact with the world. A key step to acquire this skill is to identify what part of the object affords each action, which is called affordance grounding. In this paper, we address this problem and propose a framework called LOCATE that can identify matching object parts across images, to transfer knowledge from images where an object is being used (exocentric images used for learning), to images where the object is inactive (egocentric ones used to test). To this end, we first find interaction areas and extract their feature embeddings. Then we learn to aggregate the embeddings into compact prototypes (human, object part, and background), and select the one representing the object part. Finally, we use the selected prototype to guide affordance grounding. We do this in a weakly supervised manner, learning only from image-level affordance and object labels. Extensive experiments demonstrate that our approach outperforms state-of-the-art methods by a large margin on both seen and unseen objects.
<div id='section'>Paperid: <span id='pid'>367, <a href='https://arxiv.org/pdf/2303.09608.pdf' target='_blank'>https://arxiv.org/pdf/2303.09608.pdf</a></span>   <span><a href='https://github.com/arushirai1/CLaNDataset' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Arushi Rai, Adriana Kovashka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.09608">VEIL: Vetting Extracted Image Labels from In-the-Wild Captions for Weakly-Supervised Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The use of large-scale vision-language datasets is limited for object detection due to the negative impact of label noise on localization. Prior methods have shown how such large-scale datasets can be used for pretraining, which can provide initial signal for localization, but is insufficient without clean bounding-box data for at least some categories. We propose a technique to "vet" labels extracted from noisy captions, and use them for weakly-supervised object detection (WSOD), without any bounding boxes. We analyze and annotate the types of label noise in captions in our Caption Label Noise dataset, and train a classifier that predicts if an extracted label is actually present in the image or not. Our classifier generalizes across dataset boundaries and across categories. We compare the classifier to nine baselines on five datasets, and demonstrate that it can improve WSOD without label vetting by 30% (31.2 to 40.5 mAP when evaluated on PASCAL VOC). See dataset at: https://github.com/arushirai1/CLaNDataset.
<div id='section'>Paperid: <span id='pid'>368, <a href='https://arxiv.org/pdf/2303.08686.pdf' target='_blank'>https://arxiv.org/pdf/2303.08686.pdf</a></span>   <span><a href='https://github.com/weakmono3d/weakmono3d' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Runzhou Tao, Wencheng Han, Zhongying Qiu, Cheng-zhong Xu, Jianbing Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.08686">Weakly Supervised Monocular 3D Object Detection using Multi-View Projection and Direction Consistency</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monocular 3D object detection has become a mainstream approach in automatic driving for its easy application. A prominent advantage is that it does not need LiDAR point clouds during the inference. However, most current methods still rely on 3D point cloud data for labeling the ground truths used in the training phase. This inconsistency between the training and inference makes it hard to utilize the large-scale feedback data and increases the data collection expenses. To bridge this gap, we propose a new weakly supervised monocular 3D objection detection method, which can train the model with only 2D labels marked on images. To be specific, we explore three types of consistency in this task, i.e. the projection, multi-view and direction consistency, and design a weakly-supervised architecture based on these consistencies. Moreover, we propose a new 2D direction labeling method in this task to guide the model for accurate rotation direction prediction. Experiments show that our weakly-supervised method achieves comparable performance with some fully supervised methods. When used as a pre-training method, our model can significantly outperform the corresponding fully-supervised baseline with only 1/3 3D labels. https://github.com/weakmono3d/weakmono3d
<div id='section'>Paperid: <span id='pid'>369, <a href='https://arxiv.org/pdf/2303.08578.pdf' target='_blank'>https://arxiv.org/pdf/2303.08578.pdf</a></span>   <span><a href='https://github.com/lslrh/SIM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruihuang Li, Chenhang He, Yabin Zhang, Shuai Li, Liyi Chen, Lei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.08578">SIM: Semantic-aware Instance Mask Generation for Box-Supervised Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised instance segmentation using only bounding box annotations has recently attracted much research attention. Most of the current efforts leverage low-level image features as extra supervision without explicitly exploiting the high-level semantic information of the objects, which will become ineffective when the foreground objects have similar appearances to the background or other objects nearby. We propose a new box-supervised instance segmentation approach by developing a Semantic-aware Instance Mask (SIM) generation paradigm. Instead of heavily relying on local pair-wise affinities among neighboring pixels, we construct a group of category-wise feature centroids as prototypes to identify foreground objects and assign them semantic-level pseudo labels. Considering that the semantic-aware prototypes cannot distinguish different instances of the same semantics, we propose a self-correction mechanism to rectify the falsely activated regions while enhancing the correct ones. Furthermore, to handle the occlusions between objects, we tailor the Copy-Paste operation for the weakly-supervised instance segmentation task to augment challenging training data. Extensive experimental results demonstrate the superiority of our proposed SIM approach over other state-of-the-art methods. The source code: https://github.com/lslrh/SIM.
<div id='section'>Paperid: <span id='pid'>370, <a href='https://arxiv.org/pdf/2303.08446.pdf' target='_blank'>https://arxiv.org/pdf/2303.08446.pdf</a></span>   <span><a href='https://github.com/invoker-LL/WSI-finetuning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Honglin Li, Chenglu Zhu, Yunlong Zhang, Yuxuan Sun, Zhongyi Shui, Wenwei Kuang, Sunyi Zheng, Lin Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.08446">Task-specific Fine-tuning via Variational Information Bottleneck for Weakly-supervised Pathology Whole Slide Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While Multiple Instance Learning (MIL) has shown promising results in digital Pathology Whole Slide Image (WSI) classification, such a paradigm still faces performance and generalization problems due to challenges in high computational costs on Gigapixel WSIs and limited sample size for model training. To deal with the computation problem, most MIL methods utilize a frozen pretrained model from ImageNet to obtain representations first. This process may lose essential information owing to the large domain gap and hinder the generalization of model due to the lack of image-level training-time augmentations. Though Self-supervised Learning (SSL) proposes viable representation learning schemes, the improvement of the downstream task still needs to be further explored in the conversion from the task-agnostic features of SSL to the task-specifics under the partial label supervised learning. To alleviate the dilemma of computation cost and performance, we propose an efficient WSI fine-tuning framework motivated by the Information Bottleneck theory. The theory enables the framework to find the minimal sufficient statistics of WSI, thus supporting us to fine-tune the backbone into a task-specific representation only depending on WSI-level weak labels. The WSI-MIL problem is further analyzed to theoretically deduce our fine-tuning method. Our framework is evaluated on five pathology WSI datasets on various WSI heads. The experimental results of our fine-tuned representations show significant improvements in both accuracy and generalization compared with previous works. Source code will be available at https://github.com/invoker-LL/WSI-finetuning.
<div id='section'>Paperid: <span id='pid'>371, <a href='https://arxiv.org/pdf/2303.07853.pdf' target='_blank'>https://arxiv.org/pdf/2303.07853.pdf</a></span>   <span><a href='https://github.com/bharathprabakaran/ReFit' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bharath Srinivas Prabakaran, Erik Ostrowski, Muhammad Shafique
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.07853">ReFit: A Framework for Refinement of Weakly Supervised Semantic Segmentation using Object Border Fitting for Medical Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Semantic Segmentation (WSSS) relying only on image-level supervision is a promising approach to deal with the need for Segmentation networks, especially for generating a large number of pixel-wise masks in a given dataset. However, most state-of-the-art image-level WSSS techniques lack an understanding of the geometric features embedded in the images since the network cannot derive any object boundary information from just image-level labels. We define a boundary here as the line separating an object and its background, or two different objects. To address this drawback, we are proposing our novel ReFit framework, which deploys state-of-the-art class activation maps combined with various post-processing techniques in order to achieve fine-grained higher-accuracy segmentation masks. To achieve this, we investigate a state-of-the-art unsupervised segmentation network that can be used to construct a boundary map, which enables ReFit to predict object locations with sharper boundaries. By applying our method to WSSS predictions, we achieved up to 10% improvement over the current state-of-the-art WSSS methods for medical imaging. The framework is open-source, to ensure that our results are reproducible, and accessible online at https://github.com/bharathprabakaran/ReFit.
<div id='section'>Paperid: <span id='pid'>372, <a href='https://arxiv.org/pdf/2303.06596.pdf' target='_blank'>https://arxiv.org/pdf/2303.06596.pdf</a></span>   <span><a href='https://github.com/saraao/amodal-dataset' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayang Ao, Qiuhong Ke, Krista A. Ehinger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.06596">Amodal Intra-class Instance Segmentation: Synthetic Datasets and Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Images of realistic scenes often contain intra-class objects that are heavily occluded from each other, making the amodal perception task that requires parsing the occluded parts of the objects challenging. Although important for downstream tasks such as robotic grasping systems, the lack of large-scale amodal datasets with detailed annotations makes it difficult to model intra-class occlusions explicitly. This paper introduces two new amodal datasets for image amodal completion tasks, which contain a total of over 267K images of intra-class occlusion scenarios, annotated with multiple masks, amodal bounding boxes, dual order relations and full appearance for instances and background. We also present a point-supervised scheme with layer priors for amodal instance segmentation specifically designed for intra-class occlusion scenarios. Experiments show that our weakly supervised approach outperforms the SOTA fully supervised methods, while our layer priors design exhibits remarkable performance improvements in the case of intra-class occlusion in both synthetic and real images.
<div id='section'>Paperid: <span id='pid'>373, <a href='https://arxiv.org/pdf/2303.06242.pdf' target='_blank'>https://arxiv.org/pdf/2303.06242.pdf</a></span>   <span><a href='https://github.com/paolomandica/HYSP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Luca Franco, Paolo Mandica, Bharti Munjal, Fabio Galasso
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.06242">HYperbolic Self-Paced Learning for Self-Supervised Skeleton-based Action Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-paced learning has been beneficial for tasks where some initial knowledge is available, such as weakly supervised learning and domain adaptation, to select and order the training sample sequence, from easy to complex. However its applicability remains unexplored in unsupervised learning, whereby the knowledge of the task matures during training. We propose a novel HYperbolic Self-Paced model (HYSP) for learning skeleton-based action representations. HYSP adopts self-supervision: it uses data augmentations to generate two views of the same sample, and it learns by matching one (named online) to the other (the target). We propose to use hyperbolic uncertainty to determine the algorithmic learning pace, under the assumption that less uncertain samples should be more strongly driving the training, with a larger weight and pace. Hyperbolic uncertainty is a by-product of the adopted hyperbolic neural networks, it matures during training and it comes with no extra cost, compared to the established Euclidean SSL framework counterparts. When tested on three established skeleton-based action recognition datasets, HYSP outperforms the state-of-the-art on PKU-MMD I, as well as on 2 out of 3 downstream tasks on NTU-60 and NTU-120. Additionally, HYSP only uses positive pairs and bypasses therefore the complex and computationally-demanding mining procedures required for the negatives in contrastive techniques. Code is available at https://github.com/paolomandica/HYSP.
<div id='section'>Paperid: <span id='pid'>374, <a href='https://arxiv.org/pdf/2303.05646.pdf' target='_blank'>https://arxiv.org/pdf/2303.05646.pdf</a></span>   <span><a href='https://github.com/Whileherham/IMR-HSNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haohan Wang, Liang Liu, Wuhao Zhang, Jiangning Zhang, Zhenye Gan, Yabiao Wang, Chengjie Wang, Haoqian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.05646">Iterative Few-shot Semantic Segmentation from Image Label Text</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Few-shot semantic segmentation aims to learn to segment unseen class objects with the guidance of only a few support images. Most previous methods rely on the pixel-level label of support images. In this paper, we focus on a more challenging setting, in which only the image-level labels are available. We propose a general framework to firstly generate coarse masks with the help of the powerful vision-language model CLIP, and then iteratively and mutually refine the mask predictions of support and query images. Extensive experiments on PASCAL-5i and COCO-20i datasets demonstrate that our method not only outperforms the state-of-the-art weakly supervised approaches by a significant margin, but also achieves comparable or better results to recent supervised methods. Moreover, our method owns an excellent generalization ability for the images in the wild and uncommon classes. Code will be available at https://github.com/Whileherham/IMR-HSNet.
<div id='section'>Paperid: <span id='pid'>375, <a href='https://arxiv.org/pdf/2303.05164.pdf' target='_blank'>https://arxiv.org/pdf/2303.05164.pdf</a></span>   <span><a href='https://github.com/wu-zhonghua/RAC-Net' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhonghua Wu, Yicheng Wu, Guosheng Lin, Jianfei Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.05164">Reliability-Adaptive Consistency Regularization for Weakly-Supervised Point Cloud Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised point cloud segmentation with extremely limited labels is highly desirable to alleviate the expensive costs of collecting densely annotated 3D points. This paper explores applying the consistency regularization that is commonly used in weakly-supervised learning, for its point cloud counterpart with multiple data-specific augmentations, which has not been well studied. We observe that the straightforward way of applying consistency constraints to weakly-supervised point cloud segmentation has two major limitations: noisy pseudo labels due to the conventional confidence-based selection and insufficient consistency constraints due to discarding unreliable pseudo labels. Therefore, we propose a novel Reliability-Adaptive Consistency Network (RAC-Net) to use both prediction confidence and model uncertainty to measure the reliability of pseudo labels and apply consistency training on all unlabeled points while with different consistency constraints for different points based on the reliability of corresponding pseudo labels. Experimental results on the S3DIS and ScanNet-v2 benchmark datasets show that our model achieves superior performance in weakly-supervised point cloud segmentation. The code will be released publicly at https://github.com/wu-zhonghua/RAC-Net.
<div id='section'>Paperid: <span id='pid'>376, <a href='https://arxiv.org/pdf/2303.05123.pdf' target='_blank'>https://arxiv.org/pdf/2303.05123.pdf</a></span>   <span><a href='https://prime-slam.github.io/place-recognition-db/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Anastasiia Kornilova, Ivan Moskalenko, Timofei Pushkin, Fakhriddin Tojiboev, Rahim Tariverdizadeh, Gonzalo Ferrer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.05123">Dominating Set Database Selection for Visual Place Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents an approach for creating a visual place recognition (VPR) database for localization in indoor environments from RGBD scanning sequences. The proposed approach is formulated as a minimization problem in terms of dominating set algorithm for graph, constructed from spatial information, and referred as DominatingSet. Our algorithm shows better scene coverage in comparison to other methodologies that are used for database creation. Also, we demonstrate that using DominatingSet, a database size could be up to 250-1400 times smaller than the original scanning sequence while maintaining a recall rate of more than 80% on testing sequences. We evaluated our algorithm on 7-scenes and BundleFusion datasets and an additionally recorded sequence in a highly repetitive office setting. In addition, the database selection can produce weakly-supervised labels for fine-tuning neural place recognition algorithms to particular settings, improving even more their accuracy. The paper also presents a fully automated pipeline for VPR database creation from RGBD scanning sequences, as well as a set of metrics for VPR database evaluation. The code and released data are available on our web-page~ -- https://prime-slam.github.io/place-recognition-db/
<div id='section'>Paperid: <span id='pid'>377, <a href='https://arxiv.org/pdf/2303.01267.pdf' target='_blank'>https://arxiv.org/pdf/2303.01267.pdf</a></span>   <span><a href='https://github.com/rulixiang/ToCo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lixiang Ru, Heliang Zheng, Yibing Zhan, Bo Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.01267">Token Contrast for Weakly-Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-Supervised Semantic Segmentation (WSSS) using image-level labels typically utilizes Class Activation Map (CAM) to generate the pseudo labels. Limited by the local structure perception of CNN, CAM usually cannot identify the integral object regions. Though the recent Vision Transformer (ViT) can remedy this flaw, we observe it also brings the over-smoothing issue, \ie, the final patch tokens incline to be uniform. In this work, we propose Token Contrast (ToCo) to address this issue and further explore the virtue of ViT for WSSS. Firstly, motivated by the observation that intermediate layers in ViT can still retain semantic diversity, we designed a Patch Token Contrast module (PTC). PTC supervises the final patch tokens with the pseudo token relations derived from intermediate layers, allowing them to align the semantic regions and thus yield more accurate CAM. Secondly, to further differentiate the low-confidence regions in CAM, we devised a Class Token Contrast module (CTC) inspired by the fact that class tokens in ViT can capture high-level semantics. CTC facilitates the representation consistency between uncertain local regions and global objects by contrasting their class tokens. Experiments on the PASCAL VOC and MS COCO datasets show the proposed ToCo can remarkably surpass other single-stage competitors and achieve comparable performance with state-of-the-art multi-stage methods. Code is available at https://github.com/rulixiang/ToCo.
<div id='section'>Paperid: <span id='pid'>378, <a href='https://arxiv.org/pdf/2302.13765.pdf' target='_blank'>https://arxiv.org/pdf/2302.13765.pdf</a></span>   <span><a href='https://github.com/Rongtao-Xu/RepresentationLearning/tree/main/SCD-AAAI2023' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rongtao Xu, Changwei Wang, Jiaxi Sun, Shibiao Xu, Weiliang Meng, Xiaopeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.13765">Self Correspondence Distillation for End-to-End Weakly-Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficiently training accurate deep models for weakly supervised semantic segmentation (WSSS) with image-level labels is challenging and important. Recently, end-to-end WSSS methods have become the focus of research due to their high training efficiency. However, current methods suffer from insufficient extraction of comprehensive semantic information, resulting in low-quality pseudo-labels and sub-optimal solutions for end-to-end WSSS. To this end, we propose a simple and novel Self Correspondence Distillation (SCD) method to refine pseudo-labels without introducing external supervision. Our SCD enables the network to utilize feature correspondence derived from itself as a distillation target, which can enhance the network's feature learning process by complementing semantic information. In addition, to further improve the segmentation accuracy, we design a Variation-aware Refine Module to enhance the local consistency of pseudo-labels by computing pixel-level variation. Finally, we present an efficient end-to-end Transformer-based framework (TSCD) via SCD and Variation-aware Refine Module for the accurate WSSS task. Extensive experiments on the PASCAL VOC 2012 and MS COCO 2014 datasets demonstrate that our method significantly outperforms other state-of-the-art methods.
  Our code is available at {https://github.com/Rongtao-Xu/RepresentationLearning/tree/main/SCD-AAAI2023}.
<div id='section'>Paperid: <span id='pid'>379, <a href='https://arxiv.org/pdf/2302.09891.pdf' target='_blank'>https://arxiv.org/pdf/2302.09891.pdf</a></span>   <span><a href='https://github.com/dhiyu/UPLLRS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Shi, Ning Xu, Hua Yuan, Xin Geng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.09891">Unreliable Partial Label Learning with Recursive Separation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partial label learning (PLL) is a typical weakly supervised learning problem in which each instance is associated with a candidate label set, and among which only one is true. However, the assumption that the ground-truth label is always among the candidate label set would be unrealistic, as the reliability of the candidate label sets in real-world applications cannot be guaranteed by annotators. Therefore, a generalized PLL named Unreliable Partial Label Learning (UPLL) is proposed, in which the true label may not be in the candidate label set. Due to the challenges posed by unreliable labeling, previous PLL methods will experience a marked decline in performance when applied to UPLL. To address the issue, we propose a two-stage framework named Unreliable Partial Label Learning with Recursive Separation (UPLLRS). In the first stage, the self-adaptive recursive separation strategy is proposed to separate the training set into a reliable subset and an unreliable subset. In the second stage, a disambiguation strategy is employed to progressively identify the ground-truth labels in the reliable subset. Simultaneously, semi-supervised learning methods are adopted to extract valuable information from the unreliable subset. Our method demonstrates state-of-the-art performance as evidenced by experimental results, particularly in situations of high unreliability. Code and supplementary materials are available at https://github.com/dhiyu/UPLLRS.
<div id='section'>Paperid: <span id='pid'>380, <a href='https://arxiv.org/pdf/2302.05087.pdf' target='_blank'>https://arxiv.org/pdf/2302.05087.pdf</a></span>   <span><a href='https://github.com/fudanyliu/GVAED' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Liu, Dingkang Yang, Yan Wang, Jing Liu, Jun Liu, Azzedine Boukerche, Peng Sun, Liang Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.05087">Generalized Video Anomaly Event Detection: Systematic Taxonomy and Comparison of Deep Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Anomaly Detection (VAD) serves as a pivotal technology in the intelligent surveillance systems, enabling the temporal or spatial identification of anomalous events within videos. While existing reviews predominantly concentrate on conventional unsupervised methods, they often overlook the emergence of weakly-supervised and fully-unsupervised approaches. To address this gap, this survey extends the conventional scope of VAD beyond unsupervised methods, encompassing a broader spectrum termed Generalized Video Anomaly Event Detection (GVAED). By skillfully incorporating recent advancements rooted in diverse assumptions and learning frameworks, this survey introduces an intuitive taxonomy that seamlessly navigates through unsupervised, weakly-supervised, supervised and fully-unsupervised VAD methodologies, elucidating the distinctions and interconnections within these research trajectories. In addition, this survey facilitates prospective researchers by assembling a compilation of research resources, including public datasets, available codebases, programming tools, and pertinent literature. Furthermore, this survey quantitatively assesses model performance, delves into research challenges and directions, and outlines potential avenues for future exploration.
<div id='section'>Paperid: <span id='pid'>381, <a href='https://arxiv.org/pdf/2302.04607.pdf' target='_blank'>https://arxiv.org/pdf/2302.04607.pdf</a></span>   <span><a href='https://github.com/jiabeiwangTJU/DICL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiabei Wang, Yanwei Pang, Jiale Cao, Hanqing Sun, Zhuang Shao, Xuelong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.04607">Deep Intra-Image Contrastive Learning for Weakly Supervised One-Step Person Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised person search aims to perform joint pedestrian detection and re-identification (re-id) with only person bounding-box annotations. Recently, the idea of contrastive learning is initially applied to weakly supervised person search, where two common contrast strategies are memory-based contrast and intra-image contrast. We argue that current intra-image contrast is shallow, which suffers from spatial-level and occlusion-level variance. In this paper, we present a novel deep intra-image contrastive learning using a Siamese network. Two key modules are spatial-invariant contrast (SIC) and occlusion-invariant contrast (OIC). SIC performs many-to-one contrasts between two branches of Siamese network and dense prediction contrasts in one branch of Siamese network. With these many-to-one and dense contrasts, SIC tends to learn discriminative scale-invariant and location-invariant features to solve spatial-level variance. OIC enhances feature consistency with the masking strategy to learn occlusion-invariant features. Extensive experiments are performed on two person search datasets CUHK-SYSU and PRW, respectively. Our method achieves a state-of-the-art performance among weakly supervised one-step person search approaches. We hope that our simple intra-image contrastive learning can provide more paradigms on weakly supervised person search. The source code is available at \url{https://github.com/jiabeiwangTJU/DICL}.
<div id='section'>Paperid: <span id='pid'>382, <a href='https://arxiv.org/pdf/2302.04549.pdf' target='_blank'>https://arxiv.org/pdf/2302.04549.pdf</a></span>   <span><a href='https://github.com/yzhao062/wsad' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Minqi Jiang, Chaochuan Hou, Ao Zheng, Xiyang Hu, Songqiao Han, Hailiang Huang, Xiangnan He, Philip S. Yu, Yue Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.04549">Weakly Supervised Anomaly Detection: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anomaly detection (AD) is a crucial task in machine learning with various applications, such as detecting emerging diseases, identifying financial frauds, and detecting fake news. However, obtaining complete, accurate, and precise labels for AD tasks can be expensive and challenging due to the cost and difficulties in data annotation. To address this issue, researchers have developed AD methods that can work with incomplete, inexact, and inaccurate supervision, collectively summarized as weakly supervised anomaly detection (WSAD) methods. In this study, we present the first comprehensive survey of WSAD methods by categorizing them into the above three weak supervision settings across four data modalities (i.e., tabular, graph, time-series, and image/video data). For each setting, we provide formal definitions, key algorithms, and potential future directions. To support future research, we conduct experiments on a selected setting and release the source code, along with a collection of WSAD methods and data.
<div id='section'>Paperid: <span id='pid'>383, <a href='https://arxiv.org/pdf/2301.12053.pdf' target='_blank'>https://arxiv.org/pdf/2301.12053.pdf</a></span>   <span><a href='https://github.com/wangjuan313/wsis-beyond-tightBB' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Juan Wang, Bin Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.12053">Weakly Supervised Image Segmentation Beyond Tight Bounding Box Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised image segmentation approaches in the literature usually achieve high segmentation performance using tight bounding box supervision and decrease the performance greatly when supervised by loose bounding boxes. However, compared with loose bounding box, it is much more difficult to acquire tight bounding box due to its strict requirements on the precise locations of the four sides of the box. To resolve this issue, this study investigates whether it is possible to maintain good segmentation performance when loose bounding boxes are used as supervision. For this purpose, this work extends our previous parallel transformation based multiple instance learning (MIL) for tight bounding box supervision by integrating an MIL strategy based on polar transformation to assist image segmentation. The proposed polar transformation based MIL formulation works for both tight and loose bounding boxes, in which a positive bag is defined as pixels in a polar line of a bounding box with one endpoint located inside the object enclosed by the box and the other endpoint located at one of the four sides of the box. Moreover, a weighted smooth maximum approximation is introduced to incorporate the observation that pixels closer to the origin of the polar transformation are more likely to belong to the object in the box. The proposed approach was evaluated on two public datasets using dice coefficient when bounding boxes at different precision levels were considered in the experiments. The results demonstrate that the proposed approach achieves state-of-the-art performance for bounding boxes at all precision levels and is robust to mild and moderate errors in the loose bounding box annotations. The codes are available at \url{https://github.com/wangjuan313/wsis-beyond-tightBB}.
<div id='section'>Paperid: <span id='pid'>384, <a href='https://arxiv.org/pdf/2301.09640.pdf' target='_blank'>https://arxiv.org/pdf/2301.09640.pdf</a></span>   <span><a href='https://github.com/fyshelab/QA-ZRE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Saeed Najafi, Alona Fyshe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.09640">Weakly-Supervised Questions for Zero-Shot Relation Extraction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Zero-Shot Relation Extraction (ZRE) is the task of Relation Extraction where the training and test sets have no shared relation types. This very challenging domain is a good test of a model's ability to generalize. Previous approaches to ZRE reframed relation extraction as Question Answering (QA), allowing for the use of pre-trained QA models. However, this method required manually creating gold question templates for each new relation. Here, we do away with these gold templates and instead learn a model that can generate questions for unseen relations. Our technique can successfully translate relation descriptions into relevant questions, which are then leveraged to generate the correct tail entity. On tail entity extraction, we outperform the previous state-of-the-art by more than 16 F1 points without using gold question templates. On the RE-QA dataset where no previous baseline for relation extraction exists, our proposed algorithm comes within 0.7 F1 points of a system that uses gold question templates. Our model also outperforms the state-of-the-art ZRE baselines on the FewRel and WikiZSL datasets, showing that QA models no longer need template questions to match the performance of models specifically tailored to the ZRE task. Our implementation is available at https://github.com/fyshelab/QA-ZRE.
<div id='section'>Paperid: <span id='pid'>385, <a href='https://arxiv.org/pdf/2212.09506.pdf' target='_blank'>https://arxiv.org/pdf/2212.09506.pdf</a></span>   <span><a href='https://github.com/linyq2117/CLIP-ES' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqi Lin, Minghao Chen, Wenxiao Wang, Boxi Wu, Ke Li, Binbin Lin, Haifeng Liu, Xiaofei He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.09506">CLIP is Also an Efficient Segmenter: A Text-Driven Approach for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation (WSSS) with image-level labels is a challenging task. Mainstream approaches follow a multi-stage framework and suffer from high training costs. In this paper, we explore the potential of Contrastive Language-Image Pre-training models (CLIP) to localize different categories with only image-level labels and without further training. To efficiently generate high-quality segmentation masks from CLIP, we propose a novel WSSS framework called CLIP-ES. Our framework improves all three stages of WSSS with special designs for CLIP: 1) We introduce the softmax function into GradCAM and exploit the zero-shot ability of CLIP to suppress the confusion caused by non-target classes and backgrounds. Meanwhile, to take full advantage of CLIP, we re-explore text inputs under the WSSS setting and customize two text-driven strategies: sharpness-based prompt selection and synonym fusion. 2) To simplify the stage of CAM refinement, we propose a real-time class-aware attention-based affinity (CAA) module based on the inherent multi-head self-attention (MHSA) in CLIP-ViTs. 3) When training the final segmentation model with the masks generated by CLIP, we introduced a confidence-guided loss (CGL) focus on confident regions. Our CLIP-ES achieves SOTA performance on Pascal VOC 2012 and MS COCO 2014 while only taking 10% time of previous methods for the pseudo mask generation. Code is available at https://github.com/linyq2117/CLIP-ES.
<div id='section'>Paperid: <span id='pid'>386, <a href='https://arxiv.org/pdf/2212.08902.pdf' target='_blank'>https://arxiv.org/pdf/2212.08902.pdf</a></span>   <span><a href='https://github.com/wbbeyourself/DTE' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/wbbeyourself/DTE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bing Wang, Yan Gao, Zhoujun Li, Jian-Guang Lou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.08902">Know What I don't Know: Handling Ambiguous and Unanswerable Questions for Text-to-SQL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The task of text-to-SQL aims to convert a natural language question into its corresponding SQL query within the context of relational tables. Existing text-to-SQL parsers generate a "plausible" SQL query for an arbitrary user question, thereby failing to correctly handle problematic user questions. To formalize this problem, we conduct a preliminary study on the observed ambiguous and unanswerable cases in text-to-SQL and summarize them into 6 feature categories. Correspondingly, we identify the causes behind each category and propose requirements for handling ambiguous and unanswerable questions. Following this study, we propose a simple yet effective counterfactual example generation approach that automatically produces ambiguous and unanswerable text-to-SQL examples. Furthermore, we propose a weakly supervised DTE (Detecting-Then-Explaining) model for error detection, localization, and explanation. Experimental results show that our model achieves the best result on both real-world examples and generated examples compared with various baselines. We release our data and code at: \href{https://github.com/wbbeyourself/DTE}{https://github.com/wbbeyourself/DTE}.
<div id='section'>Paperid: <span id='pid'>387, <a href='https://arxiv.org/pdf/2212.07047.pdf' target='_blank'>https://arxiv.org/pdf/2212.07047.pdf</a></span>   <span><a href='https://github.com/sunjiayuanro/SCFeat.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayuan Sun, Jiewen Zhu, Luping Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.07047">Shared Coupling-bridge for Weakly Supervised Local Feature Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sparse local feature extraction is usually believed to be of important significance in typical vision tasks such as simultaneous localization and mapping, image matching and 3D reconstruction. At present, it still has some deficiencies needing further improvement, mainly including the discrimination power of extracted local descriptors, the localization accuracy of detected keypoints, and the efficiency of local feature learning. This paper focuses on promoting the currently popular sparse local feature learning with camera pose supervision. Therefore, it pertinently proposes a Shared Coupling-bridge scheme with four light-weight yet effective improvements for weakly-supervised local feature (SCFeat) learning. It mainly contains: i) the \emph{Feature-Fusion-ResUNet Backbone} (F2R-Backbone) for local descriptors learning, ii) a shared coupling-bridge normalization to improve the decoupling training of description network and detection network, iii) an improved detection network with peakiness measurement to detect keypoints and iv) the fundamental matrix error as a reward factor to further optimize feature detection training. Extensive experiments prove that our SCFeat improvement is effective. It could often obtain a state-of-the-art performance on classic image matching and visual localization. In terms of 3D reconstruction, it could still achieve competitive results. For sharing and communication, our source codes are available at https://github.com/sunjiayuanro/SCFeat.git.
<div id='section'>Paperid: <span id='pid'>388, <a href='https://arxiv.org/pdf/2212.05566.pdf' target='_blank'>https://arxiv.org/pdf/2212.05566.pdf</a></span>   <span><a href='https://github.com/llmir/YoloCurvSeg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Li Lin, Linkai Peng, Huaqing He, Pujin Cheng, Jiewei Wu, Kenneth K. Y. Wong, Xiaoying Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.05566">YoloCurvSeg: You Only Label One Noisy Skeleton for Vessel-style Curvilinear Structure Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised learning (WSL) has been proposed to alleviate the conflict between data annotation cost and model performance through employing sparsely-grained (i.e., point-, box-, scribble-wise) supervision and has shown promising performance, particularly in the image segmentation field. However, it is still a very challenging task due to the limited supervision, especially when only a small number of labeled samples are available. Additionally, almost all existing WSL segmentation methods are designed for star-convex structures which are very different from curvilinear structures such as vessels and nerves. In this paper, we propose a novel sparsely annotated segmentation framework for curvilinear structures, named YoloCurvSeg. A very essential component of YoloCurvSeg is image synthesis. Specifically, a background generator delivers image backgrounds that closely match the real distributions through inpainting dilated skeletons. The extracted backgrounds are then combined with randomly emulated curves generated by a Space Colonization Algorithm-based foreground generator and through a multilayer patch-wise contrastive learning synthesizer. In this way, a synthetic dataset with both images and curve segmentation labels is obtained, at the cost of only one or a few noisy skeleton annotations. Finally, a segmenter is trained with the generated dataset and possibly an unlabeled dataset. The proposed YoloCurvSeg is evaluated on four publicly available datasets (OCTA500, CORN, DRIVE and CHASEDB1) and the results show that YoloCurvSeg outperforms state-of-the-art WSL segmentation methods by large margins. With only one noisy skeleton annotation (respectively 0.14\%, 0.03\%, 1.40\%, and 0.65\% of the full annotation), YoloCurvSeg achieves more than 97\% of the fully-supervised performance on each dataset. Code and datasets will be released at https://github.com/llmir/YoloCurvSeg.
<div id='section'>Paperid: <span id='pid'>389, <a href='https://arxiv.org/pdf/2212.05136.pdf' target='_blank'>https://arxiv.org/pdf/2212.05136.pdf</a></span>   <span><a href='https://github.com/joos2010kj/CLIP-TSA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyekang Kevin Joo, Khoa Vo, Kashu Yamazaki, Ngan Le
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.05136">CLIP-TSA: CLIP-Assisted Temporal Self-Attention for Weakly-Supervised Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video anomaly detection (VAD) -- commonly formulated as a multiple-instance learning problem in a weakly-supervised manner due to its labor-intensive nature -- is a challenging problem in video surveillance where the frames of anomaly need to be localized in an untrimmed video. In this paper, we first propose to utilize the ViT-encoded visual features from CLIP, in contrast with the conventional C3D or I3D features in the domain, to efficiently extract discriminative representations in the novel technique. We then model temporal dependencies and nominate the snippets of interest by leveraging our proposed Temporal Self-Attention (TSA). The ablation study confirms the effectiveness of TSA and ViT feature. The extensive experiments show that our proposed CLIP-TSA outperforms the existing state-of-the-art (SOTA) methods by a large margin on three commonly-used benchmark datasets in the VAD problem (UCF-Crime, ShanghaiTech Campus, and XD-Violence). Our source code is available at https://github.com/joos2010kj/CLIP-TSA.
<div id='section'>Paperid: <span id='pid'>390, <a href='https://arxiv.org/pdf/2212.04875.pdf' target='_blank'>https://arxiv.org/pdf/2212.04875.pdf</a></span>   <span><a href='https://github.com/minhlong94/Random-Mixup]' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Minh-Long Luu, Zeyi Huang, Eric P. Xing, Yong Jae Lee, Haohan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.04875">Expeditious Saliency-guided Mix-up through Random Gradient Thresholding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mix-up training approaches have proven to be effective in improving the generalization ability of Deep Neural Networks. Over the years, the research community expands mix-up methods into two directions, with extensive efforts to improve saliency-guided procedures but minimal focus on the arbitrary path, leaving the randomization domain unexplored. In this paper, inspired by the superior qualities of each direction over one another, we introduce a novel method that lies at the junction of the two routes. By combining the best elements of randomness and saliency utilization, our method balances speed, simplicity, and accuracy. We name our method R-Mix following the concept of "Random Mix-up". We demonstrate its effectiveness in generalization, weakly supervised object localization, calibration, and robustness to adversarial attacks. Finally, in order to address the question of whether there exists a better decision protocol, we train a Reinforcement Learning agent that decides the mix-up policies based on the classifier's performance, reducing dependency on human-designed objectives and hyperparameter tuning. Extensive experiments further show that the agent is capable of performing at the cutting-edge level, laying the foundation for a fully automatic mix-up. Our code is released at [https://github.com/minhlong94/Random-Mixup].
<div id='section'>Paperid: <span id='pid'>391, <a href='https://arxiv.org/pdf/2212.03504.pdf' target='_blank'>https://arxiv.org/pdf/2212.03504.pdf</a></span>   <span><a href='https://github.com/Serenos/LWSIS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiang Li, Junbo Yin, Botian Shi, Yikang Li, Ruigang Yang, Jianbing Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.03504">LWSIS: LiDAR-guided Weakly Supervised Instance Segmentation for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image instance segmentation is a fundamental research topic in autonomous driving, which is crucial for scene understanding and road safety. Advanced learning-based approaches often rely on the costly 2D mask annotations for training. In this paper, we present a more artful framework, LiDAR-guided Weakly Supervised Instance Segmentation (LWSIS), which leverages the off-the-shelf 3D data, i.e., Point Cloud, together with the 3D boxes, as natural weak supervisions for training the 2D image instance segmentation models. Our LWSIS not only exploits the complementary information in multimodal data during training, but also significantly reduces the annotation cost of the dense 2D masks. In detail, LWSIS consists of two crucial modules, Point Label Assignment (PLA) and Graph-based Consistency Regularization (GCR). The former module aims to automatically assign the 3D point cloud as 2D point-wise labels, while the latter further refines the predictions by enforcing geometry and appearance consistency of the multimodal data. Moreover, we conduct a secondary instance segmentation annotation on the nuScenes, named nuInsSeg, to encourage further research on multimodal perception tasks. Extensive experiments on the nuInsSeg, as well as the large-scale Waymo, show that LWSIS can substantially improve existing weakly supervised segmentation models by only involving 3D data during training. Additionally, LWSIS can also be incorporated into 3D object detectors like PointPainting to boost the 3D detection performance for free. The code and dataset are available at https://github.com/Serenos/LWSIS.
<div id='section'>Paperid: <span id='pid'>392, <a href='https://arxiv.org/pdf/2211.17071.pdf' target='_blank'>https://arxiv.org/pdf/2211.17071.pdf</a></span>   <span><a href='https://github.com/InkiInki/MI-UAP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu-Xuan Zhang, Hua Meng, Xue-Mei Cao, Zhengchun Zhou, Mei Yang, Avik Ranjan Adhikary
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.17071">Interpreting Vulnerabilities of Multi-Instance Learning to Adversarial Perturbations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Instance Learning (MIL) is a recent machine learning paradigm which is immensely useful in various real-life applications, like image analysis, video anomaly detection, text classification, etc. It is well known that most of the existing machine learning classifiers are highly vulnerable to adversarial perturbations. Since MIL is a weakly supervised learning, where information is available for a set of instances, called bag and not for every instances, adversarial perturbations can be fatal. In this paper, we have proposed two adversarial perturbation methods to analyze the effect of adversarial perturbations to interpret the vulnerability of MIL methods. Out of the two algorithms, one can be customized for every bag, and the other is a universal one, which can affect all bags in a given data set and thus has some generalizability. Through simulations, we have also shown the effectiveness of the proposed algorithms to fool the state-of-the-art (SOTA) MIL methods. Finally, we have discussed through experiments, about taking care of these kind of adversarial perturbations through a simple strategy. Source codes are available at https://github.com/InkiInki/MI-UAP.
<div id='section'>Paperid: <span id='pid'>393, <a href='https://arxiv.org/pdf/2211.11478.pdf' target='_blank'>https://arxiv.org/pdf/2211.11478.pdf</a></span>   <span><a href='https://github.com/tsingqguo/bgmix' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Huang, Ruofei Wang, Qing Guo, Jieda Wei, Yuxiang Zhang, Wei Fan, Yang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.11478">Background-Mixed Augmentation for Weakly Supervised Change Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Change detection (CD) is to decouple object changes (i.e., object missing or appearing) from background changes (i.e., environment variations) like light and season variations in two images captured in the same scene over a long time span, presenting critical applications in disaster management, urban development, etc. In particular, the endless patterns of background changes require detectors to have a high generalization against unseen environment variations, making this task significantly challenging. Recent deep learning-based methods develop novel network architectures or optimization strategies with paired-training examples, which do not handle the generalization issue explicitly and require huge manual pixel-level annotation efforts. In this work, for the first attempt in the CD community, we study the generalization issue of CD from the perspective of data augmentation and develop a novel weakly supervised training algorithm that only needs image-level labels. Different from general augmentation techniques for classification, we propose the background-mixed augmentation that is specifically designed for change detection by augmenting examples under the guidance of a set of background-changing images and letting deep CD models see diverse environment variations. Moreover, we propose the augmented & real data consistency loss that encourages the generalization increase significantly. Our method as a general framework can enhance a wide range of existing deep learning-based detectors. We conduct extensive experiments in two public datasets and enhance four state-of-the-art methods, demonstrating the advantages of our method. We release the code at https://github.com/tsingqguo/bgmix.
<div id='section'>Paperid: <span id='pid'>394, <a href='https://arxiv.org/pdf/2211.01174.pdf' target='_blank'>https://arxiv.org/pdf/2211.01174.pdf</a></span>   <span><a href='http://zhiyongsu.github.io/Project/WHCN.html' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuheng Lu, Peng Zhang, Yuewei Dai, Weiqing Li, Zhiyong Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.01174">Hypergraph Convolutional Network based Weakly Supervised Point Cloud Semantic Segmentation with Scene-Level Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Point cloud segmentation with scene-level annotations is a promising but challenging task. Currently, the most popular way is to employ the class activation map (CAM) to locate discriminative regions and then generate point-level pseudo labels from scene-level annotations. However, these methods always suffer from the point imbalance among categories, as well as the sparse and incomplete supervision from CAM. In this paper, we propose a novel weighted hypergraph convolutional network-based method, called WHCN, to confront the challenges of learning point-wise labels from scene-level annotations. Firstly, in order to simultaneously overcome the point imbalance among different categories and reduce the model complexity, superpoints of a training point cloud are generated by exploiting the geometrically homogeneous partition. Then, a hypergraph is constructed based on the high-confidence superpoint-level seeds which are converted from scene-level annotations. Secondly, the WHCN takes the hypergraph as input and learns to predict high-precision point-level pseudo labels by label propagation. Besides the backbone network consisting of spectral hypergraph convolution blocks, a hyperedge attention module is learned to adjust the weights of hyperedges in the WHCN. Finally, a segmentation network is trained by these pseudo point cloud labels. We comprehensively conduct experiments on the ScanNet and S3DIS segmentation datasets. Experimental results demonstrate that the proposed WHCN is effective to predict the point labels with scene annotations, and yields state-of-the-art results in the community. The source code is available at http://zhiyongsu.github.io/Project/WHCN.html.
<div id='section'>Paperid: <span id='pid'>395, <a href='https://arxiv.org/pdf/2210.12444.pdf' target='_blank'>https://arxiv.org/pdf/2210.12444.pdf</a></span>   <span><a href='https://github.com/zjuchenlong/WSAG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Long Chen, Yulei Niu, Brian Chen, Xudong Lin, Guangxing Han, Christopher Thomas, Hammad Ayyubi, Heng Ji, Shih-Fu Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.12444">Weakly-Supervised Temporal Article Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Given a long untrimmed video and natural language queries, video grounding (VG) aims to temporally localize the semantically-aligned video segments. Almost all existing VG work holds two simple but unrealistic assumptions: 1) All query sentences can be grounded in the corresponding video. 2) All query sentences for the same video are always at the same semantic scale. Unfortunately, both assumptions make today's VG models fail to work in practice. For example, in real-world multimodal assets (eg, news articles), most of the sentences in the article can not be grounded in their affiliated videos, and they typically have rich hierarchical relations (ie, at different semantic scales). To this end, we propose a new challenging grounding task: Weakly-Supervised temporal Article Grounding (WSAG). Specifically, given an article and a relevant video, WSAG aims to localize all ``groundable'' sentences to the video, and these sentences are possibly at different semantic scales. Accordingly, we collect the first WSAG dataset to facilitate this task: YouwikiHow, which borrows the inherent multi-scale descriptions in wikiHow articles and plentiful YouTube videos. In addition, we propose a simple but effective method DualMIL for WSAG, which consists of a two-level MIL loss and a single-/cross- sentence constraint loss. These training objectives are carefully designed for these relaxed assumptions. Extensive ablations have verified the effectiveness of DualMIL.
<div id='section'>Paperid: <span id='pid'>396, <a href='https://arxiv.org/pdf/2210.06742.pdf' target='_blank'>https://arxiv.org/pdf/2210.06742.pdf</a></span>   <span><a href='https://github.com/yangxue0827/h2rbox-jittor' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/yangxue0827/h2rbox-mmrotate' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/yangxue0827/h2rbox-mmrotate' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/yangxue0827/h2rbox-jittor' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xue Yang, Gefan Zhang, Wentong Li, Xuehui Wang, Yue Zhou, Junchi Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.06742">H2RBox: Horizontal Box Annotation is All You Need for Oriented Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Oriented object detection emerges in many applications from aerial images to autonomous driving, while many existing detection benchmarks are annotated with horizontal bounding box only which is also less costive than fine-grained rotated box, leading to a gap between the readily available training corpus and the rising demand for oriented object detection. This paper proposes a simple yet effective oriented object detection approach called H2RBox merely using horizontal box annotation for weakly-supervised training, which closes the above gap and shows competitive performance even against those trained with rotated boxes. The cores of our method are weakly- and self-supervised learning, which predicts the angle of the object by learning the consistency of two different views. To our best knowledge, H2RBox is the first horizontal box annotation-based oriented object detector. Compared to an alternative i.e. horizontal box-supervised instance segmentation with our post adaption to oriented object detection, our approach is not susceptible to the prediction quality of mask and can perform more robustly in complex scenes containing a large number of dense objects and outliers. Experimental results show that H2RBox has significant performance and speed advantages over horizontal box-supervised instance segmentation methods, as well as lower memory requirements. While compared to rotated box-supervised oriented object detectors, our method shows very close performance and speed. The source code is available at PyTorch-based \href{https://github.com/yangxue0827/h2rbox-mmrotate}{MMRotate} and Jittor-based \href{https://github.com/yangxue0827/h2rbox-jittor}{JDet}.
<div id='section'>Paperid: <span id='pid'>397, <a href='https://arxiv.org/pdf/2210.06688.pdf' target='_blank'>https://arxiv.org/pdf/2210.06688.pdf</a></span>   <span><a href='https://github.com/wjtan99/BERT_Anomaly_Video_Classification' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weijun Tan, Qi Yao, Jingfeng Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.06688">Overlooked Video Classification in Weakly Supervised Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current weakly supervised video anomaly detection algorithms mostly use multiple instance learning (MIL) or their varieties. Almost all recent approaches focus on how to select the correct snippets for training to improve the performance. They overlook or do not realize the power of video classification in boosting the performance of anomaly detection. In this paper, we study explicitly the power of video classification supervision using a BERT or LSTM. With this BERT or LSTM, CNN features of all snippets of a video can be aggregated into a single feature which can be used for video classification. This simple yet powerful video classification supervision, combined into the MIL framework, brings extraordinary performance improvement on all three major video anomaly detection datasets. Particularly it improves the mean average precision (mAP) on the XD-Violence from SOTA 78.84\% to new 82.10\%. The source code is available at https://github.com/wjtan99/BERT_Anomaly_Video_Classification.
<div id='section'>Paperid: <span id='pid'>398, <a href='https://arxiv.org/pdf/2210.05242.pdf' target='_blank'>https://arxiv.org/pdf/2210.05242.pdf</a></span>   <span><a href='https://github.com/Bravo5542/VSCG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanyuan Jiang, Jianqin Yin, Yonghao Dang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.05242">Leveraging the Video-level Semantic Consistency of Event for Audio-visual Event Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-visual event (AVE) localization has attracted much attention in recent years. Most existing methods are often limited to independently encoding and classifying each video segment separated from the full video (which can be regarded as the segment-level representations of events). However, they ignore the semantic consistency of the event within the same full video (which can be considered as the video-level representations of events). In contrast to existing methods, we propose a novel video-level semantic consistency guidance network for the AVE localization task. Specifically, we propose an event semantic consistency modeling (ESCM) module to explore video-level semantic information for semantic consistency modeling. It consists of two components: a cross-modal event representation extractor (CERE) and an intra-modal semantic consistency enhancer (ISCE). CERE is proposed to obtain the event semantic information at the video level. Furthermore, ISCE takes video-level event semantics as prior knowledge to guide the model to focus on the semantic continuity of an event within each modality. Moreover, we propose a new negative pair filter loss to encourage the network to filter out the irrelevant segment pairs and a new smooth loss to further increase the gap between different categories of events in the weakly-supervised setting. We perform extensive experiments on the public AVE dataset and outperform the state-of-the-art methods in both fully- and weakly-supervised settings, thus verifying the effectiveness of our method.The code is available at https://github.com/Bravo5542/VSCG.
<div id='section'>Paperid: <span id='pid'>399, <a href='https://arxiv.org/pdf/2210.05174.pdf' target='_blank'>https://arxiv.org/pdf/2210.05174.pdf</a></span>   <span><a href='https://github.com/hustvl/BoxTeacher' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/hustvl/BoxTeacher' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianheng Cheng, Xinggang Wang, Shaoyu Chen, Qian Zhang, Wenyu Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.05174">BoxTeacher: Exploring High-Quality Pseudo Labels for Weakly Supervised Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Labeling objects with pixel-wise segmentation requires a huge amount of human labor compared to bounding boxes. Most existing methods for weakly supervised instance segmentation focus on designing heuristic losses with priors from bounding boxes. While, we find that box-supervised methods can produce some fine segmentation masks and we wonder whether the detectors could learn from these fine masks while ignoring low-quality masks. To answer this question, we present BoxTeacher, an efficient and end-to-end training framework for high-performance weakly supervised instance segmentation, which leverages a sophisticated teacher to generate high-quality masks as pseudo labels. Considering the massive noisy masks hurt the training, we present a mask-aware confidence score to estimate the quality of pseudo masks and propose the noise-aware pixel loss and noise-reduced affinity loss to adaptively optimize the student with pseudo masks. Extensive experiments can demonstrate the effectiveness of the proposed BoxTeacher. Without bells and whistles, BoxTeacher remarkably achieves 35.0 mask AP and 36.5 mask AP with ResNet-50 and ResNet-101 respectively on the challenging COCO dataset, which outperforms the previous state-of-the-art methods by a significant margin and bridges the gap between box-supervised and mask-supervised methods. The code and models will be available at https://github.com/hustvl/BoxTeacher.
<div id='section'>Paperid: <span id='pid'>400, <a href='https://arxiv.org/pdf/2208.09350.pdf' target='_blank'>https://arxiv.org/pdf/2208.09350.pdf</a></span>   <span><a href='https://github.com/HiLab-git/PyMIC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guotai Wang, Xiangde Luo, Ran Gu, Shuojue Yang, Yijie Qu, Shuwei Zhai, Qianfei Zhao, Kang Li, Shaoting Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.09350">PyMIC: A deep learning toolkit for annotation-efficient medical image segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Background and Objective: Open-source deep learning toolkits are one of the driving forces for developing medical image segmentation models. Existing toolkits mainly focus on fully supervised segmentation and require full and accurate pixel-level annotations that are time-consuming and difficult to acquire for segmentation tasks, which makes learning from imperfect labels highly desired for reducing the annotation cost. We aim to develop a new deep learning toolkit to support annotation-efficient learning for medical image segmentation.
  Methods: Our proposed toolkit named PyMIC is a modular deep learning library for medical image segmentation tasks. In addition to basic components that support development of high-performance models for fully supervised segmentation, it contains several advanced components tailored for learning from imperfect annotations, such as loading annotated and unannounced images, loss functions for unannotated, partially or inaccurately annotated images, and training procedures for co-learning between multiple networks, etc. PyMIC supports development of semi-supervised, weakly supervised and noise-robust learning methods for medical image segmentation.
  Results: We present several illustrative medical image segmentation tasks based on PyMIC: (1) Achieving competitive performance on fully supervised learning; (2) Semi-supervised cardiac structure segmentation with only 10% training images annotated; (3) Weakly supervised segmentation using scribble annotations; and (4) Learning from noisy labels for chest radiograph segmentation.
  Conclusions: The PyMIC toolkit is easy to use and facilitates efficient development of medical image segmentation models with imperfect annotations. It is modular and flexible, which enables researchers to develop high-performance models with low annotation cost. The source code is available at: https://github.com/HiLab-git/PyMIC.
<div id='section'>Paperid: <span id='pid'>401, <a href='https://arxiv.org/pdf/2208.01838.pdf' target='_blank'>https://arxiv.org/pdf/2208.01838.pdf</a></span>   <span><a href='https://github.com/su-hui-zz/ReAttentionTransformer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hui Su, Yue Ye, Zhiwei Chen, Mingli Song, Lechao Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.01838">Re-Attention Transformer for Weakly Supervised Object Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised object localization is a challenging task which aims to localize objects with coarse annotations such as image categories. Existing deep network approaches are mainly based on class activation map, which focuses on highlighting discriminative local region while ignoring the full object. In addition, the emerging transformer-based techniques constantly put a lot of emphasis on the backdrop that impedes the ability to identify complete objects. To address these issues, we present a re-attention mechanism termed token refinement transformer (TRT) that captures the object-level semantics to guide the localization well. Specifically, TRT introduces a novel module named token priority scoring module (TPSM) to suppress the effects of background noise while focusing on the target object. Then, we incorporate the class activation map as the semantically aware input to restrain the attention map to the target object. Extensive experiments on two benchmarks showcase the superiority of our proposed method against existing methods with image category annotations. Source code is available in \url{https://github.com/su-hui-zz/ReAttentionTransformer}.
<div id='section'>Paperid: <span id='pid'>402, <a href='https://arxiv.org/pdf/2207.10447.pdf' target='_blank'>https://arxiv.org/pdf/2207.10447.pdf</a></span>   <span><a href='https://github.com/164140757/SCM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haotian Bai, Ruimao Zhang, Jiong Wang, Xiang Wan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.10447">Weakly Supervised Object Localization via Transformer with Implicit Spatial Calibration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Object Localization (WSOL), which aims to localize objects by only using image-level labels, has attracted much attention because of its low annotation cost in real applications. Recent studies leverage the advantage of self-attention in visual Transformer for long-range dependency to re-active semantic regions, aiming to avoid partial activation in traditional class activation mapping (CAM). However, the long-range modeling in Transformer neglects the inherent spatial coherence of the object, and it usually diffuses the semantic-aware regions far from the object boundary, making localization results significantly larger or far smaller. To address such an issue, we introduce a simple yet effective Spatial Calibration Module (SCM) for accurate WSOL, incorporating semantic similarities of patch tokens and their spatial relationships into a unified diffusion model. Specifically, we introduce a learnable parameter to dynamically adjust the semantic correlations and spatial context intensities for effective information propagation. In practice, SCM is designed as an external module of Transformer, and can be removed during inference to reduce the computation cost. The object-sensitive localization ability is implicitly embedded into the Transformer encoder through optimization in the training phase. It enables the generated attention maps to capture the sharper object boundaries and filter the object-irrelevant background area. Extensive experimental results demonstrate the effectiveness of the proposed method, which significantly outperforms its counterpart TS-CAM on both CUB-200 and ImageNet-1K benchmarks. The code is available at https://github.com/164140757/SCM.
<div id='section'>Paperid: <span id='pid'>403, <a href='https://arxiv.org/pdf/2207.05205.pdf' target='_blank'>https://arxiv.org/pdf/2207.05205.pdf</a></span>   <span><a href='https://github.com/tmlabonte/weakly-supervised-DETR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tyler LaBonte, Yale Song, Xin Wang, Vibhav Vineet, Neel Joshi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.05205">Scaling Novel Object Detection with Weakly Supervised Detection Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A critical object detection task is finetuning an existing model to detect novel objects, but the standard workflow requires bounding box annotations which are time-consuming and expensive to collect. Weakly supervised object detection (WSOD) offers an appealing alternative, where object detectors can be trained using image-level labels. However, the practical application of current WSOD models is limited, as they only operate at small data scales and require multiple rounds of training and refinement. To address this, we propose the Weakly Supervised Detection Transformer, which enables efficient knowledge transfer from a large-scale pretraining dataset to WSOD finetuning on hundreds of novel objects. Additionally, we leverage pretrained knowledge to improve the multiple instance learning (MIL) framework often used in WSOD methods. Our experiments show that our approach outperforms previous state-of-the-art models on large-scale novel object detection datasets, and our scaling study reveals that class quantity is more important than image quantity for WSOD pretraining. The code is available at https://github.com/tmlabonte/weakly-supervised-DETR.
<div id='section'>Paperid: <span id='pid'>404, <a href='https://arxiv.org/pdf/2205.00400.pdf' target='_blank'>https://arxiv.org/pdf/2205.00400.pdf</a></span>   <span><a href='https://github.com/Qinying-Liu/C3BN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qinying Liu, Zilei Wang, Ruoxi Chen, Zhilin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2205.00400">Convex Combination Consistency between Neighbors for Weakly-supervised Action Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised temporal action localization (WTAL) intends to detect action instances with only weak supervision, e.g., video-level labels. The current~\textit{de facto} pipeline locates action instances by thresholding and grouping continuous high-score regions on temporal class activation sequences. In this route, the capacity of the model to recognize the relationships between adjacent snippets is of vital importance which determines the quality of the action boundaries. However, it is error-prone since the variations between adjacent snippets are typically subtle, and unfortunately this is overlooked in the literature. To tackle the issue, we propose a novel WTAL approach named Convex Combination Consistency between Neighbors (C$^3$BN). C$^3$BN consists of two key ingredients: a micro data augmentation strategy that increases the diversity in-between adjacent snippets by convex combination of adjacent snippets, and a macro-micro consistency regularization that enforces the model to be invariant to the transformations~\textit{w.r.t.} video semantics, snippet predictions, and snippet representations. Consequently, fine-grained patterns in-between adjacent snippets are enforced to be explored, thereby resulting in a more robust action boundary localization. Experimental results demonstrate the effectiveness of C$^3$BN on top of various baselines for WTAL with video-level and point-level supervisions. Code is at https://github.com/Qinying-Liu/C3BN.
<div id='section'>Paperid: <span id='pid'>405, <a href='https://arxiv.org/pdf/2204.08154.pdf' target='_blank'>https://arxiv.org/pdf/2204.08154.pdf</a></span>   <span><a href='https://github.com/zijinxuxu/SMHR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinwei Ren, Jianke Zhu, Jialiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2204.08154">End-to-end Weakly-supervised Single-stage Multiple 3D Hand Mesh Reconstruction from a Single RGB Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we consider the challenging task of simultaneously locating and recovering multiple hands from a single 2D image. Previous studies either focus on single hand reconstruction or solve this problem in a multi-stage way. Moreover, the conventional two-stage pipeline firstly detects hand areas, and then estimates 3D hand pose from each cropped patch. To reduce the computational redundancy in preprocessing and feature extraction, for the first time, we propose a concise but efficient single-stage pipeline for multi-hand reconstruction. Specifically, we design a multi-head auto-encoder structure, where each head network shares the same feature map and outputs the hand center, pose and texture, respectively. Besides, we adopt a weakly-supervised scheme to alleviate the burden of expensive 3D real-world data annotations. To this end, we propose a series of losses optimized by a stage-wise training scheme, where a multi-hand dataset with 2D annotations is generated based on the publicly available single hand datasets. In order to further improve the accuracy of the weakly supervised model, we adopt several feature consistency constraints in both single and multiple hand settings. Specifically, the keypoints of each hand estimated from local features should be consistent with the re-projected points predicted from global features. Extensive experiments on public benchmarks including FreiHAND, HO3D, InterHand2.6M and RHD demonstrate that our method outperforms the state-of-the-art model-based methods in both weakly-supervised and fully-supervised manners. The code and models are available at {https://github.com/zijinxuxu/SMHR}.
<div id='section'>Paperid: <span id='pid'>406, <a href='https://arxiv.org/pdf/2204.06754.pdf' target='_blank'>https://arxiv.org/pdf/2204.06754.pdf</a></span>   <span><a href='https://github.com/shjo-april/RecurSeed_and_EdgePredictMix' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sanghyun Jo, In-Jae Yu, Kyungsu Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2204.06754">RecurSeed and EdgePredictMix: Pseudo-Label Refinement Learning for Weakly Supervised Semantic Segmentation across Single- and Multi-Stage Frameworks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although weakly supervised semantic segmentation using only image-level labels (WSSS-IL) is potentially useful, its low performance and implementation complexity still limit its application. The main causes are (a) non-detection and (b) false-detection phenomena: (a) The class activation maps refined from existing WSSS-IL methods still only represent partial regions for large-scale objects, and (b) for small-scale objects, over-activation causes them to deviate from the object edges. We propose RecurSeed, which alternately reduces non- and false detections through recursive iterations, thereby implicitly finding an optimal junction that minimizes both errors. We also propose a novel data augmentation (DA) approach called EdgePredictMix, which further expresses an object's edge by utilizing the probability difference information between adjacent pixels in combining the segmentation results, thereby compensating for the shortcomings when applying the existing DA methods to WSSS. We achieved new state-of-the-art performances on both the PASCAL VOC 2012 and MS COCO 2014 benchmarks (VOC val: 74.4%, COCO val: 46.4%). The code is available at https://github.com/shjo-april/RecurSeed_and_EdgePredictMix.
<div id='section'>Paperid: <span id='pid'>407, <a href='https://arxiv.org/pdf/2204.03845.pdf' target='_blank'>https://arxiv.org/pdf/2204.03845.pdf</a></span>   <span><a href='https://github.com/palm-ml/idgp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Congyu Qiao, Ning Xu, Xin Geng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2204.03845">Decompositional Generation Process for Instance-Dependent Partial Label Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partial label learning (PLL) is a typical weakly supervised learning problem, where each training example is associated with a set of candidate labels among which only one is true. Most existing PLL approaches assume that the incorrect labels in each training example are randomly picked as the candidate labels and model the generation process of the candidate labels in a simple way. However, these approaches usually do not perform as well as expected due to the fact that the generation process of the candidate labels is always instance-dependent. Therefore, it deserves to be modeled in a refined way. In this paper, we consider instance-dependent PLL and assume that the generation process of the candidate labels could decompose into two sequential parts, where the correct label emerges first in the mind of the annotator but then the incorrect labels related to the feature are also selected with the correct label as candidate labels due to uncertainty of labeling. Motivated by this consideration, we propose a novel PLL method that performs Maximum A Posterior (MAP) based on an explicitly modeled generation process of candidate labels via decomposed probability distribution models. Extensive experiments on manually corrupted benchmark datasets and real-world datasets validate the effectiveness of the proposed method. Source code is available at https://github.com/palm-ml/idgp.
<div id='section'>Paperid: <span id='pid'>408, <a href='https://arxiv.org/pdf/2204.03842.pdf' target='_blank'>https://arxiv.org/pdf/2204.03842.pdf</a></span>   <span><a href='https://github.com/weiguangzhao/DF_MVR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiguang Zhao, Chaolong Yang, Jianan Ye, Rui Zhang, Yuyao Yan, Xi Yang, Bin Dong, Amir Hussain, Kaizhu Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2204.03842">From 2D Images to 3D Model:Weakly Supervised Multi-View Face Reconstruction with Deep Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While weakly supervised multi-view face reconstruction (MVR) is garnering increased attention, one critical issue still remains open: how to effectively interact and fuse multiple image information to reconstruct high-precision 3D models. In this regard, we propose a novel pipeline called Deep Fusion MVR (DF-MVR) to explore the feature correspondences between multi-view images and reconstruct high-precision 3D faces. Specifically, we present a novel multi-view feature fusion backbone that utilizes face masks to align features from multiple encoders and integrates one multi-layer attention mechanism to enhance feature interaction and fusion, resulting in one unified facial representation. Additionally, we develop one concise face mask mechanism that facilitates multi-view feature fusion and facial reconstruction by identifying common areas and guiding the network's focus on critical facial features (e.g., eyes, brows, nose, and mouth). Experiments on Pixel-Face and Bosphorus datasets indicate the superiority of our model. Without 3D annotation, DF-MVR achieves 5.2% and 3.0% RMSE improvement over the existing weakly supervised MVRs respectively on Pixel-Face and Bosphorus dataset. Code will be available publicly at https://github.com/weiguangzhao/DF_MVR.
<div id='section'>Paperid: <span id='pid'>409, <a href='https://arxiv.org/pdf/2203.16782.pdf' target='_blank'>https://arxiv.org/pdf/2203.16782.pdf</a></span>   <span><a href='https://github.com/DearCaat/wsplin' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sheng Huang, Wenhao Tang, Guixin Huang, Luwen Huangfu, Dan Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.16782">Weakly Supervised Patch Label Inference Networks for Efficient Pavement Distress Detection and Recognition in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic image-based pavement distress detection and recognition are vital for pavement maintenance and management. However, existing deep learning-based methods largely omit the specific characteristics of pavement images, such as high image resolution and low distress area ratio, and are not end-to-end trainable. In this paper, we present a series of simple yet effective end-to-end deep learning approaches named Weakly Supervised Patch Label Inference Networks (WSPLIN) for efficiently addressing these tasks under various application settings. WSPLIN transforms the fully supervised pavement image classification problem into a weakly supervised pavement patch classification problem for solutions. Specifically, WSPLIN first divides the pavement image under different scales into patches with different collection strategies and then employs a Patch Label Inference Network (PLIN) to infer the labels of these patches to fully exploit the resolution and scale information. Notably, we design a patch label sparsity constraint based on the prior knowledge of distress distribution and leverage the Comprehensive Decision Network (CDN) to guide the training of PLIN in a weakly supervised way. Therefore, the patch labels produced by PLIN provide interpretable intermediate information, such as the rough location and the type of distress. We evaluate our method on a large-scale bituminous pavement distress dataset named CQU-BPDD and the augmented Crack500 (Crack500-PDD) dataset, which is a newly constructed pavement distress detection dataset augmented from the Crack500. Extensive results demonstrate the superiority of our method over baselines in both performance and efficiency. The source codes of WSPLIN are released on https://github.com/DearCaat/wsplin.
<div id='section'>Paperid: <span id='pid'>410, <a href='https://arxiv.org/pdf/2203.04279.pdf' target='_blank'>https://arxiv.org/pdf/2203.04279.pdf</a></span>   <span><a href='https://github.com/PruneTruong/DenseMatching' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Prune Truong, Martin Danelljan, Fisher Yu, Luc Van Gool
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.04279">Probabilistic Warp Consistency for Weakly-Supervised Semantic Correspondences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose Probabilistic Warp Consistency, a weakly-supervised learning objective for semantic matching. Our approach directly supervises the dense matching scores predicted by the network, encoded as a conditional probability distribution. We first construct an image triplet by applying a known warp to one of the images in a pair depicting different instances of the same object class. Our probabilistic learning objectives are then derived using the constraints arising from the resulting image triplet. We further account for occlusion and background clutter present in real image pairs by extending our probabilistic output space with a learnable unmatched state. To supervise it, we design an objective between image pairs depicting different object classes. We validate our method by applying it to four recent semantic matching architectures. Our weakly-supervised approach sets a new state-of-the-art on four challenging semantic matching benchmarks. Lastly, we demonstrate that our objective also brings substantial improvements in the strongly-supervised regime, when combined with keypoint annotations.
<div id='section'>Paperid: <span id='pid'>411, <a href='https://arxiv.org/pdf/2203.02909.pdf' target='_blank'>https://arxiv.org/pdf/2203.02909.pdf</a></span>   <span><a href='https://github.com/chenqi1126/SIPE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Chen, Lingxiao Yang, Jianhuang Lai, Xiaohua Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.02909">Self-supervised Image-specific Prototype Exploration for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Semantic Segmentation (WSSS) based on image-level labels has attracted much attention due to low annotation costs. Existing methods often rely on Class Activation Mapping (CAM) that measures the correlation between image pixels and classifier weight. However, the classifier focuses only on the discriminative regions while ignoring other useful information in each image, resulting in incomplete localization maps. To address this issue, we propose a Self-supervised Image-specific Prototype Exploration (SIPE) that consists of an Image-specific Prototype Exploration (IPE) and a General-Specific Consistency (GSC) loss. Specifically, IPE tailors prototypes for every image to capture complete regions, formed our Image-Specific CAM (IS-CAM), which is realized by two sequential steps. In addition, GSC is proposed to construct the consistency of general CAM and our specific IS-CAM, which further optimizes the feature representation and empowers a self-correction ability of prototype exploration. Extensive experiments are conducted on PASCAL VOC 2012 and MS COCO 2014 segmentation benchmark and results show our SIPE achieves new state-of-the-art performance using only image-level labels. The code is available at https://github.com/chenqi1126/SIPE.
<div id='section'>Paperid: <span id='pid'>412, <a href='https://arxiv.org/pdf/2202.08195.pdf' target='_blank'>https://arxiv.org/pdf/2202.08195.pdf</a></span>   <span><a href='https://github.com/hust-linyi/SC-Net' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Lin, Zhiyong Qu, Hao Chen, Zhongke Gao, Yuexiang Li, Lili Xia, Kai Ma, Yefeng Zheng, Kwang-Ting Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.08195">Nuclei Segmentation with Point Annotations from Pathology Images via Self-Supervised Learning and Co-Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nuclei segmentation is a crucial task for whole slide image analysis in digital pathology. Generally, the segmentation performance of fully-supervised learning heavily depends on the amount and quality of the annotated data. However, it is time-consuming and expensive for professional pathologists to provide accurate pixel-level ground truth, while it is much easier to get coarse labels such as point annotations. In this paper, we propose a weakly-supervised learning method for nuclei segmentation that only requires point annotations for training. First, coarse pixel-level labels are derived from the point annotations based on the Voronoi diagram and the k-means clustering method to avoid overfitting. Second, a co-training strategy with an exponential moving average method is designed to refine the incomplete supervision of the coarse labels. Third, a self-supervised visual representation learning method is tailored for nuclei segmentation of pathology images that transforms the hematoxylin component images into the H&E stained images to gain better understanding of the relationship between the nuclei and cytoplasm. We comprehensively evaluate the proposed method using two public datasets. Both visual and quantitative results demonstrate the superiority of our method to the state-of-the-art methods, and its competitive performance compared to the fully-supervised methods. Code: https://github.com/hust-linyi/SC-Net
<div id='section'>Paperid: <span id='pid'>413, <a href='https://arxiv.org/pdf/2202.06997.pdf' target='_blank'>https://arxiv.org/pdf/2202.06997.pdf</a></span>   <span><a href='https://github.com/M-3LAB/awesome-multimodal-brain-image-systhesis' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guoyang Xie, Yawen Huang, Jinbao Wang, Jiayi Lyu, Feng Zheng, Yefeng Zheng, Yaochu Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.06997">Cross-Modality Neuroimage Synthesis: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-modality imaging improves disease diagnosis and reveals distinct deviations in tissues with anatomical properties. The existence of completely aligned and paired multi-modality neuroimaging data has proved its effectiveness in brain research. However, collecting fully aligned and paired data is expensive or even impractical, since it faces many difficulties, including high cost, long acquisition time, image corruption, and privacy issues. An alternative solution is to explore unsupervised or weakly supervised learning methods to synthesize the absent neuroimaging data. In this paper, we provide a comprehensive review of cross-modality synthesis for neuroimages, from the perspectives of weakly supervised and unsupervised settings, loss functions, evaluation metrics, imaging modalities, datasets, and downstream applications based on synthesis. We begin by highlighting several opening challenges for cross-modality neuroimage synthesis. Then, we discuss representative architectures of cross-modality synthesis methods under different supervisions. This is followed by a stepwise in-depth analysis to evaluate how cross-modality neuroimage synthesis improves the performance of its downstream tasks. Finally, we summarize the existing research findings and point out future research directions. All resources are available at https://github.com/M-3LAB/awesome-multimodal-brain-image-systhesis
<div id='section'>Paperid: <span id='pid'>414, <a href='https://arxiv.org/pdf/2110.15720.pdf' target='_blank'>https://arxiv.org/pdf/2110.15720.pdf</a></span>   <span><a href='https://github.com/lujiaying/GT-doc2graph' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaying Lu, Xiangjue Dong, Carl Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2110.15720">Weakly Supervised Concept Map Generation through Task-Guided Graph Translation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent years have witnessed the rapid development of concept map generation techniques due to their advantages in providing well-structured summarization of knowledge from free texts. Traditional unsupervised methods do not generate task-oriented concept maps, whereas deep generative models require large amounts of training data. In this work, we present GT-D2G (Graph Translation-based Document To Graph), an automatic concept map generation framework that leverages generalized NLP pipelines to derive semantic-rich initial graphs, and translates them into more concise structures under the weak supervision of downstream task labels. The concept maps generated by GT-D2G can provide interpretable summarization of structured knowledge for the input texts, which are demonstrated through human evaluation and case studies on three real-world corpora. Further experiments on the downstream task of document classification show that GT-D2G beats other concept map generation methods. Moreover, we specifically validate the labeling efficiency of GT-D2G in the label-efficient learning setting and the flexibility of generated graph sizes in controlled hyper-parameter studies.
<div id='section'>Paperid: <span id='pid'>415, <a href='https://arxiv.org/pdf/2108.11926.pdf' target='_blank'>https://arxiv.org/pdf/2108.11926.pdf</a></span>   <span><a href='https://vios-s.github.io/adversarial-test-time-training' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gabriele Valvano, Andrea Leo, Sotirios A. Tsaftaris
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2108.11926">Re-using Adversarial Mask Discriminators for Test-time Training under Distribution Shifts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Thanks to their ability to learn flexible data-driven losses, Generative Adversarial Networks (GANs) are an integral part of many semi- and weakly-supervised methods for medical image segmentation. GANs jointly optimise a generator and an adversarial discriminator on a set of training data. After training is complete, the discriminator is usually discarded, and only the generator is used for inference. But should we discard discriminators? In this work, we argue that training stable discriminators produces expressive loss functions that we can re-use at inference to detect and \textit{correct} segmentation mistakes. First, we identify key challenges and suggest possible solutions to make discriminators re-usable at inference. Then, we show that we can combine discriminators with image reconstruction costs (via decoders) to endow a causal perspective to test-time training and further improve the model. Our method is simple and improves the test-time performance of pre-trained GANs. Moreover, we show that it is compatible with standard post-processing techniques and it has the potential to be used for Online Continual Learning. With our work, we open new research avenues for re-using adversarial discriminators at inference. Our code is available at https://vios-s.github.io/adversarial-test-time-training.
<div id='section'>Paperid: <span id='pid'>416, <a href='https://arxiv.org/pdf/2107.14449.pdf' target='_blank'>https://arxiv.org/pdf/2107.14449.pdf</a></span>   <span><a href='https://github.com/acasamitjana/SynthByReg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>AdriÃ  Casamitjana, Matteo Mancini, Juan Eugenio Iglesias
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2107.14449">Synth-by-Reg (SbR): Contrastive learning for synthesis-based registration of paired images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nonlinear inter-modality registration is often challenging due to the lack of objective functions that are good proxies for alignment. Here we propose a synthesis-by-registration method to convert this problem into an easier intra-modality task. We introduce a registration loss for weakly supervised image translation between domains that does not require perfectly aligned training data. This loss capitalises on a registration U-Net with frozen weights, to drive a synthesis CNN towards the desired translation. We complement this loss with a structure preserving constraint based on contrastive learning, which prevents blurring and content shifts due to overfitting. We apply this method to the registration of histological sections to MRI slices, a key step in 3D histology reconstruction. Results on two different public datasets show improvements over registration based on mutual information (13% reduction in landmark error) and synthesis-based algorithms such as CycleGAN (11% reduction), and are comparable to a registration CNN with label supervision. Code and data are publicly available at \url{https://github.com/acasamitjana/SynthByReg}
<div id='section'>Paperid: <span id='pid'>417, <a href='https://arxiv.org/pdf/2104.04891.pdf' target='_blank'>https://arxiv.org/pdf/2104.04891.pdf</a></span>   <span><a href='https://github.com/QingyongHu/SQN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingyong Hu, Bo Yang, Guangchi Fang, Yulan Guo, Ales Leonardis, Niki Trigoni, Andrew Markham
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2104.04891">SQN: Weakly-Supervised Semantic Segmentation of Large-Scale 3D Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Labelling point clouds fully is highly time-consuming and costly. As larger point cloud datasets with billions of points become more common, we ask whether the full annotation is even necessary, demonstrating that existing baselines designed under a fully annotated assumption only degrade slightly even when faced with 1% random point annotations. However, beyond this point, e.g., at 0.1% annotations, segmentation accuracy is unacceptably low. We observe that, as point clouds are samples of the 3D world, the distribution of points in a local neighborhood is relatively homogeneous, exhibiting strong semantic similarity. Motivated by this, we propose a new weak supervision method to implicitly augment highly sparse supervision signals. Extensive experiments demonstrate the proposed Semantic Query Network (SQN) achieves promising performance on seven large-scale open datasets under weak supervision schemes, while requiring only 0.1% randomly annotated points for training, greatly reducing annotation cost and effort. The code is available at https://github.com/QingyongHu/SQN.
<div id='section'>Paperid: <span id='pid'>418, <a href='https://arxiv.org/pdf/2103.01403.pdf' target='_blank'>https://arxiv.org/pdf/2103.01403.pdf</a></span>   <span><a href='https://liqing-ustc.github.io/HINT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qing Li, Siyuan Huang, Yining Hong, Yixin Zhu, Ying Nian Wu, Song-Chun Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2103.01403">A Minimalist Dataset for Systematic Generalization of Perception, Syntax, and Semantics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inspired by humans' exceptional ability to master arithmetic and generalize to new problems, we present a new dataset, Handwritten arithmetic with INTegers (HINT), to examine machines' capability of learning generalizable concepts at three levels: perception, syntax, and semantics. In HINT, machines are tasked with learning how concepts are perceived from raw signals such as images (i.e., perception), how multiple concepts are structurally combined to form a valid expression (i.e., syntax), and how concepts are realized to afford various reasoning tasks (i.e., semantics), all in a weakly supervised manner. Focusing on systematic generalization, we carefully design a five-fold test set to evaluate both the interpolation and the extrapolation of learned concepts w.r.t. the three levels. Further, we design a few-shot learning split to determine whether or not models can rapidly learn new concepts and generalize them to more complex scenarios. To comprehend existing models' limitations, we undertake extensive experiments with various sequence-to-sequence models, including RNNs, Transformers, and GPT-3 (with the chain of thought prompting). The results indicate that current models struggle to extrapolate to long-range syntactic dependency and semantics. Models exhibit a considerable gap toward human-level generalization when evaluated with new concepts in a few-shot setting. Moreover, we discover that it is infeasible to solve HINT by merely scaling up the dataset and the model size; this strategy contributes little to the extrapolation of syntax and semantics. Finally, in zero-shot GPT-3 experiments, the chain of thought prompting exhibits impressive results and significantly boosts the test accuracy. We believe the HINT dataset and the experimental findings are of great interest to the learning community on systematic generalization.
<div id='section'>Paperid: <span id='pid'>419, <a href='https://arxiv.org/pdf/2101.09858.pdf' target='_blank'>https://arxiv.org/pdf/2101.09858.pdf</a></span>   <span><a href='https://github.com/praveena2j/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>R. Gnana Praveen, Patrick Cardinal, Eric Granger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2101.09858">Weakly Supervised Learning for Facial Behavior Analysis : A Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the recent years, there has been a shift in facial behavior analysis from the laboratory-controlled conditions to the challenging in-the-wild conditions due to the superior performance of deep learning based approaches for many real world applications.However, the performance of deep learning approaches relies on the amount of training data. One of the major problems with data acquisition is the requirement of annotations for large amount of training data. Labeling process of huge training data demands lot of human support with strong domain expertise for facial expressions or action units, which is difficult to obtain in real-time environments.Moreover, labeling process is highly vulnerable to ambiguity of expressions or action units, especially for intensities due to the bias induced by the domain experts. Therefore, there is an imperative need to address the problem of facial behavior analysis with weak annotations. In this paper, we provide a comprehensive review of weakly supervised learning (WSL) approaches for facial behavior analysis with both categorical as well as dimensional labels along with the challenges and potential research directions associated with it. First, we introduce various types of weak annotations in the context of facial behavior analysis and the corresponding challenges associated with it. We then systematically review the existing state-of-the-art approaches and provide a taxonomy of these approaches along with their insights and limitations. In addition, widely used data-sets in the reviewed literature and the performance of these approaches along with evaluation principles are summarized. Finally, we discuss the remaining challenges and opportunities along with the potential research directions in order to apply facial behavior analysis with weak labels in real life situations.
<div id='section'>Paperid: <span id='pid'>420, <a href='https://arxiv.org/pdf/2004.13324.pdf' target='_blank'>https://arxiv.org/pdf/2004.13324.pdf</a></span>   <span><a href='https://qianqianwang68.github.io/CAPS/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qianqian Wang, Xiaowei Zhou, Bharath Hariharan, Noah Snavely
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2004.13324">Learning Feature Descriptors using Camera Pose Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent research on learned visual descriptors has shown promising improvements in correspondence estimation, a key component of many 3D vision tasks. However, existing descriptor learning frameworks typically require ground-truth correspondences between feature points for training, which are challenging to acquire at scale. In this paper we propose a novel weakly-supervised framework that can learn feature descriptors solely from relative camera poses between images. To do so, we devise both a new loss function that exploits the epipolar constraint given by camera poses, and a new model architecture that makes the whole pipeline differentiable and efficient. Because we no longer need pixel-level ground-truth correspondences, our framework opens up the possibility of training on much larger and more diverse datasets for better and unbiased descriptors. We call the resulting descriptors CAmera Pose Supervised, or CAPS, descriptors. Though trained with weak supervision, CAPS descriptors outperform even prior fully-supervised descriptors and achieve state-of-the-art performance on a variety of geometric tasks. Project Page: https://qianqianwang68.github.io/CAPS/
<div id='section'>Paperid: <span id='pid'>421, <a href='https://arxiv.org/pdf/1910.13601.pdf' target='_blank'>https://arxiv.org/pdf/1910.13601.pdf</a></span>   <span><a href='https://github.com/mala-lab/PReNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guansong Pang, Chunhua Shen, Huidong Jin, Anton van den Hengel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1910.13601">Deep Weakly-supervised Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent semi-supervised anomaly detection methods that are trained using small labeled anomaly examples and large unlabeled data (mostly normal data) have shown largely improved performance over unsupervised methods. However, these methods often focus on fitting abnormalities illustrated by the given anomaly examples only (i.e.,, seen anomalies), and consequently they fail to generalize to those that are not, i.e., new types/classes of anomaly unseen during training. To detect both seen and unseen anomalies, we introduce a novel deep weakly-supervised approach, namely Pairwise Relation prediction Network (PReNet), that learns pairwise relation features and anomaly scores by predicting the relation of any two randomly sampled training instances, in which the pairwise relation can be anomaly-anomaly, anomaly-unlabeled, or unlabeled-unlabeled. Since unlabeled instances are mostly normal, the relation prediction enforces a joint learning of anomaly-anomaly, anomaly-normal, and normal-normal pairwise discriminative patterns, respectively. PReNet can then detect any seen/unseen abnormalities that fit the learned pairwise abnormal patterns, or deviate from the normal patterns. Further, this pairwise approach also seamlessly and significantly augments the training anomaly data. Empirical results on 12 real-world datasets show that PReNet significantly outperforms nine competing methods in detecting seen and unseen anomalies. We also theoretically and empirically justify the robustness of our model w.r.t. anomaly contamination in the unlabeled data. The code is available at https://github.com/mala-lab/PReNet.
<div id='section'>Paperid: <span id='pid'>422, <a href='https://arxiv.org/pdf/2502.12917.pdf' target='_blank'>https://arxiv.org/pdf/2502.12917.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haicheng Wang, Chen Ju, Weixiong Lin, Chaofan Ma, Shuai Xiao, Ya Zhang, Yanfeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.12917">Contrast-Unity for Partially-Supervised Temporal Sentence Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Temporal sentence grounding aims to detect event timestamps described by the natural language query from given untrimmed videos. The existing fully-supervised setting achieves great results but requires expensive annotation costs; while the weakly-supervised setting adopts cheap labels but performs poorly. To pursue high performance with less annotation costs, this paper introduces an intermediate partially-supervised setting, i.e., only short-clip is available during training. To make full use of partial labels, we specially design one contrast-unity framework, with the two-stage goal of implicit-explicit progressive grounding. In the implicit stage, we align event-query representations at fine granularity using comprehensive quadruple contrastive learning: event-query gather, event-background separation, intra-cluster compactness and inter-cluster separability. Then, high-quality representations bring acceptable grounding pseudo-labels. In the explicit stage, to explicitly optimize grounding objectives, we train one fully-supervised model using obtained pseudo-labels for grounding refinement and denoising. Extensive experiments and thoroughly ablations on Charades-STA and ActivityNet Captions demonstrate the significance of partial supervision, as well as our superior performance.
<div id='section'>Paperid: <span id='pid'>423, <a href='https://arxiv.org/pdf/2505.16294.pdf' target='_blank'>https://arxiv.org/pdf/2505.16294.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufei Yin, Lechao Cheng, Wengang Zhou, Jiajun Deng, Zhou Yu, Houqiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16294">Self-Classification Enhancement and Correction for Weakly Supervised Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, weakly supervised object detection (WSOD) has attracted much attention due to its low labeling cost. The success of recent WSOD models is often ascribed to the two-stage multi-class classification (MCC) task, i.e., multiple instance learning and online classification refinement. Despite achieving non-trivial progresses, these methods overlook potential classification ambiguities between these two MCC tasks and fail to leverage their unique strengths. In this work, we introduce a novel WSOD framework to ameliorate these two issues. For one thing, we propose a self-classification enhancement module that integrates intra-class binary classification (ICBC) to bridge the gap between the two distinct MCC tasks. The ICBC task enhances the network's discrimination between positive and mis-located samples in a class-wise manner and forges a mutually reinforcing relationship with the MCC task. For another, we propose a self-classification correction algorithm during inference, which combines the results of both MCC tasks to effectively reduce the mis-classified predictions. Extensive experiments on the prevalent VOC 2007 & 2012 datasets demonstrate the superior performance of our framework.
<div id='section'>Paperid: <span id='pid'>424, <a href='https://arxiv.org/pdf/2305.17384.pdf' target='_blank'>https://arxiv.org/pdf/2305.17384.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuo Li, Huangzhao Zhang, Zhi Jin, Ge Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.17384">WELL: Applying Bug Detectors to Bug Localization via Weakly Supervised Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Bug localization, which is used to help programmers identify the location of bugs in source code, is an essential task in software development. Researchers have already made efforts to harness the powerful deep learning (DL) techniques to automate it. However, training bug localization model is usually challenging because it requires a large quantity of data labeled with the bug's exact location, which is difficult and time-consuming to collect. By contrast, obtaining bug detection data with binary labels of whether there is a bug in the source code is much simpler. This paper proposes a WEakly supervised bug LocaLization (WELL) method, which only uses the bug detection data with binary labels to train a bug localization model. With CodeBERT finetuned on the buggy-or-not binary labeled data, WELL can address bug localization in a weakly supervised manner. The evaluations on three method-level synthetic datasets and one file-level real-world dataset show that WELL is significantly better than the existing SOTA model in typical bug localization tasks such as variable misuse and other programming bugs.
<div id='section'>Paperid: <span id='pid'>425, <a href='https://arxiv.org/pdf/2306.03492.pdf' target='_blank'>https://arxiv.org/pdf/2306.03492.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanxi Li, Jingqi Wu, Deyin Liu, Lin Wu, Hao Chen, Mingwen Wang, Chunhua Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.03492">Industrial Anomaly Detection and Localization Using Weakly-Supervised Residual Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in industrial anomaly detection (AD) have demonstrated that incorporating a small number of anomalous samples during training can significantly enhance accuracy. However, this improvement often comes at the cost of extensive annotation efforts, which are impractical for many real-world applications. In this paper, we introduce a novel framework, Weak}ly-supervised RESidual Transformer (WeakREST), designed to achieve high anomaly detection accuracy while minimizing the reliance on manual annotations. First, we reformulate the pixel-wise anomaly localization task into a block-wise classification problem. Second, we introduce a residual-based feature representation called Positional Fast Anomaly Residuals (PosFAR) which captures anomalous patterns more effectively. To leverage this feature, we adapt the Swin Transformer for enhanced anomaly detection and localization. Additionally, we propose a weak annotation approach, utilizing bounding boxes and image tags to define anomalous regions. This approach establishes a semi-supervised learning context that reduces the dependency on precise pixel-level labels. To further improve the learning process, we develop a novel ResMixMatch algorithm, capable of handling the interplay between weak labels and residual-based representations.
  On the benchmark dataset MVTec-AD, our method achieves an Average Precision (AP) of $83.0\%$, surpassing the previous best result of $82.7\%$ in the unsupervised setting. In the supervised AD setting, WeakREST attains an AP of $87.6\%$, outperforming the previous best of $86.0\%$. Notably, even when using weaker annotations such as bounding boxes, WeakREST exceeds the performance of leading methods relying on pixel-wise supervision, achieving an AP of $87.1\%$ compared to the prior best of $86.0\%$ on MVTec-AD.
<div id='section'>Paperid: <span id='pid'>426, <a href='https://arxiv.org/pdf/2305.15483.pdf' target='_blank'>https://arxiv.org/pdf/2305.15483.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chi Chen, Peng Li, Maosong Sun, Yang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.15483">Weakly Supervised Vision-and-Language Pre-training with Relative Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised vision-and-language pre-training (WVLP), which learns cross-modal representations with limited cross-modal supervision, has been shown to effectively reduce the data cost of pre-training while maintaining decent performance on downstream tasks. However, current WVLP methods use only local descriptions of images, i.e., object tags, as cross-modal anchors to construct weakly-aligned image-text pairs for pre-training. This affects the data quality and thus the effectiveness of pre-training. In this paper, we propose to directly take a small number of aligned image-text pairs as anchors, and represent each unaligned image and text by its similarities to these anchors, i.e., relative representations. We build a WVLP framework based on the relative representations, namely RELIT, which collects high-quality weakly-aligned image-text pairs from large-scale image-only and text-only data for pre-training through relative representation-based retrieval and generation. Experiments on four downstream tasks show that RELIT achieves new state-of-the-art results under the weakly supervised setting.
<div id='section'>Paperid: <span id='pid'>427, <a href='https://arxiv.org/pdf/2303.02449.pdf' target='_blank'>https://arxiv.org/pdf/2303.02449.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiren Mai, Fei Zhang, Junjie Ye, Marcus Kalander, Xian Zhang, WanKou Yang, Tongliang Liu, Bo Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.02449">Exploit CAM by itself: Complementary Learning System for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Semantic Segmentation (WSSS) with image-level labels has long been suffering from fragmentary object regions led by Class Activation Map (CAM), which is incapable of generating fine-grained masks for semantic segmentation. To guide CAM to find more non-discriminating object patterns, this paper turns to an interesting working mechanism in agent learning named Complementary Learning System (CLS). CLS holds that the neocortex builds a sensation of general knowledge, while the hippocampus specially learns specific details, completing the learned patterns. Motivated by this simple but effective learning pattern, we propose a General-Specific Learning Mechanism (GSLM) to explicitly drive a coarse-grained CAM to a fine-grained pseudo mask. Specifically, GSLM develops a General Learning Module (GLM) and a Specific Learning Module (SLM). The GLM is trained with image-level supervision to extract coarse and general localization representations from CAM. Based on the general knowledge in the GLM, the SLM progressively exploits the specific spatial knowledge from the localization representations, expanding the CAM in an explicit way. To this end, we propose the Seed Reactivation to help SLM reactivate non-discriminating regions by setting a boundary for activation values, which successively identifies more regions of CAM. Without extra refinement processes, our method is able to achieve breakthrough improvements for CAM of over 20.0% mIoU on PASCAL VOC 2012 and 10.0% mIoU on MS COCO 2014 datasets, representing a new state-of-the-art among existing WSSS methods.
<div id='section'>Paperid: <span id='pid'>428, <a href='https://arxiv.org/pdf/2505.04905.pdf' target='_blank'>https://arxiv.org/pdf/2505.04905.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xi Yang, Songsong Duan, Nannan Wang, Xinbo Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04905">Pro2SAM: Mask Prompt to SAM with Grid Points for Weakly Supervised Object Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Object Localization (WSOL), which aims to localize objects by only using image-level labels, has attracted much attention because of its low annotation cost in real applications. Current studies focus on the Class Activation Map (CAM) of CNN and the self-attention map of transformer to identify the region of objects. However, both CAM and self-attention maps can not learn pixel-level fine-grained information on the foreground objects, which hinders the further advance of WSOL. To address this problem, we initiatively leverage the capability of zero-shot generalization and fine-grained segmentation in Segment Anything Model (SAM) to boost the activation of integral object regions. Further, to alleviate the semantic ambiguity issue accrued in single point prompt-based SAM, we propose an innovative mask prompt to SAM (Pro2SAM) network with grid points for WSOL task. First, we devise a Global Token Transformer (GTFormer) to generate a coarse-grained foreground map as a flexible mask prompt, where the GTFormer jointly embeds patch tokens and novel global tokens to learn foreground semantics. Secondly, we deliver grid points as dense prompts into SAM to maximize the probability of foreground mask, which avoids the lack of objects caused by a single point/box prompt. Finally, we propose a pixel-level similarity metric to come true the mask matching from mask prompt to SAM, where the mask with the highest score is viewed as the final localization map. Experiments show that the proposed Pro2SAM achieves state-of-the-art performance on both CUB-200-2011 and ILSVRC, with 84.03\% and 66.85\% Top-1 Loc, respectively.
<div id='section'>Paperid: <span id='pid'>429, <a href='https://arxiv.org/pdf/2312.02483.pdf' target='_blank'>https://arxiv.org/pdf/2312.02483.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guozhang Li, Xinpeng Ding, De Cheng, Jie Li, Nannan Wang, Xinbo Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02483">EtC: Temporal Boundary Expand then Clarify for Weakly Supervised Video Grounding with Multimodal Large Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Early weakly supervised video grounding (WSVG) methods often struggle with incomplete boundary detection due to the absence of temporal boundary annotations. To bridge the gap between video-level and boundary-level annotation, explicit-supervision methods, i.e., generating pseudo-temporal boundaries for training, have achieved great success. However, data augmentations in these methods might disrupt critical temporal information, yielding poor pseudo boundaries. In this paper, we propose a new perspective that maintains the integrity of the original temporal content while introducing more valuable information for expanding the incomplete boundaries. To this end, we propose EtC (Expand then Clarify), first use the additional information to expand the initial incomplete pseudo boundaries, and subsequently refine these expanded ones to achieve precise boundaries. Motivated by video continuity, i.e., visual similarity across adjacent frames, we use powerful multimodal large language models (MLLMs) to annotate each frame within initial pseudo boundaries, yielding more comprehensive descriptions for expanded boundaries. To further clarify the noise of expanded boundaries, we combine mutual learning with a tailored proposal-level contrastive objective to use a learnable approach to harmonize a balance between incomplete yet clean (initial) and comprehensive yet noisy (expanded) boundaries for more precise ones. Experiments demonstrate the superiority of our method on two challenging WSVG datasets.
<div id='section'>Paperid: <span id='pid'>430, <a href='https://arxiv.org/pdf/2304.12616.pdf' target='_blank'>https://arxiv.org/pdf/2304.12616.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guozhang Li, De Cheng, Xinpeng Ding, Nannan Wang, Jie Li, Xinbo Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.12616">Weakly-Supervised Temporal Action Localization with Bidirectional Semantic Consistency Constraint</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Temporal Action Localization (WTAL) aims to classify and localize temporal boundaries of actions for the video, given only video-level category labels in the training datasets. Due to the lack of boundary information during training, existing approaches formulate WTAL as a classificationproblem, i.e., generating the temporal class activation map (T-CAM) for localization. However, with only classification loss, the model would be sub-optimized, i.e., the action-related scenes are enough to distinguish different class labels. Regarding other actions in the action-related scene ( i.e., the scene same as positive actions) as co-scene actions, this sub-optimized model would misclassify the co-scene actions as positive actions. To address this misclassification, we propose a simple yet efficient method, named bidirectional semantic consistency constraint (Bi-SCC), to discriminate the positive actions from co-scene actions. The proposed Bi-SCC firstly adopts a temporal context augmentation to generate an augmented video that breaks the correlation between positive actions and their co-scene actions in the inter-video; Then, a semantic consistency constraint (SCC) is used to enforce the predictions of the original video and augmented video to be consistent, hence suppressing the co-scene actions. However, we find that this augmented video would destroy the original temporal context. Simply applying the consistency constraint would affect the completeness of localized positive actions. Hence, we boost the SCC in a bidirectional way to suppress co-scene actions while ensuring the integrity of positive actions, by cross-supervising the original and augmented videos. Finally, our proposed Bi-SCC can be applied to current WTAL approaches, and improve their performance. Experimental results show that our approach outperforms the state-of-the-art methods on THUMOS14 and ActivityNet.
<div id='section'>Paperid: <span id='pid'>431, <a href='https://arxiv.org/pdf/2406.17988.pdf' target='_blank'>https://arxiv.org/pdf/2406.17988.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingxuan Wu, Zhiyang Dou, Sirui Xu, Soshi Shimada, Chen Wang, Zhengming Yu, Yuan Liu, Cheng Lin, Zeyu Cao, Taku Komura, Vladislav Golyanik, Christian Theobalt, Wenping Wang, Lingjie Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.17988">DICE: End-to-end Deformation Capture of Hand-Face Interactions from a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing 3D hand-face interactions with deformations from a single image is a challenging yet crucial task with broad applications in AR, VR, and gaming. The challenges stem from self-occlusions during single-view hand-face interactions, diverse spatial relationships between hands and face, complex deformations, and the ambiguity of the single-view setting. The first and only method for hand-face interaction recovery, Decaf, introduces a global fitting optimization guided by contact and deformation estimation networks trained on studio-collected data with 3D annotations. However, Decaf suffers from a time-consuming optimization process and limited generalization capability due to its reliance on 3D annotations of hand-face interaction data. To address these issues, we present DICE, the first end-to-end method for Deformation-aware hand-face Interaction reCovEry from a single image. DICE estimates the poses of hands and faces, contacts, and deformations simultaneously using a Transformer-based architecture. It features disentangling the regression of local deformation fields and global mesh vertex locations into two network branches, enhancing deformation and contact estimation for precise and robust hand-face mesh recovery. To improve generalizability, we propose a weakly-supervised training approach that augments the training set using in-the-wild images without 3D ground-truth annotations, employing the depths of 2D keypoints estimated by off-the-shelf models and adversarial priors of poses for supervision. Our experiments demonstrate that DICE achieves state-of-the-art performance on a standard benchmark and in-the-wild data in terms of accuracy and physical plausibility. Additionally, our method operates at an interactive rate (20 fps) on an Nvidia 4090 GPU, whereas Decaf requires more than 15 seconds for a single image. Our code will be publicly available upon publication.
<div id='section'>Paperid: <span id='pid'>432, <a href='https://arxiv.org/pdf/2511.12229.pdf' target='_blank'>https://arxiv.org/pdf/2511.12229.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhipeng Xue, Zhipeng Gao, Tongtong Xu, Xing Hu, Xin Xia, Shanping Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.12229">Actionable Warning Is Not Enough: Recommending Valid Actionable Warnings with Weak Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The use of static analysis tools has gained increasing popularity among developers in the last few years. However, the widespread adoption of static analysis tools is hindered by their high false alarm rates. Previous studies have introduced the concept of actionable warnings and built a machine-learning method to distinguish actionable warnings from false alarms. However, according to our empirical observation, the current assumption used for actionable warning(s) collection is rather shaky and inaccurate, leading to a large number of invalid actionable warnings. To address this problem, in this study, we build the first large actionable warning dataset by mining 68,274 reversions from Top-500 GitHub C repositories, we then take one step further by assigning each actionable warning a weak label regarding its likelihood of being a real bug. Following that, we propose a two-stage framework called ACWRecommender to automatically recommend the actionable warnings with high probability to be real bugs (AWHB). Our approach warms up the pre-trained model UniXcoder by identifying actionable warnings task (coarse-grained detection stage) and rerank AWHB to the top by weakly supervised learning (fine-grained reranking stage). Experimental results show that our proposed model outperforms several baselines by a large margin in terms of nDCG and MRR for AWHB recommendation. Moreover, we ran our tool on 6 randomly selected projects and manually checked the top-ranked warnings from 2,197 reported warnings, we reported top-10 recommended warnings to developers, 27 of them were already confirmed by developers as real bugs. Developers can quickly find real bugs among the massive amount of reported warnings, which verifies the practical usage of our tool.
<div id='section'>Paperid: <span id='pid'>433, <a href='https://arxiv.org/pdf/2306.17103.pdf' target='_blank'>https://arxiv.org/pdf/2306.17103.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Le Zhuo, Ruibin Yuan, Jiahao Pan, Yinghao Ma, Yizhi LI, Ge Zhang, Si Liu, Roger Dannenberg, Jie Fu, Chenghua Lin, Emmanouil Benetos, Wei Xue, Yike Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.17103">LyricWhiz: Robust Multilingual Zero-shot Lyrics Transcription by Whispering to ChatGPT</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce LyricWhiz, a robust, multilingual, and zero-shot automatic lyrics transcription method achieving state-of-the-art performance on various lyrics transcription datasets, even in challenging genres such as rock and metal. Our novel, training-free approach utilizes Whisper, a weakly supervised robust speech recognition model, and GPT-4, today's most performant chat-based large language model. In the proposed method, Whisper functions as the "ear" by transcribing the audio, while GPT-4 serves as the "brain," acting as an annotator with a strong performance for contextualized output selection and correction. Our experiments show that LyricWhiz significantly reduces Word Error Rate compared to existing methods in English and can effectively transcribe lyrics across multiple languages. Furthermore, we use LyricWhiz to create the first publicly available, large-scale, multilingual lyrics transcription dataset with a CC-BY-NC-SA copyright license, based on MTG-Jamendo, and offer a human-annotated subset for noise level estimation and evaluation. We anticipate that our proposed method and dataset will advance the development of multilingual lyrics transcription, a challenging and emerging task.
<div id='section'>Paperid: <span id='pid'>434, <a href='https://arxiv.org/pdf/2208.10833.pdf' target='_blank'>https://arxiv.org/pdf/2208.10833.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongcheng Guo, Yuhui Guo, Renjie Chen, Jian Yang, Jiaheng Liu, Zhoujun Li, Tieqiao Zheng, Weichao Hou, Liangfan Zheng, Bo Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.10833">LogLG: Weakly Supervised Log Anomaly Detection via Log-Event Graph Construction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fully supervised log anomaly detection methods suffer the heavy burden of annotating massive unlabeled log data. Recently, many semi-supervised methods have been proposed to reduce annotation costs with the help of parsed templates. However, these methods consider each keyword independently, which disregards the correlation between keywords and the contextual relationships among log sequences. In this paper, we propose a novel weakly supervised log anomaly detection framework, named LogLG, to explore the semantic connections among keywords from sequences. Specifically, we design an end-to-end iterative process, where the keywords of unlabeled logs are first extracted to construct a log-event graph. Then, we build a subgraph annotator to generate pseudo labels for unlabeled log sequences. To ameliorate the annotation quality, we adopt a self-supervised task to pre-train a subgraph annotator. After that, a detection model is trained with the generated pseudo labels. Conditioned on the classification results, we re-extract the keywords from the log sequences and update the log-event graph for the next iteration. Experiments on five benchmarks validate the effectiveness of LogLG for detecting anomalies on unlabeled log data and demonstrate that LogLG, as the state-of-the-art weakly supervised method, achieves significant performance improvements compared to existing methods.
<div id='section'>Paperid: <span id='pid'>435, <a href='https://arxiv.org/pdf/2506.20923.pdf' target='_blank'>https://arxiv.org/pdf/2506.20923.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinping Zhao, Xinshuo Hu, Zifei Shan, Shouzheng Huang, Yao Zhou, Zetian Sun, Zhenyu Liu, Dongfang Li, Xinyuan Wei, Qian Chen, Youcheng Pan, Yang Xiang, Meishan Zhang, Haofen Wang, Jun Yu, Baotian Hu, Min Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20923">KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose KaLM-Embedding-V2, a versatile and compact embedding model, which achieves impressive performance in general-purpose text embedding tasks by leveraging superior training techniques and data. Our key innovations include: (1) To better align the architecture with representation learning, we remove the causal attention mask and adopt a fully bidirectional transformer with simple yet effective mean-pooling to produce fixed-length embeddings; (2) We employ a multi-stage training pipeline: (i) pre-training on large-scale weakly supervised open-source corpora; (ii) fine-tuning on high-quality retrieval and non-retrieval datasets; and (iii) model-soup parameter averaging for robust generalization. Besides, we introduce a focal-style reweighting mechanism that concentrates learning on difficult samples and an online hard-negative mixing strategy to continuously enrich hard negatives without expensive offline mining; (3) We collect over 20 categories of data for pre-training and 100 categories of data for fine-tuning, to boost both the performance and generalization of the embedding model. Extensive evaluations on the Massive Text Embedding Benchmark (MTEB) Chinese and English show that our model significantly outperforms others of comparable size, and competes with 3x, 14x, 18x, and 26x larger embedding models, setting a new standard for a versatile and compact embedding model with less than 1B parameters.
<div id='section'>Paperid: <span id='pid'>436, <a href='https://arxiv.org/pdf/2505.06557.pdf' target='_blank'>https://arxiv.org/pdf/2505.06557.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lu Dong, Haiyu Zhang, Hongjie Zhang, Yifei Huang, Zhen-Hua Ling, Yu Qiao, Limin Wang, Yali Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06557">Weakly Supervised Temporal Sentence Grounding via Positive Sample Mining</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The task of weakly supervised temporal sentence grounding (WSTSG) aims to detect temporal intervals corresponding to a language description from untrimmed videos with only video-level video-language correspondence. For an anchor sample, most existing approaches generate negative samples either from other videos or within the same video for contrastive learning. However, some training samples are highly similar to the anchor sample, directly regarding them as negative samples leads to difficulties for optimization and ignores the correlations between these similar samples and the anchor sample. To address this, we propose Positive Sample Mining (PSM), a novel framework that mines positive samples from the training set to provide more discriminative supervision. Specifically, for a given anchor sample, we partition the remaining training set into semantically similar and dissimilar subsets based on the similarity of their text queries. To effectively leverage these correlations, we introduce a PSM-guided contrastive loss to ensure that the anchor proposal is closer to similar samples and further from dissimilar ones. Additionally, we design a PSM-guided rank loss to ensure that similar samples are closer to the anchor proposal than to the negative intra-video proposal, aiming to distinguish the anchor proposal and the negative intra-video proposal. Experiments on the WSTSG and grounded VideoQA tasks demonstrate the effectiveness and superiority of our method.
<div id='section'>Paperid: <span id='pid'>437, <a href='https://arxiv.org/pdf/2506.15757.pdf' target='_blank'>https://arxiv.org/pdf/2506.15757.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruoyu Wang, Tong Yu, Junda Wu, Yao Liu, Julian McAuley, Lina Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.15757">Weakly-supervised VLM-guided Partial Contrastive Learning for Visual Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Language Navigation (VLN) is a fundamental task within the field of Embodied AI, focusing on the ability of agents to navigate complex environments based on natural language instructions. Despite the progress made by existing methods, these methods often present some common challenges. First, they rely on pre-trained backbone models for visual perception, which struggle with the dynamic viewpoints in VLN scenarios. Second, the performance is limited when using pre-trained LLMs or VLMs without fine-tuning, due to the absence of VLN domain knowledge. Third, while fine-tuning LLMs and VLMs can improve results, their computational costs are higher than those without fine-tuning. To address these limitations, we propose Weakly-supervised Partial Contrastive Learning (WPCL), a method that enhances an agent's ability to identify objects from dynamic viewpoints in VLN scenarios by effectively integrating pre-trained VLM knowledge into the perception process, without requiring VLM fine-tuning. Our method enhances the agent's ability to interpret and respond to environmental cues while ensuring computational efficiency. Experimental results have shown that our method outperforms the baseline methods on multiple benchmarks, which validate the effectiveness, robustness and generalizability of our method.
<div id='section'>Paperid: <span id='pid'>438, <a href='https://arxiv.org/pdf/2305.01915.pdf' target='_blank'>https://arxiv.org/pdf/2305.01915.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dong Yao, Shengyu Zhang, Zhou Zhao, Jieming Zhu, Wenqiao Zhang, Rui Zhang, Xiaofei He, Fei Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.01915">Denoising Multi-modal Sequential Recommenders with Contrastive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There is a rapidly-growing research interest in engaging users with multi-modal data for accurate user modeling on recommender systems. Existing multimedia recommenders have achieved substantial improvements by incorporating various modalities and devising delicate modules. However, when users decide to interact with items, most of them do not fully read the content of all modalities. We refer to modalities that directly cause users' behaviors as point-of-interests, which are important aspects to capture users' interests. In contrast, modalities that do not cause users' behaviors are potential noises and might mislead the learning of a recommendation model. Not surprisingly, little research in the literature has been devoted to denoising such potential noises due to the inaccessibility of users' explicit feedback on their point-of-interests. To bridge the gap, we propose a weakly-supervised framework based on contrastive learning for denoising multi-modal recommenders (dubbed Demure). In a weakly-supervised manner, Demure circumvents the requirement of users' explicit feedback and identifies the noises by analyzing the modalities of all interacted items from a given user.
<div id='section'>Paperid: <span id='pid'>439, <a href='https://arxiv.org/pdf/2203.09287.pdf' target='_blank'>https://arxiv.org/pdf/2203.09287.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Liang, Yannan He, Chengfeng Zhao, Mutian Li, Jingya Wang, Jingyi Yu, Lan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.09287">HybridCap: Inertia-aid Monocular Capture of Challenging Human Motions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monocular 3D motion capture (mocap) is beneficial to many applications. The use of a single camera, however, often fails to handle occlusions of different body parts and hence it is limited to capture relatively simple movements. We present a light-weight, hybrid mocap technique called HybridCap that augments the camera with only 4 Inertial Measurement Units (IMUs) in a learning-and-optimization framework. We first employ a weakly-supervised and hierarchical motion inference module based on cooperative Gated Recurrent Unit (GRU) blocks that serve as limb, body and root trackers as well as an inverse kinematics solver. Our network effectively narrows the search space of plausible motions via coarse-to-fine pose estimation and manages to tackle challenging movements with high efficiency. We further develop a hybrid optimization scheme that combines inertial feedback and visual cues to improve tracking accuracy. Extensive experiments on various datasets demonstrate HybridCap can robustly handle challenging movements ranging from fitness actions to Latin dance. It also achieves real-time performance up to 60 fps with state-of-the-art accuracy.
<div id='section'>Paperid: <span id='pid'>440, <a href='https://arxiv.org/pdf/2501.12632.pdf' target='_blank'>https://arxiv.org/pdf/2501.12632.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shakeeb Murtaza, Soufiane Belharbi, Marco Pedersoli, Eric Granger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.12632">TeD-Loc: Text Distillation for Weakly Supervised Object Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised object localization (WSOL) using classification models trained with only image-class labels remains an important challenge in computer vision. Given their reliance on classification objectives, traditional WSOL methods like class activation mapping focus on the most discriminative object parts, often missing the full spatial extent. In contrast, recent WSOL methods based on vision-language models like CLIP require ground truth classes or external classifiers to produce a localization map, limiting their deployment in downstream tasks. Moreover, methods like GenPromp attempt to address these issues but introduce considerable complexity due to their reliance on conditional denoising processes and intricate prompt learning. This paper introduces Text Distillation for Localization (TeD-Loc), an approach that directly distills knowledge from CLIP text embeddings into the model backbone and produces patch-level localization. Multiple instance learning of these image patches allows for accurate localization and classification using one model without requiring external classifiers. Such integration of textual and visual modalities addresses the longstanding challenge of achieving accurate localization and classification concurrently, as WSOL methods in the literature typically converge at different epochs. Extensive experiments show that leveraging text embeddings and localization cues provides a cost-effective WSOL model. TeD-Loc improves Top-1 LOC accuracy over state-of-the-art models by about 5% on both CUB and ILSVRC datasets, while significantly reducing computational complexity compared to GenPromp.
<div id='section'>Paperid: <span id='pid'>441, <a href='https://arxiv.org/pdf/2407.06018.pdf' target='_blank'>https://arxiv.org/pdf/2407.06018.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shakeeb Murtaza, Marco Pedersoli, Aydin Sarraf, Eric Granger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.06018">Leveraging Transformers for Weakly Supervised Object Localization in Unconstrained Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-Supervised Video Object Localization (WSVOL) involves localizing an object in videos using only video-level labels, also referred to as tags. State-of-the-art WSVOL methods like Temporal CAM (TCAM) rely on class activation mapping (CAM) and typically require a pre-trained CNN classifier. However, their localization accuracy is affected by their tendency to minimize the mutual information between different instances of a class and exploit temporal information during training for downstream tasks, e.g., detection and tracking. In the absence of bounding box annotation, it is challenging to exploit precise information about objects from temporal cues because the model struggles to locate objects over time. To address these issues, a novel method called transformer based CAM for videos (TrCAM-V), is proposed for WSVOL. It consists of a DeiT backbone with two heads for classification and localization. The classification head is trained using standard classification loss (CL), while the localization head is trained using pseudo-labels that are extracted using a pre-trained CLIP model. From these pseudo-labels, the high and low activation values are considered to be foreground and background regions, respectively. Our TrCAM-V method allows training a localization network by sampling pseudo-pixels on the fly from these regions. Additionally, a conditional random field (CRF) loss is employed to align the object boundaries with the foreground map. During inference, the model can process individual frames for real-time localization applications. Extensive experiments on challenging YouTube-Objects unconstrained video datasets show that our TrCAM-V method achieves new state-of-the-art performance in terms of classification and localization accuracy.
<div id='section'>Paperid: <span id='pid'>442, <a href='https://arxiv.org/pdf/2404.10034.pdf' target='_blank'>https://arxiv.org/pdf/2404.10034.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shakeeb Murtaza, Soufiane Belharbi, Marco Pedersoli, Eric Granger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10034">A Realistic Protocol for Evaluation of Weakly Supervised Object Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Object Localization (WSOL) allows training deep learning models for classification and localization (LOC) using only global class-level labels. The absence of bounding box (bbox) supervision during training raises challenges in the literature for hyper-parameter tuning, model selection, and evaluation. WSOL methods rely on a validation set with bbox annotations for model selection, and a test set with bbox annotations for threshold estimation for producing bboxes from localization maps. This approach, however, is not aligned with the WSOL setting as these annotations are typically unavailable in real-world scenarios. Our initial empirical analysis shows a significant decline in LOC performance when model selection and threshold estimation rely solely on class labels and the image itself, respectively, compared to using manual bbox annotations. This highlights the importance of incorporating bbox labels for optimal model performance. In this paper, a new WSOL evaluation protocol is proposed that provides LOC information without the need for manual bbox annotations. In particular, we generated noisy pseudo-boxes from a pretrained off-the-shelf region proposal method such as Selective Search, CLIP, and RPN for model selection. These bboxes are also employed to estimate the threshold from LOC maps, circumventing the need for test-set bbox annotations. Our experiments with several WSOL methods on ILSVRC and CUB datasets show that using the proposed pseudo-bboxes for validation facilitates the model selection and threshold estimation, with LOC performance comparable to those selected using GT bboxes on the validation set and threshold estimation on the test set. It also outperforms models selected using class-level labels, and then dynamically thresholded based solely on LOC maps.
<div id='section'>Paperid: <span id='pid'>443, <a href='https://arxiv.org/pdf/2310.06196.pdf' target='_blank'>https://arxiv.org/pdf/2310.06196.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shakeeb Murtaza, Soufiane Belharbi, Marco Pedersoli, Aydin Sarraf, Eric Granger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.06196">DiPS: Discriminative Pseudo-Label Sampling with Self-Supervised Transformers for Weakly Supervised Object Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-supervised vision transformers (SSTs) have shown great potential to yield rich localization maps that highlight different objects in an image. However, these maps remain class-agnostic since the model is unsupervised. They often tend to decompose the image into multiple maps containing different objects while being unable to distinguish the object of interest from background noise objects. In this paper, Discriminative Pseudo-label Sampling (DiPS) is introduced to leverage these class-agnostic maps for weakly-supervised object localization (WSOL), where only image-class labels are available. Given multiple attention maps, DiPS relies on a pre-trained classifier to identify the most discriminative regions of each attention map. This ensures that the selected ROIs cover the correct image object while discarding the background ones, and, as such, provides a rich pool of diverse and discriminative proposals to cover different parts of the object. Subsequently, these proposals are used as pseudo-labels to train our new transformer-based WSOL model designed to perform classification and localization tasks. Unlike standard WSOL methods, DiPS optimizes performance in both tasks by using a transformer encoder and a dedicated output head for each task, each trained using dedicated loss functions. To avoid overfitting a single proposal and promote better object coverage, a single proposal is randomly selected among the top ones for a training image at each training step. Experimental results on the challenging CUB, ILSVRC, OpenImages, and TelDrone datasets indicate that our architecture, in combination with our transformer-based proposals, can yield better localization performance than state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>444, <a href='https://arxiv.org/pdf/2303.10848.pdf' target='_blank'>https://arxiv.org/pdf/2303.10848.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyan Zu, Haiyang Yu, Bin Li, Xiangyang Xue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.10848">Weakly-Supervised Text Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text segmentation is a challenging vision task with many downstream applications. Current text segmentation methods require pixel-level annotations, which are expensive in the cost of human labor and limited in application scenarios. In this paper, we take the first attempt to perform weakly-supervised text instance segmentation by bridging text recognition and text segmentation. The insight is that text recognition methods provide precise attention position of each text instance, and the attention location can feed to both a text adaptive refinement head (TAR) and a text segmentation head. Specifically, the proposed TAR generates pseudo labels by performing two-stage iterative refinement operations on the attention location to fit the accurate boundaries of the corresponding text instance. Meanwhile, the text segmentation head takes the rough attention location to predict segmentation masks which are supervised by the aforementioned pseudo labels. In addition, we design a mask-augmented contrastive learning by treating our segmentation result as an augmented version of the input text image, thus improving the visual representation and further enhancing the performance of both recognition and segmentation. The experimental results demonstrate that the proposed method significantly outperforms weakly-supervised instance segmentation methods on ICDAR13-FST (18.95$\%$ improvement) and TextSeg (17.80$\%$ improvement) benchmarks.
<div id='section'>Paperid: <span id='pid'>445, <a href='https://arxiv.org/pdf/2303.09044.pdf' target='_blank'>https://arxiv.org/pdf/2303.09044.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soufiane Belharbi, Shakeeb Murtaza, Marco Pedersoli, Ismail Ben Ayed, Luke McCaffrey, Eric Granger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.09044">CoLo-CAM: Class Activation Mapping for Object Co-Localization in Weakly-Labeled Unconstrained Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Leveraging spatiotemporal information in videos is critical for weakly supervised video object localization (WSVOL) tasks. However, state-of-the-art methods only rely on visual and motion cues, while discarding discriminative information, making them susceptible to inaccurate localizations. Recently, discriminative models have been explored for WSVOL tasks using a temporal class activation mapping (CAM) method. Although their results are promising, objects are assumed to have limited movement from frame to frame, leading to degradation in performance for relatively long-term dependencies. This paper proposes a novel CAM method for WSVOL that exploits spatiotemporal information in activation maps during training without constraining an object's position. Its training relies on Co-Localization, hence, the name CoLo-CAM. Given a sequence of frames, localization is jointly learned based on color cues extracted across the corresponding maps, by assuming that an object has similar color in consecutive frames. CAM activations are constrained to respond similarly over pixels with similar colors, achieving co-localization. This improves localization performance because the joint learning creates direct communication among pixels across all image locations and over all frames, allowing for transfer, aggregation, and correction of localizations. Co-localization is integrated into training by minimizing the color term of a conditional random field (CRF) loss over a sequence of frames/CAMs. Extensive experiments on two challenging YouTube-Objects datasets of unconstrained videos show the merits of our method, and its robustness to long-term dependencies, leading to new state-of-the-art performance for WSVOL task.
<div id='section'>Paperid: <span id='pid'>446, <a href='https://arxiv.org/pdf/2402.01922.pdf' target='_blank'>https://arxiv.org/pdf/2402.01922.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Chen, Jindong Wang, Lei Feng, Xiang Li, Yidong Wang, Xing Xie, Masashi Sugiyama, Rita Singh, Bhiksha Raj
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.01922">A General Framework for Learning from Weak Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised learning generally faces challenges in applicability to various scenarios with diverse weak supervision and in scalability due to the complexity of existing algorithms, thereby hindering the practical deployment. This paper introduces a general framework for learning from weak supervision (GLWS) with a novel algorithm. Central to GLWS is an Expectation-Maximization (EM) formulation, adeptly accommodating various weak supervision sources, including instance partial labels, aggregate statistics, pairwise observations, and unlabeled data. We further present an advanced algorithm that significantly simplifies the EM computational demands using a Non-deterministic Finite Automaton (NFA) along with a forward-backward algorithm, which effectively reduces time complexity from quadratic or factorial often required in existing solutions to linear scale. The problem of learning from arbitrary weak supervision is therefore converted to the NFA modeling of them. GLWS not only enhances the scalability of machine learning models but also demonstrates superior performance and versatility across 11 weak supervision scenarios. We hope our work paves the way for further advancements and practical deployment in this field.
<div id='section'>Paperid: <span id='pid'>447, <a href='https://arxiv.org/pdf/2410.13786.pdf' target='_blank'>https://arxiv.org/pdf/2410.13786.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fengqi Liu, Hexiang Wang, Jingyu Gong, Ran Yi, Qianyu Zhou, Xuequan Lu, Jiangbo Lu, Lizhuang Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13786">Emphasizing Semantic Consistency of Salient Posture for Speech-Driven Gesture Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech-driven gesture generation aims at synthesizing a gesture sequence synchronized with the input speech signal. Previous methods leverage neural networks to directly map a compact audio representation to the gesture sequence, ignoring the semantic association of different modalities and failing to deal with salient gestures. In this paper, we propose a novel speech-driven gesture generation method by emphasizing the semantic consistency of salient posture. Specifically, we first learn a joint manifold space for the individual representation of audio and body pose to exploit the inherent semantic association between two modalities, and propose to enforce semantic consistency via a consistency loss. Furthermore, we emphasize the semantic consistency of salient postures by introducing a weakly-supervised detector to identify salient postures, and reweighting the consistency loss to focus more on learning the correspondence between salient postures and the high-level semantics of speech content. In addition, we propose to extract audio features dedicated to facial expression and body gesture separately, and design separate branches for face and body gesture synthesis. Extensive experimental results demonstrate the superiority of our method over the state-of-the-art approaches.
<div id='section'>Paperid: <span id='pid'>448, <a href='https://arxiv.org/pdf/2307.11299.pdf' target='_blank'>https://arxiv.org/pdf/2307.11299.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haitao Lin, Yanwei Fu, Xiangyang Xue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.11299">PourIt!: Weakly-supervised Liquid Perception from a Single Image for Visual Closed-Loop Robotic Pouring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Liquid perception is critical for robotic pouring tasks. It usually requires the robust visual detection of flowing liquid. However, while recent works have shown promising results in liquid perception, they typically require labeled data for model training, a process that is both time-consuming and reliant on human labor. To this end, this paper proposes a simple yet effective framework PourIt!, to serve as a tool for robotic pouring tasks. We design a simple data collection pipeline that only needs image-level labels to reduce the reliance on tedious pixel-wise annotations. Then, a binary classification model is trained to generate Class Activation Map (CAM) that focuses on the visual difference between these two kinds of collected data, i.e., the existence of liquid drop or not. We also devise a feature contrast strategy to improve the quality of the CAM, thus entirely and tightly covering the actual liquid regions. Then, the container pose is further utilized to facilitate the 3D point cloud recovery of the detected liquid region. Finally, the liquid-to-container distance is calculated for visual closed-loop control of the physical robot. To validate the effectiveness of our proposed method, we also contribute a novel dataset for our task and name it PourIt! dataset. Extensive results on this dataset and physical Franka robot have shown the utility and effectiveness of our method in the robotic pouring tasks. Our dataset, code and pre-trained models will be available on the project page.
<div id='section'>Paperid: <span id='pid'>449, <a href='https://arxiv.org/pdf/2303.05725.pdf' target='_blank'>https://arxiv.org/pdf/2303.05725.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiangbin Zheng, Yile Wang, Cheng Tan, Siyuan Li, Ge Wang, Jun Xia, Yidong Chen, Stan Z. Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.05725">CVT-SLR: Contrastive Visual-Textual Transformation for Sign Language Recognition with Variational Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sign language recognition (SLR) is a weakly supervised task that annotates sign videos as textual glosses. Recent studies show that insufficient training caused by the lack of large-scale available sign datasets becomes the main bottleneck for SLR. Most SLR works thereby adopt pretrained visual modules and develop two mainstream solutions. The multi-stream architectures extend multi-cue visual features, yielding the current SOTA performances but requiring complex designs and might introduce potential noise. Alternatively, the advanced single-cue SLR frameworks using explicit cross-modal alignment between visual and textual modalities are simple and effective, potentially competitive with the multi-cue framework. In this work, we propose a novel contrastive visual-textual transformation for SLR, CVT-SLR, to fully explore the pretrained knowledge of both the visual and language modalities. Based on the single-cue cross-modal alignment framework, we propose a variational autoencoder (VAE) for pretrained contextual knowledge while introducing the complete pretrained language module. The VAE implicitly aligns visual and textual modalities while benefiting from pretrained contextual knowledge as the traditional contextual module. Meanwhile, a contrastive cross-modal alignment algorithm is designed to explicitly enhance the consistency constraints. Extensive experiments on public datasets (PHOENIX-2014 and PHOENIX-2014T) demonstrate that our proposed CVT-SLR consistently outperforms existing single-cue methods and even outperforms SOTA multi-cue methods.
<div id='section'>Paperid: <span id='pid'>450, <a href='https://arxiv.org/pdf/2509.26281.pdf' target='_blank'>https://arxiv.org/pdf/2509.26281.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Teng Zhang, Ziqian Fan, Mingxin Liu, Xin Zhang, Xudong Lu, Wentong Li, Yue Zhou, Yi Yu, Xiang Li, Junchi Yan, Xue Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26281">Point2RBox-v3: Self-Bootstrapping from Point Annotations via Integrated Pseudo-Label Refinement and Utilization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Driven by the growing need for Oriented Object Detection (OOD), learning from point annotations under a weakly-supervised framework has emerged as a promising alternative to costly and laborious manual labeling. In this paper, we discuss two deficiencies in existing point-supervised methods: inefficient utilization and poor quality of pseudo labels. Therefore, we present Point2RBox-v3. At the core are two principles: 1) Progressive Label Assignment (PLA). It dynamically estimates instance sizes in a coarse yet intelligent manner at different stages of the training process, enabling the use of label assignment methods. 2) Prior-Guided Dynamic Mask Loss (PGDM-Loss). It is an enhancement of the Voronoi Watershed Loss from Point2RBox-v2, which overcomes the shortcomings of Watershed in its poor performance in sparse scenes and SAM's poor performance in dense scenes. To our knowledge, Point2RBox-v3 is the first model to employ dynamic pseudo labels for label assignment, and it creatively complements the advantages of SAM model with the watershed algorithm, which achieves excellent performance in both sparse and dense scenes. Our solution gives competitive performance, especially in scenarios with large variations in object size or sparse object occurrences: 66.09%/56.86%/41.28%/46.40%/19.60%/45.96% on DOTA-v1.0/DOTA-v1.5/DOTA-v2.0/DIOR/STAR/RSAR.
<div id='section'>Paperid: <span id='pid'>451, <a href='https://arxiv.org/pdf/2406.19394.pdf' target='_blank'>https://arxiv.org/pdf/2406.19394.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liujuan Cao, Jianghang Lin, Zebo Hong, Yunhang Shen, Shaohui Lin, Chao Chen, Rongrong Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.19394">HUWSOD: Holistic Self-training for Unified Weakly Supervised Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most WSOD methods rely on traditional object proposals to generate candidate regions and are confronted with unstable training, which easily gets stuck in a poor local optimum. In this paper, we introduce a unified, high-capacity weakly supervised object detection (WSOD) network called HUWSOD, which utilizes a comprehensive self-training framework without needing external modules or additional supervision. HUWSOD innovatively incorporates a self-supervised proposal generator and an autoencoder proposal generator with a multi-rate resampling pyramid to replace traditional object proposals, enabling end-to-end WSOD training and inference. Additionally, we implement a holistic self-training scheme that refines detection scores and coordinates through step-wise entropy minimization and consistency-constraint regularization, ensuring consistent predictions across stochastic augmentations of the same image. Extensive experiments on PASCAL VOC and MS COCO demonstrate that HUWSOD competes with state-of-the-art WSOD methods, eliminating the need for offline proposals and additional data. The peak performance of HUWSOD approaches that of fully-supervised Faster R-CNN. Our findings also indicate that randomly initialized boxes, although significantly different from well-designed offline object proposals, are effective for WSOD training.
<div id='section'>Paperid: <span id='pid'>452, <a href='https://arxiv.org/pdf/2309.12766.pdf' target='_blank'>https://arxiv.org/pdf/2309.12766.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryandhimas E. Zezario, Yu-Wen Chen, Szu-Wei Fu, Yu Tsao, Hsin-Min Wang, Chiou-Shann Fuh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.12766">A Study on Incorporating Whisper for Robust Speech Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This research introduces an enhanced version of the multi-objective speech assessment model--MOSA-Net+, by leveraging the acoustic features from Whisper, a large-scaled weakly supervised model. We first investigate the effectiveness of Whisper in deploying a more robust speech assessment model. After that, we explore combining representations from Whisper and SSL models. The experimental results reveal that Whisper's embedding features can contribute to more accurate prediction performance. Moreover, combining the embedding features from Whisper and SSL models only leads to marginal improvement. As compared to intrusive methods, MOSA-Net, and other SSL-based speech assessment models, MOSA-Net+ yields notable improvements in estimating subjective quality and intelligibility scores across all evaluation metrics in Taiwan Mandarin Hearing In Noise test - Quality & Intelligibility (TMHINT-QI) dataset. To further validate its robustness, MOSA-Net+ was tested in the noisy-and-enhanced track of the VoiceMOS Challenge 2023, where it obtained the top-ranked performance among nine systems.
<div id='section'>Paperid: <span id='pid'>453, <a href='https://arxiv.org/pdf/2510.04091.pdf' target='_blank'>https://arxiv.org/pdf/2510.04091.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Wang, Tianhao Ma, Ming-Kun Xie, Gang Niu, Masashi Sugiyama
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04091">Rethinking Consistent Multi-Label Classification under Inexact Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partial multi-label learning and complementary multi-label learning are two popular weakly supervised multi-label classification paradigms that aim to alleviate the high annotation costs of collecting precisely annotated multi-label data. In partial multi-label learning, each instance is annotated with a candidate label set, among which only some labels are relevant; in complementary multi-label learning, each instance is annotated with complementary labels indicating the classes to which the instance does not belong. Existing consistent approaches for the two paradigms either require accurate estimation of the generation process of candidate or complementary labels or assume a uniform distribution to eliminate the estimation problem. However, both conditions are usually difficult to satisfy in real-world scenarios. In this paper, we propose consistent approaches that do not rely on the aforementioned conditions to handle both problems in a unified way. Specifically, we propose two unbiased risk estimators based on first- and second-order strategies. Theoretically, we prove consistency w.r.t. two widely used multi-label classification evaluation metrics and derive convergence rates for the estimation errors of the proposed risk estimators. Empirically, extensive experimental results validate the effectiveness of our proposed approaches against state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>454, <a href='https://arxiv.org/pdf/2509.24228.pdf' target='_blank'>https://arxiv.org/pdf/2509.24228.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Wang, Dong-Dong Wu, Ming Li, Jingxiong Zhang, Gang Niu, Masashi Sugiyama
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24228">Accessible, Realistic, and Fair Evaluation of Positive-Unlabeled Learning Algorithms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Positive-unlabeled (PU) learning is a weakly supervised binary classification problem, in which the goal is to learn a binary classifier from only positive and unlabeled data, without access to negative data. In recent years, many PU learning algorithms have been developed to improve model performance. However, experimental settings are highly inconsistent, making it difficult to identify which algorithm performs better. In this paper, we propose the first PU learning benchmark to systematically compare PU learning algorithms. During our implementation, we identify subtle yet critical factors that affect the realistic and fair evaluation of PU learning algorithms. On the one hand, many PU learning algorithms rely on a validation set that includes negative data for model selection. This is unrealistic in traditional PU learning settings, where no negative data are available. To handle this problem, we systematically investigate model selection criteria for PU learning. On the other hand, the problem settings and solutions of PU learning have different families, i.e., the one-sample and two-sample settings. However, existing evaluation protocols are heavily biased towards the one-sample setting and neglect the significant difference between them. We identify the internal label shift problem of unlabeled training data for the one-sample setting and propose a simple yet effective calibration approach to ensure fair comparisons within and across families. We hope our framework will provide an accessible, realistic, and fair environment for evaluating PU learning algorithms in the future.
<div id='section'>Paperid: <span id='pid'>455, <a href='https://arxiv.org/pdf/2502.10184.pdf' target='_blank'>https://arxiv.org/pdf/2502.10184.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Wang, Dong-Dong Wu, Jindong Wang, Gang Niu, Min-Ling Zhang, Masashi Sugiyama
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10184">Realistic Evaluation of Deep Partial-Label Learning Algorithms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partial-label learning (PLL) is a weakly supervised learning problem in which each example is associated with multiple candidate labels and only one is the true label. In recent years, many deep PLL algorithms have been developed to improve model performance. However, we find that some early developed algorithms are often underestimated and can outperform many later algorithms with complicated designs. In this paper, we delve into the empirical perspective of PLL and identify several critical but previously overlooked issues. First, model selection for PLL is non-trivial, but has never been systematically studied. Second, the experimental settings are highly inconsistent, making it difficult to evaluate the effectiveness of the algorithms. Third, there is a lack of real-world image datasets that can be compatible with modern network architectures. Based on these findings, we propose PLENCH, the first Partial-Label learning bENCHmark to systematically compare state-of-the-art deep PLL algorithms. We investigate the model selection problem for PLL for the first time, and propose novel model selection criteria with theoretical guarantees. We also create Partial-Label CIFAR-10 (PLCIFAR10), an image dataset of human-annotated partial labels collected from Amazon Mechanical Turk, to provide a testbed for evaluating the performance of PLL algorithms in more realistic scenarios. Researchers can quickly and conveniently perform a comprehensive and fair evaluation and verify the effectiveness of newly developed algorithms based on PLENCH. We hope that PLENCH will facilitate standardized, fair, and practical evaluation of PLL algorithms in the future.
<div id='section'>Paperid: <span id='pid'>456, <a href='https://arxiv.org/pdf/2311.15502.pdf' target='_blank'>https://arxiv.org/pdf/2311.15502.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Wang, Takashi Ishida, Yu-Jie Zhang, Gang Niu, Masashi Sugiyama
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.15502">Learning with Complementary Labels Revisited: The Selected-Completely-at-Random Setting Is More Practical</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Complementary-label learning is a weakly supervised learning problem in which each training example is associated with one or multiple complementary labels indicating the classes to which it does not belong. Existing consistent approaches have relied on the uniform distribution assumption to model the generation of complementary labels, or on an ordinary-label training set to estimate the transition matrix in non-uniform cases. However, either condition may not be satisfied in real-world scenarios. In this paper, we propose a novel consistent approach that does not rely on these conditions. Inspired by the positive-unlabeled (PU) learning literature, we propose an unbiased risk estimator based on the Selected-Completely-at-Random assumption for complementary-label learning. We then introduce a risk-correction approach to address overfitting problems. Furthermore, we find that complementary-label learning can be expressed as a set of negative-unlabeled binary classification problems when using the one-versus-rest strategy. Extensive experimental results on both synthetic and real-world benchmark datasets validate the superiority of our proposed approach over state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>457, <a href='https://arxiv.org/pdf/2310.05632.pdf' target='_blank'>https://arxiv.org/pdf/2310.05632.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Wang, Lei Feng, Yuchen Jiang, Gang Niu, Min-Ling Zhang, Masashi Sugiyama
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.05632">Binary Classification with Confidence Difference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, learning with soft labels has been shown to achieve better performance than learning with hard labels in terms of model generalization, calibration, and robustness. However, collecting pointwise labeling confidence for all training examples can be challenging and time-consuming in real-world scenarios. This paper delves into a novel weakly supervised binary classification problem called confidence-difference (ConfDiff) classification. Instead of pointwise labeling confidence, we are given only unlabeled data pairs with confidence difference that specifies the difference in the probabilities of being positive. We propose a risk-consistent approach to tackle this problem and show that the estimation error bound achieves the optimal convergence rate. We also introduce a risk correction approach to mitigate overfitting problems, whose consistency and convergence rate are also proven. Extensive experiments on benchmark data sets and a real-world recommender system data set validate the effectiveness of our proposed approaches in exploiting the supervision information of the confidence difference.
<div id='section'>Paperid: <span id='pid'>458, <a href='https://arxiv.org/pdf/2305.08344.pdf' target='_blank'>https://arxiv.org/pdf/2305.08344.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei-I Lin, Gang Niu, Hsuan-Tien Lin, Masashi Sugiyama
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.08344">Enhancing Label Sharing Efficiency in Complementary-Label Learning with Label Augmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Complementary-label Learning (CLL) is a form of weakly supervised learning that trains an ordinary classifier using only complementary labels, which are the classes that certain instances do not belong to. While existing CLL studies typically use novel loss functions or training techniques to solve this problem, few studies focus on how complementary labels collectively provide information to train the ordinary classifier. In this paper, we fill the gap by analyzing the implicit sharing of complementary labels on nearby instances during training. Our analysis reveals that the efficiency of implicit label sharing is closely related to the performance of existing CLL models. Based on this analysis, we propose a novel technique that enhances the sharing efficiency via complementary-label augmentation, which explicitly propagates additional complementary labels to each instance. We carefully design the augmentation process to enrich the data with new and accurate complementary labels, which provide CLL models with fresh and valuable information to enhance the sharing efficiency. We then verify our proposed technique by conducting thorough experiments on both synthetic and real-world datasets. Our results confirm that complementary-label augmentation can systematically improve empirical performance over state-of-the-art CLL models.
<div id='section'>Paperid: <span id='pid'>459, <a href='https://arxiv.org/pdf/2202.00395.pdf' target='_blank'>https://arxiv.org/pdf/2202.00395.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Takashi Ishida, Ikko Yamane, Nontawat Charoenphakdee, Gang Niu, Masashi Sugiyama
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.00395">Is the Performance of My Deep Network Too Good to Be True? A Direct Approach to Estimating the Bayes Error in Binary Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There is a fundamental limitation in the prediction performance that a machine learning model can achieve due to the inevitable uncertainty of the prediction target. In classification problems, this can be characterized by the Bayes error, which is the best achievable error with any classifier. The Bayes error can be used as a criterion to evaluate classifiers with state-of-the-art performance and can be used to detect test set overfitting. We propose a simple and direct Bayes error estimator, where we just take the mean of the labels that show \emph{uncertainty} of the class assignments. Our flexible approach enables us to perform Bayes error estimation even for weakly supervised data. In contrast to others, our method is model-free and even instance-free. Moreover, it has no hyperparameters and gives a more accurate estimate of the Bayes error than several baselines empirically. Experiments using our method suggest that recently proposed deep networks such as the Vision Transformer may have reached, or is about to reach, the Bayes error for benchmark datasets. Finally, we discuss how we can study the inherent difficulty of the acceptance/rejection decision for scientific articles, by estimating the Bayes error of the ICLR papers from 2017 to 2023.
<div id='section'>Paperid: <span id='pid'>460, <a href='https://arxiv.org/pdf/2406.14958.pdf' target='_blank'>https://arxiv.org/pdf/2406.14958.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei Chen, Dingkang Yang, Yuxuan Lei, Lihua Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.14958">Skip and Skip: Segmenting Medical Images with Prompts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most medical image lesion segmentation methods rely on hand-crafted accurate annotations of the original image for supervised learning. Recently, a series of weakly supervised or unsupervised methods have been proposed to reduce the dependence on pixel-level annotations. However, these methods are essentially based on pixel-level annotation, ignoring the image-level diagnostic results of the current massive medical images. In this paper, we propose a dual U-shaped two-stage framework that utilizes image-level labels to prompt the segmentation. In the first stage, we pre-train a classification network with image-level labels, which is used to obtain the hierarchical pyramid features and guide the learning of downstream branches. In the second stage, we feed the hierarchical features obtained from the classification branch into the downstream branch through short-skip and long-skip and get the lesion masks under the supervised learning of pixel-level labels. Experiments show that our framework achieves better results than networks simply using pixel-level annotations.
<div id='section'>Paperid: <span id='pid'>461, <a href='https://arxiv.org/pdf/2512.02224.pdf' target='_blank'>https://arxiv.org/pdf/2512.02224.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Feng, Tianhao Peng, Fan Zhang, David Bull
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.02224">Towards Unified Video Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent works in video quality assessment (VQA) typically employ monolithic models that typically predict a single quality score for each test video. These approaches cannot provide diagnostic, interpretable feedback, offering little insight into why the video quality is degraded. Most of them are also specialized, format-specific metrics rather than truly ``generic" solutions, as they are designed to learn a compromised representation from disparate perceptual domains. To address these limitations, this paper proposes Unified-VQA, a framework that provides a single, unified quality model applicable to various distortion types within multiple video formats by recasting generic VQA as a Diagnostic Mixture-of-Experts (MoE) problem. Unified-VQA employs multiple ``perceptual experts'' dedicated to distinct perceptual domains. A novel multi-proxy expert training strategy is designed to optimize each expert using a ranking-inspired loss, guided by the most suitable proxy metric for its domain. We also integrated a diagnostic multi-task head into this framework to generate a global quality score and an interpretable multi-dimensional artifact vector, which is optimized using a weakly-supervised learning strategy, leveraging the known properties of the large-scale training database generated for this work. With static model parameters (without retraining or fine-tuning), Unified-VQA demonstrates consistent and superior performance compared to over 18 benchmark methods for both generic VQA and diagnostic artifact detection tasks across 17 databases containing diverse streaming artifacts in HD, UHD, HDR and HFR formats. This work represents an important step towards practical, actionable, and interpretable video quality assessment.
<div id='section'>Paperid: <span id='pid'>462, <a href='https://arxiv.org/pdf/2505.04339.pdf' target='_blank'>https://arxiv.org/pdf/2505.04339.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Peng, Xiang Huang, Shuo Sun, Ruitong Zhang, Philip S. Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04339">Adaptive and Robust DBSCAN with Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>DBSCAN, a well-known density-based clustering algorithm, has gained widespread popularity and usage due to its effectiveness in identifying clusters of arbitrary shapes and handling noisy data. However, it encounters challenges in producing satisfactory cluster results when confronted with datasets of varying density scales, a common scenario in real-world applications. In this paper, we propose a novel Adaptive and Robust DBSCAN with Multi-agent Reinforcement Learning cluster framework, namely AR-DBSCAN. First, we model the initial dataset as a two-level encoding tree and categorize the data vertices into distinct density partitions according to the information uncertainty determined in the encoding tree. Each partition is then assigned to an agent to find the best clustering parameters without manual assistance. The allocation is density-adaptive, enabling AR-DBSCAN to effectively handle diverse density distributions within the dataset by utilizing distinct agents for different partitions. Second, a multi-agent deep reinforcement learning guided automatic parameter searching process is designed. The process of adjusting the parameter search direction by perceiving the clustering environment is modeled as a Markov decision process. Using a weakly-supervised reward training policy network, each agent adaptively learns the optimal clustering parameters by interacting with the clusters. Third, a recursive search mechanism adaptable to the data's scale is presented, enabling efficient and controlled exploration of large parameter spaces. Extensive experiments are conducted on nine artificial datasets and a real-world dataset. The results of offline and online tasks show that AR-DBSCAN not only improves clustering accuracy by up to 144.1% and 175.3% in the NMI and ARI metrics, respectively, but also is capable of robustly finding dominant parameters.
<div id='section'>Paperid: <span id='pid'>463, <a href='https://arxiv.org/pdf/2504.00844.pdf' target='_blank'>https://arxiv.org/pdf/2504.00844.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdelrahman Elskhawy, Mengze Li, Nassir Navab, Benjamin Busam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.00844">PRISM-0: A Predicate-Rich Scene Graph Generation Framework for Zero-Shot Open-Vocabulary Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In Scene Graph Generation (SGG), structured representations are extracted from visual inputs as object nodes and connecting predicates, enabling image-based reasoning for diverse downstream tasks. While fully supervised SGG has improved steadily, it suffers from training bias due to limited curated data and long-tail predicate distributions, leading to poor predicate diversity and degraded downstream performance. We present PRISM-0, a zero-shot open-vocabulary SGG framework that leverages foundation models in a bottom-up pipeline to capture a broad spectrum of predicates. Detected object pairs are filtered, described via a Vision-Language Model (VLM), and processed by a Large Language Model (LLM) to generate fine- and coarse-grained predicates, which are then validated by a Visual Question Answering (VQA) model. PRISM-0 modular, dataset-independent design enriches existing SGG datasets such as Visual Genome and produces diverse, unbiased graphs. While operating entirely in a zero-shot setting, PRISM-0 achieves performance on par with state-of-the-art weakly-supervised models on SGG benchmarks and even state-of-the-art supervised methods in tasks such as Sentence-to-Graph Retrieval.
<div id='section'>Paperid: <span id='pid'>464, <a href='https://arxiv.org/pdf/2405.11868.pdf' target='_blank'>https://arxiv.org/pdf/2405.11868.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Ju, Yifan Wang, Yifang Qin, Zhengyang Mao, Zhiping Xiao, Junyu Luo, Junwei Yang, Yiyang Gu, Dongjie Wang, Qingqing Long, Siyu Yi, Xiao Luo, Ming Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.11868">Towards Graph Contrastive Learning: A Survey and Beyond</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, deep learning on graphs has achieved remarkable success in various domains. However, the reliance on annotated graph data remains a significant bottleneck due to its prohibitive cost and time-intensive nature. To address this challenge, self-supervised learning (SSL) on graphs has gained increasing attention and has made significant progress. SSL enables machine learning models to produce informative representations from unlabeled graph data, reducing the reliance on expensive labeled data. While SSL on graphs has witnessed widespread adoption, one critical component, Graph Contrastive Learning (GCL), has not been thoroughly investigated in the existing literature. Thus, this survey aims to fill this gap by offering a dedicated survey on GCL. We provide a comprehensive overview of the fundamental principles of GCL, including data augmentation strategies, contrastive modes, and contrastive optimization objectives. Furthermore, we explore the extensions of GCL to other aspects of data-efficient graph learning, such as weakly supervised learning, transfer learning, and related scenarios. We also discuss practical applications spanning domains such as drug discovery, genomics analysis, recommender systems, and finally outline the challenges and potential future directions in this field.
<div id='section'>Paperid: <span id='pid'>465, <a href='https://arxiv.org/pdf/2112.02814.pdf' target='_blank'>https://arxiv.org/pdf/2112.02814.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qihan Huang, Haofei Zhang, Mengqi Xue, Jie Song, Mingli Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2112.02814">A Survey of Deep Learning for Low-Shot Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object detection has achieved a huge breakthrough with deep neural networks and massive annotated data. However, current detection methods cannot be directly transferred to the scenario where the annotated data is scarce due to the severe overfitting problem. Although few-shot learning and zero-shot learning have been extensively explored in the field of image classification, it is indispensable to design new methods for object detection in the data-scarce scenario since object detection has an additional challenging localization task. Low-Shot Object Detection (LSOD) is an emerging research topic of detecting objects from a few or even no annotated samples, consisting of One-Shot Object Detection (OSOD), Few-Shot Object Detection (FSOD) and Zero-Shot Object Detection (ZSD). This survey provides a comprehensive review of LSOD methods. First, we propose a thorough taxonomy of LSOD methods and analyze them systematically, comprising some extensional topics of LSOD (semi-supervised LSOD, weakly-supervised LSOD, and incremental LSOD). Then, we indicate the pros and cons of current LSOD methods with a comparison of their performance. Finally, we discuss the challenges and promising directions of LSOD to provide guidance for future works.
<div id='section'>Paperid: <span id='pid'>466, <a href='https://arxiv.org/pdf/2505.20106.pdf' target='_blank'>https://arxiv.org/pdf/2505.20106.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zuyao Chen, Jinlin Wu, Zhen Lei, Chang Wen Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20106">From Data to Modeling: Fully Open-vocabulary Scene Graph Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present OvSGTR, a novel transformer-based framework for fully open-vocabulary scene graph generation that overcomes the limitations of traditional closed-set models. Conventional methods restrict both object and relationship recognition to a fixed vocabulary, hindering their applicability to real-world scenarios where novel concepts frequently emerge. In contrast, our approach jointly predicts objects (nodes) and their inter-relationships (edges) beyond predefined categories. OvSGTR leverages a DETR-like architecture featuring a frozen image backbone and text encoder to extract high-quality visual and semantic features, which are then fused via a transformer decoder for end-to-end scene graph prediction. To enrich the model's understanding of complex visual relations, we propose a relation-aware pre-training strategy that synthesizes scene graph annotations in a weakly supervised manner. Specifically, we investigate three pipelines--scene parser-based, LLM-based, and multimodal LLM-based--to generate transferable supervision signals with minimal manual annotation. Furthermore, we address the common issue of catastrophic forgetting in open-vocabulary settings by incorporating a visual-concept retention mechanism coupled with a knowledge distillation strategy, ensuring that the model retains rich semantic cues during fine-tuning. Extensive experiments on the VG150 benchmark demonstrate that OvSGTR achieves state-of-the-art performance across multiple settings, including closed-set, open-vocabulary object detection-based, relation-based, and fully open-vocabulary scenarios. Our results highlight the promise of large-scale relation-aware pre-training and transformer architectures for advancing scene graph generation towards more generalized and reliable visual understanding.
<div id='section'>Paperid: <span id='pid'>467, <a href='https://arxiv.org/pdf/2406.00919.pdf' target='_blank'>https://arxiv.org/pdf/2406.00919.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinxing Zhou, Dan Guo, Yiran Zhong, Meng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00919">Advancing Weakly-Supervised Audio-Visual Video Parsing via Segment-wise Pseudo Labeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Audio-Visual Video Parsing task aims to identify and temporally localize the events that occur in either or both the audio and visual streams of audible videos. It often performs in a weakly-supervised manner, where only video event labels are provided, \ie, the modalities and the timestamps of the labels are unknown. Due to the lack of densely annotated labels, recent work attempts to leverage pseudo labels to enrich the supervision. A commonly used strategy is to generate pseudo labels by categorizing the known video event labels for each modality. However, the labels are still confined to the video level, and the temporal boundaries of events remain unlabeled. In this paper, we propose a new pseudo label generation strategy that can explicitly assign labels to each video segment by utilizing prior knowledge learned from the open world. Specifically, we exploit the large-scale pretrained models, namely CLIP and CLAP, to estimate the events in each video segment and generate segment-level visual and audio pseudo labels, respectively. We then propose a new loss function to exploit these pseudo labels by taking into account their category-richness and segment-richness. A label denoising strategy is also adopted to further improve the visual pseudo labels by flipping them whenever abnormally large forward losses occur. We perform extensive experiments on the LLP dataset and demonstrate the effectiveness of each proposed design and we achieve state-of-the-art video parsing performance on all types of event parsing, \ie, audio event, visual event, and audio-visual event. We also examine the proposed pseudo label generation strategy on a relevant weakly-supervised audio-visual event localization task and the experimental results again verify the benefits and generalization of our method.
<div id='section'>Paperid: <span id='pid'>468, <a href='https://arxiv.org/pdf/2311.09819.pdf' target='_blank'>https://arxiv.org/pdf/2311.09819.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhen Sun, Huan Xu, Jinlin Wu, Zhen Chen, Zhen Lei, Hongbin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.09819">PWISeg: Point-based Weakly-supervised Instance Segmentation for Surgical Instruments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In surgical procedures, correct instrument counting is essential. Instance segmentation is a location method that locates not only an object's bounding box but also each pixel's specific details. However, obtaining mask-level annotations is labor-intensive in instance segmentation. To address this issue, we propose a novel yet effective weakly-supervised surgical instrument instance segmentation approach, named Point-based Weakly-supervised Instance Segmentation (PWISeg). PWISeg adopts an FCN-based architecture with point-to-box and point-to-mask branches to model the relationships between feature points and bounding boxes, as well as feature points and segmentation masks on FPN, accomplishing instrument detection and segmentation jointly in a single model. Since mask level annotations are hard to available in the real world, for point-to-mask training, we introduce an unsupervised projection loss, utilizing the projected relation between predicted masks and bboxes as supervision signal. On the other hand, we annotate a few pixels as the key pixel for each instrument. Based on this, we further propose a key pixel association loss and a key pixel distribution loss, driving the point-to-mask branch to generate more accurate segmentation predictions. To comprehensively evaluate this task, we unveil a novel surgical instrument dataset with manual annotations, setting up a benchmark for further research. Our comprehensive research trial validated the superior performance of our PWISeg. The results show that the accuracy of surgical instrument segmentation is improved, surpassing most methods of instance segmentation via weakly supervised bounding boxes. This improvement is consistently observed in our proposed dataset and when applied to the public HOSPI-Tools dataset.
<div id='section'>Paperid: <span id='pid'>469, <a href='https://arxiv.org/pdf/2309.02578.pdf' target='_blank'>https://arxiv.org/pdf/2309.02578.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Philip MÃ¼ller, Felix Meissen, Johannes Brandt, Georgios Kaissis, Daniel Rueckert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.02578">Anatomy-Driven Pathology Detection on Chest X-rays</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pathology detection and delineation enables the automatic interpretation of medical scans such as chest X-rays while providing a high level of explainability to support radiologists in making informed decisions. However, annotating pathology bounding boxes is a time-consuming task such that large public datasets for this purpose are scarce. Current approaches thus use weakly supervised object detection to learn the (rough) localization of pathologies from image-level annotations, which is however limited in performance due to the lack of bounding box supervision. We therefore propose anatomy-driven pathology detection (ADPD), which uses easy-to-annotate bounding boxes of anatomical regions as proxies for pathologies. We study two training approaches: supervised training using anatomy-level pathology labels and multiple instance learning (MIL) with image-level pathology labels. Our results show that our anatomy-level training approach outperforms weakly supervised methods and fully supervised detection with limited training samples, and our MIL approach is competitive with both baseline approaches, therefore demonstrating the potential of our approach.
<div id='section'>Paperid: <span id='pid'>470, <a href='https://arxiv.org/pdf/2303.02344.pdf' target='_blank'>https://arxiv.org/pdf/2303.02344.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinxing Zhou, Dan Guo, Yiran Zhong, Meng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.02344">Improving Audio-Visual Video Parsing with Pseudo Visual Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-Visual Video Parsing is a task to predict the events that occur in video segments for each modality. It often performs in a weakly supervised manner, where only video event labels are provided, i.e., the modalities and the timestamps of the labels are unknown. Due to the lack of densely annotated labels, recent work attempts to leverage pseudo labels to enrich the supervision. A commonly used strategy is to generate pseudo labels by categorizing the known event labels for each modality. However, the labels are still limited to the video level, and the temporal boundaries of event timestamps remain unlabeled. In this paper, we propose a new pseudo label generation strategy that can explicitly assign labels to each video segment by utilizing prior knowledge learned from the open world. Specifically, we exploit the CLIP model to estimate the events in each video segment based on visual modality to generate segment-level pseudo labels. A new loss function is proposed to regularize these labels by taking into account their category-richness and segmentrichness. A label denoising strategy is adopted to improve the pseudo labels by flipping them whenever high forward binary cross entropy loss occurs. We perform extensive experiments on the LLP dataset and demonstrate that our method can generate high-quality segment-level pseudo labels with the help of our newly proposed loss and the label denoising strategy. Our method achieves state-of-the-art audio-visual video parsing performance.
<div id='section'>Paperid: <span id='pid'>471, <a href='https://arxiv.org/pdf/2302.12986.pdf' target='_blank'>https://arxiv.org/pdf/2302.12986.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benzhi Wang, Yang Yang, Jinlin Wu, Guo-jun Qi, Zhen Lei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.12986">Self-similarity Driven Scale-invariant Learning for Weakly Supervised Person Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised person search aims to jointly detect and match persons with only bounding box annotations. Existing approaches typically focus on improving the features by exploring relations of persons. However, scale variation problem is a more severe obstacle and under-studied that a person often owns images with different scales (resolutions). On the one hand, small-scale images contain less information of a person, thus affecting the accuracy of the generated pseudo labels. On the other hand, the similarity of cross-scale images is often smaller than that of images with the same scale for a person, which will increase the difficulty of matching. In this paper, we address this problem by proposing a novel one-step framework, named Self-similarity driven Scale-invariant Learning (SSL). Scale invariance can be explored based on the self-similarity prior that it shows the same statistical properties of an image at different scales. To this end, we introduce a Multi-scale Exemplar Branch to guide the network in concentrating on the foreground and learning scale-invariant features by hard exemplars mining. To enhance the discriminative power of the features in an unsupervised manner, we introduce a dynamic multi-label prediction which progressively seeks true labels for training. It is adaptable to different types of unlabeled data and serves as a compensation for clustering based strategy. Experiments on PRW and CUHK-SYSU databases demonstrate the effectiveness of our method.
<div id='section'>Paperid: <span id='pid'>472, <a href='https://arxiv.org/pdf/2301.09506.pdf' target='_blank'>https://arxiv.org/pdf/2301.09506.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Keyan Chen, Xiaolong Jiang, Yao Hu, Xu Tang, Yan Gao, Jianqi Chen, Weidi Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.09506">OvarNet: Towards Open-vocabulary Object Attribute Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we consider the problem of simultaneously detecting objects and inferring their visual attributes in an image, even for those with no manual annotations provided at the training stage, resembling an open-vocabulary scenario. To achieve this goal, we make the following contributions: (i) we start with a naive two-stage approach for open-vocabulary object detection and attribute classification, termed CLIP-Attr. The candidate objects are first proposed with an offline RPN and later classified for semantic category and attributes; (ii) we combine all available datasets and train with a federated strategy to finetune the CLIP model, aligning the visual representation with attributes, additionally, we investigate the efficacy of leveraging freely available online image-caption pairs under weakly supervised learning; (iii) in pursuit of efficiency, we train a Faster-RCNN type model end-to-end with knowledge distillation, that performs class-agnostic object proposals and classification on semantic categories and attributes with classifiers generated from a text encoder; Finally, (iv) we conduct extensive experiments on VAW, MS-COCO, LSA, and OVAD datasets, and show that recognition of semantic category and attributes is complementary for visual scene understanding, i.e., jointly training object detection and attributes prediction largely outperform existing approaches that treat the two tasks independently, demonstrating strong generalization ability to novel attributes and categories.
<div id='section'>Paperid: <span id='pid'>473, <a href='https://arxiv.org/pdf/2503.20685.pdf' target='_blank'>https://arxiv.org/pdf/2503.20685.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhao Huang, Ao Chang, Haoran Dou, Xing Tao, Xinrui Zhou, Yan Cao, Ruobing Huang, Alejandro F Frangi, Lingyun Bao, Xin Yang, Dong Ni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20685">Flip Learning: Weakly Supervised Erase to Segment Nodules in Breast Ultrasound</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate segmentation of nodules in both 2D breast ultrasound (BUS) and 3D automated breast ultrasound (ABUS) is crucial for clinical diagnosis and treatment planning. Therefore, developing an automated system for nodule segmentation can enhance user independence and expedite clinical analysis. Unlike fully-supervised learning, weakly-supervised segmentation (WSS) can streamline the laborious and intricate annotation process. However, current WSS methods face challenges in achieving precise nodule segmentation, as many of them depend on inaccurate activation maps or inefficient pseudo-mask generation algorithms. In this study, we introduce a novel multi-agent reinforcement learning-based WSS framework called Flip Learning, which relies solely on 2D/3D boxes for accurate segmentation. Specifically, multiple agents are employed to erase the target from the box to facilitate classification tag flipping, with the erased region serving as the predicted segmentation mask. The key contributions of this research are as follows: (1) Adoption of a superpixel/supervoxel-based approach to encode the standardized environment, capturing boundary priors and expediting the learning process. (2) Introduction of three meticulously designed rewards, comprising a classification score reward and two intensity distribution rewards, to steer the agents' erasing process precisely, thereby avoiding both under- and over-segmentation. (3) Implementation of a progressive curriculum learning strategy to enable agents to interact with the environment in a progressively challenging manner, thereby enhancing learning efficiency. Extensively validated on the large in-house BUS and ABUS datasets, our Flip Learning method outperforms state-of-the-art WSS methods and foundation models, and achieves comparable performance as fully-supervised learning algorithms.
<div id='section'>Paperid: <span id='pid'>474, <a href='https://arxiv.org/pdf/2308.08269.pdf' target='_blank'>https://arxiv.org/pdf/2308.08269.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Zhou, Dong Ni, Ao Chang, Xinrui Zhou, Rusi Chen, Yanlin Chen, Lian Liu, Jiamin Liang, Yuhao Huang, Tong Han, Zhe Liu, Deng-Ping Fan, Xin Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.08269">OnUVS: Online Feature Decoupling Framework for High-Fidelity Ultrasound Video Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ultrasound (US) imaging is indispensable in clinical practice. To diagnose certain diseases, sonographers must observe corresponding dynamic anatomic structures to gather comprehensive information. However, the limited availability of specific US video cases causes teaching difficulties in identifying corresponding diseases, which potentially impacts the detection rate of such cases. The synthesis of US videos may represent a promising solution to this issue. Nevertheless, it is challenging to accurately animate the intricate motion of dynamic anatomic structures while preserving image fidelity. To address this, we present a novel online feature-decoupling framework called OnUVS for high-fidelity US video synthesis. Our highlights can be summarized by four aspects. First, we introduced anatomic information into keypoint learning through a weakly-supervised training strategy, resulting in improved preservation of anatomical integrity and motion while minimizing the labeling burden. Second, to better preserve the integrity and textural information of US images, we implemented a dual-decoder that decouples the content and textural features in the generator. Third, we adopted a multiple-feature discriminator to extract a comprehensive range of visual cues, thereby enhancing the sharpness and fine details of the generated videos. Fourth, we constrained the motion trajectories of keypoints during online learning to enhance the fluidity of generated videos. Our validation and user studies on in-house echocardiographic and pelvic floor US videos showed that OnUVS synthesizes US videos with high fidelity.
<div id='section'>Paperid: <span id='pid'>475, <a href='https://arxiv.org/pdf/2306.04160.pdf' target='_blank'>https://arxiv.org/pdf/2306.04160.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyi Cui, Weiran Huang, Yifei Wang, Yisen Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.04160">Rethinking Weak Supervision in Helping Contrastive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contrastive learning has shown outstanding performances in both supervised and unsupervised learning, and has recently been introduced to solve weakly supervised learning problems such as semi-supervised learning and noisy label learning. Despite the empirical evidence showing that semi-supervised labels improve the representations of contrastive learning, it remains unknown if noisy supervised information can be directly used in training instead of after manual denoising. Therefore, to explore the mechanical differences between semi-supervised and noisy-labeled information in helping contrastive learning, we establish a unified theoretical framework of contrastive learning under weak supervision. Specifically, we investigate the most intuitive paradigm of jointly training supervised and unsupervised contrastive losses. By translating the weakly supervised information into a similarity graph under the framework of spectral clustering based on the posterior probability of weak labels, we establish the downstream classification error bound. We prove that semi-supervised labels improve the downstream error bound whereas noisy labels have limited effects under such a paradigm. Our theoretical findings here provide new insights for the community to rethink the role of weak supervision in helping contrastive learning.
<div id='section'>Paperid: <span id='pid'>476, <a href='https://arxiv.org/pdf/2306.02548.pdf' target='_blank'>https://arxiv.org/pdf/2306.02548.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinrui Zhou, Yuhao Huang, Wufeng Xue, Xin Yang, Yuxin Zou, Qilong Ying, Yuanji Zhang, Jia Liu, Jie Ren, Dong Ni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.02548">Inflated 3D Convolution-Transformer for Weakly-supervised Carotid Stenosis Grading with Ultrasound Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Localization of the narrowest position of the vessel and corresponding vessel and remnant vessel delineation in carotid ultrasound (US) are essential for carotid stenosis grading (CSG) in clinical practice. However, the pipeline is time-consuming and tough due to the ambiguous boundaries of plaque and temporal variation. To automatize this procedure, a large number of manual delineations are usually required, which is not only laborious but also not reliable given the annotation difficulty. In this study, we present the first video classification framework for automatic CSG. Our contribution is three-fold. First, to avoid the requirement of laborious and unreliable annotation, we propose a novel and effective video classification network for weakly-supervised CSG. Second, to ease the model training, we adopt an inflation strategy for the network, where pre-trained 2D convolution weights can be adapted into the 3D counterpart in our network for an effective warm start. Third, to enhance the feature discrimination of the video, we propose a novel attention-guided multi-dimension fusion (AMDF) transformer encoder to model and integrate global dependencies within and across spatial and temporal dimensions, where two lightweight cross-dimensional attention mechanisms are designed. Our approach is extensively validated on a large clinically collected carotid US video dataset, demonstrating state-of-the-art performance compared with strong competitors.
<div id='section'>Paperid: <span id='pid'>477, <a href='https://arxiv.org/pdf/2411.16803.pdf' target='_blank'>https://arxiv.org/pdf/2411.16803.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marta Ligero, Tim Lenz, Georg WÃ¶lflein, Omar S. M. El Nahhas, Daniel Truhn, Jakob Nikolas Kather
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16803">Abnormality-Driven Representation Learning for Radiology Imaging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To date, the most common approach for radiology deep learning pipelines is the use of end-to-end 3D networks based on models pre-trained on other tasks, followed by fine-tuning on the task at hand. In contrast, adjacent medical fields such as pathology, which focus on 2D images, have effectively adopted task-agnostic foundational models based on self-supervised learning (SSL), combined with weakly-supervised deep learning (DL). However, the field of radiology still lacks task-agnostic representation models due to the computational and data demands of 3D imaging and the anatomical complexity inherent to radiology scans. To address this gap, we propose CLEAR, a framework for radiology images that uses extracted embeddings from 2D slices along with attention-based aggregation for efficiently predicting clinical endpoints. As part of this framework, we introduce lesion-enhanced contrastive learning (LeCL), a novel approach to obtain visual representations driven by abnormalities in 2D axial slices across different locations of the CT scans. Specifically, we trained single-domain contrastive learning approaches using three different architectures: Vision Transformers, Vision State Space Models and Gated Convolutional Neural Networks. We evaluate our approach across three clinical tasks: tumor lesion location, lung disease detection, and patient staging, benchmarking against four state-of-the-art foundation models, including BiomedCLIP. Our findings demonstrate that CLEAR using representations learned through LeCL, outperforms existing foundation models, while being substantially more compute- and data-efficient.
<div id='section'>Paperid: <span id='pid'>478, <a href='https://arxiv.org/pdf/2408.15823.pdf' target='_blank'>https://arxiv.org/pdf/2408.15823.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peter Neidlinger, Omar S. M. El Nahhas, Hannah Sophie Muti, Tim Lenz, Michael Hoffmeister, Hermann Brenner, Marko van Treeck, Rupert Langer, Bastian Dislich, Hans Michael Behrens, Christoph RÃ¶cken, Sebastian Foersch, Daniel Truhn, Antonio Marra, Oliver Lester Saldanha, Jakob Nikolas Kather
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.15823">Benchmarking foundation models as feature extractors for weakly-supervised computational pathology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advancements in artificial intelligence have driven the development of numerous pathology foundation models capable of extracting clinically relevant information. However, there is currently limited literature independently evaluating these foundation models on truly external cohorts and clinically-relevant tasks to uncover adjustments for future improvements. In this study, we benchmarked 19 histopathology foundation models on 13 patient cohorts with 6,818 patients and 9,528 slides from lung, colorectal, gastric, and breast cancers. The models were evaluated on weakly-supervised tasks related to biomarkers, morphological properties, and prognostic outcomes. We show that a vision-language foundation model, CONCH, yielded the highest performance when compared to vision-only foundation models, with Virchow2 as close second. The experiments reveal that foundation models trained on distinct cohorts learn complementary features to predict the same label, and can be fused to outperform the current state of the art. An ensemble combining CONCH and Virchow2 predictions outperformed individual models in 55% of tasks, leveraging their complementary strengths in classification scenarios. Moreover, our findings suggest that data diversity outweighs data volume for foundation models. Our work highlights actionable adjustments to improve pathology foundation models.
<div id='section'>Paperid: <span id='pid'>479, <a href='https://arxiv.org/pdf/2403.03891.pdf' target='_blank'>https://arxiv.org/pdf/2403.03891.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Omar S. M. El Nahhas, Georg WÃ¶lflein, Marta Ligero, Tim Lenz, Marko van Treeck, Firas Khader, Daniel Truhn, Jakob Nikolas Kather
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.03891">Joint multi-task learning improves weakly-supervised biomarker prediction in computational pathology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep Learning (DL) can predict biomarkers directly from digitized cancer histology in a weakly-supervised setting. Recently, the prediction of continuous biomarkers through regression-based DL has seen an increasing interest. Nonetheless, clinical decision making often requires a categorical outcome. Consequently, we developed a weakly-supervised joint multi-task Transformer architecture which has been trained and evaluated on four public patient cohorts for the prediction of two key predictive biomarkers, microsatellite instability (MSI) and homologous recombination deficiency (HRD), trained with auxiliary regression tasks related to the tumor microenvironment. Moreover, we perform a comprehensive benchmark of 16 approaches of task balancing for weakly-supervised joint multi-task learning in computational pathology. Using our novel approach, we improve over the state-of-the-art area under the receiver operating characteristic by +7.7% and +4.1%, as well as yielding better clustering of latent embeddings by +8% and +5% for the prediction of MSI and HRD in external cohorts, respectively.
<div id='section'>Paperid: <span id='pid'>480, <a href='https://arxiv.org/pdf/2311.11772.pdf' target='_blank'>https://arxiv.org/pdf/2311.11772.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Georg WÃ¶lflein, Dyke Ferber, Asier R. Meneghetti, Omar S. M. El Nahhas, Daniel Truhn, Zunamys I. Carrero, David J. Harrison, Ognjen ArandjeloviÄ, Jakob Nikolas Kather
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.11772">Benchmarking Pathology Feature Extractors for Whole Slide Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised whole slide image classification is a key task in computational pathology, which involves predicting a slide-level label from a set of image patches constituting the slide. Constructing models to solve this task involves multiple design choices, often made without robust empirical or conclusive theoretical justification. To address this, we conduct a comprehensive benchmarking of feature extractors to answer three critical questions: 1) Is stain normalisation still a necessary preprocessing step? 2) Which feature extractors are best for downstream slide-level classification? 3) How does magnification affect downstream performance? Our study constitutes the most comprehensive evaluation of publicly available pathology feature extractors to date, involving more than 10,000 training runs across 14 feature extractors, 9 tasks, 5 datasets, 3 downstream architectures, 2 levels of magnification, and various preprocessing setups. Our findings challenge existing assumptions: 1) We observe empirically, and by analysing the latent space, that skipping stain normalisation and image augmentations does not degrade performance, while significantly reducing memory and computational demands. 2) We develop a novel evaluation metric to compare relative downstream performance, and show that the choice of feature extractor is the most consequential factor for downstream performance. 3) We find that lower-magnification slides are sufficient for accurate slide-level classification. Contrary to previous patch-level benchmarking studies, our approach emphasises clinical relevance by focusing on slide-level biomarker prediction tasks in a weakly supervised setting with external validation cohorts. Our findings stand to streamline digital pathology workflows by minimising preprocessing needs and informing the selection of feature extractors.
<div id='section'>Paperid: <span id='pid'>481, <a href='https://arxiv.org/pdf/2304.14293.pdf' target='_blank'>https://arxiv.org/pdf/2304.14293.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wangchunshu Zhou, Yuchen Eleanor Jiang, Ethan Wilcox, Ryan Cotterell, Mrinmaya Sachan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.14293">Controlled Text Generation with Natural Language Instructions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models generate fluent texts and can follow natural language instructions to solve a wide range of tasks without task-specific training. Nevertheless, it is notoriously difficult to control their generation to satisfy the various constraints required by different applications. In this work, we present InstructCTG, a controlled text generation framework that incorporates different constraints by conditioning on natural language descriptions and demonstrations of the constraints. In particular, we first extract the underlying constraints of natural texts through a combination of off-the-shelf NLP tools and simple heuristics. We then verbalize the constraints into natural language instructions to form weakly supervised training data. By prepending natural language descriptions of the constraints and a few demonstrations, we fine-tune a pre-trained language model to incorporate various types of constraints. Compared to existing search-based or score-based methods, InstructCTG is more flexible to different constraint types and has a much smaller impact on the generation quality and speed because it does not modify the decoding procedure. Additionally, InstructCTG allows the model to adapt to new constraints without re-training through the use of few-shot task generalization and in-context learning abilities of instruction-tuned language models.
<div id='section'>Paperid: <span id='pid'>482, <a href='https://arxiv.org/pdf/2409.11223.pdf' target='_blank'>https://arxiv.org/pdf/2409.11223.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuta Kaneko, Abu Saleh Musa Miah, Najmul Hassan, Hyoun-Sup Lee, Si-Woong Jang, Jungpil Shin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11223">Multimodal Attention-Enhanced Feature Fusion-based Weekly Supervised Anomaly Violence Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised video anomaly detection (WS-VAD) is a crucial area in computer vision for developing intelligent surveillance systems. This system uses three feature streams: RGB video, optical flow, and audio signals, where each stream extracts complementary spatial and temporal features using an enhanced attention module to improve detection accuracy and robustness. In the first stream, we employed an attention-based, multi-stage feature enhancement approach to improve spatial and temporal features from the RGB video where the first stage consists of a ViT-based CLIP module, with top-k features concatenated in parallel with I3D and Temporal Contextual Aggregation (TCA) based rich spatiotemporal features. The second stage effectively captures temporal dependencies using the Uncertainty-Regulated Dual Memory Units (UR-DMU) model, which learns representations of normal and abnormal data simultaneously, and the third stage is employed to select the most relevant spatiotemporal features. The second stream extracted enhanced attention-based spatiotemporal features from the flow data modality-based feature by taking advantage of the integration of the deep learning and attention module. The audio stream captures auditory cues using an attention module integrated with the VGGish model, aiming to detect anomalies based on sound patterns. These streams enrich the model by incorporating motion and audio signals often indicative of abnormal events undetectable through visual analysis alone. The concatenation of the multimodal fusion leverages the strengths of each modality, resulting in a comprehensive feature set that significantly improves anomaly detection accuracy and robustness across three datasets. The extensive experiment and high performance with the three benchmark datasets proved the effectiveness of the proposed system over the existing state-of-the-art system.
<div id='section'>Paperid: <span id='pid'>483, <a href='https://arxiv.org/pdf/2312.12437.pdf' target='_blank'>https://arxiv.org/pdf/2312.12437.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianghang Lin, Yunhang Shen, Bingquan Wang, Shaohui Lin, Ke Li, Liujuan Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.12437">Weakly Supervised Open-Vocabulary Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite weakly supervised object detection (WSOD) being a promising step toward evading strong instance-level annotations, its capability is confined to closed-set categories within a single training dataset. In this paper, we propose a novel weakly supervised open-vocabulary object detection framework, namely WSOVOD, to extend traditional WSOD to detect novel concepts and utilize diverse datasets with only image-level annotations. To achieve this, we explore three vital strategies, including dataset-level feature adaptation, image-level salient object localization, and region-level vision-language alignment. First, we perform data-aware feature extraction to produce an input-conditional coefficient, which is leveraged into dataset attribute prototypes to identify dataset bias and help achieve cross-dataset generalization. Second, a customized location-oriented weakly supervised region proposal network is proposed to utilize high-level semantic layouts from the category-agnostic segment anything model to distinguish object boundaries. Lastly, we introduce a proposal-concept synchronized multiple-instance network, i.e., object mining and refinement with visual-semantic alignment, to discover objects matched to the text embeddings of concepts. Extensive experiments on Pascal VOC and MS COCO demonstrate that the proposed WSOVOD achieves new state-of-the-art compared with previous WSOD methods in both close-set object localization and detection tasks. Meanwhile, WSOVOD enables cross-dataset and open-vocabulary learning to achieve on-par or even better performance than well-established fully-supervised open-vocabulary object detection (FSOVOD).
<div id='section'>Paperid: <span id='pid'>484, <a href='https://arxiv.org/pdf/2305.12606.pdf' target='_blank'>https://arxiv.org/pdf/2305.12606.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrew Rouditchenko, Sameer Khurana, Samuel Thomas, Rogerio Feris, Leonid Karlinsky, Hilde Kuehne, David Harwath, Brian Kingsbury, James Glass
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.12606">Comparison of Multilingual Self-Supervised and Weakly-Supervised Speech Pre-Training for Adaptation to Unseen Languages</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent models such as XLS-R and Whisper have made multilingual speech technologies more accessible by pre-training on audio from around 100 spoken languages each. However, there are thousands of spoken languages worldwide, and adapting to new languages is an important problem. In this work, we aim to understand which model adapts better to languages unseen during pre-training. We fine-tune both models on 13 unseen languages and 18 seen languages. Our results show that the number of hours seen per language and language family during pre-training is predictive of how the models compare, despite the significant differences in the pre-training methods.
<div id='section'>Paperid: <span id='pid'>485, <a href='https://arxiv.org/pdf/2107.08228.pdf' target='_blank'>https://arxiv.org/pdf/2107.08228.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lisha Tang, Yi Wang, Lap-Pui Chau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2107.08228">Weakly-supervised Part-Attention and Mentored Networks for Vehicle Re-Identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vehicle re-identification (Re-ID) aims to retrieve images with the same vehicle ID across different cameras. Current part-level feature learning methods typically detect vehicle parts via uniform division, outside tools, or attention modeling. However, such part features often require expensive additional annotations and cause sub-optimal performance in case of unreliable part mask predictions. In this paper, we propose a weakly-supervised Part-Attention Network (PANet) and Part-Mentored Network (PMNet) for Vehicle Re-ID. Firstly, PANet localizes vehicle parts via part-relevant channel recalibration and cluster-based mask generation without vehicle part supervisory information. Secondly, PMNet leverages teacher-student guided learning to distill vehicle part-specific features from PANet and performs multi-scale global-part feature extraction. During inference, PMNet can adaptively extract discriminative part features without part localization by PANet, preventing unstable part mask predictions. We address this Re-ID issue as a multi-task problem and adopt Homoscedastic Uncertainty to learn the optimal weighing of ID losses. Experiments are conducted on two public benchmarks, showing that our approach outperforms recent methods, which require no extra annotations by an average increase of 3.0% in CMC@5 on VehicleID and over 1.4% in mAP on VeRi776. Moreover, our method can extend to the occluded vehicle Re-ID task and exhibits good generalization ability.
<div id='section'>Paperid: <span id='pid'>486, <a href='https://arxiv.org/pdf/2309.10649.pdf' target='_blank'>https://arxiv.org/pdf/2309.10649.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyu Zhang, Huitong Yang, Dai-Jie Wu, Jacky Keung, Xuesong Li, Xinge Zhu, Yuexin Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.10649">Cross-modal and Cross-domain Knowledge Transfer for Label-free 3D Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current state-of-the-art point cloud-based perception methods usually rely on large-scale labeled data, which requires expensive manual annotations. A natural option is to explore the unsupervised methodology for 3D perception tasks. However, such methods often face substantial performance-drop difficulties. Fortunately, we found that there exist amounts of image-based datasets and an alternative can be proposed, i.e., transferring the knowledge in the 2D images to 3D point clouds. Specifically, we propose a novel approach for the challenging cross-modal and cross-domain adaptation task by fully exploring the relationship between images and point clouds and designing effective feature alignment strategies. Without any 3D labels, our method achieves state-of-the-art performance for 3D point cloud semantic segmentation on SemanticKITTI by using the knowledge of KITTI360 and GTA5, compared to existing unsupervised and weakly-supervised baselines.
<div id='section'>Paperid: <span id='pid'>487, <a href='https://arxiv.org/pdf/2203.03668.pdf' target='_blank'>https://arxiv.org/pdf/2203.03668.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Felix Friedrich, Wolfgang Stammer, Patrick Schramowski, Kristian Kersting
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.03668">A Typology for Exploring the Mitigation of Shortcut Behavior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As machine learning models become increasingly larger, trained weakly supervised on large, possibly uncurated data sets, it becomes increasingly important to establish mechanisms for inspecting, interacting, and revising models to mitigate learning shortcuts and guarantee their learned knowledge is aligned with human knowledge. The recently proposed XIL framework was developed for this purpose, and several such methods have been introduced, each with individual motivations and methodological details. In this work, we provide a unification of various XIL methods into a single typology by establishing a common set of basic modules. In doing so, we pave the way for a principled comparison of existing, but, importantly, also future XIL approaches. In addition, we discuss existing and introduce novel measures and benchmarks for evaluating the overall abilities of a XIL method. Given this extensive toolbox, including our typology, measures, and benchmarks, we finally compare several recent XIL methods methodologically and quantitatively. In our evaluations, all methods prove to revise a model successfully. However, we found remarkable differences in individual benchmark tasks, revealing valuable application-relevant aspects for integrating these benchmarks in developing future methods.
<div id='section'>Paperid: <span id='pid'>488, <a href='https://arxiv.org/pdf/2510.10111.pdf' target='_blank'>https://arxiv.org/pdf/2510.10111.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Chen, Bin Liu, Changtao Miao, Xinghao Wang, Yi Li, Tao Gong, Qi Chu, Nenghai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10111">Training-Free In-Context Forensic Chain for Image Manipulation Detection and Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advances in image tampering pose serious security threats, underscoring the need for effective image manipulation localization (IML). While supervised IML achieves strong performance, it depends on costly pixel-level annotations. Existing weakly supervised or training-free alternatives often underperform and lack interpretability. We propose the In-Context Forensic Chain (ICFC), a training-free framework that leverages multi-modal large language models (MLLMs) for interpretable IML tasks. ICFC integrates an objectified rule construction with adaptive filtering to build a reliable knowledge base and a multi-step progressive reasoning pipeline that mirrors expert forensic workflows from coarse proposals to fine-grained forensics results. This design enables systematic exploitation of MLLM reasoning for image-level classification, pixel-level localization, and text-level interpretability. Across multiple benchmarks, ICFC not only surpasses state-of-the-art training-free methods but also achieves competitive or superior performance compared to weakly and fully supervised approaches.
<div id='section'>Paperid: <span id='pid'>489, <a href='https://arxiv.org/pdf/2503.20294.pdf' target='_blank'>https://arxiv.org/pdf/2503.20294.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinghao Wang, Tao Gong, Qi Chu, Bin Liu, Nenghai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20294">Context-Aware Weakly Supervised Image Manipulation Localization with SAM Refinement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Malicious image manipulation poses societal risks, increasing the importance of effective image manipulation detection methods. Recent approaches in image manipulation detection have largely been driven by fully supervised approaches, which require labor-intensive pixel-level annotations. Thus, it is essential to explore weakly supervised image manipulation localization methods that only require image-level binary labels for training. However, existing weakly supervised image manipulation methods overlook the importance of edge information for accurate localization, leading to suboptimal localization performance. To address this, we propose a Context-Aware Boundary Localization (CABL) module to aggregate boundary features and learn context-inconsistency for localizing manipulated areas. Furthermore, by leveraging Class Activation Mapping (CAM) and Segment Anything Model (SAM), we introduce the CAM-Guided SAM Refinement (CGSR) module to generate more accurate manipulation localization maps. By integrating two modules, we present a novel weakly supervised framework based on a dual-branch Transformer-CNN architecture. Our method achieves outstanding localization performance across multiple datasets.
<div id='section'>Paperid: <span id='pid'>490, <a href='https://arxiv.org/pdf/2401.07854.pdf' target='_blank'>https://arxiv.org/pdf/2401.07854.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Quan Liu, Jiawen Yao, Lisha Yao, Xin Chen, Jingren Zhou, Le Lu, Ling Zhang, Zaiyi Liu, Yuankai Huo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.07854">$M^{2}$Fusion: Bayesian-based Multimodal Multi-level Fusion on Colorectal Cancer Microsatellite Instability Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Colorectal cancer (CRC) micro-satellite instability (MSI) prediction on histopathology images is a challenging weakly supervised learning task that involves multi-instance learning on gigapixel images. To date, radiology images have proven to have CRC MSI information and efficient patient imaging techniques. Different data modalities integration offers the opportunity to increase the accuracy and robustness of MSI prediction. Despite the progress in representation learning from the whole slide images (WSI) and exploring the potential of making use of radiology data, CRC MSI prediction remains a challenge to fuse the information from multiple data modalities (e.g., pathology WSI and radiology CT image). In this paper, we propose $M^{2}$Fusion: a Bayesian-based multimodal multi-level fusion pipeline for CRC MSI. The proposed fusion model $M^{2}$Fusion is capable of discovering more novel patterns within and across modalities that are beneficial for predicting MSI than using a single modality alone, as well as other fusion methods. The contribution of the paper is three-fold: (1) $M^{2}$Fusion is the first pipeline of multi-level fusion on pathology WSI and 3D radiology CT image for MSI prediction; (2) CT images are the first time integrated into multimodal fusion for CRC MSI prediction; (3) feature-level fusion strategy is evaluated on both Transformer-based and CNN-based method. Our approach is validated on cross-validation of 352 cases and outperforms either feature-level (0.8177 vs. 0.7908) or decision-level fusion strategy (0.8177 vs. 0.7289) on AUC score.
<div id='section'>Paperid: <span id='pid'>491, <a href='https://arxiv.org/pdf/2502.09967.pdf' target='_blank'>https://arxiv.org/pdf/2502.09967.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuming Wang, Yihao Zheng, Jiarui Li, Yaofei Wu, Yan Huang, Zun Li, Lifang Wu, Liang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09967">VicKAM: Visual Conceptual Knowledge Guided Action Map for Weakly Supervised Group Activity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing weakly supervised group activity recognition methods rely on object detectors or attention mechanisms to capture key areas automatically. However, they overlook the semantic information associated with captured areas, which may adversely affect the recognition performance. In this paper, we propose a novel framework named Visual Conceptual Knowledge Guided Action Map (VicKAM) which effectively captures the locations of individual actions and integrates them with action semantics for weakly supervised group activity recognition.It generates individual action prototypes from training set as visual conceptual knowledge to bridge action semantics and visual representations. Guided by this knowledge, VicKAM produces action maps that indicate the likelihood of each action occurring at various locations, based on image correlation theorem. It further augments individual action maps using group activity related statistical information, representing individual action distribution under different group activities, to establish connections between action maps and specific group activities. The augmented action map is incorporated with action semantic representations for group activity recognition.Extensive experiments on two public benchmarks, the Volleyball and the NBA datasets, demonstrate the effectiveness of our proposed method, even in cases of limited training data. The code will be released later.
<div id='section'>Paperid: <span id='pid'>492, <a href='https://arxiv.org/pdf/2407.16182.pdf' target='_blank'>https://arxiv.org/pdf/2407.16182.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Chen, Fanman Meng, Chenhao Wu, Haoran Wei, Runtong Zhang, Qingbo Wu, Linfeng Xu, Hongliang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.16182">No Re-Train, More Gain: Upgrading Backbones with Diffusion model for Pixel-Wise and Weakly-Supervised Few-Shot Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Few-Shot Segmentation (FSS) aims to segment novel classes using only a few annotated images. Despite considerable progress under pixel-wise support annotation, current FSS methods still face three issues: the inflexibility of backbone upgrade without re-training, the inability to uniformly handle various types of annotations (e.g., scribble, bounding box, mask, and text), and the difficulty in accommodating different annotation quantity. To address these issues simultaneously, we propose DiffUp, a novel framework that conceptualizes the FSS task as a conditional generative problem using a diffusion process. For the first issue, we introduce a backbone-agnostic feature transformation module that converts different segmentation cues into unified coarse priors, facilitating seamless backbone upgrade without re-training. For the second issue, due to the varying granularity of transformed priors from diverse annotation types (scribble, bounding box, mask, and text), we conceptualize these multi-granular transformed priors as analogous to noisy intermediates at different steps of a diffusion model. This is implemented via a self-conditioned modulation block coupled with a dual-level quality modulation branch. For the third issue, we incorporate an uncertainty-aware information fusion module to harmonize the variability across zero-shot, one-shot, and many-shot scenarios. Evaluated through rigorous benchmarks, DiffUp significantly outperforms existing FSS models in terms of flexibility and accuracy.
<div id='section'>Paperid: <span id='pid'>493, <a href='https://arxiv.org/pdf/2401.00368.pdf' target='_blank'>https://arxiv.org/pdf/2401.00368.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.00368">Improving Text Embeddings with Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce a novel and simple method for obtaining high-quality text embeddings using only synthetic data and less than 1k training steps. Unlike existing methods that often depend on multi-stage intermediate pre-training with billions of weakly-supervised text pairs, followed by fine-tuning with a few labeled datasets, our method does not require building complex training pipelines or relying on manually collected datasets that are often constrained by task diversity and language coverage. We leverage proprietary LLMs to generate diverse synthetic data for hundreds of thousands of text embedding tasks across 93 languages. We then fine-tune open-source decoder-only LLMs on the synthetic data using standard contrastive loss. Experiments demonstrate that our method achieves strong performance on highly competitive text embedding benchmarks without using any labeled data. Furthermore, when fine-tuned with a mixture of synthetic and labeled data, our model sets new state-of-the-art results on the BEIR and MTEB benchmarks.
<div id='section'>Paperid: <span id='pid'>494, <a href='https://arxiv.org/pdf/2212.03533.pdf' target='_blank'>https://arxiv.org/pdf/2212.03533.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.03533">Text Embeddings by Weakly-Supervised Contrastive Pre-training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents E5, a family of state-of-the-art text embeddings that transfer well to a wide range of tasks. The model is trained in a contrastive manner with weak supervision signals from our curated large-scale text pair dataset (called CCPairs). E5 can be readily used as a general-purpose embedding model for any tasks requiring a single-vector representation of texts such as retrieval, clustering, and classification, achieving strong performance in both zero-shot and fine-tuned settings. We conduct extensive evaluations on 56 datasets from the BEIR and MTEB benchmarks. For zero-shot settings, E5 is the first model that outperforms the strong BM25 baseline on the BEIR retrieval benchmark without using any labeled data. When fine-tuned, E5 obtains the best results on the MTEB benchmark, beating existing embedding models with 40x more parameters.
<div id='section'>Paperid: <span id='pid'>495, <a href='https://arxiv.org/pdf/2509.26004.pdf' target='_blank'>https://arxiv.org/pdf/2509.26004.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicola Messina, Rosario Leonardi, Luca Ciampi, Fabio Carrara, Giovanni Maria Farinella, Fabrizio Falchi, Antonino Furnari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26004">Learning Egocentric In-Hand Object Segmentation through Weak Supervision from Human Narrations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pixel-level recognition of objects manipulated by the user from egocentric images enables key applications spanning assistive technologies, industrial safety, and activity monitoring. However, progress in this area is currently hindered by the scarcity of annotated datasets, as existing approaches rely on costly manual labels. In this paper, we propose to learn human-object interaction detection leveraging narrations -- natural language descriptions of the actions performed by the camera wearer which contain clues about manipulated objects (e.g., "I am pouring vegetables from the chopping board to the pan"). Narrations provide a form of weak supervision that is cheap to acquire and readily available in state-of-the-art egocentric datasets. We introduce Narration-Supervised in-Hand Object Segmentation (NS-iHOS), a novel task where models have to learn to segment in-hand objects by learning from natural-language narrations. Narrations are then not employed at inference time. We showcase the potential of the task by proposing Weakly-Supervised In-hand Object Segmentation from Human Narrations (WISH), an end-to-end model distilling knowledge from narrations to learn plausible hand-object associations and enable in-hand object segmentation without using narrations at test time. We benchmark WISH against different baselines based on open-vocabulary object detectors and vision-language models, showing the superiority of its design. Experiments on EPIC-Kitchens and Ego4D show that WISH surpasses all baselines, recovering more than 50% of the performance of fully supervised methods, without employing fine-grained pixel-wise annotations.
<div id='section'>Paperid: <span id='pid'>496, <a href='https://arxiv.org/pdf/2405.14230.pdf' target='_blank'>https://arxiv.org/pdf/2405.14230.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangyu Guo, Jiawen Yao, Yingda Xia, Tony C. W. Mok, Zhilin Zheng, Junwei Han, Le Lu, Dingwen Zhang, Jian Zhou, Ling Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14230">Boosting Medical Image-based Cancer Detection via Text-guided Supervision from Reports</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The absence of adequately sufficient expert-level tumor annotations hinders the effectiveness of supervised learning based opportunistic cancer screening on medical imaging. Clinical reports (that are rich in descriptive textual details) can offer a "free lunch'' supervision information and provide tumor location as a type of weak label to cope with screening tasks, thus saving human labeling workloads, if properly leveraged. However, predicting cancer only using such weak labels can be very changeling since tumors are usually presented in small anatomical regions compared to the whole 3D medical scans. Weakly semi-supervised learning (WSSL) utilizes a limited set of voxel-level tumor annotations and incorporates alongside a substantial number of medical images that have only off-the-shelf clinical reports, which may strike a good balance between minimizing expert annotation workload and optimizing screening efficacy. In this paper, we propose a novel text-guided learning method to achieve highly accurate cancer detection results. Through integrating diagnostic and tumor location text prompts into the text encoder of a vision-language model (VLM), optimization of weakly supervised learning can be effectively performed in the latent space of VLM, thereby enhancing the stability of training. Our approach can leverage clinical knowledge by large-scale pre-trained VLM to enhance generalization ability, and produce reliable pseudo tumor masks to improve cancer detection. Our extensive quantitative experimental results on a large-scale cancer dataset, including 1,651 unique patients, validate that our approach can reduce human annotation efforts by at least 70% while maintaining comparable cancer detection accuracy to competing fully supervised methods (AUC value 0.961 versus 0.966).
<div id='section'>Paperid: <span id='pid'>497, <a href='https://arxiv.org/pdf/2303.07806.pdf' target='_blank'>https://arxiv.org/pdf/2303.07806.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zelin Peng, Guanchun Wang, Lingxi Xie, Dongsheng Jiang, Wei Shen, Qi Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.07806">USAGE: A Unified Seed Area Generation Paradigm for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Seed area generation is usually the starting point of weakly supervised semantic segmentation (WSSS). Computing the Class Activation Map (CAM) from a multi-label classification network is the de facto paradigm for seed area generation, but CAMs generated from Convolutional Neural Networks (CNNs) and Transformers are prone to be under- and over-activated, respectively, which makes the strategies to refine CAMs for CNNs usually inappropriate for Transformers, and vice versa. In this paper, we propose a Unified optimization paradigm for Seed Area GEneration (USAGE) for both types of networks, in which the objective function to be optimized consists of two terms: One is a generation loss, which controls the shape of seed areas by a temperature parameter following a deterministic principle for different types of networks; The other is a regularization loss, which ensures the consistency between the seed areas that are generated by self-adaptive network adjustment from different views, to overturn false activation in seed areas. Experimental results show that USAGE consistently improves seed area generation for both CNNs and Transformers by large margins, e.g., outperforming state-of-the-art methods by a mIoU of 4.1% on PASCAL VOC. Moreover, based on the USAGE-generated seed areas on Transformers, we achieve state-of-the-art WSSS results on both PASCAL VOC and MS COCO.
<div id='section'>Paperid: <span id='pid'>498, <a href='https://arxiv.org/pdf/2211.12268.pdf' target='_blank'>https://arxiv.org/pdf/2211.12268.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zesen Cheng, Pengchong Qiao, Kehan Li, Siheng Li, Pengxu Wei, Xiangyang Ji, Li Yuan, Chang Liu, Jie Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.12268">Out-of-Candidate Rectification for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation is typically inspired by class activation maps, which serve as pseudo masks with class-discriminative regions highlighted. Although tremendous efforts have been made to recall precise and complete locations for each class, existing methods still commonly suffer from the unsolicited Out-of-Candidate (OC) error predictions that not belongs to the label candidates, which could be avoidable since the contradiction with image-level class tags is easy to be detected. In this paper, we develop a group ranking-based Out-of-Candidate Rectification (OCR) mechanism in a plug-and-play fashion. Firstly, we adaptively split the semantic categories into In-Candidate (IC) and OC groups for each OC pixel according to their prior annotation correlation and posterior prediction correlation. Then, we derive a differentiable rectification loss to force OC pixels to shift to the IC group. Incorporating our OCR with seminal baselines (e.g., AffinityNet, SEAM, MCTformer), we can achieve remarkable performance gains on both Pascal VOC (+3.2%, +3.3%, +0.8% mIoU) and MS COCO (+1.0%, +1.3%, +0.5% mIoU) datasets with negligible extra training overhead, which justifies the effectiveness and generality of our OCR.
<div id='section'>Paperid: <span id='pid'>499, <a href='https://arxiv.org/pdf/2303.12332.pdf' target='_blank'>https://arxiv.org/pdf/2303.12332.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wulian Yun, Mengshi Qi, Chuanming Wang, Huadong Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.12332">Weakly-Supervised Temporal Action Localization by Inferring Salient Snippet-Feature</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised temporal action localization aims to locate action regions and identify action categories in untrimmed videos simultaneously by taking only video-level labels as the supervision. Pseudo label generation is a promising strategy to solve the challenging problem, but the current methods ignore the natural temporal structure of the video that can provide rich information to assist such a generation process. In this paper, we propose a novel weakly-supervised temporal action localization method by inferring salient snippet-feature. First, we design a saliency inference module that exploits the variation relationship between temporal neighbor snippets to discover salient snippet-features, which can reflect the significant dynamic change in the video. Secondly, we introduce a boundary refinement module that enhances salient snippet-features through the information interaction unit. Then, a discrimination enhancement module is introduced to enhance the discriminative nature of snippet-features. Finally, we adopt the refined snippet-features to produce high-fidelity pseudo labels, which could be used to supervise the training of the action localization network. Extensive experiments on two publicly available datasets, i.e., THUMOS14 and ActivityNet v1.3, demonstrate our proposed method achieves significant improvements compared to the state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>500, <a href='https://arxiv.org/pdf/2511.19527.pdf' target='_blank'>https://arxiv.org/pdf/2511.19527.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyu Lyu, Thomas Monninger, Julie Stephany Berrio Perez, Mao Shan, Zhenxing Ming, Stewart Worrall
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19527">MapRF: Weakly Supervised Online HD Map Construction via NeRF-Guided Self-Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous driving systems benefit from high-definition (HD) maps that provide critical information about road infrastructure. The online construction of HD maps offers a scalable approach to generate local maps from on-board sensors. However, existing methods typically rely on costly 3D map annotations for training, which limits their generalization and scalability across diverse driving environments. In this work, we propose MapRF, a weakly supervised framework that learns to construct 3D maps using only 2D image labels. To generate high-quality pseudo labels, we introduce a novel Neural Radiance Fields (NeRF) module conditioned on map predictions, which reconstructs view-consistent 3D geometry and semantics. These pseudo labels are then iteratively used to refine the map network in a self-training manner, enabling progressive improvement without additional supervision. Furthermore, to mitigate error accumulation during self-training, we propose a Map-to-Ray Matching strategy that aligns map predictions with camera rays derived from 2D labels. Extensive experiments on the Argoverse 2 and nuScenes datasets demonstrate that MapRF achieves performance comparable to fully supervised methods, attaining around 75% of the baseline while surpassing several approaches using only 2D labels. This highlights the potential of MapRF to enable scalable and cost-effective online HD map construction for autonomous driving.
<div id='section'>Paperid: <span id='pid'>501, <a href='https://arxiv.org/pdf/2509.11312.pdf' target='_blank'>https://arxiv.org/pdf/2509.11312.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenchao Gu, Yupan Chen, Yanlin Wang, Hongyu Zhang, Cuiyun Gao, Michael R. Lyu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11312">Weakly Supervised Vulnerability Localization via Multiple Instance Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Software vulnerability detection has emerged as a significant concern in the field of software security recently, capturing the attention of numerous researchers and developers. Most previous approaches focus on coarse-grained vulnerability detection, such as at the function or file level. However, the developers would still encounter the challenge of manually inspecting a large volume of code inside the vulnerable function to identify the specific vulnerable statements for modification, indicating the importance of vulnerability localization. Training the model for vulnerability localization usually requires ground-truth labels at the statement-level, and labeling vulnerable statements demands expert knowledge, which incurs high costs. Hence, the demand for an approach that eliminates the need for additional labeling at the statement-level is on the rise. To tackle this problem, we propose a novel approach called WAVES for WeAkly supervised Vulnerability Localization via multiplE inStance learning, which does not need the additional statement-level labels during the training. WAVES has the capability to determine whether a function is vulnerable (i.e., vulnerability detection) and pinpoint the vulnerable statements (i.e., vulnerability localization). Specifically, inspired by the concept of multiple instance learning, WAVES converts the ground-truth label at the function-level into pseudo labels for individual statements, eliminating the need for additional statement-level labeling. These pseudo labels are utilized to train the classifiers for the function-level representation vectors. Extensive experimentation on three popular benchmark datasets demonstrates that, in comparison to previous baselines, our approach achieves comparable performance in vulnerability detection and state-of-the-art performance in statement-level vulnerability localization.
<div id='section'>Paperid: <span id='pid'>502, <a href='https://arxiv.org/pdf/2412.19650.pdf' target='_blank'>https://arxiv.org/pdf/2412.19650.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongxing Xu, Feilong Tang, Zhe Chen, Yingxue Su, Zhiyi Zhao, Ge Zhang, Jionglong Su, Zongyuan Ge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19650">Toward Modality Gap: Vision Prototype Learning for Weakly-supervised Semantic Segmentation with CLIP</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The application of Contrastive Language-Image Pre-training (CLIP) in Weakly Supervised Semantic Segmentation (WSSS) research powerful cross-modal semantic understanding capabilities. Existing methods attempt to optimize input text prompts for improved alignment of images and text, by finely adjusting text prototypes to facilitate semantic matching. Nevertheless, given the modality gap between text and vision spaces, the text prototypes employed by these methods have not effectively established a close correspondence with pixel-level vision features. In this work, our theoretical analysis indicates that the inherent modality gap results in misalignment of text and region features, and that this gap cannot be sufficiently reduced by minimizing contrast loss in CLIP. To mitigate the impact of the modality gap, we propose a Vision Prototype Learning (VPL) framework, by introducing more representative vision prototypes. The core of this framework is to learn class-specific vision prototypes in vision space with the help of text prototypes, for capturing high-quality localization maps. Moreover, we propose a regional semantic contrast module that contrasts regions embedding with corresponding prototypes, leading to more comprehensive and robust feature learning. Experimental results show that our proposed framework achieves state-of-the-art performance on two benchmark datasets.
<div id='section'>Paperid: <span id='pid'>503, <a href='https://arxiv.org/pdf/2510.14532.pdf' target='_blank'>https://arxiv.org/pdf/2510.14532.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinrui Huang, Fan Xiao, Dongming He, Anqi Gao, Dandan Li, Xiaofan Zhang, Shaoting Zhang, Xudong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.14532">Towards Generalist Intelligence in Dentistry: Vision Foundation Models for Oral and Maxillofacial Radiology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Oral and maxillofacial radiology plays a vital role in dental healthcare, but radiographic image interpretation is limited by a shortage of trained professionals. While AI approaches have shown promise, existing dental AI systems are restricted by their single-modality focus, task-specific design, and reliance on costly labeled data, hindering their generalization across diverse clinical scenarios. To address these challenges, we introduce DentVFM, the first family of vision foundation models (VFMs) designed for dentistry. DentVFM generates task-agnostic visual representations for a wide range of dental applications and uses self-supervised learning on DentVista, a large curated dental imaging dataset with approximately 1.6 million multi-modal radiographic images from various medical centers. DentVFM includes 2D and 3D variants based on the Vision Transformer (ViT) architecture. To address gaps in dental intelligence assessment and benchmarks, we introduce DentBench, a comprehensive benchmark covering eight dental subspecialties, more diseases, imaging modalities, and a wide geographical distribution. DentVFM shows impressive generalist intelligence, demonstrating robust generalization to diverse dental tasks, such as disease diagnosis, treatment analysis, biomarker identification, and anatomical landmark detection and segmentation. Experimental results indicate DentVFM significantly outperforms supervised, self-supervised, and weakly supervised baselines, offering superior generalization, label efficiency, and scalability. Additionally, DentVFM enables cross-modality diagnostics, providing more reliable results than experienced dentists in situations where conventional imaging is unavailable. DentVFM sets a new paradigm for dental AI, offering a scalable, adaptable, and label-efficient model to improve intelligent dental healthcare and address critical gaps in global oral healthcare.
<div id='section'>Paperid: <span id='pid'>504, <a href='https://arxiv.org/pdf/2504.04495.pdf' target='_blank'>https://arxiv.org/pdf/2504.04495.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Wu, Wanshun Su, Guansong Pang, Yujia Sun, Qingsen Yan, Peng Wang, Yanning Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04495">AVadCLIP: Audio-Visual Collaboration for Robust Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the increasing adoption of video anomaly detection in intelligent surveillance domains, conventional visual-based detection approaches often struggle with information insufficiency and high false-positive rates in complex environments. To address these limitations, we present a novel weakly supervised framework that leverages audio-visual collaboration for robust video anomaly detection. Capitalizing on the exceptional cross-modal representation learning capabilities of Contrastive Language-Image Pretraining (CLIP) across visual, audio, and textual domains, our framework introduces two major innovations: an efficient audio-visual fusion that enables adaptive cross-modal integration through lightweight parametric adaptation while maintaining the frozen CLIP backbone, and a novel audio-visual prompt that dynamically enhances text embeddings with key multimodal information based on the semantic correlation between audio-visual features and textual labels, significantly improving CLIP's generalization for the video anomaly detection task. Moreover, to enhance robustness against modality deficiency during inference, we further develop an uncertainty-driven feature distillation module that synthesizes audio-visual representations from visual-only inputs. This module employs uncertainty modeling based on the diversity of audio-visual features to dynamically emphasize challenging features during the distillation process. Our framework demonstrates superior performance across multiple benchmarks, with audio integration significantly boosting anomaly detection accuracy in various scenarios. Notably, with unimodal data enhanced by uncertainty-driven distillation, our approach consistently outperforms current unimodal VAD methods.
<div id='section'>Paperid: <span id='pid'>505, <a href='https://arxiv.org/pdf/2409.05383.pdf' target='_blank'>https://arxiv.org/pdf/2409.05383.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Wu, Chengyu Pan, Yuting Yan, Guansong Pang, Peng Wang, Yanning Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.05383">Deep Learning for Video Anomaly Detection: A Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video anomaly detection (VAD) aims to discover behaviors or events deviating from the normality in videos. As a long-standing task in the field of computer vision, VAD has witnessed much good progress. In the era of deep learning, with the explosion of architectures of continuously growing capability and capacity, a great variety of deep learning based methods are constantly emerging for the VAD task, greatly improving the generalization ability of detection algorithms and broadening the application scenarios. Therefore, such a multitude of methods and a large body of literature make a comprehensive survey a pressing necessity. In this paper, we present an extensive and comprehensive research review, covering the spectrum of five different categories, namely, semi-supervised, weakly supervised, fully supervised, unsupervised and open-set supervised VAD, and we also delve into the latest VAD works based on pre-trained large models, remedying the limitations of past reviews in terms of only focusing on semi-supervised VAD and small model based methods. For the VAD task with different levels of supervision, we construct a well-organized taxonomy, profoundly discuss the characteristics of different types of methods, and show their performance comparisons. In addition, this review involves the public datasets, open-source codes, and evaluation metrics covering all the aforementioned VAD tasks. Finally, we provide several important research directions for the VAD community.
<div id='section'>Paperid: <span id='pid'>506, <a href='https://arxiv.org/pdf/2408.05905.pdf' target='_blank'>https://arxiv.org/pdf/2408.05905.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Wu, Xuerong Zhou, Guansong Pang, Zhiwei Yang, Qingsen Yan, Peng Wang, Yanning Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.05905">Weakly Supervised Video Anomaly Detection and Localization with Spatio-Temporal Prompts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current weakly supervised video anomaly detection (WSVAD) task aims to achieve frame-level anomalous event detection with only coarse video-level annotations available. Existing works typically involve extracting global features from full-resolution video frames and training frame-level classifiers to detect anomalies in the temporal dimension. However, most anomalous events tend to occur in localized spatial regions rather than the entire video frames, which implies existing frame-level feature based works may be misled by the dominant background information and lack the interpretation of the detected anomalies. To address this dilemma, this paper introduces a novel method called STPrompt that learns spatio-temporal prompt embeddings for weakly supervised video anomaly detection and localization (WSVADL) based on pre-trained vision-language models (VLMs). Our proposed method employs a two-stream network structure, with one stream focusing on the temporal dimension and the other primarily on the spatial dimension. By leveraging the learned knowledge from pre-trained VLMs and incorporating natural motion priors from raw videos, our model learns prompt embeddings that are aligned with spatio-temporal regions of videos (e.g., patches of individual frames) for identify specific local regions of anomalies, enabling accurate video anomaly detection while mitigating the influence of background information. Without relying on detailed spatio-temporal annotations or auxiliary object detection/tracking, our method achieves state-of-the-art performance on three public benchmarks for the WSVADL task.
<div id='section'>Paperid: <span id='pid'>507, <a href='https://arxiv.org/pdf/2404.11981.pdf' target='_blank'>https://arxiv.org/pdf/2404.11981.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chongjie Si, Xuehui Wang, Xiaokang Yang, Wei Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.11981">Tendency-driven Mutual Exclusivity for Weakly Supervised Incremental Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Incremental Learning for Semantic Segmentation (WILSS) leverages a pre-trained segmentation model to segment new classes using cost-effective and readily available image-level labels. A prevailing way to solve WILSS is the generation of seed areas for each new class, serving as a form of pixel-level supervision. However, a scenario usually arises where a pixel is concurrently predicted as an old class by the pre-trained segmentation model and a new class by the seed areas. Such a scenario becomes particularly problematic in WILSS, as the lack of pixel-level annotations on new classes makes it intractable to ascertain whether the pixel pertains to the new class or not. To surmount this issue, we propose an innovative, tendency-driven relationship of mutual exclusivity, meticulously tailored to govern the behavior of the seed areas and the predictions generated by the pre-trained segmentation model. This relationship stipulates that predictions for the new and old classes must not conflict whilst prioritizing the preservation of predictions for the old classes, which not only addresses the conflicting prediction issue but also effectively mitigates the inherent challenge of incremental learning - catastrophic forgetting. Furthermore, under the auspices of this tendency-driven mutual exclusivity relationship, we generate pseudo masks for the new classes, allowing for concurrent execution with model parameter updating via the resolution of a bi-level optimization problem. Extensive experiments substantiate the effectiveness of our framework, resulting in the establishment of new benchmarks and paving the way for further research in this field.
<div id='section'>Paperid: <span id='pid'>508, <a href='https://arxiv.org/pdf/2308.06161.pdf' target='_blank'>https://arxiv.org/pdf/2308.06161.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Xu, Yong Luo, Han Hu, Bo Du, Jialie Shen, Yonggang Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.06161">Rethinking the Localization in Weakly Supervised Object Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised object localization (WSOL) is one of the most popular and challenging tasks in computer vision. This task is to localize the objects in the images given only the image-level supervision. Recently, dividing WSOL into two parts (class-agnostic object localization and object classification) has become the state-of-the-art pipeline for this task. However, existing solutions under this pipeline usually suffer from the following drawbacks: 1) they are not flexible since they can only localize one object for each image due to the adopted single-class regression (SCR) for localization; 2) the generated pseudo bounding boxes may be noisy, but the negative impact of such noise is not well addressed. To remedy these drawbacks, we first propose to replace SCR with a binary-class detector (BCD) for localizing multiple objects, where the detector is trained by discriminating the foreground and background. Then we design a weighted entropy (WE) loss using the unlabeled data to reduce the negative impact of noisy bounding boxes. Extensive experiments on the popular CUB-200-2011 and ImageNet-1K datasets demonstrate the effectiveness of our method.
<div id='section'>Paperid: <span id='pid'>509, <a href='https://arxiv.org/pdf/2301.12077.pdf' target='_blank'>https://arxiv.org/pdf/2301.12077.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingyu Xu, Zheng Lian, Lei Feng, Bin Liu, Jianhua Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.12077">ALIM: Adjusting Label Importance Mechanism for Noisy Partial Label Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Noisy partial label learning (noisy PLL) is an important branch of weakly supervised learning. Unlike PLL where the ground-truth label must conceal in the candidate label set, noisy PLL relaxes this constraint and allows the ground-truth label may not be in the candidate label set. To address this challenging problem, most of the existing works attempt to detect noisy samples and estimate the ground-truth label for each noisy sample. However, detection errors are unavoidable. These errors can accumulate during training and continuously affect model optimization. To this end, we propose a novel framework for noisy PLL with theoretical guarantees, called ``Adjusting Label Importance Mechanism (ALIM)''. It aims to reduce the negative impact of detection errors by trading off the initial candidate set and model outputs. ALIM is a plug-in strategy that can be integrated with existing PLL approaches. Experimental results on benchmark datasets demonstrate that our method can achieve state-of-the-art performance on noisy PLL. \textcolor[rgb]{0.93,0.0,0.47}{Our code can be found in Supplementary Material}.
<div id='section'>Paperid: <span id='pid'>510, <a href='https://arxiv.org/pdf/2211.04774.pdf' target='_blank'>https://arxiv.org/pdf/2211.04774.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Lian, Mingyu Xu, Lan Chen, Licai Sun, Bin Liu, Jianhua Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.04774">IRNet: Iterative Refinement Network for Noisy Partial Label Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partial label learning (PLL) is a typical weakly supervised learning, where each sample is associated with a set of candidate labels. The basic assumption of PLL is that the ground-truth label must reside in the candidate set. However, this assumption may not be satisfied due to the unprofessional judgment of the annotators, thus limiting the practical application of PLL. In this paper, we relax this assumption and focus on a more general problem, noisy PLL, where the ground-truth label may not exist in the candidate set. To address this challenging problem, we propose a novel framework called "Iterative Refinement Network (IRNet)". It aims to purify the noisy samples by two key modules, i.e., noisy sample detection and label correction. Ideally, we can convert noisy PLL into traditional PLL if all noisy samples are corrected. To guarantee the performance of these modules, we start with warm-up training and exploit data augmentation to reduce prediction errors. Through theoretical analysis, we prove that IRNet is able to reduce the noise level of the dataset and eventually approximate the Bayes optimal classifier. Experimental results on multiple benchmark datasets demonstrate the effectiveness of our method. IRNet is superior to existing state-of-the-art approaches on noisy PLL.
<div id='section'>Paperid: <span id='pid'>511, <a href='https://arxiv.org/pdf/1909.03354.pdf' target='_blank'>https://arxiv.org/pdf/1909.03354.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>JÃ©rÃ´me Rony, Soufiane Belharbi, Jose Dolz, Ismail Ben Ayed, Luke McCaffrey, Eric Granger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1909.03354">Deep Weakly-Supervised Learning Methods for Classification and Localization in Histology Images: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Using deep learning models to diagnose cancer from histology data presents several challenges. Cancer grading and localization of regions of interest (ROIs) in these images normally relies on both image- and pixel-level labels, the latter requiring a costly annotation process. Deep weakly-supervised object localization (WSOL) methods provide different strategies for low-cost training of deep learning models. Using only image-class annotations, these methods can be trained to classify an image, and yield class activation maps (CAMs) for ROI localization. This paper provides a review of state-of-art DL methods for WSOL. We propose a taxonomy where these methods are divided into bottom-up and top-down methods according to the information flow in models. Although the latter have seen limited progress, recent bottom-up methods are currently driving much progress with deep WSOL methods. Early works focused on designing different spatial pooling functions. However, these methods reached limited localization accuracy, and unveiled a major limitation -- the under-activation of CAMs which leads to high false negative localization. Subsequent works aimed to alleviate this issue and recover complete object. Representative methods from our taxonomy are evaluated and compared in terms of classification and localization accuracy on two challenging histology datasets. Overall, the results indicate poor localization performance, particularly for generic methods that were initially designed to process natural images. Methods designed to address the challenges of histology data yielded good results. However, all methods suffer from high false positive/negative localization. Four key challenges are identified for the application of deep WSOL methods in histology -- under/over activation of CAMs, sensitivity to thresholding, and model selection.
<div id='section'>Paperid: <span id='pid'>512, <a href='https://arxiv.org/pdf/1904.04205.pdf' target='_blank'>https://arxiv.org/pdf/1904.04205.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hoel Kervadec, Jose Dolz, Jing Yuan, Christian Desrosiers, Eric Granger, Ismail Ben Ayed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1904.04205">Constrained Deep Networks: Lagrangian Optimization via Log-Barrier Extensions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study investigates imposing hard inequality constraints on the outputs of convolutional neural networks (CNN) during training. Several recent works showed that the theoretical and practical advantages of Lagrangian optimization over simple penalties do not materialize in practice when dealing with modern CNNs involving millions of parameters. Therefore, constrained CNNs are typically handled with penalties. We propose *log-barrier extensions*, which approximate Lagrangian optimization of constrained-CNN problems with a sequence of unconstrained losses. Unlike standard interior-point and log-barrier methods, our formulation does not need an initial feasible solution. The proposed extension yields an upper bound on the duality gap -- generalizing the result of standard log-barriers -- and yielding sub-optimality certificates for feasible solutions. While sub-optimality is not guaranteed for non-convex problems, this result shows that log-barrier extensions are a principled way to approximate Lagrangian optimization for constrained CNNs via implicit dual variables. We report weakly supervised image segmentation experiments, with various constraints, showing that our formulation outperforms substantially the existing constrained-CNN methods, in terms of accuracy, constraint satisfaction and training stability, more so when dealing with a large number of constraints.
<div id='section'>Paperid: <span id='pid'>513, <a href='https://arxiv.org/pdf/2505.22063.pdf' target='_blank'>https://arxiv.org/pdf/2505.22063.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingchen Shao, Xinfa Zhu, Chengyou Wang, Bingshen Mu, Hai Li, Ying Yan, Junhui Liu, Danming Xie, Lei Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22063">Weakly Supervised Data Refinement and Flexible Sequence Compression for Efficient Thai LLM-based ASR</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite remarkable achievements, automatic speech recognition (ASR) in low-resource scenarios still faces two challenges: high-quality data scarcity and high computational demands. This paper proposes EThai-ASR, the first to apply large language models (LLMs) to Thai ASR and create an efficient LLM-based ASR system. EThai-ASR comprises a speech encoder, a connection module and a Thai LLM decoder. To address the data scarcity and obtain a powerful speech encoder, EThai-ASR introduces a self-evolving data refinement strategy to refine weak labels, yielding an enhanced speech encoder. Moreover, we propose a pluggable sequence compression module used in the connection module with three modes designed to reduce the sequence length, thus decreasing computational demands while maintaining decent performance. Extensive experiments demonstrate that EThai-ASR has achieved state-of-the-art accuracy in multiple datasets. We release our refined text transcripts to promote further research.
<div id='section'>Paperid: <span id='pid'>514, <a href='https://arxiv.org/pdf/2501.13584.pdf' target='_blank'>https://arxiv.org/pdf/2501.13584.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Wang, Mingxuan Xia, Chang Yao, Lei Feng, Junbo Zhao, Gang Chen, Haobo Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13584">Towards Robust Incremental Learning under Ambiguous Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional Incremental Learning (IL) targets to handle sequential fully-supervised learning problems where novel classes emerge from time to time. However, due to inherent annotation uncertainty and ambiguity, collecting high-quality annotated data in a dynamic learning system can be extremely expensive. To mitigate this problem, we propose a novel weakly-supervised learning paradigm called Incremental Partial Label Learning (IPLL), where the sequentially arrived data relate to a set of candidate labels rather than the ground truth. Technically, we develop the Prototype-Guided Disambiguation and Replay Algorithm (PGDR) which leverages the class prototypes as a proxy to mitigate two intertwined challenges in IPLL, i.e., label ambiguity and catastrophic forgetting. To handle the former, PGDR encapsulates a momentum-based pseudo-labeling algorithm along with prototype-guided initialization, resulting in a balanced perception of classes. To alleviate forgetting, we develop a memory replay technique that collects well-disambiguated samples while maintaining representativeness and diversity. By jointly distilling knowledge from curated memory data, our framework exhibits a great disambiguation ability for samples of new tasks and achieves less forgetting of knowledge. Extensive experiments demonstrate that PGDR achieves superior
<div id='section'>Paperid: <span id='pid'>515, <a href='https://arxiv.org/pdf/2406.14183.pdf' target='_blank'>https://arxiv.org/pdf/2406.14183.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marco Fumero, Marco Pegoraro, Valentino Maiorca, Francesco Locatello, Emanuele RodolÃ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.14183">Latent Functional Maps: a spectral framework for representation alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural models learn data representations that lie on low-dimensional manifolds, yet modeling the relation between these representational spaces is an ongoing challenge. By integrating spectral geometry principles into neural modeling, we show that this problem can be better addressed in the functional domain, mitigating complexity, while enhancing interpretability and performances on downstream tasks. To this end, we introduce a multi-purpose framework to the representation learning community, which allows to: (i) compare different spaces in an interpretable way and measure their intrinsic similarity; (ii) find correspondences between them, both in unsupervised and weakly supervised settings, and (iii) to effectively transfer representations between distinct spaces. We validate our framework on various applications, ranging from stitching to retrieval tasks, and on multiple modalities, demonstrating that Latent Functional Maps can serve as a swiss-army knife for representation alignment.
<div id='section'>Paperid: <span id='pid'>516, <a href='https://arxiv.org/pdf/2307.09267.pdf' target='_blank'>https://arxiv.org/pdf/2307.09267.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zehan Wang, Haifeng Huang, Yang Zhao, Linjun Li, Xize Cheng, Yichen Zhu, Aoxiong Yin, Zhou Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.09267">Distilling Coarse-to-Fine Semantic Matching Knowledge for Weakly Supervised 3D Visual Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D visual grounding involves finding a target object in a 3D scene that corresponds to a given sentence query. Although many approaches have been proposed and achieved impressive performance, they all require dense object-sentence pair annotations in 3D point clouds, which are both time-consuming and expensive. To address the problem that fine-grained annotated data is difficult to obtain, we propose to leverage weakly supervised annotations to learn the 3D visual grounding model, i.e., only coarse scene-sentence correspondences are used to learn object-sentence links. To accomplish this, we design a novel semantic matching model that analyzes the semantic similarity between object proposals and sentences in a coarse-to-fine manner. Specifically, we first extract object proposals and coarsely select the top-K candidates based on feature and class similarity matrices. Next, we reconstruct the masked keywords of the sentence using each candidate one by one, and the reconstructed accuracy finely reflects the semantic similarity of each candidate to the query. Additionally, we distill the coarse-to-fine semantic matching knowledge into a typical two-stage 3D visual grounding model, which reduces inference costs and improves performance by taking full advantage of the well-studied structure of the existing architectures. We conduct extensive experiments on ScanRefer, Nr3D, and Sr3D, which demonstrate the effectiveness of our proposed method.
<div id='section'>Paperid: <span id='pid'>517, <a href='https://arxiv.org/pdf/2304.07939.pdf' target='_blank'>https://arxiv.org/pdf/2304.07939.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marco Fumero, Florian Wenzel, Luca Zancato, Alessandro Achille, Emanuele RodolÃ, Stefano Soatto, Bernhard SchÃ¶lkopf, Francesco Locatello
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.07939">Leveraging sparse and shared feature activations for disentangled representation learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recovering the latent factors of variation of high dimensional data has so far focused on simple synthetic settings. Mostly building on unsupervised and weakly-supervised objectives, prior work missed out on the positive implications for representation learning on real world data. In this work, we propose to leverage knowledge extracted from a diversified set of supervised tasks to learn a common disentangled representation. Assuming each supervised task only depends on an unknown subset of the factors of variation, we disentangle the feature space of a supervised multi-task model, with features activating sparsely across different tasks and information being shared as appropriate. Importantly, we never directly observe the factors of variations but establish that access to multiple tasks is sufficient for identifiability under sufficiency and minimality assumptions. We validate our approach on six real world distribution shift benchmarks, and different data modalities (images, text), demonstrating how disentangled representations can be transferred to real settings.
<div id='section'>Paperid: <span id='pid'>518, <a href='https://arxiv.org/pdf/2509.13629.pdf' target='_blank'>https://arxiv.org/pdf/2509.13629.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue He, Min Liu, Qinghao Liu, Jiazheng Wang, Yaonan Wang, Hang Zhang, Xiang Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13629">SAMIR, an efficient registration framework via robust feature learning from SAM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image registration is a fundamental task in medical image analysis. Deformations are often closely related to the morphological characteristics of tissues, making accurate feature extraction crucial. Recent weakly supervised methods improve registration by incorporating anatomical priors such as segmentation masks or landmarks, either as inputs or in the loss function. However, such weak labels are often not readily available, limiting their practical use. Motivated by the strong representation learning ability of visual foundation models, this paper introduces SAMIR, an efficient medical image registration framework that utilizes the Segment Anything Model (SAM) to enhance feature extraction. SAM is pretrained on large-scale natural image datasets and can learn robust, general-purpose visual representations. Rather than using raw input images, we design a task-specific adaptation pipeline using SAM's image encoder to extract structure-aware feature embeddings, enabling more accurate modeling of anatomical consistency and deformation patterns. We further design a lightweight 3D head to refine features within the embedding space, adapting to local deformations in medical images. Additionally, we introduce a Hierarchical Feature Consistency Loss to guide coarse-to-fine feature matching and improve anatomical alignment. Extensive experiments demonstrate that SAMIR significantly outperforms state-of-the-art methods on benchmark datasets for both intra-subject cardiac image registration and inter-subject abdomen CT image registration, achieving performance improvements of 2.68% on ACDC and 6.44% on the abdomen dataset. The source code will be publicly available on GitHub following the acceptance of this paper.
<div id='section'>Paperid: <span id='pid'>519, <a href='https://arxiv.org/pdf/2310.19351.pdf' target='_blank'>https://arxiv.org/pdf/2310.19351.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryosuke Furuta, Yoichi Sato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.19351">Seeking Flat Minima with Mean Teacher on Semi- and Weakly-Supervised Domain Generalization for Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object detectors do not work well when domains largely differ between training and testing data. To overcome this domain gap in object detection without requiring expensive annotations, we consider two problem settings: semi-supervised domain generalizable object detection (SS-DGOD) and weakly-supervised DGOD (WS-DGOD). In contrast to the conventional domain generalization for object detection that requires labeled data from multiple domains, SS-DGOD and WS-DGOD require labeled data only from one domain and unlabeled or weakly-labeled data from multiple domains for training. In this paper, we show that object detectors can be effectively trained on the two settings with the same Mean Teacher learning framework, where a student network is trained with pseudo-labels output from a teacher on the unlabeled or weakly-labeled data. We provide novel interpretations of why the Mean Teacher learning framework works well on the two settings in terms of the relationships between the generalization gap and flat minima in parameter space. On the basis of the interpretations, we also show that incorporating a simple regularization method into the Mean Teacher learning framework leads to flatter minima. The experimental results demonstrate that the regularization leads to flatter minima and boosts the performance of the detectors trained with the Mean Teacher learning framework on the two settings.
<div id='section'>Paperid: <span id='pid'>520, <a href='https://arxiv.org/pdf/2502.15370.pdf' target='_blank'>https://arxiv.org/pdf/2502.15370.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kibum Kim, Kanghoon Yoon, Yeonjun In, Jaehyeong Jeon, Jinyoung Moon, Donghyun Kim, Chanyoung Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.15370">Weakly Supervised Video Scene Graph Generation via Natural Language Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing Video Scene Graph Generation (VidSGG) studies are trained in a fully supervised manner, which requires all frames in a video to be annotated, thereby incurring high annotation cost compared to Image Scene Graph Generation (ImgSGG). Although the annotation cost of VidSGG can be alleviated by adopting a weakly supervised approach commonly used for ImgSGG (WS-ImgSGG) that uses image captions, there are two key reasons that hinder such a naive adoption: 1) Temporality within video captions, i.e., unlike image captions, video captions include temporal markers (e.g., before, while, then, after) that indicate time related details, and 2) Variability in action duration, i.e., unlike human actions in image captions, human actions in video captions unfold over varying duration. To address these issues, we propose a Natural Language-based Video Scene Graph Generation (NL-VSGG) framework that only utilizes the readily available video captions for training a VidSGG model. NL-VSGG consists of two key modules: Temporality-aware Caption Segmentation (TCS) module and Action Duration Variability-aware caption-frame alignment (ADV) module. Specifically, TCS segments the video captions into multiple sentences in a temporal order based on a Large Language Model (LLM), and ADV aligns each segmented sentence with appropriate frames considering the variability in action duration. Our approach leads to a significant enhancement in performance compared to simply applying the WS-ImgSGG pipeline to VidSGG on the Action Genome dataset. As a further benefit of utilizing the video captions as weak supervision, we show that the VidSGG model trained by NL-VSGG is able to predict a broader range of action classes that are not included in the training data, which makes our framework practical in reality.
<div id='section'>Paperid: <span id='pid'>521, <a href='https://arxiv.org/pdf/2412.10410.pdf' target='_blank'>https://arxiv.org/pdf/2412.10410.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaofei Cai, Bowei Zhang, Zihao Wang, Haowei Lin, Xiaojian Ma, Anji Liu, Yitao Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10410">GROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing agents that can follow multimodal instructions remains a fundamental challenge in robotics and AI. Although large-scale pre-training on unlabeled datasets (no language instruction) has enabled agents to learn diverse behaviors, these agents often struggle with following instructions. While augmenting the dataset with instruction labels can mitigate this issue, acquiring such high-quality annotations at scale is impractical. To address this issue, we frame the problem as a semi-supervised learning task and introduce GROOT-2, a multimodal instructable agent trained using a novel approach that combines weak supervision with latent variable models. Our method consists of two key components: constrained self-imitating, which utilizes large amounts of unlabeled demonstrations to enable the policy to learn diverse behaviors, and human intention alignment, which uses a smaller set of labeled demonstrations to ensure the latent space reflects human intentions. GROOT-2's effectiveness is validated across four diverse environments, ranging from video games to robotic manipulation, demonstrating its robust multimodal instruction-following capabilities.
<div id='section'>Paperid: <span id='pid'>522, <a href='https://arxiv.org/pdf/2402.08960.pdf' target='_blank'>https://arxiv.org/pdf/2402.08960.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoqing Wang, Xiaobo Xia, Ziye Chen, Xiao He, Yandong Guo, Mingming Gong, Tongliang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.08960">Open-Vocabulary Segmentation with Unpaired Mask-Text Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current state-of-the-art open-vocabulary segmentation methods typically rely on image-mask-text triplet annotations for supervision. However, acquiring such detailed annotations is labour-intensive and poses scalability challenges in complex real-world scenarios. While existing weakly-supervised approaches leverage image-text pairs to reduce the expansive annotation cost, the lack of mask supervision makes it difficult for the model to locate multiple instances and accurately group pixels with similar semantics, significantly hampering versatility and performance. In this paper, we introduce Unpair-Seg, a novel weakly-supervised open-vocabulary segmentation framework that learns from unpaired image-mask and image-text pairs, which can be independently and efficiently collected. Unpair-Seg initially predicts a set of binary masks and generates pseudo labels by identifying confident pairs of masks and text entities. We then train a feature adapter to align region embeddings with text embeddings based on these pseudo labels, achieving open-vocabulary segmentation. However, the inherent noise in the mask-entity correspondence poses a challenge to obtaining reliable pairs. To address this, we employ a vision-language large model to re-caption the input images and extract precise entities, and we design a multi-scale matching strategy to reduce noisy mask-entity pairs. Our Unpair-Seg framework demonstrates impressive performance, achieving 14.6\% and 19.5\% mIoU on the ADE-847 and PASCAL Context-459 datasets, significantly narrowing the gap between fully-supervised and weakly-supervised methods.
<div id='section'>Paperid: <span id='pid'>523, <a href='https://arxiv.org/pdf/2310.10404.pdf' target='_blank'>https://arxiv.org/pdf/2310.10404.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kibum Kim, Kanghoon Yoon, Jaehyeong Jeon, Yeonjun In, Jinyoung Moon, Donghyun Kim, Chanyoung Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.10404">LLM4SGG: Large Language Models for Weakly Supervised Scene Graph Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-Supervised Scene Graph Generation (WSSGG) research has recently emerged as an alternative to the fully-supervised approach that heavily relies on costly annotations. In this regard, studies on WSSGG have utilized image captions to obtain unlocalized triplets while primarily focusing on grounding the unlocalized triplets over image regions. However, they have overlooked the two issues involved in the triplet formation process from the captions: 1) Semantic over-simplification issue arises when extracting triplets from captions, where fine-grained predicates in captions are undesirably converted into coarse-grained predicates, resulting in a long-tailed predicate distribution, and 2) Low-density scene graph issue arises when aligning the triplets in the caption with entity/predicate classes of interest, where many triplets are discarded and not used in training, leading to insufficient supervision. To tackle the two issues, we propose a new approach, i.e., Large Language Model for weakly-supervised SGG (LLM4SGG), where we mitigate the two issues by leveraging the LLM's in-depth understanding of language and reasoning ability during the extraction of triplets from captions and alignment of entity/predicate classes with target data. To further engage the LLM in these processes, we adopt the idea of Chain-of-Thought and the in-context few-shot learning strategy. To validate the effectiveness of LLM4SGG, we conduct extensive experiments on Visual Genome and GQA datasets, showing significant improvements in both Recall@K and mean Recall@K compared to the state-of-the-art WSSGG methods. A further appeal is that LLM4SGG is data-efficient, enabling effective model training with a small amount of training images.
<div id='section'>Paperid: <span id='pid'>524, <a href='https://arxiv.org/pdf/2202.00315.pdf' target='_blank'>https://arxiv.org/pdf/2202.00315.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Clemens Seibold, Johannes KÃ¼nzel, Anna Hilsmann, Peter Eisert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.00315">From Explanations to Segmentation: Using Explainable AI for Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The new era of image segmentation leveraging the power of Deep Neural Nets (DNNs) comes with a price tag: to train a neural network for pixel-wise segmentation, a large amount of training samples has to be manually labeled on pixel-precision. In this work, we address this by following an indirect solution. We build upon the advances of the Explainable AI (XAI) community and extract a pixel-wise binary segmentation from the output of the Layer-wise Relevance Propagation (LRP) explaining the decision of a classification network. We show that we achieve similar results compared to an established U-Net segmentation architecture, while the generation of the training data is significantly simplified. The proposed method can be trained in a weakly supervised fashion, as the training samples must be only labeled on image-level, at the same time enabling the output of a segmentation mask. This makes it especially applicable to a wider range of real applications where tedious pixel-level labelling is often not possible.
<div id='section'>Paperid: <span id='pid'>525, <a href='https://arxiv.org/pdf/2509.03893.pdf' target='_blank'>https://arxiv.org/pdf/2509.03893.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stefan Stojanov, Linan Zhao, Yunzhi Zhang, Daniel L. K. Yamins, Jiajun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03893">Weakly-Supervised Learning of Dense Functional Correspondences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Establishing dense correspondences across image pairs is essential for tasks such as shape reconstruction and robot manipulation. In the challenging setting of matching across different categories, the function of an object, i.e., the effect that an object can cause on other objects, can guide how correspondences should be established. This is because object parts that enable specific functions often share similarities in shape and appearance. We derive the definition of dense functional correspondence based on this observation and propose a weakly-supervised learning paradigm to tackle the prediction task. The main insight behind our approach is that we can leverage vision-language models to pseudo-label multi-view images to obtain functional parts. We then integrate this with dense contrastive learning from pixel correspondences to distill both functional and spatial knowledge into a new model that can establish dense functional correspondence. Further, we curate synthetic and real evaluation datasets as task benchmarks. Our results demonstrate the advantages of our approach over baseline solutions consisting of off-the-shelf self-supervised image representations and grounded vision language models.
<div id='section'>Paperid: <span id='pid'>526, <a href='https://arxiv.org/pdf/2502.08888.pdf' target='_blank'>https://arxiv.org/pdf/2502.08888.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruichao Yang, Jing Ma, Wei Gao, Hongzhan Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.08888">LLM-Enhanced Multiple Instance Learning for Joint Rumor and Stance Detection with Social Context Information</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The proliferation of misinformation, such as rumors on social media, has drawn significant attention, prompting various expressions of stance among users. Although rumor detection and stance detection are distinct tasks, they can complement each other. Rumors can be identified by cross-referencing stances in related posts, and stances are influenced by the nature of the rumor. However, existing stance detection methods often require post-level stance annotations, which are costly to obtain. We propose a novel LLM-enhanced MIL approach to jointly predict post stance and claim class labels, supervised solely by claim labels, using an undirected microblog propagation model. Our weakly supervised approach relies only on bag-level labels of claim veracity, aligning with multi-instance learning (MIL) principles. To achieve this, we transform the multi-class problem into multiple MIL-based binary classification problems. We then employ a discriminative attention layer to aggregate the outputs from these classifiers into finer-grained classes. Experiments conducted on three rumor datasets and two stance datasets demonstrate the effectiveness of our approach, highlighting strong connections between rumor veracity and expressed stances in responding posts. Our method shows promising performance in joint rumor and stance detection compared to the state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>527, <a href='https://arxiv.org/pdf/2403.15272.pdf' target='_blank'>https://arxiv.org/pdf/2403.15272.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jialu Wang, Kaichen Zhou, Andrew Markham, Niki Trigoni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15272">WSCLoc: Weakly-Supervised Sparse-View Camera Relocalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the advancements in deep learning for camera relocalization tasks, obtaining ground truth pose labels required for the training process remains a costly endeavor. While current weakly supervised methods excel in lightweight label generation, their performance notably declines in scenarios with sparse views. In response to this challenge, we introduce WSCLoc, a system capable of being customized to various deep learning-based relocalization models to enhance their performance under weakly-supervised and sparse view conditions. This is realized with two stages. In the initial stage, WSCLoc employs a multilayer perceptron-based structure called WFT-NeRF to co-optimize image reconstruction quality and initial pose information. To ensure a stable learning process, we incorporate temporal information as input. Furthermore, instead of optimizing SE(3), we opt for $\mathfrak{sim}(3)$ optimization to explicitly enforce a scale constraint. In the second stage, we co-optimize the pre-trained WFT-NeRF and WFT-Pose. This optimization is enhanced by Time-Encoding based Random View Synthesis and supervised by inter-frame geometric constraints that consider pose, depth, and RGB information. We validate our approaches on two publicly available datasets, one outdoor and one indoor. Our experimental results demonstrate that our weakly-supervised relocalization solutions achieve superior pose estimation accuracy in sparse-view scenarios, comparable to state-of-the-art camera relocalization methods. We will make our code publicly available.
<div id='section'>Paperid: <span id='pid'>528, <a href='https://arxiv.org/pdf/2311.18173.pdf' target='_blank'>https://arxiv.org/pdf/2311.18173.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhao Zhang, Xiwen Chen, William Richardson, Bruce Z. Gao, Abolfazl Razi, Tong Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.18173">Quantification of cardiac capillarization in single-immunostained myocardial slices using weakly supervised instance segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Decreased myocardial capillary density has been reported as an important histopathological feature associated with various heart disorders. Quantitative assessment of cardiac capillarization typically involves double immunostaining of cardiomyocytes (CMs) and capillaries in myocardial slices. In contrast, single immunostaining of basement membrane components is a straightforward approach to simultaneously label CMs and capillaries, presenting fewer challenges in background staining. However, subsequent image analysis always requires manual work in identifying and segmenting CMs and capillaries. Here, we developed an image analysis tool, AutoQC, to automatically identify and segment CMs and capillaries in immunofluorescence images of collagen type IV, a predominant basement membrane protein within the myocardium. In addition, commonly used capillarization-related measurements can be derived from segmentation masks. AutoQC features a weakly supervised instance segmentation algorithm by leveraging the power of a pre-trained segmentation model via prompt engineering. AutoQC outperformed YOLOv8-Seg, a state-of-the-art instance segmentation model, in both instance segmentation and capillarization assessment. Furthermore, the training of AutoQC required only a small dataset with bounding box annotations instead of pixel-wise annotations, leading to a reduced workload during network training. AutoQC provides an automated solution for quantifying cardiac capillarization in basement-membrane-immunostained myocardial slices, eliminating the need for manual image analysis once it is trained.
<div id='section'>Paperid: <span id='pid'>529, <a href='https://arxiv.org/pdf/2310.16579.pdf' target='_blank'>https://arxiv.org/pdf/2310.16579.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruichao Yang, Wei Gao, Jing Ma, Hongzhan Lin, Zhiwei Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.16579">WSDMS: Debunk Fake News via Weakly Supervised Detection of Misinforming Sentences with Contextualized Social Wisdom</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, we witness the explosion of false and unconfirmed information (i.e., rumors) that went viral on social media and shocked the public. Rumors can trigger versatile, mostly controversial stance expressions among social media users. Rumor verification and stance detection are different yet relevant tasks. Fake news debunking primarily focuses on determining the truthfulness of news articles, which oversimplifies the issue as fake news often combines elements of both truth and falsehood. Thus, it becomes crucial to identify specific instances of misinformation within the articles. In this research, we investigate a novel task in the field of fake news debunking, which involves detecting sentence-level misinformation. One of the major challenges in this task is the absence of a training dataset with sentence-level annotations regarding veracity. Inspired by the Multiple Instance Learning (MIL) approach, we propose a model called Weakly Supervised Detection of Misinforming Sentences (WSDMS). This model only requires bag-level labels for training but is capable of inferring both sentence-level misinformation and article-level veracity, aided by relevant social media conversations that are attentively contextualized with news sentences. We evaluate WSDMS on three real-world benchmarks and demonstrate that it outperforms existing state-of-the-art baselines in debunking fake news at both the sentence and article levels.
<div id='section'>Paperid: <span id='pid'>530, <a href='https://arxiv.org/pdf/2309.09730.pdf' target='_blank'>https://arxiv.org/pdf/2309.09730.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meng Han, Xiangde Luo, Wenjun Liao, Shichuan Zhang, Shaoting Zhang, Guotai Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.09730">Scribble-based 3D Multiple Abdominal Organ Segmentation via Triple-branch Multi-dilated Network with Pixel- and Class-wise Consistency</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-organ segmentation in abdominal Computed Tomography (CT) images is of great importance for diagnosis of abdominal lesions and subsequent treatment planning. Though deep learning based methods have attained high performance, they rely heavily on large-scale pixel-level annotations that are time-consuming and labor-intensive to obtain. Due to its low dependency on annotation, weakly supervised segmentation has attracted great attention. However, there is still a large performance gap between current weakly-supervised methods and fully supervised learning, leaving room for exploration. In this work, we propose a novel 3D framework with two consistency constraints for scribble-supervised multiple abdominal organ segmentation from CT. Specifically, we employ a Triple-branch multi-Dilated network (TDNet) with one encoder and three decoders using different dilation rates to capture features from different receptive fields that are complementary to each other to generate high-quality soft pseudo labels. For more stable unsupervised learning, we use voxel-wise uncertainty to rectify the soft pseudo labels and then supervise the outputs of each decoder. To further regularize the network, class relationship information is exploited by encouraging the generated class affinity matrices to be consistent across different decoders under multi-view projection. Experiments on the public WORD dataset show that our method outperforms five existing scribble-supervised methods.
<div id='section'>Paperid: <span id='pid'>531, <a href='https://arxiv.org/pdf/2306.11490.pdf' target='_blank'>https://arxiv.org/pdf/2306.11490.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jia Fu, Tao Lu, Shaoting Zhang, Guotai Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.11490">UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for Weakly-supervised Fetal Brain Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate segmentation of the fetal brain from Magnetic Resonance Image (MRI) is important for prenatal assessment of fetal development. Although deep learning has shown the potential to achieve this task, it requires a large fine annotated dataset that is difficult to collect. To address this issue, weakly-supervised segmentation methods with image-level labels have gained attention, which are commonly based on class activation maps from a classification network trained with image tags. However, most of these methods suffer from incomplete activation regions, due to the low-resolution localization without detailed boundary cues. To this end, we propose a novel weakly-supervised method with image-level labels based on semantic features and context information exploration. We first propose an Uncertainty-weighted Multi-resolution Class Activation Map (UM-CAM) to generate high-quality pixel-level supervision. Then, we design a Geodesic distance-based Seed Expansion (GSE) method to provide context information for rectifying the ambiguous boundaries of UM-CAM. Extensive experiments on a fetal brain dataset show that our UM-CAM can provide more accurate activation regions with fewer false positive regions than existing CAM variants, and our proposed method outperforms state-of-the-art weakly-supervised methods with image-level labels.
<div id='section'>Paperid: <span id='pid'>532, <a href='https://arxiv.org/pdf/2306.05418.pdf' target='_blank'>https://arxiv.org/pdf/2306.05418.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei He, Yuqi Wang, Yuntao Chen, Zhaoxiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.05418">Weakly Supervised 3D Object Detection with Multi-Stage Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid development of large models, the need for data has become increasingly crucial. Especially in 3D object detection, costly manual annotations have hindered further advancements. To reduce the burden of annotation, we study the problem of achieving 3D object detection solely based on 2D annotations. Thanks to advanced 3D reconstruction techniques, it is now feasible to reconstruct the overall static 3D scene. However, extracting precise object-level annotations from the entire scene and generalizing these limited annotations to the entire scene remain challenges. In this paper, we introduce a novel paradigm called BA$^2$-Det, encompassing pseudo label generation and multi-stage generalization. We devise the DoubleClustering algorithm to obtain object clusters from reconstructed scene-level points, and further enhance the model's detection capabilities by developing three stages of generalization: progressing from complete to partial, static to dynamic, and close to distant. Experiments conducted on the large-scale Waymo Open Dataset show that the performance of BA$^2$-Det is on par with the fully-supervised methods using 10% annotations. Additionally, using large raw videos for pretraining,BA$^2$-Det can achieve a 20% relative improvement on the KITTI dataset. The method also has great potential for detecting open-set 3D objects in complex scenes. Project page: https://ba2det.site.
<div id='section'>Paperid: <span id='pid'>533, <a href='https://arxiv.org/pdf/2302.10834.pdf' target='_blank'>https://arxiv.org/pdf/2302.10834.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sanat Ramesh, Diego Dall'Alba, Cristians Gonzalez, Tong Yu, Pietro Mascagni, Didier Mutter, Jacques Marescaux, Paolo Fiorini, Nicolas Padoy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.10834">Weakly Supervised Temporal Convolutional Networks for Fine-grained Surgical Activity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic recognition of fine-grained surgical activities, called steps, is a challenging but crucial task for intelligent intra-operative computer assistance. The development of current vision-based activity recognition methods relies heavily on a high volume of manually annotated data. This data is difficult and time-consuming to generate and requires domain-specific knowledge. In this work, we propose to use coarser and easier-to-annotate activity labels, namely phases, as weak supervision to learn step recognition with fewer step annotated videos. We introduce a step-phase dependency loss to exploit the weak supervision signal. We then employ a Single-Stage Temporal Convolutional Network (SS-TCN) with a ResNet-50 backbone, trained in an end-to-end fashion from weakly annotated videos, for temporal activity segmentation and recognition. We extensively evaluate and show the effectiveness of the proposed method on a large video dataset consisting of 40 laparoscopic gastric bypass procedures and the public benchmark CATARACTS containing 50 cataract surgeries.
<div id='section'>Paperid: <span id='pid'>534, <a href='https://arxiv.org/pdf/2302.06294.pdf' target='_blank'>https://arxiv.org/pdf/2302.06294.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chinedu Innocent Nwoye, Tong Yu, Saurav Sharma, Aditya Murali, Deepak Alapatt, Armine Vardazaryan, Kun Yuan, Jonas Hajek, Wolfgang Reiter, Amine Yamlahi, Finn-Henri Smidt, Xiaoyang Zou, Guoyan Zheng, Bruno Oliveira, Helena R. Torres, Satoshi Kondo, Satoshi Kasai, Felix Holm, Ege Ãzsoy, Shuangchun Gui, Han Li, Sista Raviteja, Rachana Sathish, Pranav Poudel, Binod Bhattarai, Ziheng Wang, Guo Rui, Melanie Schellenberg, JoÃ£o L. VilaÃ§a, Tobias Czempiel, Zhenkun Wang, Debdoot Sheet, Shrawan Kumar Thapa, Max Berniker, Patrick Godau, Pedro Morais, Sudarshan Regmi, Thuy Nuong Tran, Jaime Fonseca, Jan-Hinrich NÃ¶lke, EstevÃ£o Lima, Eduard Vazquez, Lena Maier-Hein, Nassir Navab, Pietro Mascagni, Barbara Seeliger, Cristians Gonzalez, Didier Mutter, Nicolas Padoy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.06294">CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Formalizing surgical activities as triplets of the used instruments, actions performed, and target anatomies is becoming a gold standard approach for surgical activity modeling. The benefit is that this formalization helps to obtain a more detailed understanding of tool-tissue interaction which can be used to develop better Artificial Intelligence assistance for image-guided surgery. Earlier efforts and the CholecTriplet challenge introduced in 2021 have put together techniques aimed at recognizing these triplets from surgical footage. Estimating also the spatial locations of the triplets would offer a more precise intraoperative context-aware decision support for computer-assisted intervention. This paper presents the CholecTriplet2022 challenge, which extends surgical action triplet modeling from recognition to detection. It includes weakly-supervised bounding box localization of every visible surgical instrument (or tool), as the key actors, and the modeling of each tool-activity in the form of <instrument, verb, target> triplet. The paper describes a baseline method and 10 new deep learning algorithms presented at the challenge to solve the task. It also provides thorough methodological comparisons of the methods, an in-depth analysis of the obtained results across multiple metrics, visual and procedural challenges; their significance, and useful insights for future research directions and applications in surgery.
<div id='section'>Paperid: <span id='pid'>535, <a href='https://arxiv.org/pdf/2302.05160.pdf' target='_blank'>https://arxiv.org/pdf/2302.05160.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hang Zhou, Junqing Yu, Wei Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.05160">Dual Memory Units with Uncertainty Regulation for Weakly Supervised Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning discriminative features for effectively separating abnormal events from normality is crucial for weakly supervised video anomaly detection (WS-VAD) tasks. Existing approaches, both video and segment-level label oriented, mainly focus on extracting representations for anomaly data while neglecting the implication of normal data. We observe that such a scheme is sub-optimal, i.e., for better distinguishing anomaly one needs to understand what is a normal state, and may yield a higher false alarm rate. To address this issue, we propose an Uncertainty Regulated Dual Memory Units (UR-DMU) model to learn both the representations of normal data and discriminative features of abnormal data. To be specific, inspired by the traditional global and local structure on graph convolutional networks, we introduce a Global and Local Multi-Head Self Attention (GL-MHSA) module for the Transformer network to obtain more expressive embeddings for capturing associations in videos. Then, we use two memory banks, one additional abnormal memory for tackling hard samples, to store and separate abnormal and normal prototypes and maximize the margins between the two representations. Finally, we propose an uncertainty learning scheme to learn the normal data latent space, that is robust to noise from camera switching, object changing, scene transforming, etc. Extensive experiments on XD-Violence and UCF-Crime datasets demonstrate that our method outperforms the state-of-the-art methods by a sizable margin.
<div id='section'>Paperid: <span id='pid'>536, <a href='https://arxiv.org/pdf/2208.05669.pdf' target='_blank'>https://arxiv.org/pdf/2208.05669.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuwei Zhai, Guotai Wang, Xiangde Luo, Qiang Yue, Kang Li, Shaoting Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.05669">PA-Seg: Learning from Point Annotations for 3D Medical Image Segmentation using Contextual Regularization and Cross Knowledge Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The success of Convolutional Neural Networks (CNNs) in 3D medical image segmentation relies on massive fully annotated 3D volumes for training that are time-consuming and labor-intensive to acquire. In this paper, we propose to annotate a segmentation target with only seven points in 3D medical images, and design a two-stage weakly supervised learning framework PA-Seg. In the first stage, we employ geodesic distance transform to expand the seed points to provide more supervision signal. To further deal with unannotated image regions during training, we propose two contextual regularization strategies, i.e., multi-view Conditional Random Field (mCRF) loss and Variance Minimization (VM) loss, where the first one encourages pixels with similar features to have consistent labels, and the second one minimizes the intensity variance for the segmented foreground and background, respectively. In the second stage, we use predictions obtained by the model pre-trained in the first stage as pseudo labels. To overcome noises in the pseudo labels, we introduce a Self and Cross Monitoring (SCM) strategy, which combines self-training with Cross Knowledge Distillation (CKD) between a primary model and an auxiliary model that learn from soft labels generated by each other. Experiments on public datasets for Vestibular Schwannoma (VS) segmentation and Brain Tumor Segmentation (BraTS) demonstrated that our model trained in the first stage outperformed existing state-of-the-art weakly supervised approaches by a large margin, and after using SCM for additional training, the model's performance was close to its fully supervised counterpart on the BraTS dataset.
<div id='section'>Paperid: <span id='pid'>537, <a href='https://arxiv.org/pdf/2206.11011.pdf' target='_blank'>https://arxiv.org/pdf/2206.11011.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jia-Run Du, Jia-Chang Feng, Kun-Yu Lin, Fa-Ting Hong, Xiao-Ming Wu, Zhongang Qi, Ying Shan, Wei-Shi Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.11011">Weakly-Supervised Temporal Action Localization by Progressive Complementary Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Temporal Action Localization (WSTAL) aims to localize and classify action instances in long untrimmed videos with only video-level category labels. Due to the lack of snippet-level supervision for indicating action boundaries, previous methods typically assign pseudo labels for unlabeled snippets. However, since some action instances of different categories are visually similar, it is non-trivial to exactly label the (usually) one action category for a snippet, and incorrect pseudo labels would impair the localization performance. To address this problem, we propose a novel method from a category exclusion perspective, named Progressive Complementary Learning (ProCL), which gradually enhances the snippet-level supervision. Our method is inspired by the fact that video-level labels precisely indicate the categories that all snippets surely do not belong to, which is ignored by previous works. Accordingly, we first exclude these surely non-existent categories by a complementary learning loss. And then, we introduce the background-aware pseudo complementary labeling in order to exclude more categories for snippets of less ambiguity. Furthermore, for the remaining ambiguous snippets, we attempt to reduce the ambiguity by distinguishing foreground actions from the background. Extensive experimental results show that our method achieves new state-of-the-art performance on two popular benchmarks, namely THUMOS14 and ActivityNet1.3.
<div id='section'>Paperid: <span id='pid'>538, <a href='https://arxiv.org/pdf/2401.11847.pdf' target='_blank'>https://arxiv.org/pdf/2401.11847.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Chen, Jiaze Wang, Ziyu Guo, Jinpeng Li, Donghao Zhou, Bian Wu, Chenyong Guan, Guangyong Chen, Pheng-Ann Heng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11847">SignVTCL: Multi-Modal Continuous Sign Language Recognition Enhanced by Visual-Textual Contrastive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sign language recognition (SLR) plays a vital role in facilitating communication for the hearing-impaired community. SLR is a weakly supervised task where entire videos are annotated with glosses, making it challenging to identify the corresponding gloss within a video segment. Recent studies indicate that the main bottleneck in SLR is the insufficient training caused by the limited availability of large-scale datasets. To address this challenge, we present SignVTCL, a multi-modal continuous sign language recognition framework enhanced by visual-textual contrastive learning, which leverages the full potential of multi-modal data and the generalization ability of language model. SignVTCL integrates multi-modal data (video, keypoints, and optical flow) simultaneously to train a unified visual backbone, thereby yielding more robust visual representations. Furthermore, SignVTCL contains a visual-textual alignment approach incorporating gloss-level and sentence-level alignment to ensure precise correspondence between visual features and glosses at the level of individual glosses and sentence. Experimental results conducted on three datasets, Phoenix-2014, Phoenix-2014T, and CSL-Daily, demonstrate that SignVTCL achieves state-of-the-art results compared with previous methods.
<div id='section'>Paperid: <span id='pid'>539, <a href='https://arxiv.org/pdf/2303.17774.pdf' target='_blank'>https://arxiv.org/pdf/2303.17774.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gengxin Liu, Qian Sun, Haibin Huang, Chongyang Ma, Yulan Guo, Li Yi, Hui Huang, Ruizhen Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.17774">Semi-Weakly Supervised Object Kinematic Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Given a 3D object, kinematic motion prediction aims to identify the mobile parts as well as the corresponding motion parameters. Due to the large variations in both topological structure and geometric details of 3D objects, this remains a challenging task and the lack of large scale labeled data also constrain the performance of deep learning based approaches. In this paper, we tackle the task of object kinematic motion prediction problem in a semi-weakly supervised manner. Our key observations are two-fold. First, although 3D dataset with fully annotated motion labels is limited, there are existing datasets and methods for object part semantic segmentation at large scale. Second, semantic part segmentation and mobile part segmentation is not always consistent but it is possible to detect the mobile parts from the underlying 3D structure. Towards this end, we propose a graph neural network to learn the map between hierarchical part-level segmentation and mobile parts parameters, which are further refined based on geometric alignment. This network can be first trained on PartNet-Mobility dataset with fully labeled mobility information and then applied on PartNet dataset with fine-grained and hierarchical part-level segmentation. The network predictions yield a large scale of 3D objects with pseudo labeled mobility information and can further be used for weakly-supervised learning with pre-existing segmentation. Our experiments show there are significant performance boosts with the augmented data for previous method designed for kinematic motion prediction on 3D partial scans.
<div id='section'>Paperid: <span id='pid'>540, <a href='https://arxiv.org/pdf/2510.14668.pdf' target='_blank'>https://arxiv.org/pdf/2510.14668.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md. Abdur Rahman, Mohaimenul Azam Khan Raiaan, Sami Azam, Asif Karim, Jemima Beissbarth, Amanda Leach
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.14668">WeCKD: Weakly-supervised Chained Distillation Network for Efficient Multimodal Medical Imaging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Knowledge distillation (KD) has traditionally relied on a static teacher-student framework, where a large, well-trained teacher transfers knowledge to a single student model. However, these approaches often suffer from knowledge degradation, inefficient supervision, and reliance on either a very strong teacher model or large labeled datasets, which limits their effectiveness in real-world, limited-data scenarios. To address these, we present the first-ever Weakly-supervised Chain-based KD network (WeCKD) that redefines knowledge transfer through a structured sequence of interconnected models. Unlike conventional KD, it forms a progressive distillation chain, where each model not only learns from its predecessor but also refines the knowledge before passing it forward. This structured knowledge transfer further enhances feature learning, reduces data dependency, and mitigates the limitations of one-step KD. Each model in the distillation chain is trained on only a fraction of the dataset and demonstrates that effective learning can be achieved with minimal supervision. Extensive evaluations across four otoscopic imaging datasets demonstrate that it not only matches but in many cases surpasses the performance of existing supervised methods. Experimental results on two other datasets further underscore its generalization across diverse medical imaging modalities, including microscopic and magnetic resonance imaging. Furthermore, our evaluations resulted in cumulative accuracy gains of up to +23% over a single backbone trained on the same limited data, which highlights its potential for real-world adoption.
<div id='section'>Paperid: <span id='pid'>541, <a href='https://arxiv.org/pdf/2509.12090.pdf' target='_blank'>https://arxiv.org/pdf/2509.12090.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihong Chen, Jiancheng Yang, Deniz Sayin Mercadier, Hieu Le, Juerg Schwitter, Pascal Fua
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12090">End-to-End 4D Heart Mesh Recovery Across Full-Stack and Sparse Cardiac MRI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing cardiac motion from cine CMR sequences is critical for diagnosis, prediction, and intervention. Existing methods rely on complete CMR stacks to infer full heart motion, limiting their utility in intra-procedural scenarios where only sparse observations are available. We present TetHeart, the first end-to-end framework that unifies full 4D multi-structure heart mesh recovery from both offline full-stack acquisitions and intra-procedural sparse-slice observations. Our method leverages deep deformable tetrahedra, an explicit-implicit hybrid representation, to capture shape and motion in a coherent space shared across cardiac structures. It is initialized from high-quality pre-procedural or offline-acquired full stacks to build detailed, patient-specific heart meshes, which can then be updated using whatever slices are available, from full stacks down to a single slice. We further incorporate several key innovations: (i) an attentive mechanism for slice-adaptive 2D-3D feature assembly that dynamically integrates information from arbitrary numbers of slices at any position, combined with a distillation strategy from full-slice to sparse-slice settings to ensure accurate reconstruction under extreme sparsity; and (ii) a two-stage weakly supervised motion learning scheme requiring only keyframe (e.g., ED and ES) annotations. Trained and validated on three large public datasets and externally evaluated zero-shot on additional private interventional and public CMR datasets, TetHeart achieves state-of-the-art accuracy and strong generalization in both pre- and intra-procedural settings.
<div id='section'>Paperid: <span id='pid'>542, <a href='https://arxiv.org/pdf/2503.20190.pdf' target='_blank'>https://arxiv.org/pdf/2503.20190.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Chen, Jiawen Li, Jiali Hu, Xitong Ling, Tian Guan, Anjia Han, Yonghong He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20190">Cross-Modal Prototype Allocation: Unsupervised Slide Representation Learning via Patch-Text Contrast in Computational Pathology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancement of pathology foundation models (FMs), the representation learning of whole slide images (WSIs) attracts increasing attention. Existing studies develop high-quality patch feature extractors and employ carefully designed aggregation schemes to derive slide-level representations. However, mainstream weakly supervised slide representation learning methods, primarily based on multiple instance learning (MIL), are tailored to specific downstream tasks, which limits their generalizability. To address this issue, some studies explore unsupervised slide representation learning. However, these approaches focus solely on the visual modality of patches, neglecting the rich semantic information embedded in textual data. In this work, we propose ProAlign, a cross-modal unsupervised slide representation learning framework. Specifically, we leverage a large language model (LLM) to generate descriptive text for the prototype types present in a WSI, introducing patch-text contrast to construct initial prototype embeddings. Furthermore, we propose a parameter-free attention aggregation strategy that utilizes the similarity between patches and these prototypes to form unsupervised slide embeddings applicable to a wide range of downstream tasks. Extensive experiments on four public datasets show that ProAlign outperforms existing unsupervised frameworks and achieves performance comparable to some weakly supervised models.
<div id='section'>Paperid: <span id='pid'>543, <a href='https://arxiv.org/pdf/2503.00915.pdf' target='_blank'>https://arxiv.org/pdf/2503.00915.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xitong Ling, Yifeng Ping, Jiawen Li, Jing Peng, Yuxuan Chen, Minxi Ouyang, Yizhi Wang, Yonghong He, Tian Guan, Xiaoping Liu, Lianghui Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00915">Multimodal Distillation-Driven Ensemble Learning for Long-Tailed Histopathology Whole Slide Images Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple Instance Learning (MIL) plays a significant role in computational pathology, enabling weakly supervised analysis of Whole Slide Image (WSI) datasets. The field of WSI analysis is confronted with a severe long-tailed distribution problem, which significantly impacts the performance of classifiers. Long-tailed distributions lead to class imbalance, where some classes have sparse samples while others are abundant, making it difficult for classifiers to accurately identify minority class samples. To address this issue, we propose an ensemble learning method based on MIL, which employs expert decoders with shared aggregators and consistency constraints to learn diverse distributions and reduce the impact of class imbalance on classifier performance. Moreover, we introduce a multimodal distillation framework that leverages text encoders pre-trained on pathology-text pairs to distill knowledge and guide the MIL aggregator in capturing stronger semantic features relevant to class information. To ensure flexibility, we use learnable prompts to guide the distillation process of the pre-trained text encoder, avoiding limitations imposed by specific prompts. Our method, MDE-MIL, integrates multiple expert branches focusing on specific data distributions to address long-tailed issues. Consistency control ensures generalization across classes. Multimodal distillation enhances feature extraction. Experiments on Camelyon+-LT and PANDA-LT datasets show it outperforms state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>544, <a href='https://arxiv.org/pdf/2502.20823.pdf' target='_blank'>https://arxiv.org/pdf/2502.20823.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawen Li, Jiali Hu, Qiehe Sun, Renao Yan, Minxi Ouyang, Tian Guan, Anjia Han, Chao He, Yonghong He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20823">Can We Simplify Slide-level Fine-tuning of Pathology Foundation Models?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emergence of foundation models in computational pathology has transformed histopathological image analysis, with whole slide imaging (WSI) diagnosis being a core application. Traditionally, weakly supervised fine-tuning via multiple instance learning (MIL) has been the primary method for adapting foundation models to WSIs. However, in this work we present a key experimental finding: a simple nonlinear mapping strategy combining mean pooling and a multilayer perceptron, called SiMLP, can effectively adapt patch-level foundation models to slide-level tasks without complex MIL-based learning. Through extensive experiments across diverse downstream tasks, we demonstrate the superior performance of SiMLP with state-of-the-art methods. For instance, on a large-scale pan-cancer classification task, SiMLP surpasses popular MIL-based methods by 3.52%. Furthermore, SiMLP shows strong learning ability in few-shot classification and remaining highly competitive with slide-level foundation models pretrained on tens of thousands of slides. Finally, SiMLP exhibits remarkable robustness and transferability in lung cancer subtyping. Overall, our findings challenge the conventional MIL-based fine-tuning paradigm, demonstrating that a task-agnostic representation strategy alone can effectively adapt foundation models to WSI analysis. These insights offer a unique and meaningful perspective for future research in digital pathology, paving the way for more efficient and broadly applicable methodologies.
<div id='section'>Paperid: <span id='pid'>545, <a href='https://arxiv.org/pdf/2412.19847.pdf' target='_blank'>https://arxiv.org/pdf/2412.19847.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexandr Korchemnyi, Alexey K. Kovalev, Aleksandr I. Panov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19847">Symbolic Disentangled Representations for Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The idea of disentangled representations is to reduce the data to a set of generative factors that produce it. Typically, such representations are vectors in latent space, where each coordinate corresponds to one of the generative factors. The object can then be modified by changing the value of a particular coordinate, but it is necessary to determine which coordinate corresponds to the desired generative factor -- a difficult task if the vector representation has a high dimension. In this article, we propose ArSyD (Architecture for Symbolic Disentanglement), which represents each generative factor as a vector of the same dimension as the resulting representation. In ArSyD, the object representation is obtained as a superposition of the generative factor vector representations. We call such a representation a \textit{symbolic disentangled representation}. We use the principles of Hyperdimensional Computing (also known as Vector Symbolic Architectures), where symbols are represented as hypervectors, allowing vector operations on them. Disentanglement is achieved by construction, no additional assumptions about the underlying distributions are made during training, and the model is only trained to reconstruct images in a weakly supervised manner. We study ArSyD on the dSprites and CLEVR datasets and provide a comprehensive analysis of the learned symbolic disentangled representations. We also propose new disentanglement metrics that allow comparison of methods using latent representations of different dimensions. ArSyD allows to edit the object properties in a controlled and interpretable way, and the dimensionality of the object property representation coincides with the dimensionality of the object representation itself.
<div id='section'>Paperid: <span id='pid'>546, <a href='https://arxiv.org/pdf/2411.06702.pdf' target='_blank'>https://arxiv.org/pdf/2411.06702.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jia Syuen Lim, Yadan Luo, Zhi Chen, Tianqi Wei, Scott Chapman, Zi Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06702">Track Any Peppers: Weakly Supervised Sweet Pepper Tracking Using VLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the Detection and Multi-Object Tracking of Sweet Peppers Challenge, we present Track Any Peppers (TAP) - a weakly supervised ensemble technique for sweet peppers tracking. TAP leverages the zero-shot detection capabilities of vision-language foundation models like Grounding DINO to automatically generate pseudo-labels for sweet peppers in video sequences with minimal human intervention. These pseudo-labels, refined when necessary, are used to train a YOLOv8 segmentation network. To enhance detection accuracy under challenging conditions, we incorporate pre-processing techniques such as relighting adjustments and apply depth-based filtering during post-inference. For object tracking, we integrate the Matching by Segment Anything (MASA) adapter with the BoT-SORT algorithm. Our approach achieves a HOTA score of 80.4%, MOTA of 66.1%, Recall of 74.0%, and Precision of 90.7%, demonstrating effective tracking of sweet peppers without extensive manual effort. This work highlights the potential of foundation models for efficient and accurate object detection and tracking in agricultural settings.
<div id='section'>Paperid: <span id='pid'>547, <a href='https://arxiv.org/pdf/2409.11664.pdf' target='_blank'>https://arxiv.org/pdf/2409.11664.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xitong Ling, Minxi Ouyang, Yizhi Wang, Xinrui Chen, Renao Yan, Hongbo Chu, Junru Cheng, Tian Guan, Sufang Tian, Xiaoping Liu, Yonghong He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11664">Agent Aggregator with Mask Denoise Mechanism for Histopathology Whole Slide Image Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Histopathology analysis is the gold standard for medical diagnosis. Accurate classification of whole slide images (WSIs) and region-of-interests (ROIs) localization can assist pathologists in diagnosis. The gigapixel resolution of WSI and the absence of fine-grained annotations make direct classification and analysis challenging. In weakly supervised learning, multiple instance learning (MIL) presents a promising approach for WSI classification. The prevailing strategy is to use attention mechanisms to measure instance importance for classification. However, attention mechanisms fail to capture inter-instance information, and self-attention causes quadratic computational complexity. To address these challenges, we propose AMD-MIL, an agent aggregator with a mask denoise mechanism. The agent token acts as an intermediate variable between the query and key for computing instance importance. Mask and denoising matrices, mapped from agents-aggregated value, dynamically mask low-contribution representations and eliminate noise. AMD-MIL achieves better attention allocation by adjusting feature representations, capturing micro-metastases in cancer, and improving interpretability. Extensive experiments on CAMELYON-16, CAMELYON-17, TCGA-KIDNEY, and TCGA-LUNG show AMD-MIL's superiority over state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>548, <a href='https://arxiv.org/pdf/2408.12825.pdf' target='_blank'>https://arxiv.org/pdf/2408.12825.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingxi Ouyang, Yuqiu Fu, Renao Yan, ShanShan Shi, Xitong Ling, Lianghui Zhu, Yonghong He, Tian Guan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.12825">MergeUp-augmented Semi-Weakly Supervised Learning for WSI Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in computational pathology and artificial intelligence have significantly improved whole slide image (WSI) classification. However, the gigapixel resolution of WSIs and the scarcity of manual annotations present substantial challenges. Multiple instance learning (MIL) is a promising weakly supervised learning approach for WSI classification. Recently research revealed employing pseudo bag augmentation can encourage models to learn various data, thus bolstering models' performance. While directly inheriting the parents' labels can introduce more noise by mislabeling in training. To address this issue, we translate the WSI classification task from weakly supervised learning to semi-weakly supervised learning, termed SWS-MIL, where adaptive pseudo bag augmentation (AdaPse) is employed to assign labeled and unlabeled data based on a threshold strategy. Using the "student-teacher" pattern, we introduce a feature augmentation technique, MergeUp, which merges bags with low-priority bags to enhance inter-category information, increasing training data diversity. Experimental results on the CAMELYON-16, BRACS, and TCGA-LUNG datasets demonstrate the superiority of our method over existing state-of-the-art approaches, affirming its efficacy in WSI classification.
<div id='section'>Paperid: <span id='pid'>549, <a href='https://arxiv.org/pdf/2307.06344.pdf' target='_blank'>https://arxiv.org/pdf/2307.06344.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiehe Sun, Jiawen Li, Jin Xu, Junru Cheng, Tian Guan, Yonghong He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.06344">The Whole Pathological Slide Classification via Weakly Supervised Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to its superior efficiency in utilizing annotations and addressing gigapixel-sized images, multiple instance learning (MIL) has shown great promise as a framework for whole slide image (WSI) classification in digital pathology diagnosis. However, existing methods tend to focus on advanced aggregators with different structures, often overlooking the intrinsic features of H\&E pathological slides. To address this limitation, we introduced two pathological priors: nuclear heterogeneity of diseased cells and spatial correlation of pathological tiles. Leveraging the former, we proposed a data augmentation method that utilizes stain separation during extractor training via a contrastive learning strategy to obtain instance-level representations. We then described the spatial relationships between the tiles using an adjacency matrix. By integrating these two views, we designed a multi-instance framework for analyzing H\&E-stained tissue images based on pathological inductive bias, encompassing feature extraction, filtering, and aggregation. Extensive experiments on the Camelyon16 breast dataset and TCGA-NSCLC Lung dataset demonstrate that our proposed framework can effectively handle tasks related to cancer detection and differentiation of subtypes, outperforming state-of-the-art medical image classification methods based on MIL. The code will be released later.
<div id='section'>Paperid: <span id='pid'>550, <a href='https://arxiv.org/pdf/2510.11073.pdf' target='_blank'>https://arxiv.org/pdf/2510.11073.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Tian, Min Zhou, Yitong Chen, Fang Li, Lingzi Qi, Shuo Wang, Xieyang Xu, Yu Yu, Shiqiong Xu, Chaoyu Lei, Yankai Jiang, Rongzhao Zhang, Jia Tan, Li Wu, Hong Chen, Xiaowei Liu, Wei Lu, Lin Li, Huifang Zhou, Xuefei Song, Guangtao Zhai, Xianqun Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.11073">ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Patient face images provide a convenient mean for evaluating eye diseases, while also raising privacy concerns. Here, we introduce ROFI, a deep learning-based privacy protection framework for ophthalmology. Using weakly supervised learning and neural identity translation, ROFI anonymizes facial features while retaining disease features (over 98\% accuracy, $κ> 0.90$). It achieves 100\% diagnostic sensitivity and high agreement ($κ> 0.90$) across eleven eye diseases in three cohorts, anonymizing over 95\% of images. ROFI works with AI systems, maintaining original diagnoses ($κ> 0.80$), and supports secure image reversal (over 98\% similarity), enabling audits and long-term care. These results show ROFI's effectiveness of protecting patient privacy in the digital medicine era.
<div id='section'>Paperid: <span id='pid'>551, <a href='https://arxiv.org/pdf/2505.18989.pdf' target='_blank'>https://arxiv.org/pdf/2505.18989.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Catalina Tan, Yipeng Hu, Shaheer U. Saeed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18989">SPARS: Self-Play Adversarial Reinforcement Learning for Segmentation of Liver Tumours</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate tumour segmentation is vital for various targeted diagnostic and therapeutic procedures for cancer, e.g., planning biopsies or tumour ablations. Manual delineation is extremely labour-intensive, requiring substantial expert time. Fully-supervised machine learning models aim to automate such localisation tasks, but require a large number of costly and often subjective 3D voxel-level labels for training. The high-variance and subjectivity in such labels impacts model generalisability, even when large datasets are available. Histopathology labels may offer more objective labels but the infeasibility of acquiring pixel-level annotations to develop tumour localisation methods based on histology remains challenging in-vivo. In this work, we propose a novel weakly-supervised semantic segmentation framework called SPARS (Self-Play Adversarial Reinforcement Learning for Segmentation), which utilises an object presence classifier, trained on a small number of image-level binary cancer presence labels, to localise cancerous regions on CT scans. Such binary labels of patient-level cancer presence can be sourced more feasibly from biopsies and histopathology reports, enabling a more objective cancer localisation on medical images. Evaluating with real patient data, we observed that SPARS yielded a mean dice score of $77.3 \pm 9.4$, which outperformed other weakly-supervised methods by large margins. This performance was comparable with recent fully-supervised methods that require voxel-level annotations. Our results demonstrate the potential of using SPARS to reduce the need for extensive human-annotated labels to detect cancer in real-world healthcare settings.
<div id='section'>Paperid: <span id='pid'>552, <a href='https://arxiv.org/pdf/2505.17915.pdf' target='_blank'>https://arxiv.org/pdf/2505.17915.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lynn Karam, Yipei Wang, Veeru Kasivisvanathan, Mirabela Rusu, Yipeng Hu, Shaheer U. Saeed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17915">Promptable cancer segmentation using minimal expert-curated data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automated segmentation of cancer on medical images can aid targeted diagnostic and therapeutic procedures. However, its adoption is limited by the high cost of expert annotations required for training and inter-observer variability in datasets. While weakly-supervised methods mitigate some challenges, using binary histology labels for training as opposed to requiring full segmentation, they require large paired datasets of histology and images, which are difficult to curate. Similarly, promptable segmentation aims to allow segmentation with no re-training for new tasks at inference, however, existing models perform poorly on pathological regions, again necessitating large datasets for training. In this work we propose a novel approach for promptable segmentation requiring only 24 fully-segmented images, supplemented by 8 weakly-labelled images, for training. Curating this minimal data to a high standard is relatively feasible and thus issues with the cost and variability of obtaining labels can be mitigated. By leveraging two classifiers, one weakly-supervised and one fully-supervised, our method refines segmentation through a guided search process initiated by a single-point prompt. Our approach outperforms existing promptable segmentation methods, and performs comparably with fully-supervised methods, for the task of prostate cancer segmentation, while using substantially less annotated data (up to 100X less). This enables promptable segmentation with very minimal labelled data, such that the labels can be curated to a very high standard.
<div id='section'>Paperid: <span id='pid'>553, <a href='https://arxiv.org/pdf/2411.10356.pdf' target='_blank'>https://arxiv.org/pdf/2411.10356.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrea Agostini, DaphnÃ© Chopard, Yang Meng, Norbert Fortin, Babak Shahbaba, Stephan Mandt, Thomas M. Sutter, Julia E. Vogt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10356">Weakly-Supervised Multimodal Learning on MIMIC-CXR</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal data integration and label scarcity pose significant challenges for machine learning in medical settings. To address these issues, we conduct an in-depth evaluation of the newly proposed Multimodal Variational Mixture-of-Experts (MMVM) VAE on the challenging MIMIC-CXR dataset. Our analysis demonstrates that the MMVM VAE consistently outperforms other multimodal VAEs and fully supervised approaches, highlighting its strong potential for real-world medical applications.
<div id='section'>Paperid: <span id='pid'>554, <a href='https://arxiv.org/pdf/2403.11449.pdf' target='_blank'>https://arxiv.org/pdf/2403.11449.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hang Gao, Jiaguo Yuan, Jiangmeng Li, Peng Qiao, Fengge Wu, Changwen Zheng, Huaping Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11449">Graph Partial Label Learning with Potential Cause Discovering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph Neural Networks (GNNs) have garnered widespread attention for their potential to address the challenges posed by graph representation learning, which face complex graph-structured data across various domains. However, due to the inherent complexity and interconnectedness of graphs, accurately annotating graph data for training GNNs is extremely challenging. To address this issue, we have introduced Partial Label Learning (PLL) into graph representation learning. PLL is a critical weakly supervised learning problem where each training instance is associated with a set of candidate labels, including the ground-truth label and the additional interfering labels. PLL allows annotators to make errors, which reduces the difficulty of data labeling. Subsequently, we propose a novel graph representation learning method that enables GNN models to effectively learn discriminative information within the context of PLL. Our approach utilizes potential cause extraction to obtain graph data that holds causal relationships with the labels. By conducting auxiliary training based on the extracted graph data, our model can effectively eliminate the interfering information in the PLL scenario. We support the rationale behind our method with a series of theoretical analyses. Moreover, we conduct extensive evaluations and ablation studies on multiple datasets, demonstrating the superiority of our proposed method.
<div id='section'>Paperid: <span id='pid'>555, <a href='https://arxiv.org/pdf/2402.13778.pdf' target='_blank'>https://arxiv.org/pdf/2402.13778.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Martynas Pocius, Wen Yan, Dean C. Barratt, Mark Emberton, Matthew J. Clarkson, Yipeng Hu, Shaheer U. Saeed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.13778">Weakly supervised localisation of prostate cancer using reinforcement learning for bi-parametric MR images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper we propose a reinforcement learning based weakly supervised system for localisation. We train a controller function to localise regions of interest within an image by introducing a novel reward definition that utilises non-binarised classification probability, generated by a pre-trained binary classifier which classifies object presence in images or image crops. The object-presence classifier may then inform the controller of its localisation quality by quantifying the likelihood of the image containing an object. Such an approach allows us to minimize any potential labelling or human bias propagated via human labelling for fully supervised localisation. We evaluate our proposed approach for a task of cancerous lesion localisation on a large dataset of real clinical bi-parametric MR images of the prostate. Comparisons to the commonly used multiple-instance learning weakly supervised localisation and to a fully supervised baseline show that our proposed method outperforms the multi-instance learning and performs comparably to fully-supervised learning, using only image-level classification labels for training.
<div id='section'>Paperid: <span id='pid'>556, <a href='https://arxiv.org/pdf/2402.10728.pdf' target='_blank'>https://arxiv.org/pdf/2402.10728.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiwen Li, Yunguan Fu, Iani J. M. B. Gayo, Qianye Yang, Zhe Min, Shaheer U. Saeed, Wen Yan, Yipei Wang, J. Alison Noble, Mark Emberton, Matthew J. Clarkson, Dean C. Barratt, Victor A. Prisacariu, Yipeng Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.10728">Semi-weakly-supervised neural network training for medical image registration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For training registration networks, weak supervision from segmented corresponding regions-of-interest (ROIs) have been proven effective for (a) supplementing unsupervised methods, and (b) being used independently in registration tasks in which unsupervised losses are unavailable or ineffective. This correspondence-informing supervision entails cost in annotation that requires significant specialised effort. This paper describes a semi-weakly-supervised registration pipeline that improves the model performance, when only a small corresponding-ROI-labelled dataset is available, by exploiting unlabelled image pairs. We examine two types of augmentation methods by perturbation on network weights and image resampling, such that consistency-based unsupervised losses can be applied on unlabelled data. The novel WarpDDF and RegCut approaches are proposed to allow commutative perturbation between an image pair and the predicted spatial transformation (i.e. respective input and output of registration networks), distinct from existing perturbation methods for classification or segmentation. Experiments using 589 male pelvic MR images, labelled with eight anatomical ROIs, show the improvement in registration performance and the ablated contributions from the individual strategies. Furthermore, this study attempts to construct one of the first computational atlases for pelvic structures, enabled by registering inter-subject MRs, and quantifies the significant differences due to the proposed semi-weak supervision with a discussion on the potential clinical use of example atlas-derived statistics.
<div id='section'>Paperid: <span id='pid'>557, <a href='https://arxiv.org/pdf/2310.15098.pdf' target='_blank'>https://arxiv.org/pdf/2310.15098.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu-Cheng Chou, Bowen Li, Deng-Ping Fan, Alan Yuille, Zongwei Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.15098">Acquiring Weak Annotations for Tumor Localization in Temporal and Volumetric Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating large-scale and well-annotated datasets to train AI algorithms is crucial for automated tumor detection and localization. However, with limited resources, it is challenging to determine the best type of annotations when annotating massive amounts of unlabeled data. To address this issue, we focus on polyps in colonoscopy videos and pancreatic tumors in abdominal CT scans; both applications require significant effort and time for pixel-wise annotation due to the high dimensional nature of the data, involving either temporary or spatial dimensions. In this paper, we develop a new annotation strategy, termed Drag&Drop, which simplifies the annotation process to drag and drop. This annotation strategy is more efficient, particularly for temporal and volumetric imaging, than other types of weak annotations, such as per-pixel, bounding boxes, scribbles, ellipses, and points. Furthermore, to exploit our Drag&Drop annotations, we develop a novel weakly supervised learning method based on the watershed algorithm. Experimental results show that our method achieves better detection and localization performance than alternative weak annotations and, more importantly, achieves similar performance to that trained on detailed per-pixel annotations. Interestingly, we find that, with limited resources, allocating weak annotations from a diverse patient population can foster models more robust to unseen images than allocating per-pixel annotations for a small set of images. In summary, this research proposes an efficient annotation strategy for tumor detection and localization that is less accurate than per-pixel annotations but useful for creating large-scale datasets for screening tumors in various medical modalities.
<div id='section'>Paperid: <span id='pid'>558, <a href='https://arxiv.org/pdf/2308.11376.pdf' target='_blank'>https://arxiv.org/pdf/2308.11376.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weixi Yi, Vasilis Stavrinides, Zachary M. C. Baum, Qianye Yang, Dean C. Barratt, Matthew J. Clarkson, Yipeng Hu, Shaheer U. Saeed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.11376">Boundary-RL: Reinforcement Learning for Weakly-Supervised Prostate Segmentation in TRUS Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose Boundary-RL, a novel weakly supervised segmentation method that utilises only patch-level labels for training. We envision the segmentation as a boundary detection problem, rather than a pixel-level classification as in previous works. This outlook on segmentation may allow for boundary delineation under challenging scenarios such as where noise artefacts may be present within the region-of-interest (ROI) boundaries, where traditional pixel-level classification-based weakly supervised methods may not be able to effectively segment the ROI. Particularly of interest, ultrasound images, where intensity values represent acoustic impedance differences between boundaries, may also benefit from the boundary delineation approach. Our method uses reinforcement learning to train a controller function to localise boundaries of ROIs using a reward derived from a pre-trained boundary-presence classifier. The classifier indicates when an object boundary is encountered within a patch, as the controller modifies the patch location in a sequential Markov decision process. The classifier itself is trained using only binary patch-level labels of object presence, which are the only labels used during training of the entire boundary delineation framework, and serves as a weak signal to inform the boundary delineation. The use of a controller function ensures that a sliding window over the entire image is not necessary. It also prevents possible false-positive or -negative cases by minimising number of patches passed to the boundary-presence classifier. We evaluate our proposed approach for a clinically relevant task of prostate gland segmentation on trans-rectal ultrasound images. We show improved performance compared to other tested weakly supervised methods, using the same labels e.g., multiple instance learning.
<div id='section'>Paperid: <span id='pid'>559, <a href='https://arxiv.org/pdf/2304.07978.pdf' target='_blank'>https://arxiv.org/pdf/2304.07978.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingqiu Zhou, Linjiang Huang, Liang Wang, Si Liu, Hongsheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.07978">Improving Weakly Supervised Temporal Action Localization by Bridging Train-Test Gap in Pseudo Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The task of weakly supervised temporal action localization targets at generating temporal boundaries for actions of interest, meanwhile the action category should also be classified. Pseudo-label-based methods, which serve as an effective solution, have been widely studied recently. However, existing methods generate pseudo labels during training and make predictions during testing under different pipelines or settings, resulting in a gap between training and testing. In this paper, we propose to generate high-quality pseudo labels from the predicted action boundaries. Nevertheless, we note that existing post-processing, like NMS, would lead to information loss, which is insufficient to generate high-quality action boundaries. More importantly, transforming action boundaries into pseudo labels is quite challenging, since the predicted action instances are generally overlapped and have different confidence scores. Besides, the generated pseudo-labels can be fluctuating and inaccurate at the early stage of training. It might repeatedly strengthen the false predictions if there is no mechanism to conduct self-correction. To tackle these issues, we come up with an effective pipeline for learning better pseudo labels. Firstly, we propose a Gaussian weighted fusion module to preserve information of action instances and obtain high-quality action boundaries. Second, we formulate the pseudo-label generation as an optimization problem under the constraints in terms of the confidence scores of action instances. Finally, we introduce the idea of $Î$ pseudo labels, which enables the model with the ability of self-correction. Our method achieves superior performance to existing methods on two benchmarks, THUMOS14 and ActivityNet1.3, achieving gains of 1.9\% on THUMOS14 and 3.7\% on ActivityNet1.3 in terms of average mAP.
<div id='section'>Paperid: <span id='pid'>560, <a href='https://arxiv.org/pdf/2210.04183.pdf' target='_blank'>https://arxiv.org/pdf/2210.04183.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijia Zhao, Longteng Guo, Xingjian He, Shuai Shao, Zehuan Yuan, Jing Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.04183">MAMO: Masked Multimodal Modeling for Fine-Grained Vision-Language Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal representation learning has shown promising improvements on various vision-language tasks. Most existing methods excel at building global-level alignment between vision and language while lacking effective fine-grained image-text interaction. In this paper, we propose a jointly masked multimodal modeling method to learn fine-grained multimodal representations. Our method performs joint masking on image-text input and integrates both implicit and explicit targets for the masked signals to recover. The implicit target provides a unified and debiased objective for vision and language, where the model predicts latent multimodal representations of the unmasked input. The explicit target further enriches the multimodal representations by recovering high-level and semantically meaningful information: momentum visual features of image patches and concepts of word tokens. Through such a masked modeling process, our model not only learns fine-grained multimodal interaction, but also avoids the semantic gap between high-level representations and low- or mid-level prediction targets (e.g. image pixels), thus producing semantically rich multimodal representations that perform well on both zero-shot and fine-tuned settings. Our pre-trained model (named MAMO) achieves state-of-the-art performance on various downstream vision-language tasks, including image-text retrieval, visual question answering, visual reasoning, and weakly-supervised visual grounding.
<div id='section'>Paperid: <span id='pid'>561, <a href='https://arxiv.org/pdf/2203.01629.pdf' target='_blank'>https://arxiv.org/pdf/2203.01629.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomas M. Sutter, Laura Manduchi, Alain Ryser, Julia E. Vogt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.01629">Learning Group Importance using the Differentiable Hypergeometric Distribution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partitioning a set of elements into subsets of a priori unknown sizes is essential in many applications. These subset sizes are rarely explicitly learned - be it the cluster sizes in clustering applications or the number of shared versus independent generative latent factors in weakly-supervised learning. Probability distributions over correct combinations of subset sizes are non-differentiable due to hard constraints, which prohibit gradient-based optimization. In this work, we propose the differentiable hypergeometric distribution. The hypergeometric distribution models the probability of different group sizes based on their relative importance. We introduce reparameterizable gradients to learn the importance between groups and highlight the advantage of explicitly learning the size of subsets in two typical applications: weakly-supervised learning and clustering. In both applications, we outperform previous approaches, which rely on suboptimal heuristics to model the unknown size of groups.
<div id='section'>Paperid: <span id='pid'>562, <a href='https://arxiv.org/pdf/2510.16536.pdf' target='_blank'>https://arxiv.org/pdf/2510.16536.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Niranjana Arun Menon, Yulong Li, Iqra Farooq, Sara Ahmed, Muhammad Awais, Imran Razzak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.16536">Few-Label Multimodal Modeling of SNP Variants and ECG Phenotypes Using Large Language Models for Cardiovascular Risk Stratification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cardiovascular disease (CVD) risk stratification remains a major challenge due to its multifactorial nature and limited availability of high-quality labeled datasets. While genomic and electrophysiological data such as SNP variants and ECG phenotypes are increasingly accessible, effectively integrating these modalities in low-label settings is non-trivial. This challenge arises from the scarcity of well-annotated multimodal datasets and the high dimensionality of biological signals, which limit the effectiveness of conventional supervised models. To address this, we present a few-label multimodal framework that leverages large language models (LLMs) to combine genetic and electrophysiological information for cardiovascular risk stratification. Our approach incorporates a pseudo-label refinement strategy to adaptively distill high-confidence labels from weakly supervised predictions, enabling robust model fine-tuning with only a small set of ground-truth annotations. To enhance the interpretability, we frame the task as a Chain of Thought (CoT) reasoning problem, prompting the model to produce clinically relevant rationales alongside predictions. Experimental results demonstrate that the integration of multimodal inputs, few-label supervision, and CoT reasoning improves robustness and generalizability across diverse patient profiles. Experimental results using multimodal SNP variants and ECG-derived features demonstrated comparable performance to models trained on the full dataset, underscoring the promise of LLM-based few-label multimodal modeling for advancing personalized cardiovascular care.
<div id='section'>Paperid: <span id='pid'>563, <a href='https://arxiv.org/pdf/2510.03016.pdf' target='_blank'>https://arxiv.org/pdf/2510.03016.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dong-Dong Wu, Jiacheng Cui, Wei Wang, Zhiqiang She, Masashi Sugiyama
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03016">Learning Robust Diffusion Models from Imprecise Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conditional diffusion models have achieved remarkable success in various generative tasks recently, but their training typically relies on large-scale datasets that inevitably contain imprecise information in conditional inputs. Such supervision, often stemming from noisy, ambiguous, or incomplete labels, will cause condition mismatch and degrade generation quality. To address this challenge, we propose DMIS, a unified framework for training robust Diffusion Models from Imprecise Supervision, which is the first systematic study within diffusion models. Our framework is derived from likelihood maximization and decomposes the objective into generative and classification components: the generative component models imprecise-label distributions, while the classification component leverages a diffusion classifier to infer class-posterior probabilities, with its efficiency further improved by an optimized timestep sampling strategy. Extensive experiments on diverse forms of imprecise supervision, covering tasks of image generation, weakly supervised learning, and noisy dataset condensation demonstrate that DMIS consistently produces high-quality and class-discriminative samples.
<div id='section'>Paperid: <span id='pid'>564, <a href='https://arxiv.org/pdf/2407.09826.pdf' target='_blank'>https://arxiv.org/pdf/2407.09826.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoxu Xu, Yitian Yuan, Jinlong Li, Qiudan Zhang, Zequn Jie, Lin Ma, Hao Tang, Nicu Sebe, Xu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.09826">3D Weakly Supervised Semantic Segmentation with 2D Vision-Language Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose 3DSS-VLG, a weakly supervised approach for 3D Semantic Segmentation with 2D Vision-Language Guidance, an alternative approach that a 3D model predicts dense-embedding for each point which is co-embedded with both the aligned image and text spaces from the 2D vision-language model. Specifically, our method exploits the superior generalization ability of the 2D vision-language models and proposes the Embeddings Soft-Guidance Stage to utilize it to implicitly align 3D embeddings and text embeddings. Moreover, we introduce the Embeddings Specialization Stage to purify the feature representation with the help of a given scene-level label, specifying a better feature supervised by the corresponding text embedding. Thus, the 3D model is able to gain informative supervisions both from the image embedding and text embedding, leading to competitive segmentation performances. To the best of our knowledge, this is the first work to investigate 3D weakly supervised semantic segmentation by using the textual semantic information of text category labels. Moreover, with extensive quantitative and qualitative experiments, we present that our 3DSS-VLG is able not only to achieve the state-of-the-art performance on both S3DIS and ScanNet datasets, but also to maintain strong generalization capability.
<div id='section'>Paperid: <span id='pid'>565, <a href='https://arxiv.org/pdf/2312.09625.pdf' target='_blank'>https://arxiv.org/pdf/2312.09625.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoxu Xu, Yitian Yuan, Qiudan Zhang, Wenhui Wu, Zequn Jie, Lin Ma, Xu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.09625">Weakly-Supervised 3D Visual Grounding based on Visual Language Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning to ground natural language queries to target objects or regions in 3D point clouds is quite essential for 3D scene understanding. Nevertheless, existing 3D visual grounding approaches require a substantial number of bounding box annotations for text queries, which is time-consuming and labor-intensive to obtain. In this paper, we propose 3D-VLA, a weakly supervised approach for 3D visual grounding based on Visual Linguistic Alignment. Our 3D-VLA exploits the superior ability of current large-scale vision-language models (VLMs) on aligning the semantics between texts and 2D images, as well as the naturally existing correspondences between 2D images and 3D point clouds, and thus implicitly constructs correspondences between texts and 3D point clouds with no need for fine-grained box annotations in the training procedure. During the inference stage, the learned text-3D correspondence will help us ground the text queries to the 3D target objects even without 2D images. To the best of our knowledge, this is the first work to investigate 3D visual grounding in a weakly supervised manner by involving large scale vision-language models, and extensive experiments on ReferIt3D and ScanRefer datasets demonstrate that our 3D-VLA achieves comparable and even superior results over the fully supervised methods.
<div id='section'>Paperid: <span id='pid'>566, <a href='https://arxiv.org/pdf/2305.14546.pdf' target='_blank'>https://arxiv.org/pdf/2305.14546.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vamsikrishna Chemudupati, Marzieh Tahaei, Heitor Guimaraes, Arthur Pimentel, Anderson Avila, Mehdi Rezagholizadeh, Boxing Chen, Tiago Falk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.14546">On the Transferability of Whisper-based Representations for "In-the-Wild" Cross-Task Downstream Speech Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large self-supervised pre-trained speech models have achieved remarkable success across various speech-processing tasks. The self-supervised training of these models leads to universal speech representations that can be used for different downstream tasks, ranging from automatic speech recognition (ASR) to speaker identification. Recently, Whisper, a transformer-based model was proposed and trained on large amount of weakly supervised data for ASR; it outperformed several state-of-the-art self-supervised models. Given the superiority of Whisper for ASR, in this paper we explore the transferability of the representation for four other speech tasks in SUPERB benchmark. Moreover, we explore the robustness of Whisper representation for ``in the wild'' tasks where speech is corrupted by environment noise and room reverberation. Experimental results show Whisper achieves promising results across tasks and environmental conditions, thus showing potential for cross-task real-world deployment.
<div id='section'>Paperid: <span id='pid'>567, <a href='https://arxiv.org/pdf/2310.07795.pdf' target='_blank'>https://arxiv.org/pdf/2310.07795.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siru Ouyang, Jiaxin Huang, Pranav Pillai, Yunyi Zhang, Yu Zhang, Jiawei Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.07795">Ontology Enrichment for Effective Fine-grained Entity Typing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fine-grained entity typing (FET) is the task of identifying specific entity types at a fine-grained level for entity mentions based on their contextual information. Conventional methods for FET require extensive human annotation, which is time-consuming and costly. Recent studies have been developing weakly supervised or zero-shot approaches. We study the setting of zero-shot FET where only an ontology is provided. However, most existing ontology structures lack rich supporting information and even contain ambiguous relations, making them ineffective in guiding FET. Recently developed language models, though promising in various few-shot and zero-shot NLP tasks, may face challenges in zero-shot FET due to their lack of interaction with task-specific ontology. In this study, we propose OnEFET, where we (1) enrich each node in the ontology structure with two types of extra information: instance information for training sample augmentation and topic information to relate types to contexts, and (2) develop a coarse-to-fine typing algorithm that exploits the enriched information by training an entailment model with contrasting topics and instance-based augmented training samples. Our experiments show that OnEFET achieves high-quality fine-grained entity typing without human annotation, outperforming existing zero-shot methods by a large margin and rivaling supervised methods.
<div id='section'>Paperid: <span id='pid'>568, <a href='https://arxiv.org/pdf/2309.16964.pdf' target='_blank'>https://arxiv.org/pdf/2309.16964.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunjiao Zhou, Jianfei Yang, He Huang, Lihua Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.16964">AdaPose: Towards Cross-Site Device-Free Human Pose Estimation with Commodity WiFi</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>WiFi-based pose estimation is a technology with great potential for the development of smart homes and metaverse avatar generation. However, current WiFi-based pose estimation methods are predominantly evaluated under controlled laboratory conditions with sophisticated vision models to acquire accurately labeled data. Furthermore, WiFi CSI is highly sensitive to environmental variables, and direct application of a pre-trained model to a new environment may yield suboptimal results due to domain shift. In this paper, we proposes a domain adaptation algorithm, AdaPose, designed specifically for weakly-supervised WiFi-based pose estimation. The proposed method aims to identify consistent human poses that are highly resistant to environmental dynamics. To achieve this goal, we introduce a Mapping Consistency Loss that aligns the domain discrepancy of source and target domains based on inner consistency between input and output at the mapping level. We conduct extensive experiments on domain adaptation in two different scenes using our self-collected pose estimation dataset containing WiFi CSI frames. The results demonstrate the effectiveness and robustness of AdaPose in eliminating domain shift, thereby facilitating the widespread application of WiFi-based pose estimation in smart cities.
<div id='section'>Paperid: <span id='pid'>569, <a href='https://arxiv.org/pdf/2309.07601.pdf' target='_blank'>https://arxiv.org/pdf/2309.07601.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>JoÃ£o A. Leite, Olesya Razuvayevskaya, Kalina Bontcheva, Carolina Scarton
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.07601">Weakly Supervised Veracity Classification with LLM-Predicted Credibility Signals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Credibility signals represent a wide range of heuristics typically used by journalists and fact-checkers to assess the veracity of online content. Automating the extraction of credibility signals presents significant challenges due to the necessity of training high-accuracy, signal-specific extractors, coupled with the lack of sufficiently large annotated datasets. This paper introduces Pastel (Prompted weAk Supervision wiTh crEdibility signaLs), a weakly supervised approach that leverages large language models (LLMs) to extract credibility signals from web content, and subsequently combines them to predict the veracity of content without relying on human supervision. We validate our approach using four article-level misinformation detection datasets, demonstrating that Pastel outperforms zero-shot veracity detection by 38.3% and achieves 86.7% of the performance of the state-of-the-art system trained with human supervision. Moreover, in cross-domain settings where training and testing datasets originate from different domains, Pastel significantly outperforms the state-of-the-art supervised model by 63%. We further study the association between credibility signals and veracity, and perform an ablation study showing the impact of each signal on model performance. Our findings reveal that 12 out of the 19 proposed signals exhibit strong associations with veracity across all datasets, while some signals show domain-specific strengths.
<div id='section'>Paperid: <span id='pid'>570, <a href='https://arxiv.org/pdf/2307.01448.pdf' target='_blank'>https://arxiv.org/pdf/2307.01448.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ming Zhong, Siru Ouyang, Minhao Jiang, Vivian Hu, Yizhu Jiao, Xuan Wang, Jiawei Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.01448">ReactIE: Enhancing Chemical Reaction Extraction with Weak Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Structured chemical reaction information plays a vital role for chemists engaged in laboratory work and advanced endeavors such as computer-aided drug design. Despite the importance of extracting structured reactions from scientific literature, data annotation for this purpose is cost-prohibitive due to the significant labor required from domain experts. Consequently, the scarcity of sufficient training data poses an obstacle to the progress of related models in this domain. In this paper, we propose ReactIE, which combines two weakly supervised approaches for pre-training. Our method utilizes frequent patterns within the text as linguistic cues to identify specific characteristics of chemical reactions. Additionally, we adopt synthetic data from patent records as distant supervision to incorporate domain knowledge into the model. Experiments demonstrate that ReactIE achieves substantial improvements and outperforms all existing baselines.
<div id='section'>Paperid: <span id='pid'>571, <a href='https://arxiv.org/pdf/2306.07515.pdf' target='_blank'>https://arxiv.org/pdf/2306.07515.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meng Liu, Liqiang Nie, Yunxiao Wang, Meng Wang, Yong Rui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.07515">A Survey on Video Moment Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video moment localization, also known as video moment retrieval, aiming to search a target segment within a video described by a given natural language query. Beyond the task of temporal action localization whereby the target actions are pre-defined, video moment retrieval can query arbitrary complex activities. In this survey paper, we aim to present a comprehensive review of existing video moment localization techniques, including supervised, weakly supervised, and unsupervised ones. We also review the datasets available for video moment localization and group results of related work. In addition, we discuss promising future directions for this field, in particular large-scale datasets and interpretable video moment localization models.
<div id='section'>Paperid: <span id='pid'>572, <a href='https://arxiv.org/pdf/2301.02979.pdf' target='_blank'>https://arxiv.org/pdf/2301.02979.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng-Yen Yang, Jiajia Luo, Lu Xia, Yuyin Sun, Nan Qiao, Ke Zhang, Zhongyu Jiang, Jenq-Neng Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.02979">CameraPose: Weakly-Supervised Monocular 3D Human Pose Estimation by Leveraging In-the-wild 2D Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To improve the generalization of 3D human pose estimators, many existing deep learning based models focus on adding different augmentations to training poses. However, data augmentation techniques are limited to the "seen" pose combinations and hard to infer poses with rare "unseen" joint positions. To address this problem, we present CameraPose, a weakly-supervised framework for 3D human pose estimation from a single image, which can not only be applied on 2D-3D pose pairs but also on 2D alone annotations. By adding a camera parameter branch, any in-the-wild 2D annotations can be fed into our pipeline to boost the training diversity and the 3D poses can be implicitly learned by reprojecting back to 2D. Moreover, CameraPose introduces a refinement network module with confidence-guided loss to further improve the quality of noisy 2D keypoints extracted by 2D pose estimators. Experimental results demonstrate that the CameraPose brings in clear improvements on cross-scenario datasets. Notably, it outperforms the baseline method by 3mm on the most challenging dataset 3DPW. In addition, by combining our proposed refinement network module with existing 3D pose estimators, their performance can be improved in cross-scenario evaluation.
<div id='section'>Paperid: <span id='pid'>573, <a href='https://arxiv.org/pdf/2509.08126.pdf' target='_blank'>https://arxiv.org/pdf/2509.08126.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Houjian Yu, Zheming Zhou, Min Sun, Omid Ghasemalizadeh, Yuyin Sun, Cheng-Hao Kuo, Arnie Sen, Changhyun Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08126">Attribute-based Object Grounding and Robot Grasp Detection with Spatial Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enabling robots to grasp objects specified through natural language is essential for effective human-robot interaction, yet it remains a significant challenge. Existing approaches often struggle with open-form language expressions and typically assume unambiguous target objects without duplicates. Moreover, they frequently rely on costly, dense pixel-wise annotations for both object grounding and grasp configuration. We present Attribute-based Object Grounding and Robotic Grasping (OGRG), a novel framework that interprets open-form language expressions and performs spatial reasoning to ground target objects and predict planar grasp poses, even in scenes containing duplicated object instances. We investigate OGRG in two settings: (1) Referring Grasp Synthesis (RGS) under pixel-wise full supervision, and (2) Referring Grasp Affordance (RGA) using weakly supervised learning with only single-pixel grasp annotations. Key contributions include a bi-directional vision-language fusion module and the integration of depth information to enhance geometric reasoning, improving both grounding and grasping performance. Experiment results show that OGRG outperforms strong baselines in tabletop scenes with diverse spatial language instructions. In RGS, it operates at 17.59 FPS on a single NVIDIA RTX 2080 Ti GPU, enabling potential use in closed-loop or multi-object sequential grasping, while delivering superior grounding and grasp prediction accuracy compared to all the baselines considered. Under the weakly supervised RGA setting, OGRG also surpasses baseline grasp-success rates in both simulation and real-robot trials, underscoring the effectiveness of its spatial reasoning design. Project page: https://z.umn.edu/ogrg
<div id='section'>Paperid: <span id='pid'>574, <a href='https://arxiv.org/pdf/2508.17009.pdf' target='_blank'>https://arxiv.org/pdf/2508.17009.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wangyu Wu, Zhenhong Chen, Xiaowen Ma, Wenqiao Zhang, Xianglin Qiu, Siqi Song, Xiaowei Huang, Fei Ma, Jimin Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17009">Contrastive Prompt Clustering for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Semantic Segmentation (WSSS) with image-level labels has gained attention for its cost-effectiveness. Most existing methods emphasize inter-class separation, often neglecting the shared semantics among related categories and lacking fine-grained discrimination. To address this, we propose Contrastive Prompt Clustering (CPC), a novel WSSS framework. CPC exploits Large Language Models (LLMs) to derive category clusters that encode intrinsic inter-class relationships, and further introduces a class-aware patch-level contrastive loss to enforce intra-class consistency and inter-class separation. This hierarchical design leverages clusters as coarse-grained semantic priors while preserving fine-grained boundaries, thereby reducing confusion among visually similar categories. Experiments on PASCAL VOC 2012 and MS COCO 2014 demonstrate that CPC surpasses existing state-of-the-art methods in WSSS.
<div id='section'>Paperid: <span id='pid'>575, <a href='https://arxiv.org/pdf/2508.00235.pdf' target='_blank'>https://arxiv.org/pdf/2508.00235.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Erin Rainville, Amirhossein Rasoulian, Hassan Rivaz, Yiming Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00235">Weakly Supervised Intracranial Aneurysm Detection and Segmentation in MR angiography via Multi-task UNet with Vesselness Prior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intracranial aneurysms (IAs) are abnormal dilations of cerebral blood vessels that, if ruptured, can lead to life-threatening consequences. However, their small size and soft contrast in radiological scans often make it difficult to perform accurate and efficient detection and morphological analyses, which are critical in the clinical care of the disorder. Furthermore, the lack of large public datasets with voxel-wise expert annotations pose challenges for developing deep learning algorithms to address the issues. Therefore, we proposed a novel weakly supervised 3D multi-task UNet that integrates vesselness priors to jointly perform aneurysm detection and segmentation in time-of-flight MR angiography (TOF-MRA). Specifically, to robustly guide IA detection and segmentation, we employ the popular Frangi's vesselness filter to derive soft cerebrovascular priors for both network input and an attention block to conduct segmentation from the decoder and detection from an auxiliary branch. We train our model on the Lausanne dataset with coarse ground truth segmentation, and evaluate it on the test set with refined labels from the same database. To further assess our model's generalizability, we also validate it externally on the ADAM dataset. Our results demonstrate the superior performance of the proposed technique over the SOTA techniques for aneurysm segmentation (Dice = 0.614, 95%HD =1.38mm) and detection (false positive rate = 1.47, sensitivity = 92.9%).
<div id='section'>Paperid: <span id='pid'>576, <a href='https://arxiv.org/pdf/2412.20439.pdf' target='_blank'>https://arxiv.org/pdf/2412.20439.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wangyu Wu, Xianglin Qiu, Siqi Song, Zhenhong Chen, Xiaowei Huang, Fei Ma, Jimin Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20439">Image Augmentation Agent for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised semantic segmentation (WSSS) has achieved remarkable progress using only image-level labels. However, most existing WSSS methods focus on designing new network structures and loss functions to generate more accurate dense labels, overlooking the limitations imposed by fixed datasets, which can constrain performance improvements. We argue that more diverse trainable images provides WSSS richer information and help model understand more comprehensive semantic pattern. Therefore in this paper, we introduce a novel approach called Image Augmentation Agent (IAA) which shows that it is possible to enhance WSSS from data generation perspective. IAA mainly design an augmentation agent that leverages large language models (LLMs) and diffusion models to automatically generate additional images for WSSS. In practice, to address the instability in prompt generation by LLMs, we develop a prompt self-refinement mechanism. It allow LLMs to re-evaluate the rationality of generated prompts to produce more coherent prompts. Additionally, we insert an online filter into diffusion generation process to dynamically ensure the quality and balance of generated images. Experimental results show that our method significantly surpasses state-of-the-art WSSS approaches on the PASCAL VOC 2012 and MS COCO 2014 datasets.
<div id='section'>Paperid: <span id='pid'>577, <a href='https://arxiv.org/pdf/2412.13823.pdf' target='_blank'>https://arxiv.org/pdf/2412.13823.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wangyu Wu, Xianglin Qiu, Siqi Song, Xiaowei Huang, Fei Ma, Jimin Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13823">Prompt Categories Cluster for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Semantic Segmentation (WSSS), which leverages image-level labels, has garnered significant attention due to its cost-effectiveness. The previous methods mainly strengthen the inter-class differences to avoid class semantic ambiguity which may lead to erroneous activation. However, they overlook the positive function of some shared information between similar classes. Categories within the same cluster share some similar features. Allowing the model to recognize these features can further relieve the semantic ambiguity between these classes. To effectively identify and utilize this shared information, in this paper, we introduce a novel WSSS framework called Prompt Categories Clustering (PCC). Specifically, we explore the ability of Large Language Models (LLMs) to derive category clusters through prompts. These clusters effectively represent the intrinsic relationships between categories. By integrating this relational information into the training network, our model is able to better learn the hidden connections between categories. Experimental results demonstrate the effectiveness of our approach, showing its ability to enhance performance on the PASCAL VOC 2012 dataset and surpass existing state-of-the-art methods in WSSS.
<div id='section'>Paperid: <span id='pid'>578, <a href='https://arxiv.org/pdf/2407.10649.pdf' target='_blank'>https://arxiv.org/pdf/2407.10649.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wangyu Wu, Tianhong Dai, Zhenhong Chen, Xiaowei Huang, Jimin Xiao, Fei Ma, Renrong Ouyang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10649">Adaptive Patch Contrast for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Semantic Segmentation (WSSS) using only image-level labels has gained significant attention due to its cost-effectiveness. The typical framework involves using image-level labels as training data to generate pixel-level pseudo-labels with refinements. Recently, methods based on Vision Transformers (ViT) have demonstrated superior capabilities in generating reliable pseudo-labels, particularly in recognizing complete object regions, compared to CNN methods. However, current ViT-based approaches have some limitations in the use of patch embeddings, being prone to being dominated by certain abnormal patches, as well as many multi-stage methods being time-consuming and lengthy in training, thus lacking efficiency. Therefore, in this paper, we introduce a novel ViT-based WSSS method named \textit{Adaptive Patch Contrast} (APC) that significantly enhances patch embedding learning for improved segmentation effectiveness. APC utilizes an Adaptive-K Pooling (AKP) layer to address the limitations of previous max pooling selection methods. Additionally, we propose a Patch Contrastive Learning (PCL) to enhance patch embeddings, thereby further improving the final results. Furthermore, we improve upon the existing multi-stage training framework without CAM by transforming it into an end-to-end single-stage training approach, thereby enhancing training efficiency. The experimental results show that our approach is effective and efficient, outperforming other state-of-the-art WSSS methods on the PASCAL VOC 2012 and MS COCO 2014 dataset within a shorter training duration.
<div id='section'>Paperid: <span id='pid'>579, <a href='https://arxiv.org/pdf/2405.08911.pdf' target='_blank'>https://arxiv.org/pdf/2405.08911.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pavan Kumar Anasosalu Vasu, Hadi Pouransari, Fartash Faghri, Oncel Tuzel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.08911">CLIP with Quality Captions: A Strong Pretraining for Vision Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>CLIP models perform remarkably well on zero-shot classification and retrieval tasks. But recent studies have shown that learnt representations in CLIP are not well suited for dense prediction tasks like object detection, semantic segmentation or depth estimation. More recently, multi-stage training methods for CLIP models was introduced to mitigate the weak performance of CLIP on downstream tasks. In this work, we find that simply improving the quality of captions in image-text datasets improves the quality of CLIP's visual representations, resulting in significant improvement on downstream dense prediction vision tasks. In fact, we find that CLIP pretraining with good quality captions can surpass recent supervised, self-supervised and weakly supervised pretraining methods. We show that when CLIP model with ViT-B/16 as image encoder is trained on well aligned image-text pairs it obtains 12.1% higher mIoU and 11.5% lower RMSE on semantic segmentation and depth estimation tasks over recent state-of-the-art Masked Image Modeling (MIM) pretraining methods like Masked Autoencoder (MAE). We find that mobile architectures also benefit significantly from CLIP pretraining. A recent mobile vision architecture, MCi2, with CLIP pretraining obtains similar performance as Swin-L, pretrained on ImageNet-22k for semantic segmentation task while being 6.1$\times$ smaller. Moreover, we show that improving caption quality results in $10\times$ data efficiency when finetuning for dense prediction tasks.
<div id='section'>Paperid: <span id='pid'>580, <a href='https://arxiv.org/pdf/2404.11036.pdf' target='_blank'>https://arxiv.org/pdf/2404.11036.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Paras Sheth, Tharindu Kumarage, Raha Moraffah, Aman Chadha, Huan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.11036">Cross-Platform Hate Speech Detection with Weakly Supervised Causal Disentanglement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Content moderation faces a challenging task as social media's ability to spread hate speech contrasts with its role in promoting global connectivity. With rapidly evolving slang and hate speech, the adaptability of conventional deep learning to the fluid landscape of online dialogue remains limited. In response, causality inspired disentanglement has shown promise by segregating platform specific peculiarities from universal hate indicators. However, its dependency on available ground truth target labels for discerning these nuances faces practical hurdles with the incessant evolution of platforms and the mutable nature of hate speech. Using confidence based reweighting and contrastive regularization, this study presents HATE WATCH, a novel framework of weakly supervised causal disentanglement that circumvents the need for explicit target labeling and effectively disentangles input features into invariant representations of hate. Empirical validation across platforms two with target labels and two without positions HATE WATCH as a novel method in cross platform hate speech detection with superior performance. HATE WATCH advances scalable content moderation techniques towards developing safer online communities.
<div id='section'>Paperid: <span id='pid'>581, <a href='https://arxiv.org/pdf/2403.01156.pdf' target='_blank'>https://arxiv.org/pdf/2403.01156.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lian Xu, Mohammed Bennamoun, Farid Boussaid, Wanli Ouyang, Ferdous Sohel, Dan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.01156">Auxiliary Tasks Enhanced Dual-affinity Learning for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most existing weakly supervised semantic segmentation (WSSS) methods rely on Class Activation Mapping (CAM) to extract coarse class-specific localization maps using image-level labels. Prior works have commonly used an off-line heuristic thresholding process that combines the CAM maps with off-the-shelf saliency maps produced by a general pre-trained saliency model to produce more accurate pseudo-segmentation labels. We propose AuxSegNet+, a weakly supervised auxiliary learning framework to explore the rich information from these saliency maps and the significant inter-task correlation between saliency detection and semantic segmentation. In the proposed AuxSegNet+, saliency detection and multi-label image classification are used as auxiliary tasks to improve the primary task of semantic segmentation with only image-level ground-truth labels. We also propose a cross-task affinity learning mechanism to learn pixel-level affinities from the saliency and segmentation feature maps. In particular, we propose a cross-task dual-affinity learning module to learn both pairwise and unary affinities, which are used to enhance the task-specific features and predictions by aggregating both query-dependent and query-independent global context for both saliency detection and semantic segmentation. The learned cross-task pairwise affinity can also be used to refine and propagate CAM maps to provide better pseudo labels for both tasks. Iterative improvement of segmentation performance is enabled by cross-task affinity learning and pseudo-label updating. Extensive experiments demonstrate the effectiveness of the proposed approach with new state-of-the-art WSSS results on the challenging PASCAL VOC and MS COCO benchmarks.
<div id='section'>Paperid: <span id='pid'>582, <a href='https://arxiv.org/pdf/2402.19144.pdf' target='_blank'>https://arxiv.org/pdf/2402.19144.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xueying Jiang, Sheng Jin, Lewei Lu, Xiaoqin Zhang, Shijian Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.19144">Weakly Supervised Monocular 3D Detection with a Single-View Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monocular 3D detection (M3D) aims for precise 3D object localization from a single-view image which usually involves labor-intensive annotation of 3D detection boxes. Weakly supervised M3D has recently been studied to obviate the 3D annotation process by leveraging many existing 2D annotations, but it often requires extra training data such as LiDAR point clouds or multi-view images which greatly degrades its applicability and usability in various applications. We propose SKD-WM3D, a weakly supervised monocular 3D detection framework that exploits depth information to achieve M3D with a single-view image exclusively without any 3D annotations or other training data. One key design in SKD-WM3D is a self-knowledge distillation framework, which transforms image features into 3D-like representations by fusing depth information and effectively mitigates the inherent depth ambiguity in monocular scenarios with little computational overhead in inference. In addition, we design an uncertainty-aware distillation loss and a gradient-targeted transfer modulation strategy which facilitate knowledge acquisition and knowledge transfer, respectively. Extensive experiments show that SKD-WM3D surpasses the state-of-the-art clearly and is even on par with many fully supervised methods.
<div id='section'>Paperid: <span id='pid'>583, <a href='https://arxiv.org/pdf/2310.13259.pdf' target='_blank'>https://arxiv.org/pdf/2310.13259.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jeremy Lai, Faruk Ahmed, Supriya Vijay, Tiam Jaroensri, Jessica Loo, Saurabh Vyawahare, Saloni Agarwal, Fayaz Jamil, Yossi Matias, Greg S. Corrado, Dale R. Webster, Jonathan Krause, Yun Liu, Po-Hsuan Cameron Chen, Ellery Wulczyn, David F. Steiner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.13259">Domain-specific optimization and diverse evaluation of self-supervised models for histopathology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Task-specific deep learning models in histopathology offer promising opportunities for improving diagnosis, clinical research, and precision medicine. However, development of such models is often limited by availability of high-quality data. Foundation models in histopathology that learn general representations across a wide range of tissue types, diagnoses, and magnifications offer the potential to reduce the data, compute, and technical expertise necessary to develop task-specific deep learning models with the required level of model performance. In this work, we describe the development and evaluation of foundation models for histopathology via self-supervised learning (SSL). We first establish a diverse set of benchmark tasks involving 17 unique tissue types and 12 unique cancer types and spanning different optimal magnifications and task types. Next, we use this benchmark to explore and evaluate histopathology-specific SSL methods followed by further evaluation on held out patch-level and weakly supervised tasks. We found that standard SSL methods thoughtfully applied to histopathology images are performant across our benchmark tasks and that domain-specific methodological improvements can further increase performance. Our findings reinforce the value of using domain-specific SSL methods in pathology, and establish a set of high quality foundation models to enable further research across diverse applications.
<div id='section'>Paperid: <span id='pid'>584, <a href='https://arxiv.org/pdf/2310.09828.pdf' target='_blank'>https://arxiv.org/pdf/2310.09828.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wangyu Wu, Tianhong Dai, Xiaowei Huang, Fei Ma, Jimin Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.09828">Top-K Pooling with Patch Contrastive Learning for Weakly-Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Semantic Segmentation (WSSS) using only image-level labels has gained significant attention due to cost-effectiveness. Recently, Vision Transformer (ViT) based methods without class activation map (CAM) have shown greater capability in generating reliable pseudo labels than previous methods using CAM. However, the current ViT-based methods utilize max pooling to select the patch with the highest prediction score to map the patch-level classification to the image-level one, which may affect the quality of pseudo labels due to the inaccurate classification of the patches. In this paper, we introduce a novel ViT-based WSSS method named top-K pooling with patch contrastive learning (TKP-PCL), which employs a top-K pooling layer to alleviate the limitations of previous max pooling selection. A patch contrastive error (PCE) is also proposed to enhance the patch embeddings to further improve the final results. The experimental results show that our approach is very efficient and outperforms other state-of-the-art WSSS methods on the PASCAL VOC 2012 dataset.
<div id='section'>Paperid: <span id='pid'>585, <a href='https://arxiv.org/pdf/2310.09760.pdf' target='_blank'>https://arxiv.org/pdf/2310.09760.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wangyu Wu, Tianhong Dai, Xiaowei Huang, Fei Ma, Jimin Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.09760">Image Augmentation with Controlled Diffusion for Weakly-Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised semantic segmentation (WSSS), which aims to train segmentation models solely using image-level labels, has achieved significant attention. Existing methods primarily focus on generating high-quality pseudo labels using available images and their image-level labels. However, the quality of pseudo labels degrades significantly when the size of available dataset is limited. Thus, in this paper, we tackle this problem from a different view by introducing a novel approach called Image Augmentation with Controlled Diffusion (IACD). This framework effectively augments existing labeled datasets by generating diverse images through controlled diffusion, where the available images and image-level labels are served as the controlling information. Moreover, we also propose a high-quality image selection strategy to mitigate the potential noise introduced by the randomness of diffusion models. In the experiments, our proposed IACD approach clearly surpasses existing state-of-the-art methods. This effect is more obvious when the amount of available data is small, demonstrating the effectiveness of our method.
<div id='section'>Paperid: <span id='pid'>586, <a href='https://arxiv.org/pdf/2308.03005.pdf' target='_blank'>https://arxiv.org/pdf/2308.03005.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lian Xu, Mohammed Bennamoun, Farid Boussaid, Hamid Laga, Wanli Ouyang, Dan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.03005">MCTformer+: Multi-Class Token Transformer for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a novel transformer-based framework that aims to enhance weakly supervised semantic segmentation (WSSS) by generating accurate class-specific object localization maps as pseudo labels. Building upon the observation that the attended regions of the one-class token in the standard vision transformer can contribute to a class-agnostic localization map, we explore the potential of the transformer model to capture class-specific attention for class-discriminative object localization by learning multiple class tokens. We introduce a Multi-Class Token transformer, which incorporates multiple class tokens to enable class-aware interactions with the patch tokens. To achieve this, we devise a class-aware training strategy that establishes a one-to-one correspondence between the output class tokens and the ground-truth class labels. Moreover, a Contrastive-Class-Token (CCT) module is proposed to enhance the learning of discriminative class tokens, enabling the model to better capture the unique characteristics and properties of each class. As a result, class-discriminative object localization maps can be effectively generated by leveraging the class-to-patch attentions associated with different class tokens. To further refine these localization maps, we propose the utilization of patch-level pairwise affinity derived from the patch-to-patch transformer attention. Furthermore, the proposed framework seamlessly complements the Class Activation Mapping (CAM) method, resulting in significantly improved WSSS performance on the PASCAL VOC 2012 and MS COCO 2014 datasets. These results underline the importance of the class token for WSSS.
<div id='section'>Paperid: <span id='pid'>587, <a href='https://arxiv.org/pdf/2304.08938.pdf' target='_blank'>https://arxiv.org/pdf/2304.08938.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rongliang Wu, Yingchen Yu, Fangneng Zhan, Jiahui Zhang, Shengcai Liao, Shijian Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.08938">POCE: Pose-Controllable Expression Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Facial expression editing has attracted increasing attention with the advance of deep neural networks in recent years. However, most existing methods suffer from compromised editing fidelity and limited usability as they either ignore pose variations (unrealistic editing) or require paired training data (not easy to collect) for pose controls. This paper presents POCE, an innovative pose-controllable expression editing network that can generate realistic facial expressions and head poses simultaneously with just unpaired training images. POCE achieves the more accessible and realistic pose-controllable expression editing by mapping face images into UV space, where facial expressions and head poses can be disentangled and edited separately. POCE has two novel designs. The first is self-supervised UV completion that allows to complete UV maps sampled under different head poses, which often suffer from self-occlusions and missing facial texture. The second is weakly-supervised UV editing that allows to generate new facial expressions with minimal modification of facial identity, where the synthesized expression could be controlled by either an expression label or directly transplanted from a reference UV map via feature transfer. Extensive experiments show that POCE can learn from unpaired face images effectively, and the learned model can generate realistic and high-fidelity facial expressions under various new poses.
<div id='section'>Paperid: <span id='pid'>588, <a href='https://arxiv.org/pdf/2202.05126.pdf' target='_blank'>https://arxiv.org/pdf/2202.05126.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Jiang, Yanning Zhou, Yi Lin, Ronald CK Chan, Jiang Liu, Hao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.05126">Deep Learning for Computational Cytology: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computational cytology is a critical, rapid-developing, yet challenging topic in the field of medical image computing which analyzes the digitized cytology image by computer-aided technologies for cancer screening. Recently, an increasing number of deep learning (DL) algorithms have made significant progress in medical image analysis, leading to the boosting publications of cytological studies. To investigate the advanced methods and comprehensive applications, we survey more than 120 publications of DL-based cytology image analysis in this article. We first introduce various deep learning methods, including fully supervised, weakly supervised, unsupervised, and transfer learning. Then, we systematically summarize the public datasets, evaluation metrics, versatile cytology image analysis applications including classification, detection, segmentation, and other related tasks. Finally, we discuss current challenges and potential research directions of computational cytology.
<div id='section'>Paperid: <span id='pid'>589, <a href='https://arxiv.org/pdf/2506.23519.pdf' target='_blank'>https://arxiv.org/pdf/2506.23519.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Qin, Runmin Cong, Gen Zhan, Yiting Liao, Sam Kwong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23519">From Sight to Insight: Unleashing Eye-Tracking in Weakly Supervised Video Salient Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The eye-tracking video saliency prediction (VSP) task and video salient object detection (VSOD) task both focus on the most attractive objects in video and show the result in the form of predictive heatmaps and pixel-level saliency masks, respectively. In practical applications, eye tracker annotations are more readily obtainable and align closely with the authentic visual patterns of human eyes. Therefore, this paper aims to introduce fixation information to assist the detection of video salient objects under weak supervision. On the one hand, we ponder how to better explore and utilize the information provided by fixation, and then propose a Position and Semantic Embedding (PSE) module to provide location and semantic guidance during the feature learning process. On the other hand, we achieve spatiotemporal feature modeling under weak supervision from the aspects of feature selection and feature contrast. A Semantics and Locality Query (SLQ) Competitor with semantic and locality constraints is designed to effectively select the most matching and accurate object query for spatiotemporal modeling. In addition, an Intra-Inter Mixed Contrastive (IIMC) model improves the spatiotemporal modeling capabilities under weak supervision by forming an intra-video and inter-video contrastive learning paradigm. Experimental results on five popular VSOD benchmarks indicate that our model outperforms other competitors on various evaluation metrics.
<div id='section'>Paperid: <span id='pid'>590, <a href='https://arxiv.org/pdf/2506.18042.pdf' target='_blank'>https://arxiv.org/pdf/2506.18042.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongdong Meng, Sheng Li, Hao Wu, Suqing Tian, Wenjun Ma, Guoping Wang, Xueqing Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18042">CmFNet: Cross-modal Fusion Network for Weakly-supervised Segmentation of Medical Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate automatic medical image segmentation relies on high-quality, dense annotations, which are costly and time-consuming. Weakly supervised learning provides a more efficient alternative by leveraging sparse and coarse annotations instead of dense, precise ones. However, segmentation performance degradation and overfitting caused by sparse annotations remain key challenges. To address these issues, we propose CmFNet, a novel 3D weakly supervised cross-modal medical image segmentation approach. CmFNet consists of three main components: a modality-specific feature learning network, a cross-modal feature learning network, and a hybrid-supervised learning strategy. Specifically, the modality-specific feature learning network and the cross-modal feature learning network effectively integrate complementary information from multi-modal images, enhancing shared features across modalities to improve segmentation performance. Additionally, the hybrid-supervised learning strategy guides segmentation through scribble supervision, intra-modal regularization, and inter-modal consistency, modeling spatial and contextual relationships while promoting feature alignment. Our approach effectively mitigates overfitting, delivering robust segmentation results. It excels in segmenting both challenging small tumor regions and common anatomical structures. Extensive experiments on a clinical cross-modal nasopharyngeal carcinoma (NPC) dataset (including CT and MR imaging) and the publicly available CT Whole Abdominal Organ dataset (WORD) show that our approach outperforms state-of-the-art weakly supervised methods. In addition, our approach also outperforms fully supervised methods when full annotation is used. Our approach can facilitate clinical therapy and benefit various specialists, including physicists, radiologists, pathologists, and oncologists.
<div id='section'>Paperid: <span id='pid'>591, <a href='https://arxiv.org/pdf/2402.08697.pdf' target='_blank'>https://arxiv.org/pdf/2402.08697.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David C. Oluigboa, Bikash Santra, Tejas Sudharshan Mathai, Pritam Mukherjee, Jianfei Liu, Abhishek Jha, Mayank Patel, Karel Pacak, Ronald M. Summers
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.08697">Weakly Supervised Detection of Pheochromocytomas and Paragangliomas in CT</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pheochromocytomas and Paragangliomas (PPGLs) are rare adrenal and extra-adrenal tumors which have the potential to metastasize. For the management of patients with PPGLs, CT is the preferred modality of choice for precise localization and estimation of their progression. However, due to the myriad variations in size, morphology, and appearance of the tumors in different anatomical regions, radiologists are posed with the challenge of accurate detection of PPGLs. Since clinicians also need to routinely measure their size and track their changes over time across patient visits, manual demarcation of PPGLs is quite a time-consuming and cumbersome process. To ameliorate the manual effort spent for this task, we propose an automated method to detect PPGLs in CT studies via a proxy segmentation task. As only weak annotations for PPGLs in the form of prospectively marked 2D bounding boxes on an axial slice were available, we extended these 2D boxes into weak 3D annotations and trained a 3D full-resolution nnUNet model to directly segment PPGLs. We evaluated our approach on a dataset consisting of chest-abdomen-pelvis CTs of 255 patients with confirmed PPGLs. We obtained a precision of 70% and sensitivity of 64.1% with our proposed approach when tested on 53 CT studies. Our findings highlight the promising nature of detecting PPGLs via segmentation, and furthers the state-of-the-art in this exciting yet challenging area of rare cancer management.
<div id='section'>Paperid: <span id='pid'>592, <a href='https://arxiv.org/pdf/2402.00175.pdf' target='_blank'>https://arxiv.org/pdf/2402.00175.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Sheng, Tejas Sudharshan Mathai, Alexander Shieh, Ronald M. Summers
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.00175">Weakly-Supervised Detection of Bone Lesions in CT</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The skeletal region is one of the common sites of metastatic spread of cancer in the breast and prostate. CT is routinely used to measure the size of lesions in the bones. However, they can be difficult to spot due to the wide variations in their sizes, shapes, and appearances. Precise localization of such lesions would enable reliable tracking of interval changes (growth, shrinkage, or unchanged status). To that end, an automated technique to detect bone lesions is highly desirable. In this pilot work, we developed a pipeline to detect bone lesions (lytic, blastic, and mixed) in CT volumes via a proxy segmentation task. First, we used the bone lesions that were prospectively marked by radiologists in a few 2D slices of CT volumes and converted them into weak 3D segmentation masks. Then, we trained a 3D full-resolution nnUNet model using these weak 3D annotations to segment the lesions and thereby detected them. Our automated method detected bone lesions in CT with a precision of 96.7% and recall of 47.3% despite the use of incomplete and partial training data. To the best of our knowledge, we are the first to attempt the direct detection of bone lesions in CT via a proxy segmentation task.
<div id='section'>Paperid: <span id='pid'>593, <a href='https://arxiv.org/pdf/2311.17118.pdf' target='_blank'>https://arxiv.org/pdf/2311.17118.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaming Zhou, Hanjun Li, Kun-Yu Lin, Junwei Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17118">Towards Weakly Supervised End-to-end Learning for Long-video Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing end-to-end action recognition models on long videos is fundamental and crucial for long-video action understanding. Due to the unaffordable cost of end-to-end training on the whole long videos, existing works generally train models on short clips trimmed from long videos. However, this ``trimming-then-training'' practice requires action interval annotations for clip-level supervision, i.e., knowing which actions are trimmed into the clips. Unfortunately, collecting such annotations is very expensive and prevents model training at scale. To this end, this work aims to build a weakly supervised end-to-end framework for training recognition models on long videos, with only video-level action category labels. Without knowing the precise temporal locations of actions in long videos, our proposed weakly supervised framework, namely AdaptFocus, estimates where and how likely the actions will occur to adaptively focus on informative action clips for end-to-end training. The effectiveness of the proposed AdaptFocus framework is demonstrated on three long-video datasets. Furthermore, for downstream long-video tasks, our AdaptFocus framework provides a weakly supervised feature extraction pipeline for extracting more robust long-video features, such that the state-of-the-art methods on downstream tasks are significantly advanced. We will release the code and models.
<div id='section'>Paperid: <span id='pid'>594, <a href='https://arxiv.org/pdf/2310.17650.pdf' target='_blank'>https://arxiv.org/pdf/2310.17650.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anas Al-lahham, Nurbek Tastan, Zaigham Zaheer, Karthik Nandakumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.17650">A Coarse-to-Fine Pseudo-Labeling (C2FPL) Framework for Unsupervised Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detection of anomalous events in videos is an important problem in applications such as surveillance. Video anomaly detection (VAD) is well-studied in the one-class classification (OCC) and weakly supervised (WS) settings. However, fully unsupervised (US) video anomaly detection methods, which learn a complete system without any annotation or human supervision, have not been explored in depth. This is because the lack of any ground truth annotations significantly increases the magnitude of the VAD challenge. To address this challenge, we propose a simple-but-effective two-stage pseudo-label generation framework that produces segment-level (normal/anomaly) pseudo-labels, which can be further used to train a segment-level anomaly detector in a supervised manner. The proposed coarse-to-fine pseudo-label (C2FPL) generator employs carefully-designed hierarchical divisive clustering and statistical hypothesis testing to identify anomalous video segments from a set of completely unlabeled videos. The trained anomaly detector can be directly applied on segments of an unseen test video to obtain segment-level, and subsequently, frame-level anomaly predictions. Extensive studies on two large-scale public-domain datasets, UCF-Crime and XD-Violence, demonstrate that the proposed unsupervised approach achieves superior performance compared to all existing OCC and US methods , while yielding comparable performance to the state-of-the-art WS methods.
<div id='section'>Paperid: <span id='pid'>595, <a href='https://arxiv.org/pdf/2506.14912.pdf' target='_blank'>https://arxiv.org/pdf/2506.14912.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dyah Adila, Shuai Zhang, Boran Han, Bonan Min, Yuyang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14912">CrEst: Credibility Estimation for Contexts in LLMs via Weak Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of contextual information has significantly enhanced the performance of large language models (LLMs) on knowledge-intensive tasks. However, existing methods often overlook a critical challenge: the credibility of context documents can vary widely, potentially leading to the propagation of unreliable information. In this paper, we introduce CrEst, a novel weakly supervised framework for assessing the credibility of context documents during LLM inference--without requiring manual annotations. Our approach is grounded in the insight that credible documents tend to exhibit higher semantic coherence with other credible documents, enabling automated credibility estimation through inter-document agreement. To incorporate credibility into LLM inference, we propose two integration strategies: a black-box approach for models without access to internal weights or activations, and a white-box method that directly modifies attention mechanisms. Extensive experiments across three model architectures and five datasets demonstrate that CrEst consistently outperforms strong baselines, achieving up to a 26.86% improvement in accuracy and a 3.49% increase in F1 score. Further analysis shows that CrEst maintains robust performance even under high-noise conditions.
<div id='section'>Paperid: <span id='pid'>596, <a href='https://arxiv.org/pdf/2505.22230.pdf' target='_blank'>https://arxiv.org/pdf/2505.22230.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhisong Wang, Yiwen Ye, Ziyang Chen, Yong Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22230">Enjoying Information Dividend: Gaze Track-based Medical Weakly Supervised Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation (WSSS) in medical imaging struggles with effectively using sparse annotations. One promising direction for WSSS leverages gaze annotations, captured via eye trackers that record regions of interest during diagnostic procedures. However, existing gaze-based methods, such as GazeMedSeg, do not fully exploit the rich information embedded in gaze data. In this paper, we propose GradTrack, a framework that utilizes physicians' gaze track, including fixation points, durations, and temporal order, to enhance WSSS performance. GradTrack comprises two key components: Gaze Track Map Generation and Track Attention, which collaboratively enable progressive feature refinement through multi-level gaze supervision during the decoding process. Experiments on the Kvasir-SEG and NCI-ISBI datasets demonstrate that GradTrack consistently outperforms existing gaze-based methods, achieving Dice score improvements of 3.21\% and 2.61\%, respectively. Moreover, GradTrack significantly narrows the performance gap with fully supervised models such as nnUNet.
<div id='section'>Paperid: <span id='pid'>597, <a href='https://arxiv.org/pdf/2505.01809.pdf' target='_blank'>https://arxiv.org/pdf/2505.01809.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoqi Li, Jiaming Liu, Nuowei Han, Liang Heng, Yandong Guo, Hao Dong, Yang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01809">3DWG: 3D Weakly Supervised Visual Grounding via Category and Instance-Level Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The 3D weakly-supervised visual grounding task aims to localize oriented 3D boxes in point clouds based on natural language descriptions without requiring annotations to guide model learning. This setting presents two primary challenges: category-level ambiguity and instance-level complexity. Category-level ambiguity arises from representing objects of fine-grained categories in a highly sparse point cloud format, making category distinction challenging. Instance-level complexity stems from multiple instances of the same category coexisting in a scene, leading to distractions during grounding. To address these challenges, we propose a novel weakly-supervised grounding approach that explicitly differentiates between categories and instances. In the category-level branch, we utilize extensive category knowledge from a pre-trained external detector to align object proposal features with sentence-level category features, thereby enhancing category awareness. In the instance-level branch, we utilize spatial relationship descriptions from language queries to refine object proposal features, ensuring clear differentiation among objects. These designs enable our model to accurately identify target-category objects while distinguishing instances within the same category. Compared to previous methods, our approach achieves state-of-the-art performance on three widely used benchmarks: Nr3D, Sr3D, and ScanRef.
<div id='section'>Paperid: <span id='pid'>598, <a href='https://arxiv.org/pdf/2411.12615.pdf' target='_blank'>https://arxiv.org/pdf/2411.12615.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Yang, Nitish Mehta, Xiaoling Hu, Chao Chen, Chia-Ling Tsai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.12615">A Multimodal Approach Combining Structural and Cross-domain Textual Guidance for Weakly Supervised OCT Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate segmentation of Optical Coherence Tomography (OCT) images is crucial for diagnosing and monitoring retinal diseases. However, the labor-intensive nature of pixel-level annotation limits the scalability of supervised learning with large datasets. Weakly Supervised Semantic Segmentation (WSSS) provides a promising alternative by leveraging image-level labels. In this study, we propose a novel WSSS approach that integrates structural guidance with text-driven strategies to generate high-quality pseudo labels, significantly improving segmentation performance. In terms of visual information, our method employs two processing modules that exchange raw image features and structural features from OCT images, guiding the model to identify where lesions are likely to occur. In terms of textual information, we utilize large-scale pretrained models from cross-domain sources to implement label-informed textual guidance and synthetic descriptive integration with two textual processing modules that combine local semantic features with consistent synthetic descriptions. By fusing these visual and textual components within a multimodal framework, our approach enhances lesion localization accuracy. Experimental results on three OCT datasets demonstrate that our method achieves state-of-the-art performance, highlighting its potential to improve diagnostic accuracy and efficiency in medical imaging.
<div id='section'>Paperid: <span id='pid'>599, <a href='https://arxiv.org/pdf/2410.23834.pdf' target='_blank'>https://arxiv.org/pdf/2410.23834.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cosmin I. Bercea, Philippe C. Cattin, Julia A. Schnabel, Julia Wolleb
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.23834">Denoising Diffusion Models for Anomaly Localization in Medical Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This chapter explores anomaly localization in medical images using denoising diffusion models. After providing a brief methodological background of these models, including their application to image reconstruction and their conditioning using guidance mechanisms, we provide an overview of available datasets and evaluation metrics suitable for their application to anomaly localization in medical images. In this context, we discuss supervision schemes ranging from fully supervised segmentation to semi-supervised, weakly supervised, self-supervised, and unsupervised methods, and provide insights into the effectiveness and limitations of these approaches. Furthermore, we highlight open challenges in anomaly localization, including detection bias, domain shift, computational cost, and model interpretability. Our goal is to provide an overview of the current state of the art in the field, outline research gaps, and highlight the potential of diffusion models for robust anomaly localization in medical images.
<div id='section'>Paperid: <span id='pid'>600, <a href='https://arxiv.org/pdf/2408.12814.pdf' target='_blank'>https://arxiv.org/pdf/2408.12814.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhisong Wang, Yiwen Ye, Ziyang Chen, Minglei Shu, Yanning Zhang, Yong Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.12814">From Few to More: Scribble-based Medical Image Segmentation via Masked Context Modeling and Continuous Pseudo Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scribble-based weakly supervised segmentation methods have shown promising results in medical image segmentation, significantly reducing annotation costs. However, existing approaches often rely on auxiliary tasks to enforce semantic consistency and use hard pseudo labels for supervision, overlooking the unique challenges faced by models trained with sparse annotations. These models must predict pixel-wise segmentation maps from limited data, making it crucial to handle varying levels of annotation richness effectively. In this paper, we propose MaCo, a weakly supervised model designed for medical image segmentation, based on the principle of "from few to more." MaCo leverages Masked Context Modeling (MCM) and Continuous Pseudo Labels (CPL). MCM employs an attention-based masking strategy to perturb the input image, ensuring that the model's predictions align with those of the original image. CPL converts scribble annotations into continuous pixel-wise labels by applying an exponential decay function to distance maps, producing confidence maps that represent the likelihood of each pixel belonging to a specific category, rather than relying on hard pseudo labels. We evaluate MaCo on three public datasets, comparing it with other weakly supervised methods. Our results show that MaCo outperforms competing methods across all datasets, establishing a new record in weakly supervised medical image segmentation.
<div id='section'>Paperid: <span id='pid'>601, <a href='https://arxiv.org/pdf/2406.11431.pdf' target='_blank'>https://arxiv.org/pdf/2406.11431.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenkai Yang, Shiqi Shen, Guangyao Shen, Wei Yao, Yong Liu, Zhi Gong, Yankai Lin, Ji-Rong Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.11431">Super(ficial)-alignment: Strong Models May Deceive Weak Models in Weak-to-Strong Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Superalignment, where humans act as weak supervisors for superhuman models, has become a crucial problem with the rapid development of Large Language Models (LLMs). Recent work has preliminarily studied this problem by using weak models to supervise strong models, and discovered that weakly supervised strong students can consistently outperform weak teachers towards the alignment target, leading to a weak-to-strong generalization phenomenon. However, we are concerned that behind such a promising phenomenon, whether there exists an issue of weak-to-strong deception, where strong models deceive weak models by exhibiting well-aligned in areas known to weak models but producing misaligned behaviors in cases weak models do not know. We take an initial step towards exploring this security issue in a specific but realistic multi-objective alignment case, where there may be some alignment targets conflicting with each other (e.g., helpfulness v.s. harmlessness). We aim to explore whether, in such cases, strong models might deliberately make mistakes in areas known to them but unknown to weak models within one alignment dimension, in exchange for a higher reward in another dimension. Through extensive experiments in both the reward modeling and preference optimization scenarios, we find: (1) The weak-to-strong deception phenomenon exists across all settings. (2) The deception intensifies as the capability gap between weak and strong models increases. (3) Bootstrapping with an intermediate model can mitigate the deception to some extent, though its effectiveness remains limited. Our work highlights the urgent need to pay more attention to the true reliability of superalignment.
<div id='section'>Paperid: <span id='pid'>602, <a href='https://arxiv.org/pdf/2404.05997.pdf' target='_blank'>https://arxiv.org/pdf/2404.05997.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junlin Hou, Jilan Xu, Hao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.05997">Concept-Attention Whitening for Interpretable Skin Lesion Diagnosis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The black-box nature of deep learning models has raised concerns about their interpretability for successful deployment in real-world clinical applications. To address the concerns, eXplainable Artificial Intelligence (XAI) aims to provide clear and understandable explanations of the decision-making process. In the medical domain, concepts such as attributes of lesions or abnormalities serve as key evidence for deriving diagnostic results. Existing concept-based models mainly depend on concepts that appear independently and require fine-grained concept annotations such as bounding boxes. However, a medical image usually contains multiple concepts, and the fine-grained concept annotations are difficult to acquire. In this paper, we aim to interpret representations in deep neural networks by aligning the axes of the latent space with known concepts of interest. We propose a novel Concept-Attention Whitening (CAW) framework for interpretable skin lesion diagnosis. CAW is comprised of a disease diagnosis branch and a concept alignment branch. In the former branch, we train a convolutional neural network (CNN) with an inserted CAW layer to perform skin lesion diagnosis. The CAW layer decorrelates features and aligns image features to conceptual meanings via an orthogonal matrix. In the latter branch, the orthogonal matrix is calculated under the guidance of the concept attention mask. We particularly introduce a weakly-supervised concept mask generator that only leverages coarse concept labels for filtering local regions that are relevant to certain concepts, improving the optimization of the orthogonal matrix. Extensive experiments on two public skin lesion diagnosis datasets demonstrated that CAW not only enhanced interpretability but also maintained a state-of-the-art diagnostic performance.
<div id='section'>Paperid: <span id='pid'>603, <a href='https://arxiv.org/pdf/2402.03025.pdf' target='_blank'>https://arxiv.org/pdf/2402.03025.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanyi Wang, Wei Tang, Haifeng Sun, Zirui Zhuang, Xiaoyuan Fu, Jingyu Wang, Qi Qi, Jianxin Liao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.03025">Understanding and Guiding Weakly Supervised Entity Alignment with Potential Isomorphism Propagation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Entity Alignment (EA) is the task of identifying equivalent entities across diverse knowledge graphs (KGs) using only a limited number of seed alignments. Despite substantial advances in aggregation-based weakly supervised EA, the underlying mechanisms in this setting remain unexplored. In this paper, we present a propagation perspective to analyze weakly supervised EA and explain the existing aggregation-based EA models. Our theoretical analysis reveals that these models essentially seek propagation operators for pairwise entity similarities. We further prove that, despite the structural heterogeneity of different KGs, the potentially aligned entities within aggregation-based EA models have isomorphic subgraphs, which is the core premise of EA but has not been investigated. Leveraging this insight, we introduce a potential isomorphism propagation operator to enhance the propagation of neighborhood information across KGs. We develop a general EA framework, PipEA, incorporating this operator to improve the accuracy of every type of aggregation-based model without altering the learning process. Extensive experiments substantiate our theoretical findings and demonstrate PipEA's significant performance gains over state-of-the-art weakly supervised EA methods. Our work not only advances the field but also enhances our comprehension of aggregation-based weakly supervised EA.
<div id='section'>Paperid: <span id='pid'>604, <a href='https://arxiv.org/pdf/2312.00312.pdf' target='_blank'>https://arxiv.org/pdf/2312.00312.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Zhao, Tao Zhou, Yunqi Gu, Yi Zhou, Yizhe Zhang, Ye Wu, Huazhu Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.00312">Segment Anything Model-guided Collaborative Learning Network for Scribble-supervised Polyp Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Polyp segmentation plays a vital role in accurately locating polyps at an early stage, which holds significant clinical importance for the prevention of colorectal cancer. Various polyp segmentation methods have been developed using fully-supervised deep learning techniques. However, pixel-wise annotation for polyp images by physicians during the diagnosis is both time-consuming and expensive. Moreover, visual foundation models such as the Segment Anything Model (SAM) have shown remarkable performance. Nevertheless, directly applying SAM to medical segmentation may not produce satisfactory results due to the inherent absence of medical knowledge. In this paper, we propose a novel SAM-guided Collaborative Learning Network (SAM-CLNet) for scribble-supervised polyp segmentation, enabling a collaborative learning process between our segmentation network and SAM to boost the model performance. Specifically, we first propose a Cross-level Enhancement and Aggregation Network (CEA-Net) for weakly-supervised polyp segmentation. Within CEA-Net, we propose a Cross-level Enhancement Module (CEM) that integrates the adjacent features to enhance the representation capabilities of different resolution features. Additionally, a Feature Aggregation Module (FAM) is employed to capture richer features across multiple levels. Moreover, we present a box-augmentation strategy that combines the segmentation maps generated by CEA-Net with scribble annotations to create more precise prompts. These prompts are then fed into SAM, generating segmentation SAM-guided masks, which can provide additional supervision to train CEA-Net effectively. Furthermore, we present an Image-level Filtering Mechanism to filter out unreliable SAM-guided masks. Extensive experimental results show that our SAM-CLNet outperforms state-of-the-art weakly-supervised segmentation methods.
<div id='section'>Paperid: <span id='pid'>605, <a href='https://arxiv.org/pdf/2309.13886.pdf' target='_blank'>https://arxiv.org/pdf/2309.13886.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Biao Liu, Ning Xu, Jie Wang, Xin Geng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.13886">Can Class-Priors Help Single-Positive Multi-Label Learning?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single-positive multi-label learning (SPMLL) is a typical weakly supervised multi-label learning problem, where each training example is annotated with only one positive label. Existing SPMLL methods typically assign pseudo-labels to unannotated labels with the assumption that prior probabilities of all classes are identical. However, the class-prior of each category may differ significantly in real-world scenarios, which makes the predictive model not perform as well as expected due to the unrealistic assumption on real-world application. To alleviate this issue, a novel framework named {\proposed}, i.e., Class-pRiors Induced Single-Positive multi-label learning, is proposed. Specifically, a class-priors estimator is introduced, which could estimate the class-priors that are theoretically guaranteed to converge to the ground-truth class-priors. In addition, based on the estimated class-priors, an unbiased risk estimator for classification is derived, and the corresponding risk minimizer could be guaranteed to approximately converge to the optimal risk minimizer on fully supervised data. Experimental results on ten MLL benchmark datasets demonstrate the effectiveness and superiority of our method over existing SPMLL approaches.
<div id='section'>Paperid: <span id='pid'>606, <a href='https://arxiv.org/pdf/2308.11144.pdf' target='_blank'>https://arxiv.org/pdf/2308.11144.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pingyi Chen, Chenglu Zhu, Zhongyi Shui, Jiatong Cai, Sunyi Zheng, Shichuan Zhang, Lin Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.11144">Exploring Unsupervised Cell Recognition with Prior Self-activation Maps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The success of supervised deep learning models on cell recognition tasks relies on detailed annotations. Many previous works have managed to reduce the dependency on labels. However, considering the large number of cells contained in a patch, costly and inefficient labeling is still inevitable. To this end, we explored label-free methods for cell recognition. Prior self-activation maps (PSM) are proposed to generate pseudo masks as training targets. To be specific, an activation network is trained with self-supervised learning. The gradient information in the shallow layers of the network is aggregated to generate prior self-activation maps. Afterward, a semantic clustering module is then introduced as a pipeline to transform PSMs to pixel-level semantic pseudo masks for downstream tasks. We evaluated our method on two histological datasets: MoNuSeg (cell segmentation) and BCData (multi-class cell detection). Compared with other fully-supervised and weakly-supervised methods, our method can achieve competitive performance without any manual annotations. Our simple but effective framework can also achieve multi-class cell detection which can not be done by existing unsupervised methods. The results show the potential of PSMs that might inspire other research to deal with the hunger for labels in medical area.
<div id='section'>Paperid: <span id='pid'>607, <a href='https://arxiv.org/pdf/2305.13723.pdf' target='_blank'>https://arxiv.org/pdf/2305.13723.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunyi Zhang, Minhao Jiang, Yu Meng, Yu Zhang, Jiawei Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.13723">PIEClass: Weakly-Supervised Text Classification with Prompting and Noise-Robust Iterative Ensemble Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised text classification trains a classifier using the label name of each target class as the only supervision, which largely reduces human annotation efforts. Most existing methods first use the label names as static keyword-based features to generate pseudo labels, which are then used for final classifier training. While reasonable, such a commonly adopted framework suffers from two limitations: (1) keywords can have different meanings in different contexts and some text may not have any keyword, so keyword matching can induce noisy and inadequate pseudo labels; (2) the errors made in the pseudo label generation stage will directly propagate to the classifier training stage without a chance of being corrected. In this paper, we propose a new method, PIEClass, consisting of two modules: (1) a pseudo label acquisition module that uses zero-shot prompting of pre-trained language models (PLM) to get pseudo labels based on contextualized text understanding beyond static keyword matching, and (2) a noise-robust iterative ensemble training module that iteratively trains classifiers and updates pseudo labels by utilizing two PLM fine-tuning methods that regularize each other. Extensive experiments show that PIEClass achieves overall better performance than existing strong baselines on seven benchmark datasets and even achieves similar performance to fully-supervised classifiers on sentiment classification tasks.
<div id='section'>Paperid: <span id='pid'>608, <a href='https://arxiv.org/pdf/2302.08155.pdf' target='_blank'>https://arxiv.org/pdf/2302.08155.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hua Yuan, Ning Xu, Yu Shi, Xin Geng, Yong Rui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.08155">Learning From Biased Soft Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Knowledge distillation has been widely adopted in a variety of tasks and has achieved remarkable successes. Since its inception, many researchers have been intrigued by the dark knowledge hidden in the outputs of the teacher model. Recently, a study has demonstrated that knowledge distillation and label smoothing can be unified as learning from soft labels. Consequently, how to measure the effectiveness of the soft labels becomes an important question. Most existing theories have stringent constraints on the teacher model or data distribution, and many assumptions imply that the soft labels are close to the ground-truth labels. This paper studies whether biased soft labels are still effective. We present two more comprehensive indicators to measure the effectiveness of such soft labels. Based on the two indicators, we give sufficient conditions to ensure biased soft label based learners are classifier-consistent and ERM learnable. The theory is applied to three weakly-supervised frameworks. Experimental results validate that biased soft labels can also teach good students, which corroborates the soundness of the theory.
<div id='section'>Paperid: <span id='pid'>609, <a href='https://arxiv.org/pdf/2212.03125.pdf' target='_blank'>https://arxiv.org/pdf/2212.03125.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minghao Chen, Renbo Tu, Chenxi Huang, Yuqi Lin, Boxi Wu, Deng Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.03125">Self-supervised and Weakly Supervised Contrastive Learning for Frame-wise Action Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Previous work on action representation learning focused on global representations for short video clips. In contrast, many practical applications, such as video alignment, strongly demand learning the intensive representation of long videos. In this paper, we introduce a new framework of contrastive action representation learning (CARL) to learn frame-wise action representation in a self-supervised or weakly-supervised manner, especially for long videos. Specifically, we introduce a simple but effective video encoder that considers both spatial and temporal context by combining convolution and transformer. Inspired by the recent massive progress in self-supervised learning, we propose a new sequence contrast loss (SCL) applied to two related views obtained by expanding a series of spatio-temporal data in two versions. One is the self-supervised version that optimizes embedding space by minimizing KL-divergence between sequence similarity of two augmented views and prior Gaussian distribution of timestamp distance. The other is the weakly-supervised version that builds more sample pairs among videos using video-level labels by dynamic time wrapping (DTW). Experiments on FineGym, PennAction, and Pouring datasets show that our method outperforms previous state-of-the-art by a large margin for downstream fine-grained action classification and even faster inference. Surprisingly, although without training on paired videos like in previous works, our self-supervised version also shows outstanding performance in video alignment and fine-grained frame retrieval tasks.
<div id='section'>Paperid: <span id='pid'>610, <a href='https://arxiv.org/pdf/2208.14649.pdf' target='_blank'>https://arxiv.org/pdf/2208.14649.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zilun Zhang, Cuifeng Shen, Yuan Shen, Huixin Xiong, Xinyu Zhou, Tiancheng Zhao, Jianwei Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.14649">DetailCLIP: Injecting Image Details into CLIP's Feature Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although CLIP-like Visual Language Models provide a functional joint feature space for image and text, due to the limitation of the CILP-like model's image input size (e.g., 224), subtle details are lost in the feature representation if we input high-resolution images (e.g., 2240). In this work, we introduce an efficient framework that can produce a single feature representation for a high-resolution image that injects image details and shares the same semantic space as the original CLIP. In the framework, we train a feature fusing model based on CLIP features extracted from a carefully designed image patch method that can cover objects of any scale, weakly supervised by image-agnostic class prompted queries. We validate our framework by retrieving images from class prompted queries on the real world and synthetic datasets, showing significant performance improvement on these tasks. Furthermore, to fully demonstrate our framework's detail retrieval ability, we construct a CLEVR-like synthetic dataset called CLVER-DS, which is fully annotated and has a controllable object scale.
<div id='section'>Paperid: <span id='pid'>611, <a href='https://arxiv.org/pdf/2208.05110.pdf' target='_blank'>https://arxiv.org/pdf/2208.05110.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shichao Dong, Ruibo Li, Jiacheng Wei, Fayao Liu, Guosheng Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.05110">Collaborative Propagation on Multiple Instance Graphs for 3D Instance Segmentation with Single-point Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Instance segmentation on 3D point clouds has been attracting increasing attention due to its wide applications, especially in scene understanding areas. However, most existing methods operate on fully annotated data while manually preparing ground-truth labels at point-level is very cumbersome and labor-intensive. To address this issue, we propose a novel weakly supervised method RWSeg that only requires labeling one object with one point. With these sparse weak labels, we introduce a unified framework with two branches to propagate semantic and instance information respectively to unknown regions using self-attention and a cross-graph random walk method. Specifically, we propose a Cross-graph Competing Random Walks (CRW) algorithm that encourages competition among different instance graphs to resolve ambiguities in closely placed objects, improving instance assignment accuracy. RWSeg generates high-quality instance-level pseudo labels. Experimental results on ScanNet-v2 and S3DIS datasets show that our approach achieves comparable performance with fully-supervised methods and outperforms previous weakly-supervised methods by a substantial margin.
<div id='section'>Paperid: <span id='pid'>612, <a href='https://arxiv.org/pdf/2111.04022.pdf' target='_blank'>https://arxiv.org/pdf/2111.04022.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Zhang, Shweta Garg, Yu Meng, Xiusi Chen, Jiawei Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2111.04022">MotifClass: Weakly Supervised Text Classification with Higher-order Metadata Information</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study the problem of weakly supervised text classification, which aims to classify text documents into a set of pre-defined categories with category surface names only and without any annotated training document provided. Most existing classifiers leverage textual information in each document. However, in many domains, documents are accompanied by various types of metadata (e.g., authors, venue, and year of a research paper). These metadata and their combinations may serve as strong category indicators in addition to textual contents. In this paper, we explore the potential of using metadata to help weakly supervised text classification. To be specific, we model the relationships between documents and metadata via a heterogeneous information network. To effectively capture higher-order structures in the network, we use motifs to describe metadata combinations. We propose a novel framework, named MotifClass, which (1) selects category-indicative motif instances, (2) retrieves and generates pseudo-labeled training samples based on category names and indicative motif instances, and (3) trains a text classifier using the pseudo training data. Extensive experiments on real-world datasets demonstrate the superior performance of MotifClass to existing weakly supervised text classification approaches. Further analysis shows the benefit of considering higher-order metadata information in our framework.
<div id='section'>Paperid: <span id='pid'>613, <a href='https://arxiv.org/pdf/2107.11267.pdf' target='_blank'>https://arxiv.org/pdf/2107.11267.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiacheng Wei, Guosheng Lin, Kim-Hui Yap, Fayao Liu, Tzu-Yi Hung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2107.11267">Dense Supervision Propagation for Weakly Supervised Semantic Segmentation on 3D Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semantic segmentation on 3D point clouds is an important task for 3D scene understanding. While dense labeling on 3D data is expensive and time-consuming, only a few works address weakly supervised semantic point cloud segmentation methods to relieve the labeling cost by learning from simpler and cheaper labels. Meanwhile, there are still huge performance gaps between existing weakly supervised methods and state-of-the-art fully supervised methods. In this paper, we train a semantic point cloud segmentation network with only a small portion of points being labeled. We argue that we can better utilize the limited supervision information as we densely propagate the supervision signal from the labeled points to other points within and across the input samples. Specifically, we propose a cross-sample feature reallocating module to transfer similar features and therefore re-route the gradients across two samples with common classes and an intra-sample feature redistribution module to propagate supervision signals on unlabeled points across and within point cloud samples. We conduct extensive experiments on public datasets S3DIS and ScanNet. Our weakly supervised method with only 10% and 1% of labels can produce compatible results with the fully supervised counterpart.
<div id='section'>Paperid: <span id='pid'>614, <a href='https://arxiv.org/pdf/1910.07115.pdf' target='_blank'>https://arxiv.org/pdf/1910.07115.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Zhang, Frank F. Xu, Sha Li, Yu Meng, Xuan Wang, Qi Li, Jiawei Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1910.07115">HiGitClass: Keyword-Driven Hierarchical Classification of GitHub Repositories</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>GitHub has become an important platform for code sharing and scientific exchange. With the massive number of repositories available, there is a pressing need for topic-based search. Even though the topic label functionality has been introduced, the majority of GitHub repositories do not have any labels, impeding the utility of search and topic-based analysis. This work targets the automatic repository classification problem as keyword-driven hierarchical classification. Specifically, users only need to provide a label hierarchy with keywords to supply as supervision. This setting is flexible, adaptive to the users' needs, accounts for the different granularity of topic labels and requires minimal human effort. We identify three key challenges of this problem, namely (1) the presence of multi-modal signals; (2) supervision scarcity and bias; (3) supervision format mismatch. In recognition of these challenges, we propose the HiGitClass framework, comprising of three modules: heterogeneous information network embedding; keyword enrichment; topic modeling and pseudo document generation. Experimental results on two GitHub repository collections confirm that HiGitClass is superior to existing weakly-supervised and dataless hierarchical classification methods, especially in its ability to integrate both structured and unstructured data for repository classification.
<div id='section'>Paperid: <span id='pid'>615, <a href='https://arxiv.org/pdf/2511.11407.pdf' target='_blank'>https://arxiv.org/pdf/2511.11407.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Manyu Li, Ruian He, Chenxi Ma, Weimin Tan, Bo Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.11407">MicroVQA++: High-Quality Microscopy Reasoning Dataset with Weakly Supervised Graphs for Multimodal Large Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal Large Language Models are increasingly applied to biomedical imaging, yet scientific reasoning for microscopy remains limited by the scarcity of large-scale, high-quality training data. We introduce MicroVQA++, a three-stage, large-scale and high-quality microscopy VQA corpus derived from the BIOMEDICA archive. Stage one bootstraps supervision from expert-validated figure-caption pairs sourced from peer-reviewed articles. Stage two applies HiCQA-Graph, a novel heterogeneous graph over images, captions, and QAs that fuses NLI-based textual entailment, CLIP-based vision-language alignment, and agent signals to identify and filter inconsistent samples. Stage three uses a MultiModal Large Language Model (MLLM) agent to generate multiple-choice questions (MCQ) followed by human screening. The resulting release comprises a large training split and a human-checked test split whose Bloom's level hard-sample distribution exceeds the MicroVQA benchmark. Our work delivers (i) a quality-controlled dataset that couples expert literature with graph-based filtering and human refinement; (ii) HiCQA-Graph, the first graph that jointly models (image, caption, QA) for cross-modal consistency filtering; (iii) evidence that careful data construction enables 4B-scale MLLMs to reach competitive microscopy reasoning performance (e.g., GPT-5) and achieve state-of-the-art performance among open-source MLLMs. Code and dataset will be released after the review process concludes.
<div id='section'>Paperid: <span id='pid'>616, <a href='https://arxiv.org/pdf/2511.04035.pdf' target='_blank'>https://arxiv.org/pdf/2511.04035.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongji Gao, Chenda Liao, Changliang Liu, Matthew Wiesner, Leibny Paola Garcia, Daniel Povey, Sanjeev Khudanpur, Jian Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.04035">WST: Weakly Supervised Transducer for Automatic Speech Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Recurrent Neural Network-Transducer (RNN-T) is widely adopted in end-to-end (E2E) automatic speech recognition (ASR) tasks but depends heavily on large-scale, high-quality annotated data, which are often costly and difficult to obtain. To mitigate this reliance, we propose a Weakly Supervised Transducer (WST), which integrates a flexible training graph designed to robustly handle errors in the transcripts without requiring additional confidence estimation or auxiliary pre-trained models. Empirical evaluations on synthetic and industrial datasets reveal that WST effectively maintains performance even with transcription error rates of up to 70%, consistently outperforming existing Connectionist Temporal Classification (CTC)-based weakly supervised approaches, such as Bypass Temporal Classification (BTC) and Omni-Temporal Classification (OTC). These results demonstrate the practical utility and robustness of WST in realistic ASR settings. The implementation will be publicly available.
<div id='section'>Paperid: <span id='pid'>617, <a href='https://arxiv.org/pdf/2510.05196.pdf' target='_blank'>https://arxiv.org/pdf/2510.05196.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daqian Shi, Xiaolei Diao, Jinge Wu, Honghan Wu, Xiongfeng Tang, Felix Naughton, Paulina Bondaronek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05196">Graph-based LLM over Semi-Structured Population Data for Dynamic Policy Response</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Timely and accurate analysis of population-level data is crucial for effective decision-making during public health emergencies such as the COVID-19 pandemic. However, the massive input of semi-structured data, including structured demographic information and unstructured human feedback, poses significant challenges to conventional analysis methods. Manual expert-driven assessments, though accurate, are inefficient, while standard NLP pipelines often require large task-specific labeled datasets and struggle with generalization across diverse domains. To address these challenges, we propose a novel graph-based reasoning framework that integrates large language models with structured demographic attributes and unstructured public feedback in a weakly supervised pipeline. The proposed approach dynamically models evolving citizen needs into a need-aware graph, enabling population-specific analyses based on key features such as age, gender, and the Index of Multiple Deprivation. It generates interpretable insights to inform responsive health policy decision-making. We test our method using a real-world dataset, and preliminary experimental results demonstrate its feasibility. This approach offers a scalable solution for intelligent population health monitoring in resource-constrained clinical and governmental settings.
<div id='section'>Paperid: <span id='pid'>618, <a href='https://arxiv.org/pdf/2509.18711.pdf' target='_blank'>https://arxiv.org/pdf/2509.18711.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ke Li, Di Wang, Ting Wang, Fuyu Dong, Yiming Zhang, Luyao Zhang, Xiangyu Wang, Shaofeng Li, Quan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18711">RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot Open-Vocabulary Visual Grounding in Remote Sensing Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Remote sensing visual grounding (RSVG) aims to localize objects in remote sensing images based on free-form natural language expressions. Existing approaches are typically constrained to closed-set vocabularies, limiting their applicability in open-world scenarios. While recent attempts to leverage generic foundation models for open-vocabulary RSVG, they overly rely on expensive high-quality datasets and time-consuming fine-tuning. To address these limitations, we propose \textbf{RSVG-ZeroOV}, a training-free framework that aims to explore the potential of frozen generic foundation models for zero-shot open-vocabulary RSVG. Specifically, RSVG-ZeroOV comprises three key stages: (i) Overview: We utilize a vision-language model (VLM) to obtain cross-attention\footnote[1]{In this paper, although decoder-only VLMs use self-attention over all tokens, we refer to the image-text interaction part as cross-attention to distinguish it from pure visual self-attention.}maps that capture semantic correlations between text queries and visual regions. (ii) Focus: By leveraging the fine-grained modeling priors of a diffusion model (DM), we fill in gaps in structural and shape information of objects, which are often overlooked by VLM. (iii) Evolve: A simple yet effective attention evolution module is introduced to suppress irrelevant activations, yielding purified segmentation masks over the referred objects. Without cumbersome task-specific training, RSVG-ZeroOV offers an efficient and scalable solution. Extensive experiments demonstrate that the proposed framework consistently outperforms existing weakly-supervised and zero-shot methods.
<div id='section'>Paperid: <span id='pid'>619, <a href='https://arxiv.org/pdf/2306.03407.pdf' target='_blank'>https://arxiv.org/pdf/2306.03407.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Beidi Zhao, Wenlong Deng, Zi Han, Li, Chen Zhou, Zuhua Gao, Gang Wang, Xiaoxiao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.03407">LESS: Label-efficient Multi-scale Learning for Cytological Whole Slide Image Screening</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In computational pathology, multiple instance learning (MIL) is widely used to circumvent the computational impasse in giga-pixel whole slide image (WSI) analysis. It usually consists of two stages: patch-level feature extraction and slide-level aggregation. Recently, pretrained models or self-supervised learning have been used to extract patch features, but they suffer from low effectiveness or inefficiency due to overlooking the task-specific supervision provided by slide labels. Here we propose a weakly-supervised Label-Efficient WSI Screening method, dubbed LESS, for cytological WSI analysis with only slide-level labels, which can be effectively applied to small datasets. First, we suggest using variational positive-unlabeled (VPU) learning to uncover hidden labels of both benign and malignant patches. We provide appropriate supervision by using slide-level labels to improve the learning of patch-level features. Next, we take into account the sparse and random arrangement of cells in cytological WSIs. To address this, we propose a strategy to crop patches at multiple scales and utilize a cross-attention vision transformer (CrossViT) to combine information from different scales for WSI classification. The combination of our two steps achieves task-alignment, improving effectiveness and efficiency. We validate the proposed label-efficient method on a urine cytology WSI dataset encompassing 130 samples (13,000 patches) and FNAC 2019 dataset with 212 samples (21,200 patches). The experiment shows that the proposed LESS reaches 84.79%, 85.43%, 91.79% and 78.30% on a urine cytology WSI dataset, and 96.88%, 96.86%, 98.95%, 97.06% on FNAC 2019 dataset in terms of accuracy, AUC, sensitivity and specificity. It outperforms state-of-the-art MIL methods on pathology WSIs and realizes automatic cytological WSI cancer screening.
<div id='section'>Paperid: <span id='pid'>620, <a href='https://arxiv.org/pdf/2306.01031.pdf' target='_blank'>https://arxiv.org/pdf/2306.01031.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongji Gao, Matthew Wiesner, Hainan Xu, Leibny Paola Garcia, Daniel Povey, Sanjeev Khudanpur
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.01031">Bypass Temporal Classification: Weakly Supervised Automatic Speech Recognition with Imperfect Transcripts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel algorithm for building an automatic speech recognition (ASR) model with imperfect training data. Imperfectly transcribed speech is a prevalent issue in human-annotated speech corpora, which degrades the performance of ASR models. To address this problem, we propose Bypass Temporal Classification (BTC) as an expansion of the Connectionist Temporal Classification (CTC) criterion. BTC explicitly encodes the uncertainties associated with transcripts during training. This is accomplished by enhancing the flexibility of the training graph, which is implemented as a weighted finite-state transducer (WFST) composition. The proposed algorithm improves the robustness and accuracy of ASR systems, particularly when working with imprecisely transcribed speech corpora. Our implementation will be open-sourced.
<div id='section'>Paperid: <span id='pid'>621, <a href='https://arxiv.org/pdf/2304.03282.pdf' target='_blank'>https://arxiv.org/pdf/2304.03282.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingyu Ding, Yikang Shen, Lijie Fan, Zhenfang Chen, Zitian Chen, Ping Luo, Joshua B. Tenenbaum, Chuang Gan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.03282">Visual Dependency Transformers: Dependency Tree Emerges from Reversed Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans possess a versatile mechanism for extracting structured representations of our visual world. When looking at an image, we can decompose the scene into entities and their parts as well as obtain the dependencies between them. To mimic such capability, we propose Visual Dependency Transformers (DependencyViT) that can induce visual dependencies without any labels. We achieve that with a novel neural operator called \emph{reversed attention} that can naturally capture long-range visual dependencies between image patches. Specifically, we formulate it as a dependency graph where a child token in reversed attention is trained to attend to its parent tokens and send information following a normalized probability distribution rather than gathering information in conventional self-attention. With such a design, hierarchies naturally emerge from reversed attention layers, and a dependency tree is progressively induced from leaf nodes to the root node unsupervisedly.
  DependencyViT offers several appealing benefits. (i) Entities and their parts in an image are represented by different subtrees, enabling part partitioning from dependencies; (ii) Dynamic visual pooling is made possible. The leaf nodes which rarely send messages can be pruned without hindering the model performance, based on which we propose the lightweight DependencyViT-Lite to reduce the computational and memory footprints; (iii) DependencyViT works well on both self- and weakly-supervised pretraining paradigms on ImageNet, and demonstrates its effectiveness on 8 datasets and 5 tasks, such as unsupervised part and saliency segmentation, recognition, and detection.
<div id='section'>Paperid: <span id='pid'>622, <a href='https://arxiv.org/pdf/2302.09765.pdf' target='_blank'>https://arxiv.org/pdf/2302.09765.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Moon Ye-Bin, Dongmin Choi, Yongjin Kwon, Junsik Kim, Tae-Hyun Oh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.09765">ENInst: Enhancing Weakly-supervised Low-shot Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address a weakly-supervised low-shot instance segmentation, an annotation-efficient training method to deal with novel classes effectively. Since it is an under-explored problem, we first investigate the difficulty of the problem and identify the performance bottleneck by conducting systematic analyses of model components and individual sub-tasks with a simple baseline model. Based on the analyses, we propose ENInst with sub-task enhancement methods: instance-wise mask refinement for enhancing pixel localization quality and novel classifier composition for improving classification accuracy. Our proposed method lifts the overall performance by enhancing the performance of each sub-task. We demonstrate that our ENInst is 7.5 times more efficient in achieving comparable performance to the existing fully-supervised few-shot models and even outperforms them at times.
<div id='section'>Paperid: <span id='pid'>623, <a href='https://arxiv.org/pdf/2509.01878.pdf' target='_blank'>https://arxiv.org/pdf/2509.01878.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Scarlett Raine, Tobias Fischer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01878">AI-Driven Marine Robotics: Emerging Trends in Underwater Perception and Ecosystem Monitoring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Marine ecosystems face increasing pressure due to climate change, driving the need for scalable, AI-powered monitoring solutions. This paper examines the rapid emergence of underwater AI as a major research frontier and analyzes the factors that have transformed marine perception from a niche application into a catalyst for AI innovation. We identify three convergent drivers: environmental necessity for ecosystem-scale monitoring, democratization of underwater datasets through citizen science platforms, and researcher migration from saturated terrestrial computer vision domains. Our analysis reveals how unique underwater challenges - turbidity, cryptic species detection, expert annotation bottlenecks, and cross-ecosystem generalization - are driving fundamental advances in weakly supervised learning, open-set recognition, and robust perception under degraded conditions. We survey emerging trends in datasets, scene understanding and 3D reconstruction, highlighting the paradigm shift from passive observation toward AI-driven, targeted intervention capabilities. The paper demonstrates how underwater constraints are pushing the boundaries of foundation models, self-supervised learning, and perception, with methodological innovations that extend far beyond marine applications to benefit general computer vision, robotics, and environmental monitoring.
<div id='section'>Paperid: <span id='pid'>624, <a href='https://arxiv.org/pdf/2503.12068.pdf' target='_blank'>https://arxiv.org/pdf/2503.12068.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingchen Tang, Lei Fan, Maurice Pagnucco, Yang Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12068">Prototype-Based Image Prompting for Weakly Supervised Histopathological Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised image segmentation with image-level labels has drawn attention due to the high cost of pixel-level annotations. Traditional methods using Class Activation Maps (CAMs) often highlight only the most discriminative regions, leading to incomplete masks. Recent approaches that introduce textual information struggle with histopathological images due to inter-class homogeneity and intra-class heterogeneity. In this paper, we propose a prototype-based image prompting framework for histopathological image segmentation. It constructs an image bank from the training set using clustering, extracting multiple prototype features per class to capture intra-class heterogeneity. By designing a matching loss between input features and class-specific prototypes using contrastive learning, our method addresses inter-class homogeneity and guides the model to generate more accurate CAMs. Experiments on four datasets (LUAD-HistoSeg, BCSS-WSSS, GCSS, and BCSS) show that our method outperforms existing weakly supervised segmentation approaches, setting new benchmarks in histopathological image segmentation.
<div id='section'>Paperid: <span id='pid'>625, <a href='https://arxiv.org/pdf/2412.12791.pdf' target='_blank'>https://arxiv.org/pdf/2412.12791.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiping Ge, Qiang Chen, Zhiwei Jiang, Yafeng Yin, Liu Qin, Ziyao Chen, Qing Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12791">Implicit Location-Caption Alignment via Complementary Masking for Weakly-Supervised Dense Video Captioning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-Supervised Dense Video Captioning (WSDVC) aims to localize and describe all events of interest in a video without requiring annotations of event boundaries. This setting poses a great challenge in accurately locating the temporal location of event, as the relevant supervision is unavailable. Existing methods rely on explicit alignment constraints between event locations and captions, which involve complex event proposal procedures during both training and inference. To tackle this problem, we propose a novel implicit location-caption alignment paradigm by complementary masking, which simplifies the complex event proposal and localization process while maintaining effectiveness. Specifically, our model comprises two components: a dual-mode video captioning module and a mask generation module. The dual-mode video captioning module captures global event information and generates descriptive captions, while the mask generation module generates differentiable positive and negative masks for localizing the events. These masks enable the implicit alignment of event locations and captions by ensuring that captions generated from positively and negatively masked videos are complementary, thereby forming a complete video description. In this way, even under weak supervision, the event location and event caption can be aligned implicitly. Extensive experiments on the public datasets demonstrate that our method outperforms existing weakly-supervised methods and achieves competitive results compared to fully-supervised methods.
<div id='section'>Paperid: <span id='pid'>626, <a href='https://arxiv.org/pdf/2411.11287.pdf' target='_blank'>https://arxiv.org/pdf/2411.11287.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Scarlett Raine, Frederic Maire, Niko Suenderhauf, Tobias Fischer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11287">Reducing Label Dependency for Underwater Scene Understanding: A Survey of Datasets, Techniques and Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Underwater surveys provide long-term data for informing management strategies, monitoring coral reef health, and estimating blue carbon stocks. Advances in broad-scale survey methods, such as robotic underwater vehicles, have increased the range of marine surveys but generate large volumes of imagery requiring analysis. Computer vision methods such as semantic segmentation aid automated image analysis, but typically rely on fully supervised training with extensive labelled data. While ground truth label masks for tasks like street scene segmentation can be quickly and affordably generated by non-experts through crowdsourcing services like Amazon Mechanical Turk, ecology presents greater challenges. The complexity of underwater images, coupled with the specialist expertise needed to accurately identify species at the pixel level, makes this process costly, time-consuming, and heavily dependent on domain experts. In recent years, some works have performed automated analysis of underwater imagery, and a smaller number of studies have focused on weakly supervised approaches which aim to reduce the expert-provided labelled data required. This survey focuses on approaches which reduce dependency on human expert input, while reviewing the prior and related approaches to position these works in the wider field of underwater perception. Further, we offer an overview of coastal ecosystems and the challenges of underwater imagery. We provide background on weakly and self-supervised deep learning and integrate these elements into a taxonomy that centres on the intersection of underwater monitoring, computer vision, and deep learning, while motivating approaches for weakly supervised deep learning with reduced dependency on domain expert data annotations. Lastly, the survey examines available datasets and platforms, and identifies gaps, barriers, and opportunities for automating underwater surveys.
<div id='section'>Paperid: <span id='pid'>627, <a href='https://arxiv.org/pdf/2410.20371.pdf' target='_blank'>https://arxiv.org/pdf/2410.20371.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxing Huang, Jingyi Zhang, Kai Jiang, Shijian Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20371">Open-Vocabulary Object Detection via Language Hierarchy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent studies on generalizable object detection have attracted increasing attention with additional weak supervision from large-scale datasets with image-level labels. However, weakly-supervised detection learning often suffers from image-to-box label mismatch, i.e., image-level labels do not convey precise object information. We design Language Hierarchical Self-training (LHST) that introduces language hierarchy into weakly-supervised detector training for learning more generalizable detectors. LHST expands the image-level labels with language hierarchy and enables co-regularization between the expanded labels and self-training. Specifically, the expanded labels regularize self-training by providing richer supervision and mitigating the image-to-box label mismatch, while self-training allows assessing and selecting the expanded labels according to the predicted reliability. In addition, we design language hierarchical prompt generation that introduces language hierarchy into prompt generation which helps bridge the vocabulary gaps between training and testing. Extensive experiments show that the proposed techniques achieve superior generalization performance consistently across 14 widely studied object detection datasets.
<div id='section'>Paperid: <span id='pid'>628, <a href='https://arxiv.org/pdf/2408.08092.pdf' target='_blank'>https://arxiv.org/pdf/2408.08092.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiming Xia, Hongwei Lin, Wei Ye, Hai Wu, Yadan Luo, Cheng Wang, Chenglu Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.08092">SC3D: Label-Efficient Outdoor 3D Object Detection via Single Click Annotation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LiDAR-based outdoor 3D object detection has received widespread attention. However, training 3D detectors from the LiDAR point cloud typically relies on expensive bounding box annotations. This paper presents SC3D, an innovative label-efficient method requiring only a single coarse click on the bird's eye view of the 3D point cloud for each frame. A key challenge here is the absence of complete geometric descriptions of the target objects from such simple click annotations. To address this issue, our proposed SC3D adopts a progressive pipeline. Initially, we design a mixed pseudo-label generation module that expands limited click annotations into a mixture of bounding box and semantic mask supervision. Next, we propose a mix-supervised teacher model, enabling the detector to learn mixed supervision information. Finally, we introduce a mixed-supervised student network that leverages the teacher model's generalization ability to learn unclicked instances.Experimental results on the widely used nuScenes and KITTI datasets demonstrate that our SC3D with only coarse clicks, which requires only 0.2% annotation cost, achieves state-of-the-art performance compared to weakly-supervised 3D detection methods.The code will be made publicly available.
<div id='section'>Paperid: <span id='pid'>629, <a href='https://arxiv.org/pdf/2405.01217.pdf' target='_blank'>https://arxiv.org/pdf/2405.01217.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenying Liu, Conrad Albrecht, Yi Wang, Xiao Xiang Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.01217">CromSS: Cross-modal pre-training with noisy labels for remote sensing image segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We explore the potential of large-scale noisily labeled data to enhance feature learning by pretraining semantic segmentation models within a multi-modal framework for geospatial applications. We propose a novel Cross-modal Sample Selection (CromSS) method, a weakly supervised pretraining strategy designed to improve feature representations through cross-modal consistency and noise mitigation techniques. Unlike conventional pretraining approaches, CromSS exploits massive amounts of noisy and easy-to-come-by labels for improved feature learning beneficial to semantic segmentation tasks. We investigate middle and late fusion strategies to optimize the multi-modal pretraining architecture design. We also introduce a cross-modal sample selection module to mitigate the adverse effects of label noise, which employs a cross-modal entangling strategy to refine the estimated confidence masks within each modality to guide the sampling process. Additionally, we introduce a spatial-temporal label smoothing technique to counteract overconfidence for enhanced robustness against noisy labels. To validate our approach, we assembled the multi-modal dataset, NoLDO-S12, which consists of a large-scale noisy label subset from Google's Dynamic World (DW) dataset for pretraining and two downstream subsets with high-quality labels from Google DW and OpenStreetMap (OSM) for transfer learning. Experimental results on two downstream tasks and the publicly available DFC2020 dataset demonstrate that when effectively utilized, the low-cost noisy labels can significantly enhance feature learning for segmentation tasks. All data, code, and pretrained weights will be made publicly available.
<div id='section'>Paperid: <span id='pid'>630, <a href='https://arxiv.org/pdf/2403.18504.pdf' target='_blank'>https://arxiv.org/pdf/2403.18504.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Felix Virgo, Fei Cheng, Lis Kanashiro Pereira, Masayuki Asahara, Ichiro Kobayashi, Sadao Kurohashi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.18504">AcTED: Automatic Acquisition of Typical Event Duration for Semi-supervised Temporal Commonsense QA</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a voting-driven semi-supervised approach to automatically acquire the typical duration of an event and use it as pseudo-labeled data. The human evaluation demonstrates that our pseudo labels exhibit surprisingly high accuracy and balanced coverage. In the temporal commonsense QA task, experimental results show that using only pseudo examples of 400 events, we achieve performance comparable to the existing BERT-based weakly supervised approaches that require a significant amount of training examples. When compared to the RoBERTa baselines, our best approach establishes state-of-the-art performance with a 7% improvement in Exact Match.
<div id='section'>Paperid: <span id='pid'>631, <a href='https://arxiv.org/pdf/2403.11463.pdf' target='_blank'>https://arxiv.org/pdf/2403.11463.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaolei Tan, Jianhuang Lai, Wei-Shi Zheng, Jian-Fang Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11463">Siamese Learning with Joint Alignment and Regression for Weakly-Supervised Video Paragraph Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Paragraph Grounding (VPG) is an emerging task in video-language understanding, which aims at localizing multiple sentences with semantic relations and temporal order from an untrimmed video. However, existing VPG approaches are heavily reliant on a considerable number of temporal labels that are laborious and time-consuming to acquire. In this work, we introduce and explore Weakly-Supervised Video Paragraph Grounding (WSVPG) to eliminate the need of temporal annotations. Different from previous weakly-supervised grounding frameworks based on multiple instance learning or reconstruction learning for two-stage candidate ranking, we propose a novel siamese learning framework that jointly learns the cross-modal feature alignment and temporal coordinate regression without timestamp labels to achieve concise one-stage localization for WSVPG. Specifically, we devise a Siamese Grounding TRansformer (SiamGTR) consisting of two weight-sharing branches for learning complementary supervision. An Augmentation Branch is utilized for directly regressing the temporal boundaries of a complete paragraph within a pseudo video, and an Inference Branch is designed to capture the order-guided feature correspondence for localizing multiple sentences in a normal video. We demonstrate by extensive experiments that our paradigm has superior practicability and flexibility to achieve efficient weakly-supervised or semi-supervised learning, outperforming state-of-the-art methods trained with the same or stronger supervision.
<div id='section'>Paperid: <span id='pid'>632, <a href='https://arxiv.org/pdf/2306.14848.pdf' target='_blank'>https://arxiv.org/pdf/2306.14848.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luke Robinson, Daniele De Martini, Matthew Gadd, Paul Newman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.14848">Visual Servoing on Wheels: Robust Robot Orientation Estimation in Remote Viewpoint Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work proposes a fast deployment pipeline for visually-servoed robots which does not assume anything about either the robot - e.g. sizes, colour or the presence of markers - or the deployment environment. In this, accurate estimation of robot orientation is crucial for successful navigation in complex environments; manual labelling of angular values is, though, time-consuming and possibly hard to perform. For this reason, we propose a weakly supervised pipeline that can produce a vast amount of data in a small amount of time. We evaluate our approach on a dataset of remote camera images captured in various indoor environments demonstrating high tracking performances when integrated into a fully-autonomous pipeline with a simple controller. With this, we then analyse the data requirement of our approach, showing how it is possible to deploy a new robot in a new environment in less than 30.00 min.
<div id='section'>Paperid: <span id='pid'>633, <a href='https://arxiv.org/pdf/2306.02859.pdf' target='_blank'>https://arxiv.org/pdf/2306.02859.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rongzhi Zhang, Yue Yu, Jiaming Shen, Xiquan Cui, Chao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.02859">Local Boosting for Weakly-Supervised Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Boosting is a commonly used technique to enhance the performance of a set of base models by combining them into a strong ensemble model. Though widely adopted, boosting is typically used in supervised learning where the data is labeled accurately. However, in weakly supervised learning, where most of the data is labeled through weak and noisy sources, it remains nontrivial to design effective boosting approaches. In this work, we show that the standard implementation of the convex combination of base learners can hardly work due to the presence of noisy labels. Instead, we propose $\textit{LocalBoost}$, a novel framework for weakly-supervised boosting. LocalBoost iteratively boosts the ensemble model from two dimensions, i.e., intra-source and inter-source. The intra-source boosting introduces locality to the base learners and enables each base learner to focus on a particular feature regime by training new base learners on granularity-varying error regions. For the inter-source boosting, we leverage a conditional function to indicate the weak source where the sample is more likely to appear. To account for the weak labels, we further design an estimate-then-modify approach to compute the model weights. Experiments on seven datasets show that our method significantly outperforms vanilla boosting methods and other weakly-supervised methods.
<div id='section'>Paperid: <span id='pid'>634, <a href='https://arxiv.org/pdf/2306.02615.pdf' target='_blank'>https://arxiv.org/pdf/2306.02615.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaochen Zhu, Jing Ma, Liang Wu, Qi Guo, Liangjie Hong, Jundong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.02615">Path-Specific Counterfactual Fairness for Recommender Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recommender systems (RSs) have become an indispensable part of online platforms. With the growing concerns of algorithmic fairness, RSs are not only expected to deliver high-quality personalized content, but are also demanded not to discriminate against users based on their demographic information. However, existing RSs could capture undesirable correlations between sensitive features and observed user behaviors, leading to biased recommendations. Most fair RSs tackle this problem by completely blocking the influences of sensitive features on recommendations. But since sensitive features may also affect user interests in a fair manner (e.g., race on culture-based preferences), indiscriminately eliminating all the influences of sensitive features inevitably degenerate the recommendations quality and necessary diversities. To address this challenge, we propose a path-specific fair RS (PSF-RS) for recommendations. Specifically, we summarize all fair and unfair correlations between sensitive features and observed ratings into two latent proxy mediators, where the concept of path-specific bias (PS-Bias) is defined based on path-specific counterfactual inference. Inspired by Pearl's minimal change principle, we address the PS-Bias by minimally transforming the biased factual world into a hypothetically fair world, where a fair RS model can be learned accordingly by solving a constrained optimization problem. For the technical part, we propose a feasible implementation of PSF-RS, i.e., PSF-VAE, with weakly-supervised variational inference, which robustly infers the latent mediators such that unfairness can be mitigated while necessary recommendation diversities can be maximally preserved simultaneously. Experiments conducted on semi-simulated and real-world datasets demonstrate the effectiveness of PSF-RS.
<div id='section'>Paperid: <span id='pid'>635, <a href='https://arxiv.org/pdf/2303.00973.pdf' target='_blank'>https://arxiv.org/pdf/2303.00973.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Scarlett Raine, Ross Marchant, Brano Kusy, Frederic Maire, Tobias Fischer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.00973">Image Labels Are All You Need for Coarse Seagrass Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Seagrass meadows serve as critical carbon sinks, but estimating the amount of carbon they store requires knowledge of the seagrass species present. Underwater and surface vehicles equipped with machine learning algorithms can help to accurately estimate the composition and extent of seagrass meadows at scale. However, previous approaches for seagrass detection and classification have required supervision from patch-level labels. In this paper, we reframe seagrass classification as a weakly supervised coarse segmentation problem where image-level labels are used during training (25 times fewer labels compared to patch-level labeling) and patch-level outputs are obtained at inference time. To this end, we introduce SeaFeats, an architecture that uses unsupervised contrastive pre-training and feature similarity, and SeaCLIP, a model that showcases the effectiveness of large language models as a supervisory signal in domain-specific applications. We demonstrate that an ensemble of SeaFeats and SeaCLIP leads to highly robust performance. Our method outperforms previous approaches that require patch-level labels on the multi-species 'DeepSeagrass' dataset by 6.8% (absolute) for the class-weighted F1 score, and by 12.1% (absolute) for the seagrass presence/absence F1 score on the 'Global Wetlands' dataset. We also present two case studies for real-world deployment: outlier detection on the Global Wetlands dataset, and application of our method on imagery collected by the FloatyBoat autonomous surface vehicle.
<div id='section'>Paperid: <span id='pid'>636, <a href='https://arxiv.org/pdf/2303.00747.pdf' target='_blank'>https://arxiv.org/pdf/2303.00747.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Max Bain, Jaesung Huh, Tengda Han, Andrew Zisserman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.00747">WhisperX: Time-Accurate Speech Transcription of Long-Form Audio</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale, weakly-supervised speech recognition models, such as Whisper, have demonstrated impressive results on speech recognition across domains and languages. However, their application to long audio transcription via buffered or sliding window approaches is prone to drifting, hallucination & repetition; and prohibits batched transcription due to their sequential nature. Further, timestamps corresponding each utterance are prone to inaccuracies and word-level timestamps are not available out-of-the-box. To overcome these challenges, we present WhisperX, a time-accurate speech recognition system with word-level timestamps utilising voice activity detection and forced phoneme alignment. In doing so, we demonstrate state-of-the-art performance on long-form transcription and word segmentation benchmarks. Additionally, we show that pre-segmenting audio with our proposed VAD Cut & Merge strategy improves transcription quality and enables a twelve-fold transcription speedup via batched inference.
<div id='section'>Paperid: <span id='pid'>637, <a href='https://arxiv.org/pdf/2210.07017.pdf' target='_blank'>https://arxiv.org/pdf/2210.07017.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qianying Liu, Wenyu Guan, Jianhao Shen, Fei Cheng, Sadao Kurohashi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.07017">ComSearch: Equation Searching with Combinatorial Strategy for Solving Math Word Problems with Weak Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Previous studies have introduced a weakly-supervised paradigm for solving math word problems requiring only the answer value annotation. While these methods search for correct value equation candidates as pseudo labels, they search among a narrow sub-space of the enormous equation space. To address this problem, we propose a novel search algorithm with combinatorial strategy \textbf{ComSearch}, which can compress the search space by excluding mathematically equivalent equations. The compression allows the searching algorithm to enumerate all possible equations and obtain high-quality data. We investigate the noise in the pseudo labels that hold wrong mathematical logic, which we refer to as the \textit{false-matching} problem, and propose a ranking model to denoise the pseudo labels. Our approach holds a flexible framework to utilize two existing supervised math word problem solvers to train pseudo labels, and both achieve state-of-the-art performance in the weak supervision task.
<div id='section'>Paperid: <span id='pid'>638, <a href='https://arxiv.org/pdf/2203.12023.pdf' target='_blank'>https://arxiv.org/pdf/2203.12023.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benedikt Boecking, Nicholas Roberts, Willie Neiswanger, Stefano Ermon, Frederic Sala, Artur Dubrawski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.12023">Generative Modeling Helps Weak Supervision (and Vice Versa)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many promising applications of supervised machine learning face hurdles in the acquisition of labeled data in sufficient quantity and quality, creating an expensive bottleneck. To overcome such limitations, techniques that do not depend on ground truth labels have been studied, including weak supervision and generative modeling. While these techniques would seem to be usable in concert, improving one another, how to build an interface between them is not well-understood. In this work, we propose a model fusing programmatic weak supervision and generative adversarial networks and provide theoretical justification motivating this fusion. The proposed approach captures discrete latent variables in the data alongside the weak supervision derived label estimate. Alignment of the two allows for better modeling of sample-dependent accuracies of the weak supervision sources, improving the estimate of unobserved labels. It is the first approach to enable data augmentation through weakly supervised synthetic images and pseudolabels. Additionally, its learned latent variables can be inspected qualitatively. The model outperforms baseline weak supervision label models on a number of multiclass image classification datasets, improves the quality of generated images, and further improves end-model performance through data augmentation with synthetic samples.
<div id='section'>Paperid: <span id='pid'>639, <a href='https://arxiv.org/pdf/2511.17735.pdf' target='_blank'>https://arxiv.org/pdf/2511.17735.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Samuel Stevens, Jacob Beattie, Tanya Berger-Wolf, Yu Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17735">Towards Open-Ended Visual Scientific Discovery with Sparse Autoencoders</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scientific archives now contain hundreds of petabytes of data across genomics, ecology, climate, and molecular biology that could reveal undiscovered patterns if systematically analyzed at scale. Large-scale, weakly-supervised datasets in language and vision have driven the development of foundation models whose internal representations encode structure (patterns, co-occurrences and statistical regularities) beyond their training objectives. Most existing methods extract structure only for pre-specified targets; they excel at confirmation but do not support open-ended discovery of unknown patterns. We ask whether sparse autoencoders (SAEs) can enable open-ended feature discovery from foundation model representations. We evaluate this question in controlled rediscovery studies, where the learned SAE features are tested for alignment with semantic concepts on a standard segmentation benchmark and compared against strong label-free alternatives on concept-alignment metrics. Applied to ecological imagery, the same procedure surfaces fine-grained anatomical structure without access to segmentation or part labels, providing a scientific case study with ground-truth validation. While our experiments focus on vision with an ecology case study, the method is domain-agnostic and applicable to models in other sciences (e.g., proteins, genomics, weather). Our results indicate that sparse decomposition provides a practical instrument for exploring what scientific foundation models have learned, an important prerequisite for moving from confirmation to genuine discovery.
<div id='section'>Paperid: <span id='pid'>640, <a href='https://arxiv.org/pdf/2511.12020.pdf' target='_blank'>https://arxiv.org/pdf/2511.12020.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xianglong Shi, Silin Cheng, Sirui Zhao, Yunhan Jiang, Enhong Chen, Yang Liu, Sebastien Ourselin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.12020">LIHE: Linguistic Instance-Split Hyperbolic-Euclidean Framework for Generalized Weakly-Supervised Referring Expression Comprehension</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing Weakly-Supervised Referring Expression Comprehension (WREC) methods, while effective, are fundamentally limited by a one-to-one mapping assumption, hindering their ability to handle expressions corresponding to zero or multiple targets in realistic scenarios. To bridge this gap, we introduce the Weakly-Supervised Generalized Referring Expression Comprehension task (WGREC), a more practical paradigm that handles expressions with variable numbers of referents. However, extending WREC to WGREC presents two fundamental challenges: supervisory signal ambiguity, where weak image-level supervision is insufficient for training a model to infer the correct number and identity of referents, and semantic representation collapse, where standard Euclidean similarity forces hierarchically-related concepts into non-discriminative clusters, blurring categorical boundaries. To tackle these challenges, we propose a novel WGREC framework named Linguistic Instance-Split Hyperbolic-Euclidean (LIHE), which operates in two stages. The first stage, Referential Decoupling, predicts the number of target objects and decomposes the complex expression into simpler sub-expressions. The second stage, Referent Grounding, then localizes these sub-expressions using HEMix, our innovative hybrid similarity module that synergistically combines the precise alignment capabilities of Euclidean proximity with the hierarchical modeling strengths of hyperbolic geometry. This hybrid approach effectively prevents semantic collapse while preserving fine-grained distinctions between related concepts. Extensive experiments demonstrate LIHE establishes the first effective weakly supervised WGREC baseline on gRefCOCO and Ref-ZOM, while HEMix achieves consistent improvements on standard REC benchmarks, improving IoU@0.5 by up to 2.5\%. The code is available at https://anonymous.4open.science/r/LIHE.
<div id='section'>Paperid: <span id='pid'>641, <a href='https://arxiv.org/pdf/2510.00072.pdf' target='_blank'>https://arxiv.org/pdf/2510.00072.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenhui Xu, Fuxun Yu, Michael J. Bianco, Jacob Kovarskiy, Raphael Tang, Qi Zhang, Zirui Xu, Will LeVine, Brandon Dubbs, Heming Liao, Cassandra Burgess, Suvam Bag, Jay Patravali, Rupanjali Kukal, Mikael Figueroa, Rishi Madhok, Nikolaos Karianakis, Jinjun Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00072">Geo-R1: Unlocking VLM Geospatial Reasoning with Cross-View Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Geo-R1, a reasoning-centric post-training framework that unlocks geospatial reasoning in vision-language models by combining thinking scaffolding and elevating. In the scaffolding stage, Geo-R1 instills a ``geospatial thinking paradigm" via supervised fine-tuning on synthetic chain-of-thought exemplars, enabling models to connect visual cues with geographic priors without costly human reasoning annotations. In the elevating stage, it uses GRPO-based reinforcement learning on a weakly-supervised cross-view pairing proxy. This design supplies a verifiable and scalable reward signal: teaching models to capture and reconcile features across modalities, and harnessing reasoning for accurate prediction. Geo-R1 extends geospatial modeling from domain pretraining / supervised finetuning to reasoning-first post-training, and achieves state-of-the-art performance across various geospatial reasoning benchmarks. Our model is available at https://huggingface.co/miniHui/Geo-R1.
<div id='section'>Paperid: <span id='pid'>642, <a href='https://arxiv.org/pdf/2507.11344.pdf' target='_blank'>https://arxiv.org/pdf/2507.11344.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zara Hall, Melanie Subbiah, Thomas P Zollo, Kathleen McKeown, Richard Zemel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11344">Guiding LLM Decision-Making with Fairness Reward Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models are increasingly used to support high-stakes decisions, potentially influencing who is granted bail or receives a loan. Naive chain-of-thought sampling can improve average decision accuracy, but has also been shown to amplify unfair bias. To address this challenge and enable the trustworthy use of reasoning models in high-stakes decision-making, we propose a framework for training a generalizable Fairness Reward Model (FRM). Our model assigns a fairness score to LLM reasoning, enabling the system to down-weight biased trajectories and favor equitable ones when aggregating decisions across reasoning chains. We show that a single Fairness Reward Model, trained on weakly supervised, LLM-annotated examples of biased versus unbiased reasoning, transfers across tasks, domains, and model families without additional fine-tuning. Applied to real-world decision-making tasks including recidivism prediction and social media moderation, we show that our approach consistently improves fairness while matching, or even surpassing, baseline accuracy.
<div id='section'>Paperid: <span id='pid'>643, <a href='https://arxiv.org/pdf/2506.09445.pdf' target='_blank'>https://arxiv.org/pdf/2506.09445.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ayush Gupta, Anirban Roy, Rama Chellappa, Nathaniel D. Bastian, Alvaro Velasquez, Susmit Jha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09445">TOGA: Temporally Grounded Open-Ended Video QA with Weak Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address the problem of video question answering (video QA) with temporal grounding in a weakly supervised setup, without any temporal annotations. Given a video and a question, we generate an open-ended answer grounded with the start and end time. For this task, we propose TOGA: a vision-language model for Temporally Grounded Open-Ended Video QA with Weak Supervision. We instruct-tune TOGA to jointly generate the answer and the temporal grounding. We operate in a weakly supervised setup where the temporal grounding annotations are not available. We generate pseudo labels for temporal grounding and ensure the validity of these labels by imposing a consistency constraint between the question of a grounding response and the response generated by a question referring to the same temporal segment. We notice that jointly generating the answers with the grounding improves performance on question answering as well as grounding. We evaluate TOGA on grounded QA and open-ended QA tasks. For grounded QA, we consider the NExT-GQA benchmark which is designed to evaluate weakly supervised grounded question answering. For open-ended QA, we consider the MSVD-QA and ActivityNet-QA benchmarks. We achieve state-of-the-art performance for both tasks on these benchmarks.
<div id='section'>Paperid: <span id='pid'>644, <a href='https://arxiv.org/pdf/2501.06038.pdf' target='_blank'>https://arxiv.org/pdf/2501.06038.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tsui Qin Mok, Shuyong Gao, Haozhe Xing, Miaoyang He, Yan Wang, Wenqiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.06038">A Holistically Point-guided Text Framework for Weakly-Supervised Camouflaged Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-Supervised Camouflaged Object Detection (WSCOD) has gained popularity for its promise to train models with weak labels to segment objects that visually blend into their surroundings. Recently, some methods using sparsely-annotated supervision shown promising results through scribbling in WSCOD, while point-text supervision remains underexplored. Hence, this paper introduces a novel holistically point-guided text framework for WSCOD by decomposing into three phases: segment, choose, train. Specifically, we propose Point-guided Candidate Generation (PCG), where the point's foreground serves as a correction for the text path to explicitly correct and rejuvenate the loss detection object during the mask generation process (SEGMENT). We also introduce a Qualified Candidate Discriminator (QCD) to choose the optimal mask from a given text prompt using CLIP (CHOOSE), and employ the chosen pseudo mask for training with a self-supervised Vision Transformer (TRAIN). Additionally, we developed a new point-supervised dataset (P2C-COD) and a text-supervised dataset (T-COD). Comprehensive experiments on four benchmark datasets demonstrate our method outperforms state-of-the-art methods by a large margin, and also outperforms some existing fully-supervised camouflaged object detection methods.
<div id='section'>Paperid: <span id='pid'>645, <a href='https://arxiv.org/pdf/2408.12800.pdf' target='_blank'>https://arxiv.org/pdf/2408.12800.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cairong Zhao, Chutian Wang, Zifan Song, Guosheng Hu, Haonan Chen, Xiaofan Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.12800">Cap2Sum: Learning to Summarize Videos by Generating Captions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid growth of video data on the internet, video summarization is becoming a very important AI technology. However, due to the high labelling cost of video summarization, existing studies have to be conducted on small-scale datasets, leading to limited performance and generalization capacity. In this work, we introduce the use of dense video captions as a supervision signal to train video summarization models. Motivated by this, we propose Cap2Sum, a model that learns to summarize videos by generating captions, to exploit dense video caption annotations. This weakly-supervised approach allows us to train the models on large-scale dense video caption datasets to achieve better performance and generalization capacity. To further improve the generalization capacity, we introduce a CLIP (a strong vision-language model) Prior mechanism to enhance the learning of important objects that captions may ignore in the videos. In practice, Cap2Sum can perform zero-shot video summarization or be fine-tuned by the ground-truth summary or video caption of the target dataset. To examine the performance of Cap2Sum after weakly-supervised fine-tuning by the video captions, we propose two new datasets, TVSum-Caption and SumMe-Caption, which are derived from two common video summarization datasets and will be publicly released. We conduct extensive experiments and the results demonstrate that our method achieves significant improvements in performance and generalization capacity compared with previous methods.
<div id='section'>Paperid: <span id='pid'>646, <a href='https://arxiv.org/pdf/2401.01823.pdf' target='_blank'>https://arxiv.org/pdf/2401.01823.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kumar Ashutosh, Zihui Xue, Tushar Nagarajan, Kristen Grauman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.01823">Detours for Navigating Instructional Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce the video detours problem for navigating instructional videos. Given a source video and a natural language query asking to alter the how-to video's current path of execution in a certain way, the goal is to find a related ''detour video'' that satisfies the requested alteration. To address this challenge, we propose VidDetours, a novel video-language approach that learns to retrieve the targeted temporal segments from a large repository of how-to's using video-and-text conditioned queries. Furthermore, we devise a language-based pipeline that exploits how-to video narration text to create weakly supervised training data. We demonstrate our idea applied to the domain of how-to cooking videos, where a user can detour from their current recipe to find steps with alternate ingredients, tools, and techniques. Validating on a ground truth annotated dataset of 16K samples, we show our model's significant improvements over best available methods for video retrieval and question answering, with recall rates exceeding the state of the art by 35%.
<div id='section'>Paperid: <span id='pid'>647, <a href='https://arxiv.org/pdf/2312.04601.pdf' target='_blank'>https://arxiv.org/pdf/2312.04601.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Felipe Maia Polo, Subha Maity, Mikhail Yurochkin, Moulinath Banerjee, Yuekai Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.04601">Weak Supervision Performance Evaluation via Partial Identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Programmatic Weak Supervision (PWS) enables supervised model training without direct access to ground truth labels, utilizing weak labels from heuristics, crowdsourcing, or pre-trained models. However, the absence of ground truth complicates model evaluation, as traditional metrics such as accuracy, precision, and recall cannot be directly calculated. In this work, we present a novel method to address this challenge by framing model evaluation as a partial identification problem and estimating performance bounds using FrÃ©chet bounds. Our approach derives reliable bounds on key metrics without requiring labeled data, overcoming core limitations in current weak supervision evaluation techniques. Through scalable convex optimization, we obtain accurate and computationally efficient bounds for metrics including accuracy, precision, recall, and F1-score, even in high-dimensional settings. This framework offers a robust approach to assessing model quality without ground truth labels, enhancing the practicality of weakly supervised learning for real-world applications.
<div id='section'>Paperid: <span id='pid'>648, <a href='https://arxiv.org/pdf/2305.17197.pdf' target='_blank'>https://arxiv.org/pdf/2305.17197.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxin Ge, Hongyin Luo, Yoon Kim, James Glass
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.17197">Entailment as Robust Self-Learner</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Entailment has been recognized as an important metric for evaluating natural language understanding (NLU) models, and recent studies have found that entailment pretraining benefits weakly supervised fine-tuning. In this work, we design a prompting strategy that formulates a number of different NLU tasks as contextual entailment. This approach improves the zero-shot adaptation of pretrained entailment models. Secondly, we notice that self-training entailment-based models with unlabeled data can significantly improve the adaptation performance on downstream tasks. To achieve more stable improvement, we propose the Simple Pseudo-Label Editing (SimPLE) algorithm for better pseudo-labeling quality in self-training. We also found that both pretrained entailment-based models and the self-trained models are robust against adversarial evaluation data. Experiments on binary and multi-class classification tasks show that SimPLE leads to more robust self-training results, indicating that the self-trained entailment models are more efficient and trustworthy than large language models on language understanding tasks.
<div id='section'>Paperid: <span id='pid'>649, <a href='https://arxiv.org/pdf/2211.07876.pdf' target='_blank'>https://arxiv.org/pdf/2211.07876.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingyuan Meng, Lei Bi, Dagan Feng, Jinman Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.07876">Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study, we focus on brain tumor sequence registration between pre-operative and follow-up Magnetic Resonance Imaging (MRI) scans of brain glioma patients, in the context of Brain Tumor Sequence Registration challenge (BraTS-Reg 2022). Brain tumor registration is a fundamental requirement in brain image analysis for quantifying tumor changes. This is a challenging task due to large deformations and missing correspondences between pre-operative and follow-up scans. For this task, we adopt our recently proposed Non-Iterative Coarse-to-finE registration Networks (NICE-Net) - a deep learning-based method for coarse-to-fine registering images with large deformations. To overcome missing correspondences, we extend the NICE-Net by introducing dual deep supervision, where a deep self-supervised loss based on image similarity and a deep weakly-supervised loss based on manually annotated landmarks are deeply embedded into the NICE-Net. At the BraTS-Reg 2022, our method achieved a competitive result on the validation set (mean absolute error: 3.387) and placed 4th in the final testing phase (Score: 0.3544).
<div id='section'>Paperid: <span id='pid'>650, <a href='https://arxiv.org/pdf/2209.15402.pdf' target='_blank'>https://arxiv.org/pdf/2209.15402.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weijie Wang, Nicu Sebe, Bruno Lepri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.15402">Rethinking the Learning Paradigm for Facial Expression Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the subjective crowdsourcing annotations and the inherent inter-class similarity of facial expressions, the real-world Facial Expression Recognition (FER) datasets usually exhibit ambiguous annotation. To simplify the learning paradigm, most previous methods convert ambiguous annotation results into precise one-hot annotations and train FER models in an end-to-end supervised manner. In this paper, we rethink the existing training paradigm and propose that it is better to use weakly supervised strategies to train FER models with original ambiguous annotation.
<div id='section'>Paperid: <span id='pid'>651, <a href='https://arxiv.org/pdf/2510.17384.pdf' target='_blank'>https://arxiv.org/pdf/2510.17384.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiajin Tang, Zhengxuan Wei, Ge Zheng, Sibei Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17384">Closed-Loop Transfer for Weakly-supervised Affordance Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans can perform previously unexperienced interactions with novel objects simply by observing others engage with them. Weakly-supervised affordance grounding mimics this process by learning to locate object regions that enable actions on egocentric images, using exocentric interaction images with image-level annotations. However, extracting affordance knowledge solely from exocentric images and transferring it one-way to egocentric images limits the applicability of previous works in complex interaction scenarios. Instead, this study introduces LoopTrans, a novel closed-loop framework that not only transfers knowledge from exocentric to egocentric but also transfers back to enhance exocentric knowledge extraction. Within LoopTrans, several innovative mechanisms are introduced, including unified cross-modal localization and denoising knowledge distillation, to bridge domain gaps between object-centered egocentric and interaction-centered exocentric images while enhancing knowledge transfer. Experiments show that LoopTrans achieves consistent improvements across all metrics on image and video benchmarks, even handling challenging scenarios where object interaction regions are fully occluded by the human body.
<div id='section'>Paperid: <span id='pid'>652, <a href='https://arxiv.org/pdf/2506.10233.pdf' target='_blank'>https://arxiv.org/pdf/2506.10233.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ana Lawry Aguila, Peirong Liu, Oula Puonti, Juan Eugenio Iglesias
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10233">Conditional diffusion models for guided anomaly detection in brain images using fluid-driven anomaly randomization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Supervised machine learning has enabled accurate pathology detection in brain MRI, but requires training data from diseased subjects that may not be readily available in some scenarios, for example, in the case of rare diseases. Reconstruction-based unsupervised anomaly detection, in particular using diffusion models, has gained popularity in the medical field as it allows for training on healthy images alone, eliminating the need for large disease-specific cohorts. These methods assume that a model trained on normal data cannot accurately represent or reconstruct anomalies. However, this assumption often fails with models failing to reconstruct healthy tissue or accurately reconstruct abnormal regions i.e., failing to remove anomalies. In this work, we introduce a novel conditional diffusion model framework for anomaly detection and healthy image reconstruction in brain MRI. Our weakly supervised approach integrates synthetically generated pseudo-pathology images into the modeling process to better guide the reconstruction of healthy images. To generate these pseudo-pathologies, we apply fluid-driven anomaly randomization to augment real pathology segmentation maps from an auxiliary dataset, ensuring that the synthetic anomalies are both realistic and anatomically coherent. We evaluate our model's ability to detect pathology, using both synthetic anomaly datasets and real pathology from the ATLAS dataset. In our extensive experiments, our model: (i) consistently outperforms variational autoencoders, and conditional and unconditional latent diffusion; and (ii) surpasses on most datasets, the performance of supervised inpainting methods with access to paired diseased/healthy images.
<div id='section'>Paperid: <span id='pid'>653, <a href='https://arxiv.org/pdf/2503.13895.pdf' target='_blank'>https://arxiv.org/pdf/2503.13895.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinliang Zhang, Lei Zhu, Shuang Zeng, Hangzhou He, Ourui Fu, Zhengjian Yao, Zhaoheng Xie, Yanye Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13895">Exploiting Inherent Class Label: Towards Robust Scribble Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scribble-based weakly supervised semantic segmentation leverages only a few annotated pixels as labels to train a segmentation model, presenting significant potential for reducing the human labor involved in the annotation process. This approach faces two primary challenges: first, the sparsity of scribble annotations can lead to inconsistent predictions due to limited supervision; second, the variability in scribble annotations, reflecting differing human annotator preferences, can prevent the model from consistently capturing the discriminative regions of objects, potentially leading to unstable predictions. To address these issues, we propose a holistic framework, the class-driven scribble promotion network, for robust scribble-supervised semantic segmentation. This framework not only utilizes the provided scribble annotations but also leverages their associated class labels to generate reliable pseudo-labels. Within the network, we introduce a localization rectification module to mitigate noisy labels and a distance perception module to identify reliable regions surrounding scribble annotations and pseudo-labels. In addition, we introduce new large-scale benchmarks, ScribbleCOCO and ScribbleCityscapes, accompanied by a scribble simulation algorithm that enables evaluation across varying scribble styles. Our method demonstrates competitive performance in both accuracy and robustness, underscoring its superiority over existing approaches. The datasets and the codes will be made publicly available.
<div id='section'>Paperid: <span id='pid'>654, <a href='https://arxiv.org/pdf/2502.01458.pdf' target='_blank'>https://arxiv.org/pdf/2502.01458.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Yao, Wenkai Yang, Gengze Xu, Ziqiao Wang, Yankai Lin, Yong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01458">The Capabilities and Limitations of Weak-to-Strong Generalization: Generalization and Calibration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weak-to-strong generalization, where weakly supervised strong models outperform their weaker teachers, offers a promising approach to aligning superhuman models with human values. To deepen the understanding of this approach, we provide theoretical insights into its capabilities and limitations. First, in the classification setting, we establish upper and lower generalization error bounds for the strong model, identifying the primary limitations as stemming from the weak model's generalization error and the optimization objective itself. Additionally, we derive lower and upper bounds on the calibration error of the strong model. These theoretical bounds reveal two critical insights: (1) the weak model should demonstrate strong generalization performance and maintain well-calibrated predictions, and (2) the strong model's training process must strike a careful balance, as excessive optimization could undermine its generalization capability by over-relying on the weak supervision signals. Finally, in the regression setting, we extend the work of Charikar et al. (2024) to a loss function based on Kullback-Leibler (KL) divergence, offering guarantees that the strong student can outperform its weak teacher by at least the magnitude of their disagreement. We conduct sufficient experiments to validate our theory.
<div id='section'>Paperid: <span id='pid'>655, <a href='https://arxiv.org/pdf/2409.04758.pdf' target='_blank'>https://arxiv.org/pdf/2409.04758.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuchang Ye, Mingyuan Meng, Mingjian Li, Dagan Feng, Jinman Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.04758">SGSeg: Enabling Text-free Inference in Language-guided Segmentation of Chest X-rays via Self-guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Segmentation of infected areas in chest X-rays is pivotal for facilitating the accurate delineation of pulmonary structures and pathological anomalies. Recently, multi-modal language-guided image segmentation methods have emerged as a promising solution for chest X-rays where the clinical text reports, depicting the assessment of the images, are used as guidance. Nevertheless, existing language-guided methods require clinical reports alongside the images, and hence, they are not applicable for use in image segmentation in a decision support context, but rather limited to retrospective image analysis after clinical reporting has been completed. In this study, we propose a self-guided segmentation framework (SGSeg) that leverages language guidance for training (multi-modal) while enabling text-free inference (uni-modal), which is the first that enables text-free inference in language-guided segmentation. We exploit the critical location information of both pulmonary and pathological structures depicted in the text reports and introduce a novel localization-enhanced report generation (LERG) module to generate clinical reports for self-guidance. Our LERG integrates an object detector and a location-based attention aggregator, weakly-supervised by a location-aware pseudo-label extraction module. Extensive experiments on a well-benchmarked QaTa-COV19 dataset demonstrate that our SGSeg achieved superior performance than existing uni-modal segmentation methods and closely matched the state-of-the-art performance of multi-modal language-guided segmentation methods.
<div id='section'>Paperid: <span id='pid'>656, <a href='https://arxiv.org/pdf/2403.17541.pdf' target='_blank'>https://arxiv.org/pdf/2403.17541.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Astitva Srivastava, Pranav Manu, Amit Raj, Varun Jampani, Avinash Sharma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17541">WordRobe: Text-Guided Generation of Textured 3D Garments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we tackle a new and challenging problem of text-driven generation of 3D garments with high-quality textures. We propose "WordRobe", a novel framework for the generation of unposed & textured 3D garment meshes from user-friendly text prompts. We achieve this by first learning a latent representation of 3D garments using a novel coarse-to-fine training strategy and a loss for latent disentanglement, promoting better latent interpolation. Subsequently, we align the garment latent space to the CLIP embedding space in a weakly supervised manner, enabling text-driven 3D garment generation and editing. For appearance modeling, we leverage the zero-shot generation capability of ControlNet to synthesize view-consistent texture maps in a single feed-forward inference step, thereby drastically decreasing the generation time as compared to existing methods. We demonstrate superior performance over current SOTAs for learning 3D garment latent space, garment interpolation, and text-driven texture synthesis, supported by quantitative evaluation and qualitative user study. The unposed 3D garment meshes generated using WordRobe can be directly fed to standard cloth simulation & animation pipelines without any post-processing.
<div id='section'>Paperid: <span id='pid'>657, <a href='https://arxiv.org/pdf/2403.07603.pdf' target='_blank'>https://arxiv.org/pdf/2403.07603.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Åukasz Struski, Adam Pardyl, Jacek Tabor, Bartosz ZieliÅski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07603">ProPML: Probability Partial Multi-label Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partial Multi-label Learning (PML) is a type of weakly supervised learning where each training instance corresponds to a set of candidate labels, among which only some are true. In this paper, we introduce \our{}, a novel probabilistic approach to this problem that extends the binary cross entropy to the PML setup. In contrast to existing methods, it does not require suboptimal disambiguation and, as such, can be applied to any deep architecture. Furthermore, experiments conducted on artificial and real-world datasets indicate that \our{} outperforms existing approaches, especially for high noise in a candidate set.
<div id='section'>Paperid: <span id='pid'>658, <a href='https://arxiv.org/pdf/2310.12678.pdf' target='_blank'>https://arxiv.org/pdf/2310.12678.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxu Zhang, Shaoli Huang, Zhigang Tu, Xin Chen, Xiaohang Zhan, Gang Yu, Ying Shan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.12678">TapMo: Shape-aware Motion Generation of Skeleton-free Characters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Previous motion generation methods are limited to the pre-rigged 3D human model, hindering their applications in the animation of various non-rigged characters. In this work, we present TapMo, a Text-driven Animation Pipeline for synthesizing Motion in a broad spectrum of skeleton-free 3D characters. The pivotal innovation in TapMo is its use of shape deformation-aware features as a condition to guide the diffusion model, thereby enabling the generation of mesh-specific motions for various characters. Specifically, TapMo comprises two main components - Mesh Handle Predictor and Shape-aware Diffusion Module. Mesh Handle Predictor predicts the skinning weights and clusters mesh vertices into adaptive handles for deformation control, which eliminates the need for traditional skeletal rigging. Shape-aware Motion Diffusion synthesizes motion with mesh-specific adaptations. This module employs text-guided motions and mesh features extracted during the first stage, preserving the geometric integrity of the animations by accounting for the character's shape and deformation. Trained in a weakly-supervised manner, TapMo can accommodate a multitude of non-human meshes, both with and without associated text motions. We demonstrate the effectiveness and generalizability of TapMo through rigorous qualitative and quantitative experiments. Our results reveal that TapMo consistently outperforms existing auto-animation methods, delivering superior-quality animations for both seen or unseen heterogeneous 3D characters.
<div id='section'>Paperid: <span id='pid'>659, <a href='https://arxiv.org/pdf/2308.04949.pdf' target='_blank'>https://arxiv.org/pdf/2308.04949.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Zhu, Hangzhou He, Xinliang Zhang, Qian Chen, Shuang Zeng, Qiushi Ren, Yanye Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.04949">Branches Mutual Promotion for End-to-End Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end weakly supervised semantic segmentation aims at optimizing a segmentation model in a single-stage training process based on only image annotations. Existing methods adopt an online-trained classification branch to provide pseudo annotations for supervising the segmentation branch. However, this strategy makes the classification branch dominate the whole concurrent training process, hindering these two branches from assisting each other. In our work, we treat these two branches equally by viewing them as diverse ways to generate the segmentation map, and add interactions on both their supervision and operation to achieve mutual promotion. For this purpose, a bidirectional supervision mechanism is elaborated to force the consistency between the outputs of these two branches. Thus, the segmentation branch can also give feedback to the classification branch to enhance the quality of localization seeds. Moreover, our method also designs interaction operations between these two branches to exchange their knowledge to assist each other. Experiments indicate our work outperforms existing end-to-end weakly supervised segmentation methods.
<div id='section'>Paperid: <span id='pid'>660, <a href='https://arxiv.org/pdf/2306.10535.pdf' target='_blank'>https://arxiv.org/pdf/2306.10535.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Åukasz Struski, Dawid Rymarczyk, Arkadiusz Lewicki, Robert Sabiniewicz, Jacek Tabor, Bartosz ZieliÅski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.10535">ProMIL: Probabilistic Multiple Instance Learning for Medical Imaging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple Instance Learning (MIL) is a weakly-supervised problem in which one label is assigned to the whole bag of instances. An important class of MIL models is instance-based, where we first classify instances and then aggregate those predictions to obtain a bag label. The most common MIL model is when we consider a bag as positive if at least one of its instances has a positive label. However, this reasoning does not hold in many real-life scenarios, where the positive bag label is often a consequence of a certain percentage of positive instances. To address this issue, we introduce a dedicated instance-based method called ProMIL, based on deep neural networks and Bernstein polynomial estimation. An important advantage of ProMIL is that it can automatically detect the optimal percentage level for decision-making. We show that ProMIL outperforms standard instance-based MIL in real-world medical applications. We make the code available.
<div id='section'>Paperid: <span id='pid'>661, <a href='https://arxiv.org/pdf/2305.14794.pdf' target='_blank'>https://arxiv.org/pdf/2305.14794.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengyu Dong, Zihan Wang, Jingbo Shang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.14794">Debiasing Made State-of-the-art: Revisiting the Simple Seed-based Weak Supervision for Text Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in weakly supervised text classification mostly focus on designing sophisticated methods to turn high-level human heuristics into quality pseudo-labels. In this paper, we revisit the seed matching-based method, which is arguably the simplest way to generate pseudo-labels, and show that its power was greatly underestimated. We show that the limited performance of seed matching is largely due to the label bias injected by the simple seed-match rule, which prevents the classifier from learning reliable confidence for selecting high-quality pseudo-labels. Interestingly, simply deleting the seed words present in the matched input texts can mitigate the label bias and help learn better confidence. Subsequently, the performance achieved by seed matching can be improved significantly, making it on par with or even better than the state-of-the-art. Furthermore, to handle the case when the seed words are not made known, we propose to simply delete the word tokens in the input text randomly with a high deletion ratio. Remarkably, seed matching equipped with this random deletion method can often achieve even better performance than that with seed deletion.
<div id='section'>Paperid: <span id='pid'>662, <a href='https://arxiv.org/pdf/2305.12401.pdf' target='_blank'>https://arxiv.org/pdf/2305.12401.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianle Wang, Zihan Wang, Weitang Liu, Jingbo Shang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.12401">WOT-Class: Weakly Supervised Open-world Text Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>State-of-the-art weakly supervised text classification methods, while significantly reduced the required human supervision, still requires the supervision to cover all the classes of interest. This is never easy to meet in practice when human explore new, large corpora without complete pictures. In this paper, we work on a novel yet important problem of weakly supervised open-world text classification, where supervision is only needed for a few examples from a few known classes and the machine should handle both known and unknown classes in test time. General open-world classification has been studied mostly using image classification; however, existing methods typically assume the availability of sufficient known-class supervision and strong unknown-class prior knowledge (e.g., the number and/or data distribution). We propose a novel framework WOT-Class that lifts those strong assumptions. Specifically, it follows an iterative process of (a) clustering text to new classes, (b) mining and ranking indicative words for each class, and (c) merging redundant classes by using the overlapped indicative words as a bridge. Extensive experiments on 7 popular text classification datasets demonstrate that WOT-Class outperforms strong baselines consistently with a large margin, attaining 23.33% greater average absolute macro-F1 over existing approaches across all datasets. Such competent accuracy illuminates the practical potential of further reducing human effort for text classification.
<div id='section'>Paperid: <span id='pid'>663, <a href='https://arxiv.org/pdf/2303.01038.pdf' target='_blank'>https://arxiv.org/pdf/2303.01038.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Puhua Jiang, Mingze Sun, Ruqi Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.01038">Neural Intrinsic Embedding for Non-rigid Point Cloud Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As a primitive 3D data representation, point clouds are prevailing in 3D sensing, yet short of intrinsic structural information of the underlying objects. Such discrepancy poses great challenges on directly establishing correspondences between point clouds sampled from deformable shapes. In light of this, we propose Neural Intrinsic Embedding (NIE) to embed each vertex into a high-dimensional space in a way that respects the intrinsic structure. Based upon NIE, we further present a weakly-supervised learning framework for non-rigid point cloud registration. Unlike the prior works, we do not require expansive and sensitive off-line basis construction (e.g., eigen-decomposition of Laplacians), nor do we require ground-truth correspondence labels for supervision. We empirically show that our framework performs on par with or even better than the state-of-the-art baselines, which generally require more supervision and/or more structural geometric input.
<div id='section'>Paperid: <span id='pid'>664, <a href='https://arxiv.org/pdf/2512.06849.pdf' target='_blank'>https://arxiv.org/pdf/2512.06849.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matan Atad, Alexander W. Marka, Lisa Steinhelfer, Anna Curto-Vilalta, Yannik Leonhardt, Sarah C. Foreman, Anna-Sophia Walburga Dietrich, Robert Graf, Alexandra S. Gersing, Bjoern Menze, Daniel Rueckert, Jan S. Kirschke, Hendrik Möller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.06849">Hide-and-Seek Attribution: Weakly Supervised Segmentation of Vertebral Metastases in CT</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate segmentation of vertebral metastasis in CT is clinically important yet difficult to scale, as voxel-level annotations are scarce and both lytic and blastic lesions often resemble benign degenerative changes. We introduce a weakly supervised method trained solely on vertebra-level healthy/malignant labels, without any lesion masks. The method combines a Diffusion Autoencoder (DAE) that produces a classifier-guided healthy edit of each vertebra with pixel-wise difference maps that propose candidate lesion regions. To determine which regions truly reflect malignancy, we introduce Hide-and-Seek Attribution: each candidate is revealed in turn while all others are hidden, the edited image is projected back to the data manifold by the DAE, and a latent-space classifier quantifies the isolated malignant contribution of that component. High-scoring regions form the final lytic or blastic segmentation. On held-out radiologist annotations, we achieve strong blastic/lytic performance despite no mask supervision (F1: 0.91/0.85; Dice: 0.87/0.78), exceeding baselines (F1: 0.79/0.67; Dice: 0.74/0.55). These results show that vertebra-level labels can be transformed into reliable lesion masks, demonstrating that generative editing combined with selective occlusion supports accurate weakly supervised segmentation in CT.
<div id='section'>Paperid: <span id='pid'>665, <a href='https://arxiv.org/pdf/2511.20359.pdf' target='_blank'>https://arxiv.org/pdf/2511.20359.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiqing Guo, Dongdong Xi, Songlin Li, Gaobo Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.20359">From Passive Perception to Active Memory: A Weakly Supervised Image Manipulation Localization Framework Driven by Coarse-Grained Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image manipulation localization (IML) faces a fundamental trade-off between minimizing annotation cost and achieving fine-grained localization accuracy. Existing fully-supervised IML methods depend heavily on dense pixel-level mask annotations, which limits scalability to large datasets or real-world deployment.In contrast, the majority of existing weakly-supervised IML approaches are based on image-level labels, which greatly reduce annotation effort but typically lack precise spatial localization. To address this dilemma, we propose BoxPromptIML, a novel weakly-supervised IML framework that effectively balances annotation cost and localization performance. Specifically, we propose a coarse region annotation strategy, which can generate relatively accurate manipulation masks at lower cost. To improve model efficiency and facilitate deployment, we further design an efficient lightweight student model, which learns to perform fine-grained localization through knowledge distillation from a fixed teacher model based on the Segment Anything Model (SAM). Moreover, inspired by the human subconscious memory mechanism, our feature fusion module employs a dual-guidance strategy that actively contextualizes recalled prototypical patterns with real-time observational cues derived from the input. Instead of passive feature extraction, this strategy enables a dynamic process of knowledge recollection, where long-term memory is adapted to the specific context of the current image, significantly enhancing localization accuracy and robustness. Extensive experiments across both in-distribution and out-of-distribution datasets show that BoxPromptIML outperforms or rivals fully-supervised models, while maintaining strong generalization, low annotation cost, and efficient deployment characteristics.
<div id='section'>Paperid: <span id='pid'>666, <a href='https://arxiv.org/pdf/2511.14238.pdf' target='_blank'>https://arxiv.org/pdf/2511.14238.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Huang, Yongyi Su, Xin Lin, Le Zhang, Xun Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.14238">Enhancing Generalization of Depth Estimation Foundation Model via Weakly-Supervised Adaptation with Regularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emergence of foundation models has substantially advanced zero-shot generalization in monocular depth estimation (MDE), as exemplified by the Depth Anything series. However, given access to some data from downstream tasks, a natural question arises: can the performance of these models be further improved? To this end, we propose WeSTAR, a parameter-efficient framework that performs Weakly supervised Self-Training Adaptation with Regularization, designed to enhance the robustness of MDE foundation models in unseen and diverse domains. We first adopt a dense self-training objective as the primary source of structural self-supervision. To further improve robustness, we introduce semantically-aware hierarchical normalization, which exploits instance-level segmentation maps to perform more stable and multi-scale structural normalization. Beyond dense supervision, we introduce a cost-efficient weak supervision in the form of pairwise ordinal depth annotations to further guide the adaptation process, which enforces informative ordinal constraints to mitigate local topological errors. Finally, a weight regularization loss is employed to anchor the LoRA updates, ensuring training stability and preserving the model's generalizable knowledge. Extensive experiments on both realistic and corrupted out-of-distribution datasets under diverse and challenging scenarios demonstrate that WeSTAR consistently improves generalization and achieves state-of-the-art performance across a wide range of benchmarks.
<div id='section'>Paperid: <span id='pid'>667, <a href='https://arxiv.org/pdf/2507.13018.pdf' target='_blank'>https://arxiv.org/pdf/2507.13018.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Songlin Li, Guofeng Yu, Zhiqing Guo, Yunfeng Diao, Dan Ma, Gaobo Yang, Liejun Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13018">Beyond Fully Supervised Pixel Annotations: Scribble-Driven Weakly-Supervised Framework for Image Manipulation Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning-based image manipulation localization (IML) methods have achieved remarkable performance in recent years, but typically rely on large-scale pixel-level annotated datasets. To address the challenge of acquiring high-quality annotations, some recent weakly supervised methods utilize image-level labels to segment manipulated regions. However, the performance is still limited due to insufficient supervision signals. In this study, we explore a form of weak supervision that improves the annotation efficiency and detection performance, namely scribble annotation supervision. We re-annotated mainstream IML datasets with scribble labels and propose the first scribble-based IML (Sc-IML) dataset. Additionally, we propose the first scribble-based weakly supervised IML framework. Specifically, we employ self-supervised training with a structural consistency loss to encourage the model to produce consistent predictions under multi-scale and augmented inputs. In addition, we propose a prior-aware feature modulation module (PFMM) that adaptively integrates prior information from both manipulated and authentic regions for dynamic feature adjustment, further enhancing feature discriminability and prediction consistency in complex scenes. We also propose a gated adaptive fusion module (GAFM) that utilizes gating mechanisms to regulate information flow during feature fusion, guiding the model toward emphasizing potential tampered regions. Finally, we propose a confidence-aware entropy minimization loss (${\mathcal{L}}_{ {CEM }}$). This loss dynamically regularizes predictions in weakly annotated or unlabeled regions based on model uncertainty, effectively suppressing unreliable predictions. Experimental results show that our method outperforms existing fully supervised approaches in terms of average performance both in-distribution and out-of-distribution.
<div id='section'>Paperid: <span id='pid'>668, <a href='https://arxiv.org/pdf/2503.24135.pdf' target='_blank'>https://arxiv.org/pdf/2503.24135.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexis Guichemerre, Soufiane Belharbi, Mohammadhadi Shateri, Luke McCaffrey, Eric Granger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.24135">PixelCAM: Pixel Class Activation Mapping for Histology Image Classification and ROI Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised object localization (WSOL) methods allow training models to classify images and localize ROIs. WSOL only requires low-cost image-class annotations yet provides a visually interpretable classifier. Standard WSOL methods rely on class activation mapping (CAM) methods to produce spatial localization maps according to a single- or two-step strategy. While both strategies have made significant progress, they still face several limitations with histology images. Single-step methods can easily result in under- or over-activation due to the limited visual ROI saliency in histology images and scarce localization cues. They also face the well-known issue of asynchronous convergence between classification and localization tasks. The two-step approach is sub-optimal because it is constrained to a frozen classifier, limiting the capacity for localization. Moreover, these methods also struggle when applied to out-of-distribution (OOD) datasets. In this paper, a multi-task approach for WSOL is introduced for simultaneous training of both tasks to address the asynchronous convergence problem. In particular, localization is performed in the pixel-feature space of an image encoder that is shared with classification. This allows learning discriminant features and accurate delineation of foreground/background regions to support ROI localization and image classification. We propose PixelCAM, a cost-effective foreground/background pixel-wise classifier in the pixel-feature space that allows for spatial object localization. Using partial-cross entropy, PixelCAM is trained using pixel pseudo-labels collected from a pretrained WSOL model. Both image and pixel-wise classifiers are trained simultaneously using standard gradient descent. In addition, our pixel classifier can easily be integrated into CNN- and transformer-based architectures without any modifications.
<div id='section'>Paperid: <span id='pid'>669, <a href='https://arxiv.org/pdf/2502.19698.pdf' target='_blank'>https://arxiv.org/pdf/2502.19698.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangfeng Jiang, Jun Liu, Yongxuan Lv, Yuzhi Wu, Xianfei Li, Wenlong Liao, Tao He, Pai Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19698">You Only Click Once: Single Point Weakly Supervised 3D Instance Segmentation for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Outdoor LiDAR point cloud 3D instance segmentation is a crucial task in autonomous driving. However, it requires laborious human efforts to annotate the point cloud for training a segmentation model. To address this challenge, we propose a YoCo framework, which generates 3D pseudo labels using minimal coarse click annotations in the bird's eye view plane. It is a significant challenge to produce high-quality pseudo labels from sparse annotations. Our YoCo framework first leverages vision foundation models combined with geometric constraints from point clouds to enhance pseudo label generation. Second, a temporal and spatial-based label updating module is designed to generate reliable updated labels. It leverages predictions from adjacent frames and utilizes the inherent density variation of point clouds (dense near, sparse far). Finally, to further improve label quality, an IoU-guided enhancement module is proposed, replacing pseudo labels with high-confidence and high-IoU predictions. Experiments on the Waymo dataset demonstrate YoCo's effectiveness and generality, achieving state-of-the-art performance among weakly supervised methods and surpassing fully supervised Cylinder3D. Additionally, the YoCo is suitable for various networks, achieving performance comparable to fully supervised methods with minimal fine-tuning using only 0.8% of the fully labeled data, significantly reducing annotation costs.
<div id='section'>Paperid: <span id='pid'>670, <a href='https://arxiv.org/pdf/2502.03856.pdf' target='_blank'>https://arxiv.org/pdf/2502.03856.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lin Li, Chuhan Zhang, Dong Zhang, Chong Sun, Chen Li, Long Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03856">Taking A Closer Look at Interacting Objects: Interaction-Aware Open Vocabulary Scene Graph Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Today's open vocabulary scene graph generation (OVSGG) extends traditional SGG by recognizing novel objects and relationships beyond predefined categories, leveraging the knowledge from pre-trained large-scale models. Most existing methods adopt a two-stage pipeline: weakly supervised pre-training with image captions and supervised fine-tuning (SFT) on fully annotated scene graphs. Nonetheless, they omit explicit modeling of interacting objects and treat all objects equally, resulting in mismatched relation pairs. To this end, we propose an interaction-aware OVSGG framework INOVA. During pre-training, INOVA employs an interaction-aware target generation strategy to distinguish interacting objects from non-interacting ones. In SFT, INOVA devises an interaction-guided query selection tactic to prioritize interacting objects during bipartite graph matching. Besides, INOVA is equipped with an interaction-consistent knowledge distillation to enhance the robustness by pushing interacting object pairs away from the background. Extensive experiments on two benchmarks (VG and GQA) show that INOVA achieves state-of-the-art performance, demonstrating the potential of interaction-aware mechanisms for real-world applications.
<div id='section'>Paperid: <span id='pid'>671, <a href='https://arxiv.org/pdf/2501.17148.pdf' target='_blank'>https://arxiv.org/pdf/2501.17148.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengxuan Wu, Aryaman Arora, Atticus Geiger, Zheng Wang, Jing Huang, Dan Jurafsky, Christopher D. Manning, Christopher Potts
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.17148">AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse Autoencoders</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fine-grained steering of language model outputs is essential for safety and reliability. Prompting and finetuning are widely used to achieve these goals, but interpretability researchers have proposed a variety of representation-based techniques as well, including sparse autoencoders (SAEs), linear artificial tomography, supervised steering vectors, linear probes, and representation finetuning. At present, there is no benchmark for making direct comparisons between these proposals. Therefore, we introduce AxBench, a large-scale benchmark for steering and concept detection, and report experiments on Gemma-2-2B and 9B. For steering, we find that prompting outperforms all existing methods, followed by finetuning. For concept detection, representation-based methods such as difference-in-means, perform the best. On both evaluations, SAEs are not competitive. We introduce a novel weakly-supervised representational method (Rank-1 Representation Finetuning; ReFT-r1), which is competitive on both tasks while providing the interpretability advantages that prompting lacks. Along with AxBench, we train and publicly release SAE-scale feature dictionaries for ReFT-r1 and DiffMean.
<div id='section'>Paperid: <span id='pid'>672, <a href='https://arxiv.org/pdf/2411.04607.pdf' target='_blank'>https://arxiv.org/pdf/2411.04607.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chong Wang, Fengbei Liu, Yuanhong Chen, Helen Frazer, Gustavo Carneiro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04607">Cross- and Intra-image Prototypical Learning for Multi-label Disease Diagnosis and Interpretation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in prototypical learning have shown remarkable potential to provide useful decision interpretations associating activation maps and predictions with class-specific training prototypes. Such prototypical learning has been well-studied for various single-label diseases, but for quite relevant and more challenging multi-label diagnosis, where multiple diseases are often concurrent within an image, existing prototypical learning models struggle to obtain meaningful activation maps and effective class prototypes due to the entanglement of the multiple diseases. In this paper, we present a novel Cross- and Intra-image Prototypical Learning (CIPL) framework, for accurate multi-label disease diagnosis and interpretation from medical images. CIPL takes advantage of common cross-image semantics to disentangle the multiple diseases when learning the prototypes, allowing a comprehensive understanding of complicated pathological lesions. Furthermore, we propose a new two-level alignment-based regularisation strategy that effectively leverages consistent intra-image information to enhance interpretation robustness and predictive performance. Extensive experiments show that our CIPL attains the state-of-the-art (SOTA) classification accuracy in two public multi-label benchmarks of disease diagnosis: thoracic radiography and fundus images. Quantitative interpretability results show that CIPL also has superiority in weakly-supervised thoracic disease localisation over other leading saliency- and prototype-based explanation methods.
<div id='section'>Paperid: <span id='pid'>673, <a href='https://arxiv.org/pdf/2408.10069.pdf' target='_blank'>https://arxiv.org/pdf/2408.10069.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Reuben Dorent, Roya Khajavi, Tagwa Idris, Erik Ziegler, Bhanusupriya Somarouthu, Heather Jacene, Ann LaCasce, Jonathan Deissler, Jan Ehrhardt, Sofija Engelson, Stefan M. Fischer, Yun Gu, Heinz Handels, Satoshi Kasai, Satoshi Kondo, Klaus Maier-Hein, Julia A. Schnabel, Guotai Wang, Litingyu Wang, Tassilo Wald, Guang-Zhong Yang, Hanxiao Zhang, Minghui Zhang, Steve Pieper, Gordon Harris, Ron Kikinis, Tina Kapur
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.10069">LNQ 2023 challenge: Benchmark of weakly-supervised techniques for mediastinal lymph node quantification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate assessment of lymph node size in 3D CT scans is crucial for cancer staging, therapeutic management, and monitoring treatment response. Existing state-of-the-art segmentation frameworks in medical imaging often rely on fully annotated datasets. However, for lymph node segmentation, these datasets are typically small due to the extensive time and expertise required to annotate the numerous lymph nodes in 3D CT scans. Weakly-supervised learning, which leverages incomplete or noisy annotations, has recently gained interest in the medical imaging community as a potential solution. Despite the variety of weakly-supervised techniques proposed, most have been validated only on private datasets or small publicly available datasets. To address this limitation, the Mediastinal Lymph Node Quantification (LNQ) challenge was organized in conjunction with the 26th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2023). This challenge aimed to advance weakly-supervised segmentation methods by providing a new, partially annotated dataset and a robust evaluation framework. A total of 16 teams from 5 countries submitted predictions to the validation leaderboard, and 6 teams from 3 countries participated in the evaluation phase. The results highlighted both the potential and the current limitations of weakly-supervised approaches. On one hand, weakly-supervised approaches obtained relatively good performance with a median Dice score of $61.0\%$. On the other hand, top-ranked teams, with a median Dice score exceeding $70\%$, boosted their performance by leveraging smaller but fully annotated datasets to combine weak supervision and full supervision. This highlights both the promise of weakly-supervised methods and the ongoing need for high-quality, fully annotated data to achieve higher segmentation performance.
<div id='section'>Paperid: <span id='pid'>674, <a href='https://arxiv.org/pdf/2407.19507.pdf' target='_blank'>https://arxiv.org/pdf/2407.19507.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingjing Wu, Zhengyao Fang, Pengyuan Lyu, Chengquan Zhang, Fanglin Chen, Guangming Lu, Wenjie Pei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.19507">WeCromCL: Weakly Supervised Cross-Modality Contrastive Learning for Transcription-only Supervised Text Spotting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transcription-only Supervised Text Spotting aims to learn text spotters relying only on transcriptions but no text boundaries for supervision, thus eliminating expensive boundary annotation. The crux of this task lies in locating each transcription in scene text images without location annotations. In this work, we formulate this challenging problem as a Weakly Supervised Cross-modality Contrastive Learning problem, and design a simple yet effective model dubbed WeCromCL that is able to detect each transcription in a scene image in a weakly supervised manner. Unlike typical methods for cross-modality contrastive learning that focus on modeling the holistic semantic correlation between an entire image and a text description, our WeCromCL conducts atomistic contrastive learning to model the character-wise appearance consistency between a text transcription and its correlated region in a scene image to detect an anchor point for the transcription in a weakly supervised manner. The detected anchor points by WeCromCL are further used as pseudo location labels to guide the learning of text spotting. Extensive experiments on four challenging benchmarks demonstrate the superior performance of our model over other methods. Code will be released.
<div id='section'>Paperid: <span id='pid'>675, <a href='https://arxiv.org/pdf/2404.19113.pdf' target='_blank'>https://arxiv.org/pdf/2404.19113.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexis Guichemerre, Soufiane Belharbi, Tsiry Mayet, Shakeeb Murtaza, Pourya Shamsolmoali, Luke McCaffrey, Eric Granger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.19113">Source-Free Domain Adaptation of Weakly-Supervised Object Localization Models for Histology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Given the emergence of deep learning, digital pathology has gained popularity for cancer diagnosis based on histology images. Deep weakly supervised object localization (WSOL) models can be trained to classify histology images according to cancer grade and identify regions of interest (ROIs) for interpretation, using inexpensive global image-class annotations. A WSOL model initially trained on some labeled source image data can be adapted using unlabeled target data in cases of significant domain shifts caused by variations in staining, scanners, and cancer type. In this paper, we focus on source-free (unsupervised) domain adaptation (SFDA), a challenging problem where a pre-trained source model is adapted to a new target domain without using any source domain data for privacy and efficiency reasons. SFDA of WSOL models raises several challenges in histology, most notably because they are not intended to adapt for both classification and localization tasks. In this paper, 4 state-of-the-art SFDA methods, each one representative of a main SFDA family, are compared for WSOL in terms of classification and localization accuracy. They are the SFDA-Distribution Estimation, Source HypOthesis Transfer, Cross-Domain Contrastive Learning, and Adaptively Domain Statistics Alignment. Experimental results on the challenging Glas (smaller, breast cancer) and Camelyon16 (larger, colon cancer) histology datasets indicate that these SFDA methods typically perform poorly for localization after adaptation when optimized for classification.
<div id='section'>Paperid: <span id='pid'>676, <a href='https://arxiv.org/pdf/2403.00165.pdf' target='_blank'>https://arxiv.org/pdf/2403.00165.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunyi Zhang, Ruozhen Yang, Xueqiang Xu, Rui Li, Jinfeng Xiao, Jiaming Shen, Jiawei Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.00165">TELEClass: Taxonomy Enrichment and LLM-Enhanced Hierarchical Text Classification with Minimal Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hierarchical text classification aims to categorize each document into a set of classes in a label taxonomy, which is a fundamental web text mining task with broad applications such as web content analysis and semantic indexing. Most earlier works focus on fully or semi-supervised methods that require a large amount of human annotated data which is costly and time-consuming to acquire. To alleviate human efforts, in this paper, we work on hierarchical text classification with a minimal amount of supervision: using the sole class name of each node as the only supervision. Recently, large language models (LLM) have shown competitive performance on various tasks through zero-shot prompting, but this method performs poorly in the hierarchical setting because it is ineffective to include the large and structured label space in a prompt. On the other hand, previous weakly-supervised hierarchical text classification methods only utilize the raw taxonomy skeleton and ignore the rich information hidden in the text corpus that can serve as additional class-indicative features. To tackle the above challenges, we propose TELEClass, Taxonomy Enrichment and LLM-Enhanced weakly-supervised hierarchical text Classification, which combines the general knowledge of LLMs and task-specific features mined from an unlabeled corpus. TELEClass automatically enriches the raw taxonomy with class-indicative features for better label space understanding and utilizes novel LLM-based data annotation and generation methods specifically tailored for the hierarchical setting. Experiments show that TELEClass can significantly outperform previous baselines while achieving comparable performance to zero-shot prompting of LLMs with drastically less inference cost.
<div id='section'>Paperid: <span id='pid'>677, <a href='https://arxiv.org/pdf/2312.03502.pdf' target='_blank'>https://arxiv.org/pdf/2312.03502.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haojie Zhang, Yongyi Su, Xun Xu, Kui Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.03502">Improving the Generalization of Segmentation Foundation Model under Distribution Shift via Weakly Supervised Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The success of large language models has inspired the computer vision community to explore image segmentation foundation model that is able to zero/few-shot generalize through prompt engineering. Segment-Anything(SAM), among others, is the state-of-the-art image segmentation foundation model demonstrating strong zero/few-shot generalization. Despite the success, recent studies reveal the weakness of SAM under strong distribution shift. In particular, SAM performs awkwardly on corrupted natural images, camouflaged images, medical images, etc. Motivated by the observations, we aim to develop a self-training based strategy to adapt SAM to target distribution. Given the unique challenges of large source dataset, high computation cost and incorrect pseudo label, we propose a weakly supervised self-training architecture with anchor regularization and low-rank finetuning to improve the robustness and computation efficiency of adaptation. We validate the effectiveness on 5 types of downstream segmentation tasks including natural clean/corrupted images, medical images, camouflaged images and robotic images. Our proposed method is task-agnostic in nature and outperforms pre-trained SAM and state-of-the-art domain adaptation methods on almost all downstream tasks with the same testing prompt inputs.
<div id='section'>Paperid: <span id='pid'>678, <a href='https://arxiv.org/pdf/2308.10449.pdf' target='_blank'>https://arxiv.org/pdf/2308.10449.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liangrui Pan, Lian Wang, Zhichao Feng, Liwen Xu, Shaoliang Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.10449">CVFC: Attention-Based Cross-View Feature Consistency for Weakly Supervised Semantic Segmentation of Pathology Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Histopathology image segmentation is the gold standard for diagnosing cancer, and can indicate cancer prognosis. However, histopathology image segmentation requires high-quality masks, so many studies now use imagelevel labels to achieve pixel-level segmentation to reduce the need for fine-grained annotation. To solve this problem, we propose an attention-based cross-view feature consistency end-to-end pseudo-mask generation framework named CVFC based on the attention mechanism. Specifically, CVFC is a three-branch joint framework composed of two Resnet38 and one Resnet50, and the independent branch multi-scale integrated feature map to generate a class activation map (CAM); in each branch, through down-sampling and The expansion method adjusts the size of the CAM; the middle branch projects the feature matrix to the query and key feature spaces, and generates a feature space perception matrix through the connection layer and inner product to adjust and refine the CAM of each branch; finally, through the feature consistency loss and feature cross loss to optimize the parameters of CVFC in co-training mode. After a large number of experiments, An IoU of 0.7122 and a fwIoU of 0.7018 are obtained on the WSSS4LUAD dataset, which outperforms HistoSegNet, SEAM, C-CAM, WSSS-Tissue, and OEEM, respectively.
<div id='section'>Paperid: <span id='pid'>679, <a href='https://arxiv.org/pdf/2307.15904.pdf' target='_blank'>https://arxiv.org/pdf/2307.15904.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aayush Dhakal, Adeel Ahmad, Subash Khanal, Srikumar Sastry, Hannah Kerner, Nathan Jacobs
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.15904">Sat2Cap: Mapping Fine-Grained Textual Descriptions from Satellite Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a weakly supervised approach for creating maps using free-form textual descriptions. We refer to this work of creating textual maps as zero-shot mapping. Prior works have approached mapping tasks by developing models that predict a fixed set of attributes using overhead imagery. However, these models are very restrictive as they can only solve highly specific tasks for which they were trained. Mapping text, on the other hand, allows us to solve a large variety of mapping problems with minimal restrictions. To achieve this, we train a contrastive learning framework called Sat2Cap on a new large-scale dataset with 6.1M pairs of overhead and ground-level images. For a given location and overhead image, our model predicts the expected CLIP embeddings of the ground-level scenery. The predicted CLIP embeddings are then used to learn about the textual space associated with that location. Sat2Cap is also conditioned on date-time information, allowing it to model temporally varying concepts over a location. Our experimental results demonstrate that our models successfully capture ground-level concepts and allow large-scale mapping of fine-grained textual queries. Our approach does not require any text-labeled data, making the training easily scalable. The code, dataset, and models will be made publicly available.
<div id='section'>Paperid: <span id='pid'>680, <a href='https://arxiv.org/pdf/2303.15440.pdf' target='_blank'>https://arxiv.org/pdf/2303.15440.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahui Lei, Congyue Deng, Karl Schmeckpeper, Leonidas Guibas, Kostas Daniilidis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.15440">EFEM: Equivariant Neural Field Expectation Maximization for 3D Object Segmentation Without Scene Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Equivariant Neural Field Expectation Maximization (EFEM), a simple, effective, and robust geometric algorithm that can segment objects in 3D scenes without annotations or training on scenes. We achieve such unsupervised segmentation by exploiting single object shape priors. We make two novel steps in that direction. First, we introduce equivariant shape representations to this problem to eliminate the complexity induced by the variation in object configuration. Second, we propose a novel EM algorithm that can iteratively refine segmentation masks using the equivariant shape prior. We collect a novel real dataset Chairs and Mugs that contains various object configurations and novel scenes in order to verify the effectiveness and robustness of our method. Experimental results demonstrate that our method achieves consistent and robust performance across different scenes where the (weakly) supervised methods may fail. Code and data available at https://www.cis.upenn.edu/~leijh/projects/efem
<div id='section'>Paperid: <span id='pid'>681, <a href='https://arxiv.org/pdf/2509.11984.pdf' target='_blank'>https://arxiv.org/pdf/2509.11984.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meng Wei, Zhongnian Li, Peng Ying, Xinzheng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11984">Learning from Uncertain Similarity and Unlabeled Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing similarity-based weakly supervised learning approaches often rely on precise similarity annotations between data pairs, which may inadvertently expose sensitive label information and raise privacy risks. To mitigate this issue, we propose Uncertain Similarity and Unlabeled Learning (USimUL), a novel framework where each similarity pair is embedded with an uncertainty component to reduce label leakage. In this paper, we propose an unbiased risk estimator that learns from uncertain similarity and unlabeled data. Additionally, we theoretically prove that the estimator achieves statistically optimal parametric convergence rates. Extensive experiments on both benchmark and real-world datasets show that our method achieves superior classification performance compared to conventional similarity-based approaches.
<div id='section'>Paperid: <span id='pid'>682, <a href='https://arxiv.org/pdf/2508.04566.pdf' target='_blank'>https://arxiv.org/pdf/2508.04566.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinxing Zhou, Ziheng Zhou, Yanghao Zhou, Yuxin Mao, Zhangling Duan, Dan Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04566">CLASP: Cross-modal Salient Anchor-based Semantic Propagation for Weakly-supervised Dense Audio-Visual Event Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Dense Audio-Visual Event Localization (DAVEL) task aims to temporally localize events in untrimmed videos that occur simultaneously in both the audio and visual modalities. This paper explores DAVEL under a new and more challenging weakly-supervised setting (W-DAVEL task), where only video-level event labels are provided and the temporal boundaries of each event are unknown. We address W-DAVEL by exploiting \textit{cross-modal salient anchors}, which are defined as reliable timestamps that are well predicted under weak supervision and exhibit highly consistent event semantics across audio and visual modalities. Specifically, we propose a \textit{Mutual Event Agreement Evaluation} module, which generates an agreement score by measuring the discrepancy between the predicted audio and visual event classes. Then, the agreement score is utilized in a \textit{Cross-modal Salient Anchor Identification} module, which identifies the audio and visual anchor features through global-video and local temporal window identification mechanisms. The anchor features after multimodal integration are fed into an \textit{Anchor-based Temporal Propagation} module to enhance event semantic encoding in the original temporal audio and visual features, facilitating better temporal localization under weak supervision. We establish benchmarks for W-DAVEL on both the UnAV-100 and ActivityNet1.3 datasets. Extensive experiments demonstrate that our method achieves state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>683, <a href='https://arxiv.org/pdf/2507.12942.pdf' target='_blank'>https://arxiv.org/pdf/2507.12942.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yafei Zhang, Lingqi Kong, Huafeng Li, Jie Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12942">Weakly Supervised Visible-Infrared Person Re-Identification via Heterogeneous Expert Collaborative Consistency Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To reduce the reliance of visible-infrared person re-identification (ReID) models on labeled cross-modal samples, this paper explores a weakly supervised cross-modal person ReID method that uses only single-modal sample identity labels, addressing scenarios where cross-modal identity labels are unavailable. To mitigate the impact of missing cross-modal labels on model performance, we propose a heterogeneous expert collaborative consistency learning framework, designed to establish robust cross-modal identity correspondences in a weakly supervised manner. This framework leverages labeled data from each modality to independently train dedicated classification experts. To associate cross-modal samples, these classification experts act as heterogeneous predictors, predicting the identities of samples from the other modality. To improve prediction accuracy, we design a cross-modal relationship fusion mechanism that effectively integrates predictions from different experts. Under the implicit supervision provided by cross-modal identity correspondences, collaborative and consistent learning among the experts is encouraged, significantly enhancing the model's ability to extract modality-invariant features and improve cross-modal identity recognition. Experimental results on two challenging datasets validate the effectiveness of the proposed method.
<div id='section'>Paperid: <span id='pid'>684, <a href='https://arxiv.org/pdf/2507.12015.pdf' target='_blank'>https://arxiv.org/pdf/2507.12015.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoxun Li, Leyuan Qu, Jiaxi Hu, Taihao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12015">EME-TTS: Unlocking the Emphasis and Emotion Link in Speech Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, emotional Text-to-Speech (TTS) synthesis and emphasis-controllable speech synthesis have advanced significantly. However, their interaction remains underexplored. We propose Emphasis Meets Emotion TTS (EME-TTS), a novel framework designed to address two key research questions: (1) how to effectively utilize emphasis to enhance the expressiveness of emotional speech, and (2) how to maintain the perceptual clarity and stability of target emphasis across different emotions. EME-TTS employs weakly supervised learning with emphasis pseudo-labels and variance-based emphasis features. Additionally, the proposed Emphasis Perception Enhancement (EPE) block enhances the interaction between emotional signals and emphasis positions. Experimental results show that EME-TTS, when combined with large language models for emphasis position prediction, enables more natural emotional speech synthesis while preserving stable and distinguishable target emphasis across emotions. Synthesized samples are available on-line.
<div id='section'>Paperid: <span id='pid'>685, <a href='https://arxiv.org/pdf/2507.06744.pdf' target='_blank'>https://arxiv.org/pdf/2507.06744.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yafei Zhang, Yongle Shang, Huafeng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06744">Dual-Granularity Cross-Modal Identity Association for Weakly-Supervised Text-to-Person Image Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised text-to-person image matching, as a crucial approach to reducing models' reliance on large-scale manually labeled samples, holds significant research value. However, existing methods struggle to predict complex one-to-many identity relationships, severely limiting performance improvements. To address this challenge, we propose a local-and-global dual-granularity identity association mechanism. Specifically, at the local level, we explicitly establish cross-modal identity relationships within a batch, reinforcing identity constraints across different modalities and enabling the model to better capture subtle differences and correlations. At the global level, we construct a dynamic cross-modal identity association network with the visual modality as the anchor and introduce a confidence-based dynamic adjustment mechanism, effectively enhancing the model's ability to identify weakly associated samples while improving overall sensitivity. Additionally, we propose an information-asymmetric sample pair construction method combined with consistency learning to tackle hard sample mining and enhance model robustness. Experimental results demonstrate that the proposed method substantially boosts cross-modal matching accuracy, providing an efficient and practical solution for text-to-person image matching.
<div id='section'>Paperid: <span id='pid'>686, <a href='https://arxiv.org/pdf/2505.15123.pdf' target='_blank'>https://arxiv.org/pdf/2505.15123.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ta Duc Huy, Duy Anh Huynh, Yutong Xie, Yuankai Qi, Qi Chen, Phi Le Nguyen, Sen Kim Tran, Son Lam Phung, Anton van den Hengel, Zhibin Liao, Minh-Son To, Johan W. Verjans, Vu Minh Hieu Phan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.15123">Seeing the Trees for the Forest: Rethinking Weakly-Supervised Medical Visual Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual grounding (VG) is the capability to identify the specific regions in an image associated with a particular text description. In medical imaging, VG enhances interpretability by highlighting relevant pathological features corresponding to textual descriptions, improving model transparency and trustworthiness for wider adoption of deep learning models in clinical practice. Current models struggle to associate textual descriptions with disease regions due to inefficient attention mechanisms and a lack of fine-grained token representations. In this paper, we empirically demonstrate two key observations. First, current VLMs assign high norms to background tokens, diverting the model's attention from regions of disease. Second, the global tokens used for cross-modal learning are not representative of local disease tokens. This hampers identifying correlations between the text and disease tokens. To address this, we introduce simple, yet effective Disease-Aware Prompting (DAP) process, which uses the explainability map of a VLM to identify the appropriate image features. This simple strategy amplifies disease-relevant regions while suppressing background interference. Without any additional pixel-level annotations, DAP improves visual grounding accuracy by 20.74% compared to state-of-the-art methods across three major chest X-ray datasets.
<div id='section'>Paperid: <span id='pid'>687, <a href='https://arxiv.org/pdf/2405.20044.pdf' target='_blank'>https://arxiv.org/pdf/2405.20044.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengyu Jie, Wanquan Liu, Chenqiang Gao, Yihui Wen, Rui He, Weiping Wen, Pengcheng Li, Jintao Zhang, Deyu Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.20044">A Point-Neighborhood Learning Framework for Nasal Endoscope Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Lesion segmentation on nasal endoscopic images is challenging due to its complex lesion features. Fully-supervised deep learning methods achieve promising performance with pixel-level annotations but impose a significant annotation burden on experts. Although weakly supervised or semi-supervised methods can reduce the labelling burden, their performance is still limited. Some weakly semi-supervised methods employ a novel annotation strategy that labels weak single-point annotations for the entire training set while providing pixel-level annotations for a small subset of the data. However, the relevant weakly semi-supervised methods only mine the limited information of the point itself, while ignoring its label property and surrounding reliable information. This paper proposes a simple yet efficient weakly semi-supervised method called the Point-Neighborhood Learning (PNL) framework. PNL incorporates the surrounding area of the point, referred to as the point-neighborhood, into the learning process. In PNL, we propose a point-neighborhood supervision loss and a pseudo-label scoring mechanism to explicitly guide the model's training. Meanwhile, we proposed a more reliable data augmentation scheme. The proposed method significantly improves performance without increasing the parameters of the segmentation neural network. Extensive experiments on the NPC-LES dataset demonstrate that PNL outperforms existing methods by a significant margin. Additional validation on colonoscopic polyp segmentation datasets confirms the generalizability of the proposed PNL.
<div id='section'>Paperid: <span id='pid'>688, <a href='https://arxiv.org/pdf/2403.16469.pdf' target='_blank'>https://arxiv.org/pdf/2403.16469.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meng Wei, Zhongnian Li, Yong Zhou, Xinzheng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.16469">Learning from Reduced Labels for Long-Tailed Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Long-tailed data is prevalent in real-world classification tasks and heavily relies on supervised information, which makes the annotation process exceptionally labor-intensive and time-consuming. Unfortunately, despite being a common approach to mitigate labeling costs, existing weakly supervised learning methods struggle to adequately preserve supervised information for tail samples, resulting in a decline in accuracy for the tail classes. To alleviate this problem, we introduce a novel weakly supervised labeling setting called Reduced Label. The proposed labeling setting not only avoids the decline of supervised information for the tail samples, but also decreases the labeling costs associated with long-tailed data. Additionally, we propose an straightforward and highly efficient unbiased framework with strong theoretical guarantees to learn from these Reduced Labels. Extensive experiments conducted on benchmark datasets including ImageNet validate the effectiveness of our approach, surpassing the performance of state-of-the-art weakly supervised methods.
<div id='section'>Paperid: <span id='pid'>689, <a href='https://arxiv.org/pdf/2402.17502.pdf' target='_blank'>https://arxiv.org/pdf/2402.17502.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Li Lin, Yixiang Liu, Jiewei Wu, Pujin Cheng, Zhiyuan Cai, Kenneth K. Y. Wong, Xiaoying Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17502">FedLPPA: Learning Personalized Prompt and Aggregation for Federated Weakly-supervised Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated learning (FL) effectively mitigates the data silo challenge brought about by policies and privacy concerns, implicitly harnessing more data for deep model training. However, traditional centralized FL models grapple with diverse multi-center data, especially in the face of significant data heterogeneity, notably in medical contexts. In the realm of medical image segmentation, the growing imperative to curtail annotation costs has amplified the importance of weakly-supervised techniques which utilize sparse annotations such as points, scribbles, etc. A pragmatic FL paradigm shall accommodate diverse annotation formats across different sites, which research topic remains under-investigated. In such context, we propose a novel personalized FL framework with learnable prompt and aggregation (FedLPPA) to uniformly leverage heterogeneous weak supervision for medical image segmentation. In FedLPPA, a learnable universal knowledge prompt is maintained, complemented by multiple learnable personalized data distribution prompts and prompts representing the supervision sparsity. Integrated with sample features through a dual-attention mechanism, those prompts empower each local task decoder to adeptly adjust to both the local distribution and the supervision form. Concurrently, a dual-decoder strategy, predicated on prompt similarity, is introduced for enhancing the generation of pseudo-labels in weakly-supervised learning, alleviating overfitting and noise accumulation inherent to local data, while an adaptable aggregation method is employed to customize the task decoder on a parameter-wise basis. Extensive experiments on four distinct medical image segmentation tasks involving different modalities underscore the superiority of FedLPPA, with its efficacy closely parallels that of fully supervised centralized training. Our code and data will be available.
<div id='section'>Paperid: <span id='pid'>690, <a href='https://arxiv.org/pdf/2402.03785.pdf' target='_blank'>https://arxiv.org/pdf/2402.03785.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haihong Zhao, Chenyi Zi, Yang Liu, Chen Zhang, Yan Zhou, Jia Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.03785">Weakly Supervised Anomaly Detection via Knowledge-Data Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anomaly detection (AD) plays a pivotal role in numerous web-based applications, including malware detection, anti-money laundering, device failure detection, and network fault analysis. Most methods, which rely on unsupervised learning, are hard to reach satisfactory detection accuracy due to the lack of labels. Weakly Supervised Anomaly Detection (WSAD) has been introduced with a limited number of labeled anomaly samples to enhance model performance. Nevertheless, it is still challenging for models, trained on an inadequate amount of labeled data, to generalize to unseen anomalies. In this paper, we introduce a novel framework Knowledge-Data Alignment (KDAlign) to integrate rule knowledge, typically summarized by human experts, to supplement the limited labeled data. Specifically, we transpose these rules into the knowledge space and subsequently recast the incorporation of knowledge as the alignment of knowledge and data. To facilitate this alignment, we employ the Optimal Transport (OT) technique. We then incorporate the OT distance as an additional loss term to the original objective function of WSAD methodologies. Comprehensive experimental results on five real-world datasets demonstrate that our proposed KDAlign framework markedly surpasses its state-of-the-art counterparts, achieving superior performance across various anomaly types.
<div id='section'>Paperid: <span id='pid'>691, <a href='https://arxiv.org/pdf/2310.09114.pdf' target='_blank'>https://arxiv.org/pdf/2310.09114.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Songpengcheng Xia, Lei Chu, Ling Pei, Jiarui Yang, Wenxian Yu, Robert C. Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.09114">Timestamp-supervised Wearable-based Activity Segmentation and Recognition with Contrastive Learning and Order-Preserving Optimal Transport</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human activity recognition (HAR) with wearables is one of the serviceable technologies in ubiquitous and mobile computing applications. The sliding-window scheme is widely adopted while suffering from the multi-class windows problem. As a result, there is a growing focus on joint segmentation and recognition with deep-learning methods, aiming at simultaneously dealing with HAR and time-series segmentation issues. However, obtaining the full activity annotations of wearable data sequences is resource-intensive or time-consuming, while unsupervised methods yield poor performance. To address these challenges, we propose a novel method for joint activity segmentation and recognition with timestamp supervision, in which only a single annotated sample is needed in each activity segment. However, the limited information of sparse annotations exacerbates the gap between recognition and segmentation tasks, leading to sub-optimal model performance. Therefore, the prototypes are estimated by class-activation maps to form a sample-to-prototype contrast module for well-structured embeddings. Moreover, with the optimal transport theory, our approach generates the sample-level pseudo-labels that take advantage of unlabeled data between timestamp annotations for further performance improvement. Comprehensive experiments on four public HAR datasets demonstrate that our model trained with timestamp supervision is superior to the state-of-the-art weakly-supervised methods and achieves comparable performance to the fully-supervised approaches.
<div id='section'>Paperid: <span id='pid'>692, <a href='https://arxiv.org/pdf/2309.00828.pdf' target='_blank'>https://arxiv.org/pdf/2309.00828.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingtao Yu, Heming Du, Chen Liu, Xin Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.00828">When 3D Bounding-Box Meets SAM: Point Cloud Instance Segmentation with Weak-and-Noisy Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning from bounding-boxes annotations has shown great potential in weakly-supervised 3D point cloud instance segmentation. However, we observed that existing methods would suffer severe performance degradation with perturbed bounding box annotations. To tackle this issue, we propose a complementary image prompt-induced weakly-supervised point cloud instance segmentation (CIP-WPIS) method. CIP-WPIS leverages pretrained knowledge embedded in the 2D foundation model SAM and 3D geometric prior to achieve accurate point-wise instance labels from the bounding box annotations. Specifically, CP-WPIS first selects image views in which 3D candidate points of an instance are fully visible. Then, we generate complementary background and foreground prompts from projections to obtain SAM 2D instance mask predictions. According to these, we assign the confidence values to points indicating the likelihood of points belonging to the instance. Furthermore, we utilize 3D geometric homogeneity provided by superpoints to decide the final instance label assignments. In this fashion, we achieve high-quality 3D point-wise instance labels. Extensive experiments on both Scannet-v2 and S3DIS benchmarks demonstrate that our method is robust against noisy 3D bounding-box annotations and achieves state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>693, <a href='https://arxiv.org/pdf/2306.15644.pdf' target='_blank'>https://arxiv.org/pdf/2306.15644.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chiori Hori, Puyuan Peng, David Harwath, Xinyu Liu, Kei Ota, Siddarth Jain, Radu Corcodel, Devesh Jha, Diego Romeres, Jonathan Le Roux
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.15644">Style-transfer based Speech and Audio-visual Scene Understanding for Robot Action Sequence Acquisition from Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To realize human-robot collaboration, robots need to execute actions for new tasks according to human instructions given finite prior knowledge. Human experts can share their knowledge of how to perform a task with a robot through multi-modal instructions in their demonstrations, showing a sequence of short-horizon steps to achieve a long-horizon goal. This paper introduces a method for robot action sequence generation from instruction videos using (1) an audio-visual Transformer that converts audio-visual features and instruction speech to a sequence of robot actions called dynamic movement primitives (DMPs) and (2) style-transfer-based training that employs multi-task learning with video captioning and weakly-supervised learning with a semantic classifier to exploit unpaired video-action data. We built a system that accomplishes various cooking actions, where an arm robot executes a DMP sequence acquired from a cooking video using the audio-visual Transformer. Experiments with Epic-Kitchen-100, YouCookII, QuerYD, and in-house instruction video datasets show that the proposed method improves the quality of DMP sequences by 2.3 times the METEOR score obtained with a baseline video-to-action Transformer. The model achieved 32% of the task success rate with the task knowledge of the object.
<div id='section'>Paperid: <span id='pid'>694, <a href='https://arxiv.org/pdf/2306.03881.pdf' target='_blank'>https://arxiv.org/pdf/2306.03881.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, Bharath Hariharan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.03881">Emergent Correspondence from Image Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Finding correspondences between images is a fundamental problem in computer vision. In this paper, we show that correspondence emerges in image diffusion models without any explicit supervision. We propose a simple strategy to extract this implicit knowledge out of diffusion networks as image features, namely DIffusion FeaTures (DIFT), and use them to establish correspondences between real images. Without any additional fine-tuning or supervision on the task-specific data or annotations, DIFT is able to outperform both weakly-supervised methods and competitive off-the-shelf features in identifying semantic, geometric, and temporal correspondences. Particularly for semantic correspondence, DIFT from Stable Diffusion is able to outperform DINO and OpenCLIP by 19 and 14 accuracy points respectively on the challenging SPair-71k benchmark. It even outperforms the state-of-the-art supervised methods on 9 out of 18 categories while remaining on par for the overall performance. Project page: https://diffusionfeatures.github.io
<div id='section'>Paperid: <span id='pid'>695, <a href='https://arxiv.org/pdf/2305.18794.pdf' target='_blank'>https://arxiv.org/pdf/2305.18794.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Heinrich Dinkel, Weiji Zhuang, Zhiyong Yan, Yongqing Wang, Junbo Zhang, Yujun Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.18794">Understanding temporally weakly supervised training: A case study for keyword spotting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The currently most prominent algorithm to train keyword spotting (KWS) models with deep neural networks (DNNs) requires strong supervision i.e., precise knowledge of the spoken keyword location in time. Thus, most KWS approaches treat the presence of redundant data, such as noise, within their training set as an obstacle. A common training paradigm to deal with data redundancies is to use temporally weakly supervised learning, which only requires providing labels on a coarse scale. This study explores the limits of DNN training using temporally weak labeling with applications in KWS. We train a simple end-to-end classifier on the common Google Speech Commands dataset with increased difficulty by randomly appending and adding noise to the training dataset. Our results indicate that temporally weak labeling can achieve comparable results to strongly supervised baselines while having a less stringent labeling requirement. In the presence of noise, weakly supervised models are capable to localize and extract target keywords without explicit supervision, leading to a performance increase compared to strongly supervised approaches.
<div id='section'>Paperid: <span id='pid'>696, <a href='https://arxiv.org/pdf/2305.18533.pdf' target='_blank'>https://arxiv.org/pdf/2305.18533.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ashwin Rao, Siyi Guo, Sze-Yuh Nina Wang, Fred Morstatter, Kristina Lerman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.18533">Pandemic Culture Wars: Partisan Differences in the Moral Language of COVID-19 Discussions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective response to pandemics requires coordinated adoption of mitigation measures, like masking and quarantines, to curb a virus's spread. However, as the COVID-19 pandemic demonstrated, political divisions can hinder consensus on the appropriate response. To better understand these divisions, our study examines a vast collection of COVID-19-related tweets. We focus on five contentious issues: coronavirus origins, lockdowns, masking, education, and vaccines. We describe a weakly supervised method to identify issue-relevant tweets and employ state-of-the-art computational methods to analyze moral language and infer political ideology. We explore how partisanship and moral language shape conversations about these issues. Our findings reveal ideological differences in issue salience and moral language used by different groups. We find that conservatives use more negatively-valenced moral language than liberals and that political elites use moral rhetoric to a greater extent than non-elites across most issues. Examining the evolution and moralization on divisive issues can provide valuable insights into the dynamics of COVID-19 discussions and assist policymakers in better understanding the emergence of ideological divisions.
<div id='section'>Paperid: <span id='pid'>697, <a href='https://arxiv.org/pdf/2305.15354.pdf' target='_blank'>https://arxiv.org/pdf/2305.15354.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Feifei Shao, Yawei Luo, Lei Chen, Ping Liu, Wei Yang, Yi Yang, Jun Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.15354">Counterfactual Co-occurring Learning for Bias Mitigation in Weakly-supervised Object Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contemporary weakly-supervised object localization (WSOL) methods have primarily focused on addressing the challenge of localizing the most discriminative region while largely overlooking the relatively less explored issue of biased activation -- incorrectly spotlighting co-occurring background with the foreground feature. In this paper, we conduct a thorough causal analysis to investigate the origins of biased activation. Based on our analysis, we attribute this phenomenon to the presence of co-occurring background confounders. Building upon this profound insight, we introduce a pioneering paradigm known as Counterfactual Co-occurring Learning (CCL), meticulously engendering counterfactual representations by adeptly disentangling the foreground from the co-occurring background elements. Furthermore, we propose an innovative network architecture known as Counterfactual-CAM. This architecture seamlessly incorporates a perturbation mechanism for counterfactual representations into the vanilla CAM-based model. By training the WSOL model with these perturbed representations, we guide the model to prioritize the consistent foreground content while concurrently reducing the influence of distracting co-occurring backgrounds. To the best of our knowledge, this study represents the initial exploration of this research direction. Our extensive experiments conducted across multiple benchmarks validate the effectiveness of the proposed Counterfactual-CAM in mitigating biased activation.
<div id='section'>Paperid: <span id='pid'>698, <a href='https://arxiv.org/pdf/2305.00646.pdf' target='_blank'>https://arxiv.org/pdf/2305.00646.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziwei Yu, Chen Li, Linlin Yang, Xiaoxu Zheng, Michael Bi Mi, Gim Hee Lee, Angela Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.00646">Overcoming the Trade-off Between Accuracy and Plausibility in 3D Hand Shape Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Direct mesh fitting for 3D hand shape reconstruction is highly accurate. However, the reconstructed meshes are prone to artifacts and do not appear as plausible hand shapes. Conversely, parametric models like MANO ensure plausible hand shapes but are not as accurate as the non-parametric methods. In this work, we introduce a novel weakly-supervised hand shape estimation framework that integrates non-parametric mesh fitting with MANO model in an end-to-end fashion. Our joint model overcomes the tradeoff in accuracy and plausibility to yield well-aligned and high-quality 3D meshes, especially in challenging two-hand and hand-object interaction scenarios.
<div id='section'>Paperid: <span id='pid'>699, <a href='https://arxiv.org/pdf/2301.05336.pdf' target='_blank'>https://arxiv.org/pdf/2301.05336.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongjun Wang, Zhiwen Zhang, Zipei Fan, Jiyuan Chen, Lingyu Zhang, Ryosuke Shibasaki, Xuan Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.05336">Multitask Weakly Supervised Learning for Origin Destination Travel Time Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Travel time estimation from GPS trips is of great importance to order duration, ridesharing, taxi dispatching, etc. However, the dense trajectory is not always available due to the limitation of data privacy and acquisition, while the origin destination (OD) type of data, such as NYC taxi data, NYC bike data, and Capital Bikeshare data, is more accessible. To address this issue, this paper starts to estimate the OD trips travel time combined with the road network. Subsequently, a Multitask Weakly Supervised Learning Framework for Travel Time Estimation (MWSL TTE) has been proposed to infer transition probability between roads segments, and the travel time on road segments and intersection simultaneously. Technically, given an OD pair, the transition probability intends to recover the most possible route. And then, the output of travel time is equal to the summation of all segments' and intersections' travel time in this route. A novel route recovery function has been proposed to iteratively maximize the current route's co occurrence probability, and minimize the discrepancy between routes' probability distribution and the inverse distribution of routes' estimation loss. Moreover, the expected log likelihood function based on a weakly supervised framework has been deployed in optimizing the travel time from road segments and intersections concurrently. We conduct experiments on a wide range of real world taxi datasets in Xi'an and Chengdu and demonstrate our method's effectiveness on route recovery and travel time estimation.
<div id='section'>Paperid: <span id='pid'>700, <a href='https://arxiv.org/pdf/2301.03353.pdf' target='_blank'>https://arxiv.org/pdf/2301.03353.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ozan Ãzdemir, Matthias Kerzel, Cornelius Weber, Jae Hee Lee, Muhammad Burhan Hafez, Patrick Bruns, Stefan Wermter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.03353">Learning Bidirectional Action-Language Translation with Limited Supervision and Incongruent Input</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human infant learning happens during exploration of the environment, by interaction with objects, and by listening to and repeating utterances casually, which is analogous to unsupervised learning. Only occasionally, a learning infant would receive a matching verbal description of an action it is committing, which is similar to supervised learning. Such a learning mechanism can be mimicked with deep learning. We model this weakly supervised learning paradigm using our Paired Gated Autoencoders (PGAE) model, which combines an action and a language autoencoder. After observing a performance drop when reducing the proportion of supervised training, we introduce the Paired Transformed Autoencoders (PTAE) model, using Transformer-based crossmodal attention. PTAE achieves significantly higher accuracy in language-to-action and action-to-language translations, particularly in realistic but difficult cases when only few supervised training samples are available. We also test whether the trained model behaves realistically with conflicting multimodal input. In accordance with the concept of incongruence in psychology, conflict deteriorates the model output. Conflicting action input has a more severe impact than conflicting language input, and more conflicting features lead to larger interference. PTAE can be trained on mostly unlabelled data where labeled data is scarce, and it behaves plausibly when tested with incongruent input.
<div id='section'>Paperid: <span id='pid'>701, <a href='https://arxiv.org/pdf/2301.01060.pdf' target='_blank'>https://arxiv.org/pdf/2301.01060.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Feifei Shao, Yawei Luo, Fei Gao, Yi Yang, Jun Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.01060">Knowledge-guided Causal Intervention for Weakly-supervised Object Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Previous weakly-supervised object localization (WSOL) methods aim to expand activation map discriminative areas to cover the whole objects, yet neglect two inherent challenges when relying solely on image-level labels. First, the ``entangled context'' issue arises from object-context co-occurrence (\eg, fish and water), making the model inspection hard to distinguish object boundaries clearly. Second, the ``C-L dilemma'' issue results from the information decay caused by the pooling layers, which struggle to retain both the semantic information for precise classification and those essential details for accurate localization, leading to a trade-off in performance. In this paper, we propose a knowledge-guided causal intervention method, dubbed KG-CI-CAM, to address these two under-explored issues in one go. More specifically, we tackle the co-occurrence context confounder problem via causal intervention, which explores the causalities among image features, contexts, and categories to eliminate the biased object-context entanglement in the class activation maps. Based on the disentangled object feature, we introduce a multi-source knowledge guidance framework to strike a balance between absorbing classification knowledge and localization knowledge during model training. Extensive experiments conducted on several benchmark datasets demonstrate the effectiveness of KG-CI-CAM in learning distinct object boundaries amidst confounding contexts and mitigating the dilemma between classification and localization performance.
<div id='section'>Paperid: <span id='pid'>702, <a href='https://arxiv.org/pdf/2210.07586.pdf' target='_blank'>https://arxiv.org/pdf/2210.07586.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyunjae Kim, Jaehyo Yoo, Seunghyun Yoon, Jaewoo Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.07586">Automatic Creation of Named Entity Recognition Datasets by Querying Phrase Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most weakly supervised named entity recognition (NER) models rely on domain-specific dictionaries provided by experts. This approach is infeasible in many domains where dictionaries do not exist. While a phrase retrieval model was used to construct pseudo-dictionaries with entities retrieved from Wikipedia automatically in a recent study, these dictionaries often have limited coverage because the retriever is likely to retrieve popular entities rather than rare ones. In this study, we present a novel framework, HighGEN, that generates NER datasets with high-coverage pseudo-dictionaries. Specifically, we create entity-rich dictionaries with a novel search method, called phrase embedding search, which encourages the retriever to search a space densely populated with various entities. In addition, we use a new verification process based on the embedding distance between candidate entity mentions and entity types to reduce the false-positive noise in weak labels generated by high-coverage dictionaries. We demonstrate that HighGEN outperforms the previous best model by an average F1 score of 4.7 across five NER benchmark datasets.
<div id='section'>Paperid: <span id='pid'>703, <a href='https://arxiv.org/pdf/2209.14189.pdf' target='_blank'>https://arxiv.org/pdf/2209.14189.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meng Wei, Yong Zhou, Zhongnian Li, Xinzheng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.14189">Class-Imbalanced Complementary-Label Learning via Weighted Loss</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Complementary-label learning (CLL) is widely used in weakly supervised classification, but it faces a significant challenge in real-world datasets when confronted with class-imbalanced training samples. In such scenarios, the number of samples in one class is considerably lower than in other classes, which consequently leads to a decline in the accuracy of predictions. Unfortunately, existing CLL approaches have not investigate this problem. To alleviate this challenge, we propose a novel problem setting that enables learning from class-imbalanced complementary labels for multi-class classification. To tackle this problem, we propose a novel CLL approach called Weighted Complementary-Label Learning (WCLL). The proposed method models a weighted empirical risk minimization loss by utilizing the class-imbalanced complementary labels, which is also applicable to multi-class imbalanced training samples. Furthermore, we derive an estimation error bound to provide theoretical assurance. To evaluate our approach, we conduct extensive experiments on several widely-used benchmark datasets and a real-world dataset, and compare our method with existing state-of-the-art methods. The proposed approach shows significant improvement in these datasets, even in the case of multiple class-imbalanced scenarios. Notably, the proposed method not only utilizes complementary labels to train a classifier but also solves the problem of class imbalance.
<div id='section'>Paperid: <span id='pid'>704, <a href='https://arxiv.org/pdf/2203.12836.pdf' target='_blank'>https://arxiv.org/pdf/2203.12836.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryoma Kobayashi, Yusuke Mukuta, Tatsuya Harada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.12836">Learning from Label Proportions with Instance-wise Consistency</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning from Label Proportions (LLP) is a weakly supervised learning method that aims to perform instance classification from training data consisting of pairs of bags containing multiple instances and the class label proportions within the bags. Previous studies on multiclass LLP can be divided into two categories according to the learning task: per-instance label classification and per-bag label proportion estimation. However, these methods often results in high variance estimates of the risk when applied to complex models, or lack statistical learning theory arguments. To address this issue, we propose new learning methods based on statistical learning theory for both per-instance and per-bag policies. We demonstrate that the proposed methods are respectively risk-consistent and classifier-consistent in an instance-wise manner, and analyze the estimation error bounds. Additionally, we present a heuristic approximation method that utilizes an existing method for regressing label proportions to reduce the computational complexity of the proposed methods. Through benchmark experiments, we demonstrated the effectiveness of the proposed methods.
<div id='section'>Paperid: <span id='pid'>705, <a href='https://arxiv.org/pdf/2511.22823.pdf' target='_blank'>https://arxiv.org/pdf/2511.22823.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Miao Zhang, Junpeng Li, Changchun Hua, Yana Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.22823">A Unified and Stable Risk Minimization Framework for Weakly Supervised Learning with Theoretical Guarantees</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised learning has emerged as a practical alternative to fully supervised learning when complete and accurate labels are costly or infeasible to acquire. However, many existing methods are tailored to specific supervision patterns -- such as positive-unlabeled (PU), unlabeled-unlabeled (UU), complementary-label (CLL), partial-label (PLL), or similarity-unlabeled annotations -- and rely on post-hoc corrections to mitigate instability induced by indirect supervision. We propose a principled, unified framework that bypasses such post-hoc adjustments by directly formulating a stable surrogate risk grounded in the structure of weakly supervised data. The formulation naturally subsumes diverse settings -- including PU, UU, CLL, PLL, multi-class unlabeled, and tuple-based learning -- under a single optimization objective. We further establish a non-asymptotic generalization bound via Rademacher complexity that clarifies how supervision structure, model capacity, and sample size jointly govern performance. Beyond this, we analyze the effect of class-prior misspecification on the bound, deriving explicit terms that quantify its impact, and we study identifiability, giving sufficient conditions -- most notably via supervision stratification across groups -- under which the target risk is recoverable. Extensive experiments show consistent gains across class priors, dataset scales, and class counts -- without heuristic stabilization -- while exhibiting robustness to overfitting.
<div id='section'>Paperid: <span id='pid'>706, <a href='https://arxiv.org/pdf/2510.18406.pdf' target='_blank'>https://arxiv.org/pdf/2510.18406.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Miao Zhang, Junpeng Li, ChangChun HUa, Yana Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.18406">Learning from N-Tuple Data with M Positive Instances: Unbiased Risk Estimation and Theoretical Guarantees</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised learning often operates with coarse aggregate signals rather than instance labels. We study a setting where each training example is an $n$-tuple containing exactly m positives, while only the count m per tuple is observed. This NTMP (N-tuple with M positives) supervision arises in, e.g., image classification with region proposals and multi-instance measurements. We show that tuple counts admit a trainable unbiased risk estimator (URE) by linking the tuple-generation process to latent instance marginals. Starting from fixed (n,m), we derive a closed-form URE and extend it to variable tuple sizes, variable counts, and their combination. Identification holds whenever the effective mixing rate is separated from the class prior. We establish generalization bounds via Rademacher complexity and prove statistical consistency with standard rates under mild regularity assumptions. To improve finite-sample stability, we introduce simple ReLU corrections to the URE that preserve asymptotic correctness. Across benchmarks converted to NTMP tasks, the approach consistently outperforms representative weak-supervision baselines and yields favorable precision-recall and F1 trade-offs. It remains robust under class-prior imbalance and across diverse tuple configurations, demonstrating that count-only supervision can be exploited effectively through a theoretically grounded and practically stable objective.
<div id='section'>Paperid: <span id='pid'>707, <a href='https://arxiv.org/pdf/2509.14097.pdf' target='_blank'>https://arxiv.org/pdf/2509.14097.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaru Chen, Ruohao Guo, Liting Gao, Yang Xiang, Qingyu Luo, Zhenbo Li, Wenwu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14097">Teacher-Guided Pseudo Supervision and Cross-Modal Alignment for Audio-Visual Video Parsing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised audio-visual video parsing (AVVP) seeks to detect audible, visible, and audio-visual events without temporal annotations. Previous work has emphasized refining global predictions through contrastive or collaborative learning, but neglected stable segment-level supervision and class-aware cross-modal alignment. To address this, we propose two strategies: (1) an exponential moving average (EMA)-guided pseudo supervision framework that generates reliable segment-level masks via adaptive thresholds or top-k selection, offering stable temporal guidance beyond video-level labels; and (2) a class-aware cross-modal agreement (CMA) loss that aligns audio and visual embeddings at reliable segment-class pairs, ensuring consistency across modalities while preserving temporal structure. Evaluations on LLP and UnAV-100 datasets shows that our method achieves state-of-the-art (SOTA) performance across multiple metrics.
<div id='section'>Paperid: <span id='pid'>708, <a href='https://arxiv.org/pdf/2509.04086.pdf' target='_blank'>https://arxiv.org/pdf/2509.04086.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaru Chen, Faegheh Sardari, Peiliang Zhang, Ruohao Guo, Yang Xiang, Zhenbo Li, Wenwu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04086">TEn-CATS: Text-Enriched Audio-Visual Video Parsing with Multi-Scale Category-Aware Temporal Graph</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-Visual Video Parsing (AVVP) task aims to identify event categories and their occurrence times in a given video with weakly supervised labels. Existing methods typically fall into two categories: (i) designing enhanced architectures based on attention mechanism for better temporal modeling, and (ii) generating richer pseudo-labels to compensate for the absence of frame-level annotations. However, the first type methods treat noisy segment-level pseudo labels as reliable supervision and the second type methods let indiscriminate attention spread them across all frames, the initial errors are repeatedly amplified during training. To address this issue, we propose a method that combines the Bi-Directional Text Fusion (BiT) module and Category-Aware Temporal Graph (CATS) module. Specifically, we integrate the strengths and complementarity of the two previous research directions. We first perform semantic injection and dynamic calibration on audio and visual modality features through the BiT module, to locate and purify cleaner and richer semantic cues. Then, we leverage the CATS module for semantic propagation and connection to enable precise semantic information dissemination across time. Experimental results demonstrate that our proposed method achieves state-of-the-art (SOTA) performance in multiple key indicators on two benchmark datasets, LLP and UnAV-100.
<div id='section'>Paperid: <span id='pid'>709, <a href='https://arxiv.org/pdf/2508.05585.pdf' target='_blank'>https://arxiv.org/pdf/2508.05585.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haijing Liu, Tao Pu, Hefeng Wu, Keze Wang, Liang Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05585">DART: Dual Adaptive Refinement Transfer for Open-Vocabulary Multi-Label Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-Vocabulary Multi-Label Recognition (OV-MLR) aims to identify multiple seen and unseen object categories within an image, requiring both precise intra-class localization to pinpoint objects and effective inter-class reasoning to model complex category dependencies. While Vision-Language Pre-training (VLP) models offer a strong open-vocabulary foundation, they often struggle with fine-grained localization under weak supervision and typically fail to explicitly leverage structured relational knowledge beyond basic semantics, limiting performance especially for unseen classes. To overcome these limitations, we propose the Dual Adaptive Refinement Transfer (DART) framework. DART enhances a frozen VLP backbone via two synergistic adaptive modules. For intra-class refinement, an Adaptive Refinement Module (ARM) refines patch features adaptively, coupled with a novel Weakly Supervised Patch Selecting (WPS) loss that enables discriminative localization using only image-level labels. Concurrently, for inter-class transfer, an Adaptive Transfer Module (ATM) leverages a Class Relationship Graph (CRG), constructed using structured knowledge mined from a Large Language Model (LLM), and employs graph attention network to adaptively transfer relational information between class representations. DART is the first framework, to our knowledge, to explicitly integrate external LLM-derived relational knowledge for adaptive inter-class transfer while simultaneously performing adaptive intra-class refinement under weak supervision for OV-MLR. Extensive experiments on challenging benchmarks demonstrate that our DART achieves new state-of-the-art performance, validating its effectiveness.
<div id='section'>Paperid: <span id='pid'>710, <a href='https://arxiv.org/pdf/2507.07771.pdf' target='_blank'>https://arxiv.org/pdf/2507.07771.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuying Huang, Junpeng Li, Changchun Hua, Yana Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07771">A Unified Empirical Risk Minimization Framework for Flexible N-Tuples Weak Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To alleviate the annotation burden in supervised learning, N-tuples learning has recently emerged as a powerful weakly-supervised method. While existing N-tuples learning approaches extend pairwise learning to higher-order comparisons and accommodate various real-world scenarios, they often rely on task-specific designs and lack a unified theoretical foundation. In this paper, we propose a general N-tuples learning framework based on empirical risk minimization, which systematically integrates pointwise unlabeled data to enhance learning performance. This paper first unifies the data generation processes of N-tuples and pointwise unlabeled data under a shared probabilistic formulation. Based on this unified view, we derive an unbiased empirical risk estimator that generalizes a broad class of existing N-tuples models. We further establish a generalization error bound for theoretical support. To demonstrate the flexibility of the framework, we instantiate it in four representative weakly supervised scenarios, each recoverable as a special case of our general model. Additionally, to address overfitting issues arising from negative risk terms, we adopt correction functions to adjust the empirical risk. Extensive experiments on benchmark datasets validate the effectiveness of the proposed framework and demonstrate that leveraging pointwise unlabeled data consistently improves generalization across various N-tuples learning tasks.
<div id='section'>Paperid: <span id='pid'>711, <a href='https://arxiv.org/pdf/2506.22301.pdf' target='_blank'>https://arxiv.org/pdf/2506.22301.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Takumi Okuo, Shinnosuke Matsuo, Shota Harada, Kiyohito Tanaka, Ryoma Bise
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22301">Weakly-Supervised Domain Adaptation with Proportion-Constrained Pseudo-Labeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain shift is a significant challenge in machine learning, particularly in medical applications where data distributions differ across institutions due to variations in data collection practices, equipment, and procedures. This can degrade performance when models trained on source domain data are applied to the target domain. Domain adaptation methods have been widely studied to address this issue, but most struggle when class proportions between the source and target domains differ. In this paper, we propose a weakly-supervised domain adaptation method that leverages class proportion information from the target domain, which is often accessible in medical datasets through prior knowledge or statistical reports. Our method assigns pseudo-labels to the unlabeled target data based on class proportion (called proportion-constrained pseudo-labeling), improving performance without the need for additional annotations. Experiments on two endoscopic datasets demonstrate that our method outperforms semi-supervised domain adaptation techniques, even when 5% of the target domain is labeled. Additionally, the experimental results with noisy proportion labels highlight the robustness of our method, further demonstrating its effectiveness in real-world application scenarios.
<div id='section'>Paperid: <span id='pid'>712, <a href='https://arxiv.org/pdf/2505.22490.pdf' target='_blank'>https://arxiv.org/pdf/2505.22490.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ke Zhang, Tianyu Ding, Jiachen Jiang, Tianyi Chen, Ilya Zharkov, Vishal M. Patel, Luming Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22490">ProCrop: Learning Aesthetic Image Cropping from Professional Compositions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image cropping is crucial for enhancing the visual appeal and narrative impact of photographs, yet existing rule-based and data-driven approaches often lack diversity or require annotated training data. We introduce ProCrop, a retrieval-based method that leverages professional photography to guide cropping decisions. By fusing features from professional photographs with those of the query image, ProCrop learns from professional compositions, significantly boosting performance. Additionally, we present a large-scale dataset of 242K weakly-annotated images, generated by out-painting professional images and iteratively refining diverse crop proposals. This composition-aware dataset generation offers diverse high-quality crop proposals guided by aesthetic principles and becomes the largest publicly available dataset for image cropping. Extensive experiments show that ProCrop significantly outperforms existing methods in both supervised and weakly-supervised settings. Notably, when trained on the new dataset, our ProCrop surpasses previous weakly-supervised methods and even matches fully supervised approaches. Both the code and dataset will be made publicly available to advance research in image aesthetics and composition analysis.
<div id='section'>Paperid: <span id='pid'>713, <a href='https://arxiv.org/pdf/2411.18225.pdf' target='_blank'>https://arxiv.org/pdf/2411.18225.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zak Buzzard, Konstantin Hemker, Nikola Simidjievski, Mateja Jamnik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18225">PATHS: A Hierarchical Transformer for Efficient Whole Slide Image Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computational analysis of whole slide images (WSIs) has seen significant research progress in recent years, with applications ranging across important diagnostic and prognostic tasks such as survival or cancer subtype prediction. Many state-of-the-art models process the entire slide - which may be as large as $150,000 \times 150,000$ pixels - as a bag of many patches, the size of which necessitates computationally cheap feature aggregation methods. However, a large proportion of these patches are uninformative, such as those containing only healthy or adipose tissue, adding significant noise and size to the bag. We propose Pathology Transformer with Hierarchical Selection (PATHS), a novel top-down method for hierarchical weakly supervised representation learning on slide-level tasks in computational pathology. PATHS is inspired by the cross-magnification manner in which a human pathologist examines a slide, recursively filtering patches at each magnification level to a small subset relevant to the diagnosis. Our method overcomes the complications of processing the entire slide, enabling quadratic self-attention and providing a simple interpretable measure of region importance. We apply PATHS to five datasets of The Cancer Genome Atlas (TCGA), and achieve superior performance on slide-level prediction tasks when compared to previous methods, despite processing only a small proportion of the slide.
<div id='section'>Paperid: <span id='pid'>714, <a href='https://arxiv.org/pdf/2410.01544.pdf' target='_blank'>https://arxiv.org/pdf/2410.01544.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zaiquan Yang, Yuhao Liu, Jiaying Lin, Gerhard Hancke, Rynson W. H. Lau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01544">Boosting Weakly-Supervised Referring Image Segmentation via Progressive Comprehension</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper explores the weakly-supervised referring image segmentation (WRIS) problem, and focuses on a challenging setup where target localization is learned directly from image-text pairs. We note that the input text description typically already contains detailed information on how to localize the target object, and we also observe that humans often follow a step-by-step comprehension process (\ie, progressively utilizing target-related attributes and relations as cues) to identify the target object. Hence, we propose a novel Progressive Comprehension Network (PCNet) to leverage target-related textual cues from the input description for progressively localizing the target object. Specifically, we first use a Large Language Model (LLM) to decompose the input text description into short phrases. These short phrases are taken as target-related cues and fed into a Conditional Referring Module (CRM) in multiple stages, to allow updating the referring text embedding and enhance the response map for target localization in a multi-stage manner. Based on the CRM, we then propose a Region-aware Shrinking (RaS) loss to constrain the visual localization to be conducted progressively in a coarse-to-fine manner across different stages. Finally, we introduce an Instance-aware Disambiguation (IaD) loss to suppress instance localization ambiguity by differentiating overlapping response maps generated by different referring texts on the same image. Extensive experiments show that our method outperforms SOTA methods on three common benchmarks.
<div id='section'>Paperid: <span id='pid'>715, <a href='https://arxiv.org/pdf/2409.01030.pdf' target='_blank'>https://arxiv.org/pdf/2409.01030.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahe Tian, Peng Chen, Cai Yu, Xiaomeng Fu, Xi Wang, Jiao Dai, Jizhong Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01030">Learning to Discover Forgery Cues for Face Forgery Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Locating manipulation maps, i.e., pixel-level annotation of forgery cues, is crucial for providing interpretable detection results in face forgery detection. Related learning objects have also been widely adopted as auxiliary tasks to improve the classification performance of detectors whereas they require comparisons between paired real and forged faces to obtain manipulation maps as supervision. This requirement restricts their applicability to unpaired faces and contradicts real-world scenarios. Moreover, the used comparison methods annotate all changed pixels, including noise introduced by compression and upsampling. Using such maps as supervision hinders the learning of exploitable cues and makes models prone to overfitting. To address these issues, we introduce a weakly supervised model in this paper, named Forgery Cue Discovery (FoCus), to locate forgery cues in unpaired faces. Unlike some detectors that claim to locate forged regions in attention maps, FoCus is designed to sidestep their shortcomings of capturing partial and inaccurate forgery cues. Specifically, we propose a classification attentive regions proposal module to locate forgery cues during classification and a complementary learning module to facilitate the learning of richer cues. The produced manipulation maps can serve as better supervision to enhance face forgery detectors. Visualization of the manipulation maps of the proposed FoCus exhibits superior interpretability and robustness compared to existing methods. Experiments on five datasets and four multi-task models demonstrate the effectiveness of FoCus in both in-dataset and cross-dataset evaluations.
<div id='section'>Paperid: <span id='pid'>716, <a href='https://arxiv.org/pdf/2408.09613.pdf' target='_blank'>https://arxiv.org/pdf/2408.09613.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Herun Wan, Minnan Luo, Zihan Ma, Guang Dai, Xiang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09613">How Do Social Bots Participate in Misinformation Spread? A Comprehensive Dataset and Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Social media platforms provide an ideal environment to spread misinformation, where social bots can accelerate the spread. This paper explores the interplay between social bots and misinformation on the Sina Weibo platform. We construct a large-scale dataset that includes annotations for both misinformation and social bots. From the misinformation perspective, the dataset is multimodal, containing 11,393 pieces of misinformation and 16,416 pieces of verified information. From the social bot perspective, this dataset contains 65,749 social bots and 345,886 genuine accounts, annotated using a weakly supervised annotator. Extensive experiments demonstrate the comprehensiveness of the dataset, the clear distinction between misinformation and real information, and the high quality of social bot annotations. Further analysis illustrates that: (i) social bots are deeply involved in information spread; (ii) misinformation with the same topics has similar content, providing the basis of echo chambers, and social bots would amplify this phenomenon; and (iii) social bots generate similar content aiming to manipulate public opinions.
<div id='section'>Paperid: <span id='pid'>717, <a href='https://arxiv.org/pdf/2407.05180.pdf' target='_blank'>https://arxiv.org/pdf/2407.05180.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julien Quarez, Marc Modat, Sebastien Ourselin, Jonathan Shapey, Alejandro Granados
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05180">ReCAP: Recursive Cross Attention Network for Pseudo-Label Generation in Robotic Surgical Skill Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In surgical skill assessment, the Objective Structured Assessments of Technical Skills (OSATS) and Global Rating Scale (GRS) are well-established tools for evaluating surgeons during training. These metrics, along with performance feedback, help surgeons improve and reach practice standards. Recent research on the open-source JIGSAWS dataset, which includes both GRS and OSATS labels, has focused on regressing GRS scores from kinematic data, video, or their combination. However, we argue that regressing GRS alone is limiting, as it aggregates OSATS scores and overlooks clinically meaningful variations during a surgical trial. To address this, we developed a weakly-supervised recurrent transformer model that tracks a surgeon's performance throughout a session by mapping hidden states to six OSATS, derived from kinematic data. These OSATS scores are averaged to predict GRS, allowing us to compare our model's performance against state-of-the-art (SOTA) methods. We report Spearman's Correlation Coefficients (SCC) demonstrating that our model outperforms SOTA using kinematic data (SCC 0.83-0.88), and matches performance with video-based models. Our model also surpasses SOTA in most tasks for average OSATS predictions (SCC 0.46-0.70) and specific OSATS (SCC 0.56-0.95). The generation of pseudo-labels at the segment level translates quantitative predictions into qualitative feedback, vital for automated surgical skill assessment pipelines. A senior surgeon validated our model's outputs, agreeing with 77\% of the weakly-supervised predictions \(p=0.006\).
<div id='section'>Paperid: <span id='pid'>718, <a href='https://arxiv.org/pdf/2407.03331.pdf' target='_blank'>https://arxiv.org/pdf/2407.03331.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunzhe Li, Hongzi Zhu, Zhuohong Deng, Yunlong Cheng, Liang Zhang, Shan Chang, Minyi Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.03331">Anole: Adapting Diverse Compressed Models For Cross-Scene Prediction On Mobile Devices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Emerging Artificial Intelligence of Things (AIoT) applications desire online prediction using deep neural network (DNN) models on mobile devices. However, due to the movement of devices, unfamiliar test samples constantly appear, significantly affecting the prediction accuracy of a pre-trained DNN. In addition, unstable network connection calls for local model inference. In this paper, we propose a light-weight scheme, called Anole, to cope with the local DNN model inference on mobile devices. The core idea of Anole is to first establish an army of compact DNN models, and then adaptively select the model fitting the current test sample best for online inference. The key is to automatically identify model-friendly scenes for training scene-specific DNN models. To this end, we design a weakly-supervised scene representation learning algorithm by combining both human heuristics and feature similarity in separating scenes. Moreover, we further train a model classifier to predict the best-fit scene-specific DNN model for each test sample. We implement Anole on different types of mobile devices and conduct extensive trace-driven and real-world experiments based on unmanned aerial vehicles (UAVs). The results demonstrate that Anole outwits the method of using a versatile large DNN in terms of prediction accuracy (4.5% higher), response time (33.1% faster) and power consumption (45.1% lower).
<div id='section'>Paperid: <span id='pid'>719, <a href='https://arxiv.org/pdf/2406.14365.pdf' target='_blank'>https://arxiv.org/pdf/2406.14365.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stefan M. Fischer, Johannes Kiechle, Daniel M. Lang, Jan C. Peeken, Julia A. Schnabel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.14365">Mask the Unknown: Assessing Different Strategies to Handle Weak Annotations in the MICCAI2023 Mediastinal Lymph Node Quantification Challenge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pathological lymph node delineation is crucial in cancer diagnosis, progression assessment, and treatment planning. The MICCAI 2023 Lymph Node Quantification Challenge published the first public dataset for pathological lymph node segmentation in the mediastinum. As lymph node annotations are expensive, the challenge was formed as a weakly supervised learning task, where only a subset of all lymph nodes in the training set have been annotated. For the challenge submission, multiple methods for training on these weakly supervised data were explored, including noisy label training, loss masking of unlabeled data, and an approach that integrated the TotalSegmentator toolbox as a form of pseudo labeling in order to reduce the number of unknown voxels. Furthermore, multiple public TCIA datasets were incorporated into the training to improve the performance of the deep learning model. Our submitted model achieved a Dice score of 0.628 and an average symmetric surface distance of 5.8~mm on the challenge test set. With our submitted model, we accomplished third rank in the MICCAI2023 LNQ challenge. A finding of our analysis was that the integration of all visible, including non-pathological, lymph nodes improved the overall segmentation performance on pathological lymph nodes of the test set. Furthermore, segmentation models trained only on clinically enlarged lymph nodes, as given in the challenge scenario, could not generalize to smaller pathological lymph nodes. The code and model for the challenge submission are available at \url{https://gitlab.lrz.de/compai/MediastinalLymphNodeSegmentation}.
<div id='section'>Paperid: <span id='pid'>720, <a href='https://arxiv.org/pdf/2405.09041.pdf' target='_blank'>https://arxiv.org/pdf/2405.09041.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shinnosuke Matsuo, Daiki Suehiro, Seiichi Uchida, Hiroaki Ito, Kazuhiro Terada, Akihiko Yoshizawa, Ryoma Bise
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.09041">Learning from Partial Label Proportions for Whole Slide Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we address the segmentation of tumor subtypes in whole slide images (WSI) by utilizing incomplete label proportions. Specifically, we utilize `partial' label proportions, which give the proportions among tumor subtypes but do not give the proportion between tumor and non-tumor. Partial label proportions are recorded as the standard diagnostic information by pathologists, and we, therefore, want to use them for realizing the segmentation model that can classify each WSI patch into one of the tumor subtypes or non-tumor. We call this problem ``learning from partial label proportions (LPLP)'' and formulate the problem as a weakly supervised learning problem. Then, we propose an efficient algorithm for this challenging problem by decomposing it into two weakly supervised learning subproblems: multiple instance learning (MIL) and learning from label proportions (LLP). These subproblems are optimized efficiently in the end-to-end manner. The effectiveness of our algorithm is demonstrated through experiments conducted on two WSI datasets.
<div id='section'>Paperid: <span id='pid'>721, <a href='https://arxiv.org/pdf/2312.11024.pdf' target='_blank'>https://arxiv.org/pdf/2312.11024.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyao He, Huabin Liu, Yuxi Li, Xiao Ma, Cheng Zhong, Yang Zhang, Weiyao Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.11024">Collaborative Weakly Supervised Video Correlation Learning for Procedure-Aware Instructional Video Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Correlation Learning (VCL), which aims to analyze the relationships between videos, has been widely studied and applied in various general video tasks. However, applying VCL to instructional videos is still quite challenging due to their intrinsic procedural temporal structure. Specifically, procedural knowledge is critical for accurate correlation analyses on instructional videos. Nevertheless, current procedure-learning methods heavily rely on step-level annotations, which are costly and not scalable. To address this problem, we introduce a weakly supervised framework called Collaborative Procedure Alignment (CPA) for procedure-aware correlation learning on instructional videos. Our framework comprises two core modules: collaborative step mining and frame-to-step alignment. The collaborative step mining module enables simultaneous and consistent step segmentation for paired videos, leveraging the semantic and temporal similarity between frames. Based on the identified steps, the frame-to-step alignment module performs alignment between the frames and steps across videos. The alignment result serves as a measurement of the correlation distance between two videos. We instantiate our framework in two distinct instructional video tasks: sequence verification and action quality assessment. Extensive experiments validate the effectiveness of our approach in providing accurate and interpretable correlation analyses for instructional videos.
<div id='section'>Paperid: <span id='pid'>722, <a href='https://arxiv.org/pdf/2309.17399.pdf' target='_blank'>https://arxiv.org/pdf/2309.17399.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiancheng Huang, Donghao Zhou, Shifeng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.17399">IFAST: Weakly Supervised Interpretable Face Anti-spoofing from Single-shot Binocular NIR Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single-shot face anti-spoofing (FAS) is a key technique for securing face recognition systems, and it requires only static images as input. However, single-shot FAS remains a challenging and under-explored problem due to two main reasons: 1) on the data side, learning FAS from RGB images is largely context-dependent, and single-shot images without additional annotations contain limited semantic information. 2) on the model side, existing single-shot FAS models are infeasible to provide proper evidence for their decisions, and FAS methods based on depth estimation require expensive per-pixel annotations. To address these issues, a large binocular NIR image dataset (BNI-FAS) is constructed and published, which contains more than 300,000 real face and plane attack images, and an Interpretable FAS Transformer (IFAST) is proposed that requires only weak supervision to produce interpretable predictions. Our IFAST can produce pixel-wise disparity maps by the proposed disparity estimation Transformer with Dynamic Matching Attention (DMA) block. Besides, a well-designed confidence map generator is adopted to cooperate with the proposed dual-teacher distillation module to obtain the final discriminant results. The comprehensive experiments show that our IFAST can achieve state-of-the-art results on BNI-FAS, proving the effectiveness of the single-shot FAS based on binocular NIR images.
<div id='section'>Paperid: <span id='pid'>723, <a href='https://arxiv.org/pdf/2309.06895.pdf' target='_blank'>https://arxiv.org/pdf/2309.06895.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junha Hyung, Jaeyo Shin, Jaegul Choo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.06895">MagiCapture: High-Resolution Multi-Concept Portrait Customization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale text-to-image models including Stable Diffusion are capable of generating high-fidelity photorealistic portrait images. There is an active research area dedicated to personalizing these models, aiming to synthesize specific subjects or styles using provided sets of reference images. However, despite the plausible results from these personalization methods, they tend to produce images that often fall short of realism and are not yet on a commercially viable level. This is particularly noticeable in portrait image generation, where any unnatural artifact in human faces is easily discernible due to our inherent human bias. To address this, we introduce MagiCapture, a personalization method for integrating subject and style concepts to generate high-resolution portrait images using just a few subject and style references. For instance, given a handful of random selfies, our fine-tuned model can generate high-quality portrait images in specific styles, such as passport or profile photos. The main challenge with this task is the absence of ground truth for the composed concepts, leading to a reduction in the quality of the final output and an identity shift of the source subject. To address these issues, we present a novel Attention Refocusing loss coupled with auxiliary priors, both of which facilitate robust learning within this weakly supervised learning setting. Our pipeline also includes additional post-processing steps to ensure the creation of highly realistic outputs. MagiCapture outperforms other baselines in both quantitative and qualitative evaluations and can also be generalized to other non-human objects.
<div id='section'>Paperid: <span id='pid'>724, <a href='https://arxiv.org/pdf/2308.08822.pdf' target='_blank'>https://arxiv.org/pdf/2308.08822.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Takanori Asanomi, Shinnosuke Matsuo, Daiki Suehiro, Ryoma Bise
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.08822">MixBag: Bag-Level Data Augmentation for Learning from Label Proportions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning from label proportions (LLP) is a promising weakly supervised learning problem. In LLP, a set of instances (bag) has label proportions, but no instance-level labels are given. LLP aims to train an instance-level classifier by using the label proportions of the bag. In this paper, we propose a bag-level data augmentation method for LLP called MixBag, based on the key observation from our preliminary experiments; that the instance-level classification accuracy improves as the number of labeled bags increases even though the total number of instances is fixed. We also propose a confidence interval loss designed based on statistical theory to use the augmented bags effectively. To the best of our knowledge, this is the first attempt to propose bag-level data augmentation for LLP. The advantage of MixBag is that it can be applied to instance-level data augmentation techniques and any LLP method that uses the proportion loss. Experimental results demonstrate this advantage and the effectiveness of our method.
<div id='section'>Paperid: <span id='pid'>725, <a href='https://arxiv.org/pdf/2307.14907.pdf' target='_blank'>https://arxiv.org/pdf/2307.14907.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrew H. Song, Mane Williams, Drew F. K. Williamson, Guillaume Jaume, Andrew Zhang, Bowen Chen, Robert Serafin, Jonathan T. C. Liu, Alex Baras, Anil V. Parwani, Faisal Mahmood
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.14907">Weakly Supervised AI for Efficient Analysis of 3D Pathology Samples</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human tissue and its constituent cells form a microenvironment that is fundamentally three-dimensional (3D). However, the standard-of-care in pathologic diagnosis involves selecting a few two-dimensional (2D) sections for microscopic evaluation, risking sampling bias and misdiagnosis. Diverse methods for capturing 3D tissue morphologies have been developed, but they have yet had little translation to clinical practice; manual and computational evaluations of such large 3D data have so far been impractical and/or unable to provide patient-level clinical insights. Here we present Modality-Agnostic Multiple instance learning for volumetric Block Analysis (MAMBA), a deep-learning-based platform for processing 3D tissue images from diverse imaging modalities and predicting patient outcomes. Archived prostate cancer specimens were imaged with open-top light-sheet microscopy or microcomputed tomography and the resulting 3D datasets were used to train risk-stratification networks based on 5-year biochemical recurrence outcomes via MAMBA. With the 3D block-based approach, MAMBA achieves an area under the receiver operating characteristic curve (AUC) of 0.86 and 0.74, superior to 2D traditional single-slice-based prognostication (AUC of 0.79 and 0.57), suggesting superior prognostication with 3D morphological features. Further analyses reveal that the incorporation of greater tissue volume improves prognostic performance and mitigates risk prediction variability from sampling bias, suggesting the value of capturing larger extents of heterogeneous 3D morphology. With the rapid growth and adoption of 3D spatial biology and pathology techniques by researchers and clinicians, MAMBA provides a general and efficient framework for 3D weakly supervised learning for clinical decision support and can help to reveal novel 3D morphological biomarkers for prognosis and therapeutic response.
<div id='section'>Paperid: <span id='pid'>726, <a href='https://arxiv.org/pdf/2305.16598.pdf' target='_blank'>https://arxiv.org/pdf/2305.16598.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Farhad Moghimifar, Shilin Qu, Tongtong Wu, Yuan-Fang Li, Gholamreza Haffari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.16598">NormMark: A Weakly Supervised Markov Model for Socio-cultural Norm Discovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Norms, which are culturally accepted guidelines for behaviours, can be integrated into conversational models to generate utterances that are appropriate for the socio-cultural context. Existing methods for norm recognition tend to focus only on surface-level features of dialogues and do not take into account the interactions within a conversation. To address this issue, we propose NormMark, a probabilistic generative Markov model to carry the latent features throughout a dialogue. These features are captured by discrete and continuous latent variables conditioned on the conversation history, and improve the model's ability in norm recognition. The model is trainable on weakly annotated data using the variational technique. On a dataset with limited norm annotations, we show that our approach achieves higher F1 score, outperforming current state-of-the-art methods, including GPT3.
<div id='section'>Paperid: <span id='pid'>727, <a href='https://arxiv.org/pdf/2305.11003.pdf' target='_blank'>https://arxiv.org/pdf/2305.11003.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunming He, Kai Li, Yachao Zhang, Guoxia Xu, Longxiang Tang, Yulun Zhang, Zhenhua Guo, Xiu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.11003">Weakly-Supervised Concealed Object Segmentation with SAM-based Pseudo Labeling and Multi-scale Feature Grouping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-Supervised Concealed Object Segmentation (WSCOS) aims to segment objects well blended with surrounding environments using sparsely-annotated data for model training. It remains a challenging task since (1) it is hard to distinguish concealed objects from the background due to the intrinsic similarity and (2) the sparsely-annotated training data only provide weak supervision for model learning. In this paper, we propose a new WSCOS method to address these two challenges. To tackle the intrinsic similarity challenge, we design a multi-scale feature grouping module that first groups features at different granularities and then aggregates these grouping results. By grouping similar features together, it encourages segmentation coherence, helping obtain complete segmentation results for both single and multiple-object images. For the weak supervision challenge, we utilize the recently-proposed vision foundation model, Segment Anything Model (SAM), and use the provided sparse annotations as prompts to generate segmentation masks, which are used to train the model. To alleviate the impact of low-quality segmentation masks, we further propose a series of strategies, including multi-augmentation result ensemble, entropy-based pixel-level weighting, and entropy-based image-level selection. These strategies help provide more reliable supervision to train the segmentation model. We verify the effectiveness of our method on various WSCOS tasks, and experiments demonstrate that our method achieves state-of-the-art performance on these tasks.
<div id='section'>Paperid: <span id='pid'>728, <a href='https://arxiv.org/pdf/2303.18118.pdf' target='_blank'>https://arxiv.org/pdf/2303.18118.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Camille Garcin, Maximilien Servajean, Alexis Joly, Joseph Salmon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.18118">A two-head loss function for deep Average-K classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Average-K classification is an alternative to top-K classification in which the number of labels returned varies with the ambiguity of the input image but must average to K over all the samples. A simple method to solve this task is to threshold the softmax output of a model trained with the cross-entropy loss. This approach is theoretically proven to be asymptotically consistent, but it is not guaranteed to be optimal for a finite set of samples. In this paper, we propose a new loss function based on a multi-label classification head in addition to the classical softmax. This second head is trained using pseudo-labels generated by thresholding the softmax head while guaranteeing that K classes are returned on average. We show that this approach allows the model to better capture ambiguities between classes and, as a result, to return more consistent sets of possible classes. Experiments on two datasets from the literature demonstrate that our approach outperforms the softmax baseline, as well as several other loss functions more generally designed for weakly supervised multi-label classification. The gains are larger the higher the uncertainty, especially for classes with few samples.
<div id='section'>Paperid: <span id='pid'>729, <a href='https://arxiv.org/pdf/2303.10384.pdf' target='_blank'>https://arxiv.org/pdf/2303.10384.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aleksandr Laptev, Vladimir Bataev, Igor Gitman, Boris Ginsburg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.10384">Powerful and Extensible WFST Framework for RNN-Transducer Losses</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a framework based on Weighted Finite-State Transducers (WFST) to simplify the development of modifications for RNN-Transducer (RNN-T) loss. Existing implementations of RNN-T use CUDA-related code, which is hard to extend and debug. WFSTs are easy to construct and extend, and allow debugging through visualization. We introduce two WFST-powered RNN-T implementations: (1) "Compose-Transducer", based on a composition of the WFST graphs from acoustic and textual schema -- computationally competitive and easy to modify; (2) "Grid-Transducer", which constructs the lattice directly for further computations -- most compact, and computationally efficient. We illustrate the ease of extensibility through introduction of a new W-Transducer loss -- the adaptation of the Connectionist Temporal Classification with Wild Cards. W-Transducer (W-RNNT) consistently outperforms the standard RNN-T in a weakly-supervised data setup with missing parts of transcriptions at the beginning and end of utterances. All RNN-T losses are implemented with the k2 framework and are available in the NeMo toolkit.
<div id='section'>Paperid: <span id='pid'>730, <a href='https://arxiv.org/pdf/2302.09850.pdf' target='_blank'>https://arxiv.org/pdf/2302.09850.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Ju, Haicheng Wang, Jinxiang Liu, Chaofan Ma, Ya Zhang, Peisen Zhao, Jianlong Chang, Qi Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.09850">Constraint and Union for Partially-Supervised Temporal Sentence Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Temporal sentence grounding aims to detect the event timestamps described by the natural language query from given untrimmed videos. The existing fully-supervised setting achieves great performance but requires expensive annotation costs; while the weakly-supervised setting adopts cheap labels but performs poorly. To pursue high performance with less annotation cost, this paper introduces an intermediate partially-supervised setting, i.e., only short-clip or even single-frame labels are available during training. To take full advantage of partial labels, we propose a novel quadruple constraint pipeline to comprehensively shape event-query aligned representations, covering intra- and inter-samples, uni- and multi-modalities. The former raises intra-cluster compactness and inter-cluster separability; while the latter enables event-background separation and event-query gather. To achieve more powerful performance with explicit grounding optimization, we further introduce a partial-full union framework, i.e., bridging with an additional fully-supervised branch, to enjoy its impressive grounding bonus, and be robust to partial annotations. Extensive experiments and ablations on Charades-STA and ActivityNet Captions demonstrate the significance of partial supervision and our superior performance.
<div id='section'>Paperid: <span id='pid'>731, <a href='https://arxiv.org/pdf/2301.07700.pdf' target='_blank'>https://arxiv.org/pdf/2301.07700.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyu Su, Mostafa Rezapour, Usama Sajjad, Metin Nafi Gurcan, Muhammad Khalid Khan Niazi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.07700">Attention2Minority: A salient instance inference-based multiple instance learning for classifying small lesions in whole slide images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple instance learning (MIL) models have achieved remarkable success in analyzing whole slide images (WSIs) for disease classification problems. However, with regard to gigapixel WSI classification problems, current MIL models are often incapable of differentiating a WSI with extremely small tumor lesions. This minute tumor-to-normal area ratio in a MIL bag inhibits the attention mechanism from properly weighting the areas corresponding to minor tumor lesions. To overcome this challenge, we propose salient instance inference MIL (SiiMIL), a weakly-supervised MIL model for WSI classification. Our method initially learns representations of normal WSIs, and it then compares the normal WSIs representations with all the input patches to infer the salient instances of the input WSI. Finally, it employs attention-based MIL to perform the slide-level classification based on the selected patches of the WSI. Our experiments imply that SiiMIL can accurately identify tumor instances, which could only take up less than 1% of a WSI, so that the ratio of tumor to normal instances within a bag can increase by two to four times. It is worth mentioning that it performs equally well for large tumor lesions. As a result, SiiMIL achieves a significant improvement in performance over the state-of-the-art MIL methods.
<div id='section'>Paperid: <span id='pid'>732, <a href='https://arxiv.org/pdf/2202.10705.pdf' target='_blank'>https://arxiv.org/pdf/2202.10705.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yushuang Wu, Zizheng Yan, Shengcai Cai, Guanbin Li, Yizhou Yu, Xiaoguang Han, Shuguang Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.10705">PointMatch: A Consistency Training Framework for Weakly Supervised Semantic Segmentation of 3D Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semantic segmentation of point cloud usually relies on dense annotation that is exhausting and costly, so it attracts wide attention to investigate solutions for the weakly supervised scheme with only sparse points annotated. Existing works start from the given labels and propagate them to highly-related but unlabeled points, with the guidance of data, e.g. intra-point relation. However, it suffers from (i) the inefficient exploitation of data information, and (ii) the strong reliance on labels thus is easily suppressed when given much fewer annotations. Therefore, we propose a novel framework, PointMatch, that stands on both data and label, by applying consistency regularization to sufficiently probe information from data itself and leveraging weak labels as assistance at the same time. By doing so, meaningful information can be learned from both data and label for better representation learning, which also enables the model more robust to the extent of label sparsity. Simple yet effective, the proposed PointMatch achieves the state-of-the-art performance under various weakly-supervised schemes on both ScanNet-v2 and S3DIS datasets, especially on the settings with extremely sparse labels, e.g. surpassing SQN by 21.2% and 17.2% on the 0.01% and 0.1% setting of ScanNet-v2, respectively.
<div id='section'>Paperid: <span id='pid'>733, <a href='https://arxiv.org/pdf/2104.10127.pdf' target='_blank'>https://arxiv.org/pdf/2104.10127.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxin Mao, Jing Zhang, Zhexiong Wan, Yuchao Dai, Aixuan Li, Yunqiu Lv, Xinyu Tian, Deng-Ping Fan, Nick Barnes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2104.10127">Generative Transformer for Accurate and Reliable Salient Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transformer, which originates from machine translation, is particularly powerful at modeling long-range dependencies. Currently, the transformer is making revolutionary progress in various vision tasks, leading to significant performance improvements compared with the convolutional neural network (CNN) based frameworks. In this paper, we conduct extensive research on exploiting the contributions of transformers for accurate and reliable salient object detection. For the former, we apply transformer to a deterministic model, and explain that the effective structure modeling and global context modeling abilities lead to its superior performance compared with the CNN based frameworks. For the latter, we observe that both CNN and transformer based frameworks suffer greatly from the over-confidence issue, where the models tend to generate wrong predictions with high confidence. To estimate the reliability degree of both CNN- and transformer-based frameworks, we further present a latent variable model, namely inferential generative adversarial network (iGAN), based on the generative adversarial network (GAN). The stochastic attribute of the latent variable makes it convenient to estimate the predictive uncertainty, serving as an auxiliary output to evaluate the reliability of model prediction. Different from the conventional GAN, which defines the distribution of the latent variable as fixed standard normal distribution $\mathcal{N}(0,\mathbf{I})$, the proposed iGAN infers the latent variable by gradient-based Markov Chain Monte Carlo (MCMC), namely Langevin dynamics, leading to an input-dependent latent variable model. We apply our proposed iGAN to both fully and weakly supervised salient object detection, and explain that iGAN within the transformer framework leads to both accurate and reliable salient object detection.
<div id='section'>Paperid: <span id='pid'>734, <a href='https://arxiv.org/pdf/2511.18136.pdf' target='_blank'>https://arxiv.org/pdf/2511.18136.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunming He, Rihan Zhang, Longxiang Tang, Ziyun Yang, Kai Li, Deng-Ping Fan, Sina Farsiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.18136">SCALER: SAM-Enhanced Collaborative Learning for Label-Deficient Concealed Object Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing methods for label-deficient concealed object segmentation (LDCOS) either rely on consistency constraints or Segment Anything Model (SAM)-based pseudo-labeling. However, their performance remains limited due to the intrinsic concealment of targets and the scarcity of annotations. This study investigates two key questions: (1) Can consistency constraints and SAM-based supervision be jointly integrated to better exploit complementary information and enhance the segmenter? and (2) beyond that, can the segmenter in turn guide SAM through reciprocal supervision, enabling mutual improvement? To answer these questions, we present SCALER, a unified collaborative framework toward LDCOS that jointly optimizes a mean-teacher segmenter and a learnable SAM. SCALER operates in two alternating phases. In \textbf{Phase \uppercase\expandafter{\romannumeral1}}, the segmenter is optimized under fixed SAM supervision using entropy-based image-level and uncertainty-based pixel-level weighting to select reliable pseudo-label regions and emphasize harder examples. In \textbf{Phase \uppercase\expandafter{\romannumeral2}}, SAM is updated via augmentation invariance and noise resistance losses, leveraging its inherent robustness to perturbations. Experiments demonstrate that SCALER yields consistent performance gains across eight semi- and weakly-supervised COS tasks. The results further suggest that SCALER can serve as a general training paradigm to enhance both lightweight segmenters and large foundation models under label-scarce conditions. Code will be released.
<div id='section'>Paperid: <span id='pid'>735, <a href='https://arxiv.org/pdf/2510.23217.pdf' target='_blank'>https://arxiv.org/pdf/2510.23217.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alois Thomas, Maya Varma, Jean-Benoit Delbrouck, Curtis P. Langlotz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.23217">Process Reward Models for Sentence-Level Verification of LVLM Radiology Reports</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automating radiology report generation with Large Vision-Language Models (LVLMs) holds great potential, yet these models often produce clinically critical hallucinations, posing serious risks. Existing hallucination detection methods frequently lack the necessary sentence-level granularity or robust generalization across different LVLM generators. We introduce a novel approach: a sentence-level Process Reward Model (PRM) adapted for this vision-language task. Our PRM predicts the factual correctness of each generated sentence, conditioned on clinical context and preceding text. When fine-tuned on MIMIC-CXR with weakly-supervised labels, a lightweight 0.5B-parameter PRM outperforms existing verification techniques, demonstrating, for instance, relative improvements of 7.5% in Matthews Correlation Coefficient and 1.8% in AUROC over strong white-box baselines on outputs from one LVLM. Unlike methods reliant on internal model states, our PRM demonstrates strong generalization to an unseen LVLM. We further show its practical utility: PRM scores effectively filter low-quality reports, improving F1-CheXbert scores by 4.5% (when discarding the worst 10% of reports). Moreover, when guiding a novel weighted best-of-N selection process on the MIMIC-CXR test set, our PRM show relative improvements in clinical metrics of 7.4% for F1-CheXbert and 0.6% for BERTScore. These results demonstrate that a lightweight, context-aware PRM provides a model-agnostic safety layer for clinical LVLMs without access to internal activations
<div id='section'>Paperid: <span id='pid'>736, <a href='https://arxiv.org/pdf/2509.04737.pdf' target='_blank'>https://arxiv.org/pdf/2509.04737.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryoga Oishi, Sho Sakaino, Toshiaki Tsuji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04737">Imitation Learning Based on Disentangled Representation Learning of Behavioral Characteristics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of robot learning, coordinating robot actions through language instructions is becoming increasingly feasible. However, adapting actions to human instructions remains challenging, as such instructions are often qualitative and require exploring behaviors that satisfy varying conditions. This paper proposes a motion generation model that adapts robot actions in response to modifier directives human instructions imposing behavioral conditions during task execution. The proposed method learns a mapping from modifier directives to actions by segmenting demonstrations into short sequences, assigning weakly supervised labels corresponding to specific modifier types. We evaluated our method in wiping and pick and place tasks. Results show that it can adjust motions online in response to modifier directives, unlike conventional batch-based methods that cannot adapt during execution.
<div id='section'>Paperid: <span id='pid'>737, <a href='https://arxiv.org/pdf/2508.02179.pdf' target='_blank'>https://arxiv.org/pdf/2508.02179.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenbo Xu, Wei Lu, Xiangyang Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02179">Weakly Supervised Multimodal Temporal Forgery Localization via Multitask Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The spread of Deepfake videos has caused a trust crisis and impaired social stability. Although numerous approaches have been proposed to address the challenges of Deepfake detection and localization, there is still a lack of systematic research on the weakly supervised multimodal fine-grained temporal forgery localization (WS-MTFL). In this paper, we propose a novel weakly supervised multimodal temporal forgery localization via multitask learning (WMMT), which addresses the WS-MTFL under the multitask learning paradigm. WMMT achieves multimodal fine-grained Deepfake detection and temporal partial forgery localization using merely video-level annotations. Specifically, visual and audio modality detection are formulated as two binary classification tasks. The multitask learning paradigm is introduced to integrate these tasks into a multimodal task. Furthermore, WMMT utilizes a Mixture-of-Experts structure to adaptively select appropriate features and localization head, achieving excellent flexibility and localization precision in WS-MTFL. A feature enhancement module with temporal property preserving attention mechanism is proposed to identify the intra- and inter-modality feature deviation and construct comprehensive video features. To further explore the temporal information for weakly supervised learning, an extensible deviation perceiving loss has been proposed, which aims to enlarge the deviation of adjacent segments of the forged samples and reduce the deviation of genuine samples. Extensive experiments demonstrate the effectiveness of multitask learning for WS-MTFL, and the WMMT achieves comparable results to fully supervised approaches in several evaluation metrics.
<div id='section'>Paperid: <span id='pid'>738, <a href='https://arxiv.org/pdf/2507.19592.pdf' target='_blank'>https://arxiv.org/pdf/2507.19592.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meng Wei, Charlie Budd, Oluwatosin Alabi, Miaojing Shi, Tom Vercauteren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19592">SurgPIS: Surgical-instrument-level Instances and Part-level Semantics for Weakly-supervised Part-aware Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Consistent surgical instrument segmentation is critical for automation in robot-assisted surgery. Yet, existing methods only treat instrument-level instance segmentation (IIS) or part-level semantic segmentation (PSS) separately, without interaction between these tasks. In this work, we formulate a surgical tool segmentation as a unified part-aware instance segmentation (PIS) problem and introduce SurgPIS, the first PIS model for surgical instruments. Our method adopts a transformer-based mask classification approach and introduces part-specific queries derived from instrument-level object queries, explicitly linking parts to their parent instrument instances. In order to address the lack of large-scale datasets with both instance- and part-level labels, we propose a weakly-supervised learning strategy for SurgPIS to learn from disjoint datasets labelled for either IIS or PSS purposes. During training, we aggregate our PIS predictions into IIS or PSS masks, thereby allowing us to compute a loss against partially labelled datasets. A student-teacher approach is developed to maintain prediction consistency for missing PIS information in the partially labelled data, e.g., parts of the IIS labelled data. Extensive experiments across multiple datasets validate the effectiveness of SurgPIS, achieving state-of-the-art performance in PIS as well as IIS, PSS, and instrument-level semantic segmentation.
<div id='section'>Paperid: <span id='pid'>739, <a href='https://arxiv.org/pdf/2507.16596.pdf' target='_blank'>https://arxiv.org/pdf/2507.16596.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenbo Xu, Junyan Wu, Wei Lu, Xiangyang Luo, Qian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16596">A Multimodal Deviation Perceiving Framework for Weakly-Supervised Temporal Forgery Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current researches on Deepfake forensics often treat detection as a classification task or temporal forgery localization problem, which are usually restrictive, time-consuming, and challenging to scale for large datasets. To resolve these issues, we present a multimodal deviation perceiving framework for weakly-supervised temporal forgery localization (MDP), which aims to identify temporal partial forged segments using only video-level annotations. The MDP proposes a novel multimodal interaction mechanism (MI) and an extensible deviation perceiving loss to perceive multimodal deviation, which achieves the refined start and end timestamps localization of forged segments. Specifically, MI introduces a temporal property preserving cross-modal attention to measure the relevance between the visual and audio modalities in the probabilistic embedding space. It could identify the inter-modality deviation and construct comprehensive video features for temporal forgery localization. To explore further temporal deviation for weakly-supervised learning, an extensible deviation perceiving loss has been proposed, aiming at enlarging the deviation of adjacent segments of the forged samples and reducing that of genuine samples. Extensive experiments demonstrate the effectiveness of the proposed framework and achieve comparable results to fully-supervised approaches in several evaluation metrics.
<div id='section'>Paperid: <span id='pid'>740, <a href='https://arxiv.org/pdf/2507.10300.pdf' target='_blank'>https://arxiv.org/pdf/2507.10300.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hatef Otroshi Shahreza, SÃ©bastien Marcel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10300">FaceLLM: A Multimodal Large Language Model for Face Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal large language models (MLLMs) have shown remarkable performance in vision-language tasks. However, existing MLLMs are primarily trained on generic datasets, limiting their ability to reason on domain-specific visual cues such as those in facial images. In particular, tasks that require detailed understanding of facial structure, expression, emotion, and demographic features remain underexplored by MLLMs due to the lack of large-scale annotated face image-text datasets. In this work, we introduce FaceLLM, a multimodal large language model trained specifically for facial image understanding. To construct the training data, we propose a novel weakly supervised pipeline that uses ChatGPT with attribute-aware prompts to generate high-quality question-answer pairs based on images from the FairFace dataset. The resulting corpus, called FairFaceGPT, covers a diverse set of attributes including expression, pose, skin texture, and forensic information. Our experiments demonstrate that FaceLLM improves the performance of MLLMs on various face-centric tasks and achieves state-of-the-art performance. This work highlights the potential of synthetic supervision via language models for building domain-specialized MLLMs, and sets a precedent for trustworthy, human-centric multimodal AI systems. FairFaceGPT dataset and pretrained FaceLLM models are publicly available in the project page.
<div id='section'>Paperid: <span id='pid'>741, <a href='https://arxiv.org/pdf/2505.24656.pdf' target='_blank'>https://arxiv.org/pdf/2505.24656.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dimitrios Damianos, Georgios Paraskevopoulos, Alexandros Potamianos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24656">MSDA: Combining Pseudo-labeling and Self-Supervision for Unsupervised Domain Adaptation in ASR</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we investigate the Meta PL unsupervised domain adaptation framework for Automatic Speech Recognition (ASR). We introduce a Multi-Stage Domain Adaptation pipeline (MSDA), a sample-efficient, two-stage adaptation approach that integrates self-supervised learning with semi-supervised techniques. MSDA is designed to enhance the robustness and generalization of ASR models, making them more adaptable to diverse conditions. It is particularly effective for low-resource languages like Greek and in weakly supervised scenarios where labeled data is scarce or noisy. Through extensive experiments, we demonstrate that Meta PL can be applied effectively to ASR tasks, achieving state-of-the-art results, significantly outperforming state-of-the-art methods, and providing more robust solutions for unsupervised domain adaptation in ASR. Our ablations highlight the necessity of utilizing a cascading approach when combining self-supervision with self-training.
<div id='section'>Paperid: <span id='pid'>742, <a href='https://arxiv.org/pdf/2505.01880.pdf' target='_blank'>https://arxiv.org/pdf/2505.01880.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyan Wu, Wenbo Xu, Wei Lu, Xiangyang Luo, Rui Yang, Shize Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01880">Weakly-supervised Audio Temporal Forgery Localization via Progressive Audio-language Co-learning Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio temporal forgery localization (ATFL) aims to find the precise forgery regions of the partial spoof audio that is purposefully modified. Existing ATFL methods rely on training efficient networks using fine-grained annotations, which are obtained costly and challenging in real-world scenarios. To meet this challenge, in this paper, we propose a progressive audio-language co-learning network (LOCO) that adopts co-learning and self-supervision manners to prompt localization performance under weak supervision scenarios. Specifically, an audio-language co-learning module is first designed to capture forgery consensus features by aligning semantics from temporal and global perspectives. In this module, forgery-aware prompts are constructed by using utterance-level annotations together with learnable prompts, which can incorporate semantic priors into temporal content features dynamically. In addition, a forgery localization module is applied to produce forgery proposals based on fused forgery-class activation sequences. Finally, a progressive refinement strategy is introduced to generate pseudo frame-level labels and leverage supervised semantic contrastive learning to amplify the semantic distinction between real and fake content, thereby continuously optimizing forgery-aware features. Extensive experiments show that the proposed LOCO achieves SOTA performance on three public benchmarks.
<div id='section'>Paperid: <span id='pid'>743, <a href='https://arxiv.org/pdf/2503.04154.pdf' target='_blank'>https://arxiv.org/pdf/2503.04154.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chupeng Liu, Runkai Zhao, Weidong Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04154">CA-W3D: Leveraging Context-Aware Knowledge for Weakly Supervised Monocular 3D Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised monocular 3D detection, while less annotation-intensive, often struggles to capture the global context required for reliable 3D reasoning. Conventional label-efficient methods focus on object-centric features, neglecting contextual semantic relationships that are critical in complex scenes. In this work, we propose a Context-Aware Weak Supervision for Monocular 3D object detection, namely CA-W3D, to address this limitation in a two-stage training paradigm. Specifically, we first introduce a pre-training stage employing Region-wise Object Contrastive Matching (ROCM), which aligns regional object embeddings derived from a trainable monocular 3D encoder and a frozen open-vocabulary 2D visual grounding model. This alignment encourages the monocular encoder to discriminate scene-specific attributes and acquire richer contextual knowledge. In the second stage, we incorporate a pseudo-label training process with a Dual-to-One Distillation (D2OD) mechanism, which effectively transfers contextual priors into the monocular encoder while preserving spatial fidelity and maintaining computational efficiency during inference. Extensive experiments conducted on the public KITTI benchmark demonstrate the effectiveness of our approach, surpassing the SoTA method over all metrics, highlighting the importance of contextual-aware knowledge in weakly-supervised monocular 3D detection.
<div id='section'>Paperid: <span id='pid'>744, <a href='https://arxiv.org/pdf/2412.04383.pdf' target='_blank'>https://arxiv.org/pdf/2412.04383.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rong Li, Shijie Li, Lingdong Kong, Xulei Yang, Junwei Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04383">SeeGround: See and Ground for Zero-Shot Open-Vocabulary 3D Visual Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Visual Grounding (3DVG) aims to locate objects in 3D scenes based on textual descriptions, essential for applications like augmented reality and robotics. Traditional 3DVG approaches rely on annotated 3D datasets and predefined object categories, limiting scalability and adaptability. To overcome these limitations, we introduce SeeGround, a zero-shot 3DVG framework leveraging 2D Vision-Language Models (VLMs) trained on large-scale 2D data. SeeGround represents 3D scenes as a hybrid of query-aligned rendered images and spatially enriched text descriptions, bridging the gap between 3D data and 2D-VLMs input formats. We propose two modules: the Perspective Adaptation Module, which dynamically selects viewpoints for query-relevant image rendering, and the Fusion Alignment Module, which integrates 2D images with 3D spatial descriptions to enhance object localization. Extensive experiments on ScanRefer and Nr3D demonstrate that our approach outperforms existing zero-shot methods by large margins. Notably, we exceed weakly supervised methods and rival some fully supervised ones, outperforming previous SOTA by 7.7% on ScanRefer and 7.1% on Nr3D, showcasing its effectiveness in complex 3DVG tasks.
<div id='section'>Paperid: <span id='pid'>745, <a href='https://arxiv.org/pdf/2411.08530.pdf' target='_blank'>https://arxiv.org/pdf/2411.08530.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ravi Kant Gupta, Dadi Dharani, Shambhavi Shanker, Amit Sethi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08530">Efficient Whole Slide Image Classification through Fisher Vector Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advancement of digital pathology, particularly through computational analysis of whole slide images (WSI), is poised to significantly enhance diagnostic precision and efficiency. However, the large size and complexity of WSIs make it difficult to analyze and classify them using computers. This study introduces a novel method for WSI classification by automating the identification and examination of the most informative patches, thus eliminating the need to process the entire slide. Our method involves two-stages: firstly, it extracts only a few patches from the WSIs based on their pathological significance; and secondly, it employs Fisher vectors (FVs) for representing features extracted from these patches, which is known for its robustness in capturing fine-grained details. This approach not only accentuates key pathological features within the WSI representation but also significantly reduces computational overhead, thus making the process more efficient and scalable. We have rigorously evaluated the proposed method across multiple datasets to benchmark its performance against comprehensive WSI analysis and contemporary weakly-supervised learning methodologies. The empirical results indicate that our focused analysis of select patches, combined with Fisher vector representation, not only aligns with, but at times surpasses, the classification accuracy of standard practices. Moreover, this strategy notably diminishes computational load and resource expenditure, thereby establishing an efficient and precise framework for WSI analysis in the realm of digital pathology.
<div id='section'>Paperid: <span id='pid'>746, <a href='https://arxiv.org/pdf/2411.03551.pdf' target='_blank'>https://arxiv.org/pdf/2411.03551.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiling Yue, Yingying Fang, Liutao Yang, Nikhil Baid, Simon Walsh, Guang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03551">Enhancing Weakly Supervised Semantic Segmentation for Fibrosis via Controllable Image Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fibrotic Lung Disease (FLD) is a severe condition marked by lung stiffening and scarring, leading to respiratory decline. High-resolution computed tomography (HRCT) is critical for diagnosing and monitoring FLD; however, fibrosis appears as irregular, diffuse patterns with unclear boundaries, leading to high inter-observer variability and time-intensive manual annotation. To tackle this challenge, we propose DiffSeg, a novel weakly supervised semantic segmentation (WSSS) method that uses image-level annotations to generate pixel-level fibrosis segmentation, reducing the need for fine-grained manual labeling. Additionally, our DiffSeg incorporates a diffusion-based generative model to synthesize HRCT images with different levels of fibrosis from healthy slices, enabling the generation of the fibrosis-injected slices and their paired fibrosis location. Experiments indicate that our method significantly improves the accuracy of pseudo masks generated by existing WSSS methods, greatly reducing the complexity of manual labeling and enhancing the consistency of the generated masks.
<div id='section'>Paperid: <span id='pid'>747, <a href='https://arxiv.org/pdf/2406.00487.pdf' target='_blank'>https://arxiv.org/pdf/2406.00487.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gene Li, Lin Chen, Adel Javanmard, Vahab Mirrokni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00487">Optimistic Rates for Learning from Label Proportions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider a weakly supervised learning problem called Learning from Label Proportions (LLP), where examples are grouped into ``bags'' and only the average label within each bag is revealed to the learner. We study various learning rules for LLP that achieve PAC learning guarantees for classification loss. We establish that the classical Empirical Proportional Risk Minimization (EPRM) learning rule (Yu et al., 2014) achieves fast rates under realizability, but EPRM and similar proportion matching learning rules can fail in the agnostic setting. We also show that (1) a debiased proportional square loss, as well as (2) a recently proposed EasyLLP learning rule (Busa-Fekete et al., 2023) both achieve ``optimistic rates'' (Panchenko, 2002); in both the realizable and agnostic settings, their sample complexity is optimal (up to log factors) in terms of $Îµ, Î´$, and VC dimension.
<div id='section'>Paperid: <span id='pid'>748, <a href='https://arxiv.org/pdf/2402.10575.pdf' target='_blank'>https://arxiv.org/pdf/2402.10575.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Hossein Amani, Nicolas Mario Baldwin, Amin Mansouri, Martin Josifoski, Maxime Peyrard, Robert West
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.10575">Symbolic Autoencoding for Self-Supervised Sequence Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional language models, adept at next-token prediction in text sequences, often struggle with transduction tasks between distinct symbolic systems, particularly when parallel data is scarce. Addressing this issue, we introduce \textit{symbolic autoencoding} ($Î£$AE), a self-supervised framework that harnesses the power of abundant unparallel data alongside limited parallel data. $Î£$AE connects two generative models via a discrete bottleneck layer and is optimized end-to-end by minimizing reconstruction loss (simultaneously with supervised loss for the parallel data), such that the sequence generated by the discrete bottleneck can be read out as the transduced input sequence. We also develop gradient-based methods allowing for efficient self-supervised sequence learning despite the discreteness of the bottleneck. Our results demonstrate that $Î£$AE significantly enhances performance on transduction tasks, even with minimal parallel data, offering a promising solution for weakly supervised learning scenarios.
<div id='section'>Paperid: <span id='pid'>749, <a href='https://arxiv.org/pdf/2401.11122.pdf' target='_blank'>https://arxiv.org/pdf/2401.11122.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Chen, Yazhou Yao, Xingguo Huang, Zechao Li, Liqiang Nie, Jinhui Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11122">Spatial Structure Constraints for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The image-level label has prevailed in weakly supervised semantic segmentation tasks due to its easy availability. Since image-level labels can only indicate the existence or absence of specific categories of objects, visualization-based techniques have been widely adopted to provide object location clues. Considering class activation maps (CAMs) can only locate the most discriminative part of objects, recent approaches usually adopt an expansion strategy to enlarge the activation area for more integral object localization. However, without proper constraints, the expanded activation will easily intrude into the background region. In this paper, we propose spatial structure constraints (SSC) for weakly supervised semantic segmentation to alleviate the unwanted object over-activation of attention expansion. Specifically, we propose a CAM-driven reconstruction module to directly reconstruct the input image from deep CAM features, which constrains the diffusion of last-layer object attention by preserving the coarse spatial structure of the image content. Moreover, we propose an activation self-modulation module to refine CAMs with finer spatial structure details by enhancing regional consistency. Without external saliency models to provide background clues, our approach achieves 72.7\% and 47.0\% mIoU on the PASCAL VOC 2012 and COCO datasets, respectively, demonstrating the superiority of our proposed approach.
<div id='section'>Paperid: <span id='pid'>750, <a href='https://arxiv.org/pdf/2310.05241.pdf' target='_blank'>https://arxiv.org/pdf/2310.05241.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sunjae Yoon, Gwanhyeong Koo, Dahyun Kim, Chang D. Yoo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.05241">SCANet: Scene Complexity Aware Network for Weakly-Supervised Video Moment Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video moment retrieval aims to localize moments in video corresponding to a given language query. To avoid the expensive cost of annotating the temporal moments, weakly-supervised VMR (wsVMR) systems have been studied. For such systems, generating a number of proposals as moment candidates and then selecting the most appropriate proposal has been a popular approach. These proposals are assumed to contain many distinguishable scenes in a video as candidates. However, existing proposals of wsVMR systems do not respect the varying numbers of scenes in each video, where the proposals are heuristically determined irrespective of the video. We argue that the retrieval system should be able to counter the complexities caused by varying numbers of scenes in each video. To this end, we present a novel concept of a retrieval system referred to as Scene Complexity Aware Network (SCANet), which measures the `scene complexity' of multiple scenes in each video and generates adaptive proposals responding to variable complexities of scenes in each video. Experimental results on three retrieval benchmarks (i.e., Charades-STA, ActivityNet, TVR) achieve state-of-the-art performances and demonstrate the effectiveness of incorporating the scene complexity.
<div id='section'>Paperid: <span id='pid'>751, <a href='https://arxiv.org/pdf/2307.09184.pdf' target='_blank'>https://arxiv.org/pdf/2307.09184.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinghan Sun, Dong Wei, Zhe Xu, Donghuan Lu, Hong Liu, Liansheng Wang, Yefeng Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.09184">You've Got Two Teachers: Co-evolutionary Image and Report Distillation for Semi-supervised Anatomical Abnormality Detection in Chest X-ray</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Chest X-ray (CXR) anatomical abnormality detection aims at localizing and characterising cardiopulmonary radiological findings in the radiographs, which can expedite clinical workflow and reduce observational oversights. Most existing methods attempted this task in either fully supervised settings which demanded costly mass per-abnormality annotations, or weakly supervised settings which still lagged badly behind fully supervised methods in performance. In this work, we propose a co-evolutionary image and report distillation (CEIRD) framework, which approaches semi-supervised abnormality detection in CXR by grounding the visual detection results with text-classified abnormalities from paired radiology reports, and vice versa. Concretely, based on the classical teacher-student pseudo label distillation (TSD) paradigm, we additionally introduce an auxiliary report classification model, whose prediction is used for report-guided pseudo detection label refinement (RPDLR) in the primary vision detection task. Inversely, we also use the prediction of the vision detection model for abnormality-guided pseudo classification label refinement (APCLR) in the auxiliary report classification task, and propose a co-evolution strategy where the vision and report models mutually promote each other with RPDLR and APCLR performed alternatively. To this end, we effectively incorporate the weak supervision by reports into the semi-supervised TSD pipeline. Besides the cross-modal pseudo label refinement, we further propose an intra-image-modal self-adaptive non-maximum suppression, where the pseudo detection labels generated by the teacher vision model are dynamically rectified by high-confidence predictions by the student. Experimental results on the public MIMIC-CXR benchmark demonstrate CEIRD's superior performance to several up-to-date weakly and semi-supervised methods.
<div id='section'>Paperid: <span id='pid'>752, <a href='https://arxiv.org/pdf/2306.01016.pdf' target='_blank'>https://arxiv.org/pdf/2306.01016.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hejie Cui, Rongmei Lin, Nasser Zalmout, Chenwei Zhang, Jingbo Shang, Carl Yang, Xian Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.01016">PV2TEA: Patching Visual Modality to Textual-Established Information Extraction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Information extraction, e.g., attribute value extraction, has been extensively studied and formulated based only on text. However, many attributes can benefit from image-based extraction, like color, shape, pattern, among others. The visual modality has long been underutilized, mainly due to multimodal annotation difficulty. In this paper, we aim to patch the visual modality to the textual-established attribute information extractor. The cross-modality integration faces several unique challenges: (C1) images and textual descriptions are loosely paired intra-sample and inter-samples; (C2) images usually contain rich backgrounds that can mislead the prediction; (C3) weakly supervised labels from textual-established extractors are biased for multimodal training. We present PV2TEA, an encoder-decoder architecture equipped with three bias reduction schemes: (S1) Augmented label-smoothed contrast to improve the cross-modality alignment for loosely-paired image and text; (S2) Attention-pruning that adaptively distinguishes the visual foreground; (S3) Two-level neighborhood regularization that mitigates the label textual bias via reliability estimation. Empirical results on real-world e-Commerce datasets demonstrate up to 11.74% absolute (20.97% relatively) F1 increase over unimodal baselines.
<div id='section'>Paperid: <span id='pid'>753, <a href='https://arxiv.org/pdf/2305.17442.pdf' target='_blank'>https://arxiv.org/pdf/2305.17442.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dawei Zhu, Xiaoyu Shen, Marius Mosbach, Andreas Stephan, Dietrich Klakow
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.17442">Weaker Than You Think: A Critical Look at Weakly Supervised Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised learning is a popular approach for training machine learning models in low-resource settings. Instead of requesting high-quality yet costly human annotations, it allows training models with noisy annotations obtained from various weak sources. Recently, many sophisticated approaches have been proposed for robust training under label noise, reporting impressive results. In this paper, we revisit the setup of these approaches and find that the benefits brought by these approaches are significantly overestimated. Specifically, we find that the success of existing weakly supervised learning approaches heavily relies on the availability of clean validation samples which, as we show, can be leveraged much more efficiently by simply training on them. After using these clean labels in training, the advantages of using these sophisticated approaches are mostly wiped out. This remains true even when reducing the size of the available clean data to just five samples per class, making these approaches impractical. To understand the true value of weakly supervised learning, we thoroughly analyze diverse NLP datasets and tasks to ascertain when and why weakly supervised approaches work. Based on our findings, we provide recommendations for future research.
<div id='section'>Paperid: <span id='pid'>754, <a href='https://arxiv.org/pdf/2305.06310.pdf' target='_blank'>https://arxiv.org/pdf/2305.06310.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Naga VS Raviteja Chappa, Pha Nguyen, Alexander H Nelson, Han-Seok Seo, Xin Li, Page Daniel Dobbs, Khoa Luu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.06310">SoGAR: Self-supervised Spatiotemporal Attention-based Social Group Activity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel approach to Social Group Activity Recognition (SoGAR) using Self-supervised Transformers network that can effectively utilize unlabeled video data. To extract spatio-temporal information, we created local and global views with varying frame rates. Our self-supervised objective ensures that features extracted from contrasting views of the same video were consistent across spatio-temporal domains. Our proposed approach is efficient in using transformer-based encoders to alleviate the weakly supervised setting of group activity recognition. By leveraging the benefits of transformer models, our approach can model long-term relationships along spatio-temporal dimensions. Our proposed SoGAR method achieved state-of-the-art results on three group activity recognition benchmarks, namely JRDB-PAR, NBA, and Volleyball datasets, surpassing the current numbers in terms of F1-score, MCA, and MPCA metrics.
<div id='section'>Paperid: <span id='pid'>755, <a href='https://arxiv.org/pdf/2303.12149.pdf' target='_blank'>https://arxiv.org/pdf/2303.12149.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Naga VS Raviteja Chappa, Pha Nguyen, Alexander H Nelson, Han-Seok Seo, Xin Li, Page Daniel Dobbs, Khoa Luu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.12149">SPARTAN: Self-supervised Spatiotemporal Transformers Approach to Group Activity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a new, simple, and effective Self-supervised Spatio-temporal Transformers (SPARTAN) approach to Group Activity Recognition (GAR) using unlabeled video data. Given a video, we create local and global Spatio-temporal views with varying spatial patch sizes and frame rates. The proposed self-supervised objective aims to match the features of these contrasting views representing the same video to be consistent with the variations in spatiotemporal domains. To the best of our knowledge, the proposed mechanism is one of the first works to alleviate the weakly supervised setting of GAR using the encoders in video transformers. Furthermore, using the advantage of transformer models, our proposed approach supports long-term relationship modeling along spatio-temporal dimensions. The proposed SPARTAN approach performs well on two group activity recognition benchmarks, including NBA and Volleyball datasets, by surpassing the state-of-the-art results by a significant margin in terms of MCA and MPCA metrics.
<div id='section'>Paperid: <span id='pid'>756, <a href='https://arxiv.org/pdf/2303.00205.pdf' target='_blank'>https://arxiv.org/pdf/2303.00205.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lianyu Zhou, Dong Wei, Donghuan Lu, Wei Xue, Liansheng Wang, Yefeng Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.00205">RECIST Weakly Supervised Lesion Segmentation via Label-Space Co-Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As an essential indicator for cancer progression and treatment response, tumor size is often measured following the response evaluation criteria in solid tumors (RECIST) guideline in CT slices. By marking each lesion with its longest axis and the longest perpendicular one, laborious pixel-wise manual annotation can be avoided. However, such a coarse substitute cannot provide a rich and accurate base to allow versatile quantitative analysis of lesions. To this end, we propose a novel weakly supervised framework to exploit the existing rich RECIST annotations for pixel-wise lesion segmentation. Specifically, a pair of under- and over-segmenting masks are constructed for each lesion based on its RECIST annotation and served as the label for co-training a pair of subnets, respectively, along with the proposed label-space perturbation induced consistency loss to bridge the gap between the two subnets and enable effective co-training. Extensive experiments are conducted on a public dataset to demonstrate the superiority of the proposed framework regarding the RECIST-based weakly supervised segmentation task and its universal applicability to various backbone networks.
<div id='section'>Paperid: <span id='pid'>757, <a href='https://arxiv.org/pdf/2301.00304.pdf' target='_blank'>https://arxiv.org/pdf/2301.00304.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Georgios Paraskevopoulos, Theodoros Kouzelis, Georgios Rouvalis, Athanasios Katsamanis, Vassilis Katsouros, Alexandros Potamianos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.00304">Sample-Efficient Unsupervised Domain Adaptation of Speech Recognition Systems A case study for Modern Greek</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern speech recognition systems exhibits rapid performance degradation under domain shift. This issue is especially prevalent in data-scarce settings, such as low-resource languages, where diversity of training data is limited. In this work we propose M2DS2, a simple and sample-efficient finetuning strategy for large pretrained speech models, based on mixed source and target domain self-supervision. We find that including source domain self-supervision stabilizes training and avoids mode collapse of the latent representations. For evaluation, we collect HParl, a $120$ hour speech corpus for Greek, consisting of plenary sessions in the Greek Parliament. We merge HParl with two popular Greek corpora to create GREC-MD, a test-bed for multi-domain evaluation of Greek ASR systems. In our experiments we find that, while other Unsupervised Domain Adaptation baselines fail in this resource-constrained environment, M2DS2 yields significant improvements for cross-domain adaptation, even when a only a few hours of in-domain audio are available. When we relax the problem in a weakly supervised setting, we find that independent adaptation for audio using M2DS2 and language using simple LM augmentation techniques is particularly effective, yielding word error rates comparable to the fully supervised baselines.
<div id='section'>Paperid: <span id='pid'>758, <a href='https://arxiv.org/pdf/2203.02496.pdf' target='_blank'>https://arxiv.org/pdf/2203.02496.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianxin Zhang, Yutong Wang, Clayton Scott
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.02496">Learning from Label Proportions by Learning with Label Noise</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning from label proportions (LLP) is a weakly supervised classification problem where data points are grouped into bags, and the label proportions within each bag are observed instead of the instance-level labels. The task is to learn a classifier to predict the individual labels of future individual instances. Prior work on LLP for multi-class data has yet to develop a theoretically grounded algorithm. In this work, we provide a theoretically grounded approach to LLP based on a reduction to learning with label noise, using the forward correction (FC) loss of \citet{Patrini2017MakingDN}. We establish an excess risk bound and generalization error analysis for our approach, while also extending the theory of the FC loss which may be of independent interest. Our approach demonstrates improved empirical performance in deep learning scenarios across multiple datasets and architectures, compared to the leading existing methods.
<div id='section'>Paperid: <span id='pid'>759, <a href='https://arxiv.org/pdf/2008.10238.pdf' target='_blank'>https://arxiv.org/pdf/2008.10238.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minuk Ma, Sunjae Yoon, Junyeong Kim, Youngjoon Lee, Sunghun Kang, Chang D. Yoo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2008.10238">VLANet: Video-Language Alignment Network for Weakly-Supervised Video Moment Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Moment Retrieval (VMR) is a task to localize the temporal moment in untrimmed video specified by natural language query. For VMR, several methods that require full supervision for training have been proposed. Unfortunately, acquiring a large number of training videos with labeled temporal boundaries for each query is a labor-intensive process. This paper explores methods for performing VMR in a weakly-supervised manner (wVMR): training is performed without temporal moment labels but only with the text query that describes a segment of the video. Existing methods on wVMR generate multi-scale proposals and apply query-guided attention mechanisms to highlight the most relevant proposal. To leverage the weak supervision, contrastive learning is used which predicts higher scores for the correct video-query pairs than for the incorrect pairs. It has been observed that a large number of candidate proposals, coarse query representation, and one-way attention mechanism lead to blurry attention maps which limit the localization performance. To handle this issue, Video-Language Alignment Network (VLANet) is proposed that learns sharper attention by pruning out spurious candidate proposals and applying a multi-directional attention mechanism with fine-grained query representation. The Surrogate Proposal Selection module selects a proposal based on the proximity to the query in the joint embedding space, and thus substantially reduces candidate proposals which leads to lower computation load and sharper attention. Next, the Cascaded Cross-modal Attention module considers dense feature interactions and multi-directional attention flow to learn the multi-modal alignment. VLANet is trained end-to-end using contrastive loss which enforces semantically similar videos and queries to gather. The experiments show that the method achieves state-of-the-art performance on Charades-STA and DiDeMo datasets.
<div id='section'>Paperid: <span id='pid'>760, <a href='https://arxiv.org/pdf/2511.10241.pdf' target='_blank'>https://arxiv.org/pdf/2511.10241.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinxuan Li, Yi Zhang, Jian-Fang Hu, Chaolei Tan, Tianming Liang, Beihao Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.10241">TubeRMC: Tube-conditioned Reconstruction with Mutual Constraints for Weakly-supervised Spatio-Temporal Video Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spatio-Temporal Video Grounding (STVG) aims to localize a spatio-temporal tube that corresponds to a given language query in an untrimmed video. This is a challenging task since it involves complex vision-language understanding and spatiotemporal reasoning. Recent works have explored weakly-supervised setting in STVG to eliminate reliance on fine-grained annotations like bounding boxes or temporal stamps. However, they typically follow a simple late-fusion manner, which generates tubes independent of the text description, often resulting in failed target identification and inconsistent target tracking. To address this limitation, we propose a Tube-conditioned Reconstruction with Mutual Constraints (\textbf{TubeRMC}) framework that generates text-conditioned candidate tubes with pre-trained visual grounding models and further refine them via tube-conditioned reconstruction with spatio-temporal constraints. Specifically, we design three reconstruction strategies from temporal, spatial, and spatio-temporal perspectives to comprehensively capture rich tube-text correspondences. Each strategy is equipped with a Tube-conditioned Reconstructor, utilizing spatio-temporal tubes as condition to reconstruct the key clues in the query. We further introduce mutual constraints between spatial and temporal proposals to enhance their quality for reconstruction. TubeRMC outperforms existing methods on two public benchmarks VidSTG and HCSTVG. Further visualization shows that TubeRMC effectively mitigates both target identification errors and inconsistent tracking.
<div id='section'>Paperid: <span id='pid'>761, <a href='https://arxiv.org/pdf/2510.25413.pdf' target='_blank'>https://arxiv.org/pdf/2510.25413.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shakib Yazdani, Yasser Hamidullah, Cristina España-Bonet, Josef van Genabith
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.25413">Seeing, Signing, and Saying: A Vision-Language Model-Assisted Pipeline for Sign Language Data Acquisition and Curation from Social Media</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most existing sign language translation (SLT) datasets are limited in scale, lack multilingual coverage, and are costly to curate due to their reliance on expert annotation and controlled recording setup. Recently, Vision Language Models (VLMs) have demonstrated strong capabilities as evaluators and real-time assistants. Despite these advancements, their potential remains untapped in the context of sign language dataset acquisition. To bridge this gap, we introduce the first automated annotation and filtering framework that utilizes VLMs to reduce reliance on manual effort while preserving data quality. Our method is applied to TikTok videos across eight sign languages and to the already curated YouTube-SL-25 dataset in German Sign Language for the purpose of additional evaluation. Our VLM-based pipeline includes a face visibility detection, a sign activity recognition, a text extraction from video content, and a judgment step to validate alignment between video and text, implementing generic filtering, annotation and validation steps. Using the resulting corpus, TikTok-SL-8, we assess the performance of two off-the-shelf SLT models on our filtered dataset for German and American Sign Languages, with the goal of establishing baselines and evaluating the robustness of recent models on automatically extracted, slightly noisy data. Our work enables scalable, weakly supervised pretraining for SLT and facilitates data acquisition from social media.
<div id='section'>Paperid: <span id='pid'>762, <a href='https://arxiv.org/pdf/2508.03745.pdf' target='_blank'>https://arxiv.org/pdf/2508.03745.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenwen Li, Chia-Yu Hsu, Maosheng Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03745">Tobler's First Law in GeoAI: A Spatially Explicit Deep Learning Model for Terrain Feature Detection Under Weak Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent interest in geospatial artificial intelligence (GeoAI) has fostered a wide range of applications using artificial intelligence (AI), especially deep learning, for geospatial problem solving. However, major challenges such as a lack of training data and the neglect of spatial principles and spatial effects in AI model design remain, significantly hindering the in-depth integration of AI with geospatial research. This paper reports our work in developing a deep learning model that enables object detection, particularly of natural features, in a weakly supervised manner. Our work makes three contributions: First, we present a method of object detection using only weak labels. This is achieved by developing a spatially explicit model based on Tobler's first law of geography. Second, we incorporate attention maps into the object detection pipeline and develop a multistage training strategy to improve performance. Third, we apply this model to detect impact craters on Mars, a task that previously required extensive manual effort. The model generalizes to both natural and human-made features on the surfaces of Earth and other planets. This research advances the theoretical and methodological foundations of GeoAI.
<div id='section'>Paperid: <span id='pid'>763, <a href='https://arxiv.org/pdf/2507.01792.pdf' target='_blank'>https://arxiv.org/pdf/2507.01792.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Zheng, Ye Wang, Rui Ma, Zuxuan Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01792">FreeLoRA: Enabling Training-Free LoRA Fusion for Autoregressive Multi-Subject Personalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Subject-driven image generation plays a crucial role in applications such as virtual try-on and poster design. Existing approaches typically fine-tune pretrained generative models or apply LoRA-based adaptations for individual subjects. However, these methods struggle with multi-subject personalization, as combining independently adapted modules often requires complex re-tuning or joint optimization. We present FreeLoRA, a simple and generalizable framework that enables training-free fusion of subject-specific LoRA modules for multi-subject personalization. Each LoRA module is adapted on a few images of a specific subject using a Full Token Tuning strategy, where it is applied across all tokens in the prompt to encourage weakly supervised token-content alignment. At inference, we adopt Subject-Aware Inference, activating each module only on its corresponding subject tokens. This enables training-free fusion of multiple personalized subjects within a single image, while mitigating overfitting and mutual interference between subjects. Extensive experiments show that FreeLoRA achieves strong performance in both subject fidelity and prompt consistency.
<div id='section'>Paperid: <span id='pid'>764, <a href='https://arxiv.org/pdf/2503.13821.pdf' target='_blank'>https://arxiv.org/pdf/2503.13821.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chi Hsuan Wu, Kumar Ashutosh, Kristen Grauman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13821">Stitch-a-Recipe: Video Demonstration from Multistep Descriptions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>When obtaining visual illustrations from text descriptions, today's methods take a description with-a single text context caption, or an action description-and retrieve or generate the matching visual context. However, prior work does not permit visual illustration of multistep descriptions, e.g. a cooking recipe composed of multiple steps. Furthermore, simply handling each step description in isolation would result in an incoherent demonstration. We propose Stitch-a-Recipe, a novel retrieval-based method to assemble a video demonstration from a multistep description. The resulting video contains clips, possibly from different sources, that accurately reflect all the step descriptions, while being visually coherent. We formulate a training pipeline that creates large-scale weakly supervised data containing diverse and novel recipes and injects hard negatives that promote both correctness and coherence. Validated on in-the-wild instructional videos, Stitch-a-Recipe achieves state-of-the-art performance, with quantitative gains up to 24% as well as dramatic wins in a human preference study.
<div id='section'>Paperid: <span id='pid'>765, <a href='https://arxiv.org/pdf/2503.12905.pdf' target='_blank'>https://arxiv.org/pdf/2503.12905.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanbin Qian, Shuhan Ye, Chong Wang, Xiaojie Cai, Jiangbo Qian, Jiafei Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12905">UCF-Crime-DVS: A Novel Event-Based Dataset for Video Anomaly Detection with Spiking Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video anomaly detection plays a significant role in intelligent surveillance systems. To enhance model's anomaly recognition ability, previous works have typically involved RGB, optical flow, and text features. Recently, dynamic vision sensors (DVS) have emerged as a promising technology, which capture visual information as discrete events with a very high dynamic range and temporal resolution. It reduces data redundancy and enhances the capture capacity of moving objects compared to conventional camera. To introduce this rich dynamic information into the surveillance field, we created the first DVS video anomaly detection benchmark, namely UCF-Crime-DVS. To fully utilize this new data modality, a multi-scale spiking fusion network (MSF) is designed based on spiking neural networks (SNNs). This work explores the potential application of dynamic information from event data in video anomaly detection. Our experiments demonstrate the effectiveness of our framework on UCF-Crime-DVS and its superior performance compared to other models, establishing a new baseline for SNN-based weakly supervised video anomaly detection.
<div id='section'>Paperid: <span id='pid'>766, <a href='https://arxiv.org/pdf/2501.14148.pdf' target='_blank'>https://arxiv.org/pdf/2501.14148.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuvendu Roy, Ali Etemad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.14148">SelfPrompt: Confidence-Aware Semi-Supervised Tuning for Robust Vision-Language Model Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present SelfPrompt, a novel prompt-tuning approach for vision-language models (VLMs) in a semi-supervised learning setup. Existing methods for tuning VLMs in semi-supervised setups struggle with the negative impact of the miscalibrated VLMs on pseudo-labelling, and the accumulation of noisy pseudo-labels. SelfPrompt addresses these challenges by introducing a cluster-guided pseudo-labelling method that improves pseudo-label accuracy, and a confidence-aware semi-supervised learning module that maximizes the utilization of unlabelled data by combining supervised learning and weakly-supervised learning. Additionally, we investigate our method in an active semi-supervised learning setup, where the labelled set is strategically selected to ensure the best utilization of a limited labelling budget. To this end, we propose a weakly-supervised sampling technique that selects a diverse and representative labelled set, which can be seamlessly integrated into existing methods to enhance their performance. We conduct extensive evaluations across 13 datasets, significantly surpassing state-of-the-art performances with average improvements of 6.23% in standard semi-supervised learning, 6.25% in active semi-supervised learning, and 4.9% in base-to-novel generalization, using a 2-shot setup. Furthermore, SelfPrompt shows excellent generalization in single-shot settings, achieving an average improvement of 11.78%.
<div id='section'>Paperid: <span id='pid'>767, <a href='https://arxiv.org/pdf/2501.13497.pdf' target='_blank'>https://arxiv.org/pdf/2501.13497.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qijie Shao, Linhao Dong, Kun Wei, Sining Sun, Lei Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13497">DQ-Data2vec: Decoupling Quantization for Multilingual Speech Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data2vec is a self-supervised learning (SSL) approach that employs a teacher-student architecture for contextual representation learning via masked prediction, demonstrating remarkable performance in monolingual ASR. Previous studies have revealed that data2vec's shallow layers capture speaker and language information, middle layers encode phoneme and word features, while deep layers are responsible for reconstruction. Language and phoneme features are crucial for multilingual ASR. However, data2vec's masked representation generation relies on multi-layer averaging, inevitably coupling these features. To address this limitation, we propose a decoupling quantization based data2vec (DQ-Data2vec) for multilingual ASR, which includes a data2vec backbone and two improved online K-means quantizers. Our core idea is using the K-means quantizer with specified cluster numbers to decouple language and phoneme information for masked prediction. Specifically, in the language quantization, considering that the number of languages is significantly different from other irrelevant features (e.g., speakers), we assign the cluster number to match the number of languages, explicitly decoupling shallow layers' language-related information from irrelevant features. This strategy is also applied to decoupling middle layers' phoneme and word features. In a self-supervised scenario, experiments on the CommonVoice dataset demonstrate that DQ-Data2vec achieves a relative reduction of 9.51% in phoneme error rate (PER) and 11.58% in word error rate (WER) compared to data2vec and UniData2vec. Moreover, in a weakly-supervised scenario incorporating language labels and high-resource language text labels, the relative reduction is 18.09% and 1.55%, respectively.
<div id='section'>Paperid: <span id='pid'>768, <a href='https://arxiv.org/pdf/2412.14579.pdf' target='_blank'>https://arxiv.org/pdf/2412.14579.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qianpu Sun, Changyong Shu, Sifan Zhou, Zichen Yu, Yan Chen, Dawei Yang, Yuan Chun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14579">GSRender: Deduplicated Occupancy Prediction via Weakly Supervised 3D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D occupancy perception is gaining increasing attention due to its capability to offer detailed and precise environment representations. Previous weakly-supervised NeRF methods balance efficiency and accuracy, with mIoU varying by 5-10 points due to sampling count along camera rays. Recently, real-time Gaussian splatting has gained widespread popularity in 3D reconstruction, and the occupancy prediction task can also be viewed as a reconstruction task. Consequently, we propose GSRender, which naturally employs 3D Gaussian Splatting for occupancy prediction, simplifying the sampling process. In addition, the limitations of 2D supervision result in duplicate predictions along the same camera ray. We implemented the Ray Compensation (RC) module, which mitigates this issue by compensating for features from adjacent frames. Finally, we redesigned the loss to eliminate the impact of dynamic objects from adjacent frames. Extensive experiments demonstrate that our approach achieves SOTA (state-of-the-art) results in RayIoU (+6.0), while narrowing the gap with 3D supervision methods. Our code will be released soon.
<div id='section'>Paperid: <span id='pid'>769, <a href='https://arxiv.org/pdf/2408.00672.pdf' target='_blank'>https://arxiv.org/pdf/2408.00672.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kumar Ashutosh, Tushar Nagarajan, Georgios Pavlakos, Kris Kitani, Kristen Grauman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.00672">ExpertAF: Expert Actionable Feedback from Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Feedback is essential for learning a new skill or improving one's current skill-level. However, current methods for skill-assessment from video only provide scores or compare demonstrations, leaving the burden of knowing what to do differently on the user. We introduce a novel method to generate actionable feedback (AF) from video of a person doing a physical activity, such as basketball or soccer. Our method takes a video demonstration and its accompanying 3D body pose and generates (1) free-form expert commentary describing what the person is doing well and what they could improve, and (2) a visual expert demonstration that incorporates the required corrections. We show how to leverage Ego-Exo4D's [29] videos of skilled activity and expert commentary together with a strong language model to create a weakly-supervised training dataset for this task, and we devise a multimodal video-language model to infer coaching feedback. Our method is able to reason across multi-modal input combinations to output full spectrum, actionable coaching-expert commentary, expert video retrieval, and expert pose generation-outperforming strong vision-language models on both established metrics and human preference studies.
<div id='section'>Paperid: <span id='pid'>770, <a href='https://arxiv.org/pdf/2406.02251.pdf' target='_blank'>https://arxiv.org/pdf/2406.02251.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lukas Christ, Shahin Amiriparian, Manuel Milling, Ilhan Aslan, BjÃ¶rn W. Schuller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.02251">Modeling Emotional Trajectories in Written Stories Utilizing Transformers and Weakly-Supervised Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Telling stories is an integral part of human communication which can evoke emotions and influence the affective states of the audience. Automatically modeling emotional trajectories in stories has thus attracted considerable scholarly interest. However, as most existing works have been limited to unsupervised dictionary-based approaches, there is no benchmark for this task. We address this gap by introducing continuous valence and arousal labels for an existing dataset of children's stories originally annotated with discrete emotion categories. We collect additional annotations for this data and map the categorical labels to the continuous valence and arousal space. For predicting the thus obtained emotionality signals, we fine-tune a DeBERTa model and improve upon this baseline via a weakly supervised learning approach. The best configuration achieves a Concordance Correlation Coefficient (CCC) of $.8221$ for valence and $.7125$ for arousal on the test set, demonstrating the efficacy of our proposed approach. A detailed analysis shows the extent to which the results vary depending on factors such as the author, the individual story, or the section within the story. In addition, we uncover the weaknesses of our approach by investigating examples that prove to be difficult to predict.
<div id='section'>Paperid: <span id='pid'>771, <a href='https://arxiv.org/pdf/2405.20402.pdf' target='_blank'>https://arxiv.org/pdf/2405.20402.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhong-Qiu Wang, Anurag Kumar, Shinji Watanabe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.20402">Cross-Talk Reduction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While far-field multi-talker mixtures are recorded, each speaker can wear a close-talk microphone so that close-talk mixtures can be recorded at the same time. Although each close-talk mixture has a high signal-to-noise ratio (SNR) of the wearer, it has a very limited range of applications, as it also contains significant cross-talk speech by other speakers and is not clean enough. In this context, we propose a novel task named cross-talk reduction (CTR) which aims at reducing cross-talk speech, and a novel solution named CTRnet which is based on unsupervised or weakly-supervised neural speech separation. In unsupervised CTRnet, close-talk and far-field mixtures are stacked as input for a DNN to estimate the close-talk speech of each speaker. It is trained in an unsupervised, discriminative way such that the DNN estimate for each speaker can be linearly filtered to cancel out the speaker's cross-talk speech captured at other microphones. In weakly-supervised CTRnet, we assume the availability of each speaker's activity timestamps during training, and leverage them to improve the training of unsupervised CTRnet. Evaluation results on a simulated two-speaker CTR task and on a real-recorded conversational speech separation and recognition task show the effectiveness and potential of CTRnet.
<div id='section'>Paperid: <span id='pid'>772, <a href='https://arxiv.org/pdf/2405.18012.pdf' target='_blank'>https://arxiv.org/pdf/2405.18012.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Adi Nugroho, Sangmin Woo, Sumin Lee, Jinyoung Park, Yooseung Wang, Donguk Kim, Changick Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.18012">Flow-Assisted Motion Learning Network for Weakly-Supervised Group Activity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-Supervised Group Activity Recognition (WSGAR) aims to understand the activity performed together by a group of individuals with the video-level label and without actor-level labels. We propose Flow-Assisted Motion Learning Network (Flaming-Net) for WSGAR, which consists of the motion-aware actor encoder to extract actor features and the two-pathways relation module to infer the interaction among actors and their activity. Flaming-Net leverages an additional optical flow modality in the training stage to enhance its motion awareness when finding locally active actors. The first pathway of the relation module, the actor-centric path, initially captures the temporal dynamics of individual actors and then constructs inter-actor relationships. In parallel, the group-centric path starts by building spatial connections between actors within the same timeframe and then captures simultaneous spatio-temporal dynamics among them. We demonstrate that Flaming-Net achieves new state-of-the-art WSGAR results on two benchmarks, including a 2.8%p higher MPCA score on the NBA dataset. Importantly, we use the optical flow modality only for training and not for inference.
<div id='section'>Paperid: <span id='pid'>773, <a href='https://arxiv.org/pdf/2403.08801.pdf' target='_blank'>https://arxiv.org/pdf/2403.08801.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Woojung Han, Seil Kang, Kyobin Choo, Seong Jae Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.08801">CoBra: Complementary Branch Fusing Class and Semantic Knowledge for Robust Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Leveraging semantically precise pseudo masks derived from image-level class knowledge for segmentation, namely image-level Weakly Supervised Semantic Segmentation (WSSS), still remains challenging. While Class Activation Maps (CAMs) using CNNs have steadily been contributing to the success of WSSS, the resulting activation maps often narrowly focus on class-specific parts (e.g., only face of human). On the other hand, recent works based on vision transformers (ViT) have shown promising results based on their self-attention mechanism to capture the semantic parts but fail in capturing complete class-specific details (e.g., entire body parts of human but also with a dog nearby). In this work, we propose Complementary Branch (CoBra), a novel dual branch framework consisting of two distinct architectures which provide valuable complementary knowledge of class (from CNN) and semantic (from ViT) to each branch. In particular, we learn Class-Aware Projection (CAP) for the CNN branch and Semantic-Aware Projection (SAP) for the ViT branch to explicitly fuse their complementary knowledge and facilitate a new type of extra patch-level supervision. Our model, through CoBra, fuses CNN and ViT's complementary outputs to create robust pseudo masks that integrate both class and semantic information effectively. Extensive experiments qualitatively and quantitatively investigate how CNN and ViT complement each other on the PASCAL VOC 2012 dataset, showing a state-of-the-art WSSS result. This includes not only the masks generated by our model, but also the segmentation results derived from utilizing these masks as pseudo labels.
<div id='section'>Paperid: <span id='pid'>774, <a href='https://arxiv.org/pdf/2401.13537.pdf' target='_blank'>https://arxiv.org/pdf/2401.13537.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tobias Golling, Lukas Heinrich, Michael Kagan, Samuel Klein, Matthew Leigh, Margarita Osadchy, John Andrew Raine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.13537">Masked Particle Modeling on Sets: Towards Self-Supervised High Energy Physics Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose masked particle modeling (MPM) as a self-supervised method for learning generic, transferable, and reusable representations on unordered sets of inputs for use in high energy physics (HEP) scientific data. This work provides a novel scheme to perform masked modeling based pre-training to learn permutation invariant functions on sets. More generally, this work provides a step towards building large foundation models for HEP that can be generically pre-trained with self-supervised learning and later fine-tuned for a variety of down-stream tasks. In MPM, particles in a set are masked and the training objective is to recover their identity, as defined by a discretized token representation of a pre-trained vector quantized variational autoencoder. We study the efficacy of the method in samples of high energy jets at collider physics experiments, including studies on the impact of discretization, permutation invariance, and ordering. We also study the fine-tuning capability of the model, showing that it can be adapted to tasks such as supervised and weakly supervised jet classification, and that the model can transfer efficiently with small fine-tuning data sets to new classes and new data domains.
<div id='section'>Paperid: <span id='pid'>775, <a href='https://arxiv.org/pdf/2312.06799.pdf' target='_blank'>https://arxiv.org/pdf/2312.06799.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaobo Xia, Jun Yue, Kacper Kania, Leyuan Fang, Andrea Tagliasacchi, Kwang Moo Yi, Weiwei Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.06799">Weakly Supervised Point Cloud Segmentation via Conservative Propagation of Scene-level Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a weakly supervised semantic segmentation method for point clouds that predicts "per-point" labels from just "whole-scene" annotations. The key challenge here is the discrepancy between the target of dense per-point semantic prediction and training losses derived from only scene-level labels. To address this, in addition to the typical weakly-supervised setup that supervises all points with the scene label, we propose to conservatively propagate the scene-level labels to points selectively. Specifically, we over-segment point cloud features via unsupervised clustering in the entire dataset and form primitives. We then associate scene-level labels with primitives through bipartite matching. Then, we allow labels to pass through this primitive-label relationship, while further encouraging features to form narrow clusters around the primitives. Importantly, through bipartite matching, this additional pathway through which labels flow, only propagates scene labels to the most relevant points, reducing the potential negative impact caused by the global approach that existing methods take. We evaluate our method on ScanNet and S3DIS datasets, outperforming the state of the art by a large margin.
<div id='section'>Paperid: <span id='pid'>776, <a href='https://arxiv.org/pdf/2305.01586.pdf' target='_blank'>https://arxiv.org/pdf/2305.01586.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weixuan Sun, Zheyuan Liu, Yanhao Zhang, Yiran Zhong, Nick Barnes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.01586">An Alternative to WSSS? An Empirical Study of the Segment Anything Model (SAM) on Weakly-Supervised Semantic Segmentation Problems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Segment Anything Model (SAM) has demonstrated exceptional performance and versatility, making it a promising tool for various related tasks. In this report, we explore the application of SAM in Weakly-Supervised Semantic Segmentation (WSSS). Particularly, we adapt SAM as the pseudo-label generation pipeline given only the image-level class labels. While we observed impressive results in most cases, we also identify certain limitations. Our study includes performance evaluations on PASCAL VOC and MS-COCO, where we achieved remarkable improvements over the latest state-of-the-art methods on both datasets. We anticipate that this report encourages further explorations of adopting SAM in WSSS, as well as wider real-world applications.
<div id='section'>Paperid: <span id='pid'>777, <a href='https://arxiv.org/pdf/2211.08179.pdf' target='_blank'>https://arxiv.org/pdf/2211.08179.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joseph B. Choi, Phong C. H. Nguyen, Oishik Sen, H. S. Udaykumar, Stephen Baek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.08179">Artificial intelligence approaches for materials-by-design of energetic materials: state-of-the-art, challenges, and future directions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Artificial intelligence (AI) is rapidly emerging as an enabling tool for solving various complex materials design problems. This paper aims to review recent advances in AI-driven materials-by-design and their applications to energetic materials (EM). Trained with data from numerical simulations and/or physical experiments, AI models can assimilate trends and patterns within the design parameter space, identify optimal material designs (micro-morphologies, combinations of materials in composites, etc.), and point to designs with superior/targeted property and performance metrics. We review approaches focusing on such capabilities with respect to the three main stages of materials-by-design, namely representation learning of microstructure morphology (i.e., shape descriptors), structure-property-performance (S-P-P) linkage estimation, and optimization/design exploration. We provide a perspective view of these methods in terms of their potential, practicality, and efficacy towards the realization of materials-by-design. Specifically, methods in the literature are evaluated in terms of their capacity to learn from a small/limited number of data, computational complexity, generalizability/scalability to other material species and operating conditions, interpretability of the model predictions, and the burden of supervision/data annotation. Finally, we suggest a few promising future research directions for EM materials-by-design, such as meta-learning, active learning, Bayesian learning, and semi-/weakly-supervised learning, to bridge the gap between machine learning research and EM research.
<div id='section'>Paperid: <span id='pid'>778, <a href='https://arxiv.org/pdf/2511.18319.pdf' target='_blank'>https://arxiv.org/pdf/2511.18319.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xian Yeow Lee, Lasitha Vidyaratne, Gregory Sin, Ahmed Farahat, Chetan Gupta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.18319">Weakly-supervised Latent Models for Task-specific Visual-Language Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous inspection in hazardous environments requires AI agents that can interpret high-level goals and execute precise control. A key capability for such agents is spatial grounding, for example when a drone must center a detected object in its camera view to enable reliable inspection. While large language models provide a natural interface for specifying goals, using them directly for visual control achieves only 58\% success in this task. We envision that equipping agents with a world model as a tool would allow them to roll out candidate actions and perform better in spatially grounded settings, but conventional world models are data and compute intensive. To address this, we propose a task-specific latent dynamics model that learns state-specific action-induced shifts in a shared latent space using only goal-state supervision. The model leverages global action embeddings and complementary training losses to stabilize learning. In experiments, our approach achieves 71\% success and generalizes to unseen images and instructions, highlighting the potential of compact, domain-specific latent dynamics models for spatial alignment in autonomous inspection.
<div id='section'>Paperid: <span id='pid'>779, <a href='https://arxiv.org/pdf/2509.01899.pdf' target='_blank'>https://arxiv.org/pdf/2509.01899.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhimeng Luo, Zhendong Wang, Rui Meng, Diyang Xue, Adam Frisch, Daqing He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01899">Weakly Supervised Medical Entity Extraction and Linking for Chief Complaints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A Chief complaint (CC) is the reason for the medical visit as stated in the patient's own words. It helps medical professionals to quickly understand a patient's situation, and also serves as a short summary for medical text mining. However, chief complaint records often take a variety of entering methods, resulting in a wide variation of medical notations, which makes it difficult to standardize across different medical institutions for record keeping or text mining. In this study, we propose a weakly supervised method to automatically extract and link entities in chief complaints in the absence of human annotation. We first adopt a split-and-match algorithm to produce weak annotations, including entity mention spans and class labels, on 1.2 million real-world de-identified and IRB approved chief complaint records. Then we train a BERT-based model with generated weak labels to locate entity mentions in chief complaint text and link them to a pre-defined ontology. We conducted extensive experiments, and the results showed that our Weakly Supervised Entity Extraction and Linking (\ours) method produced superior performance over previous methods without any human annotation.
<div id='section'>Paperid: <span id='pid'>780, <a href='https://arxiv.org/pdf/2508.10104.pdf' target='_blank'>https://arxiv.org/pdf/2508.10104.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Oriane SimÃ©oni, Huy V. Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, MichaÃ«l Ramamonjisoa, Francisco Massa, Daniel Haziza, Luca Wehrstedt, Jianyuan Wang, TimothÃ©e Darcet, ThÃ©o Moutakanni, Leonel Sentana, Claire Roberts, Andrea Vedaldi, Jamie Tolan, John Brandt, Camille Couprie, Julien Mairal, HervÃ© JÃ©gou, Patrick Labatut, Piotr Bojanowski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10104">DINOv3</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-supervised learning holds the promise of eliminating the need for manual data annotation, enabling models to scale effortlessly to massive datasets and larger architectures. By not being tailored to specific tasks or domains, this training paradigm has the potential to learn visual representations from diverse sources, ranging from natural to aerial images -- using a single algorithm. This technical report introduces DINOv3, a major milestone toward realizing this vision by leveraging simple yet effective strategies. First, we leverage the benefit of scaling both dataset and model size by careful data preparation, design, and optimization. Second, we introduce a new method called Gram anchoring, which effectively addresses the known yet unsolved issue of dense feature maps degrading during long training schedules. Finally, we apply post-hoc strategies that further enhance our models' flexibility with respect to resolution, model size, and alignment with text. As a result, we present a versatile vision foundation model that outperforms the specialized state of the art across a broad range of settings, without fine-tuning. DINOv3 produces high-quality dense features that achieve outstanding performance on various vision tasks, significantly surpassing previous self- and weakly-supervised foundation models. We also share the DINOv3 suite of vision models, designed to advance the state of the art on a wide spectrum of tasks and data by providing scalable solutions for diverse resource constraints and deployment scenarios.
<div id='section'>Paperid: <span id='pid'>781, <a href='https://arxiv.org/pdf/2505.17905.pdf' target='_blank'>https://arxiv.org/pdf/2505.17905.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xie Ting, Ye Huang, Zhilin Liu, Lixin Duan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17905">Semantic segmentation with reward</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In real-world scenarios, pixel-level labeling is not always available. Sometimes, we need a semantic segmentation network, and even a visual encoder can have a high compatibility, and can be trained using various types of feedback beyond traditional labels, such as feedback that indicates the quality of the parsing results. To tackle this issue, we proposed RSS (Reward in Semantic Segmentation), the first practical application of reward-based reinforcement learning on pure semantic segmentation offered in two granular levels (pixel-level and image-level). RSS incorporates various novel technologies, such as progressive scale rewards (PSR) and pair-wise spatial difference (PSD), to ensure that the reward facilitates the convergence of the semantic segmentation network, especially under image-level rewards. Experiments and visualizations on benchmark datasets demonstrate that the proposed RSS can successfully ensure the convergence of the semantic segmentation network on two levels of rewards. Additionally, the RSS, which utilizes an image-level reward, outperforms existing weakly supervised methods that also rely solely on image-level signals during training.
<div id='section'>Paperid: <span id='pid'>782, <a href='https://arxiv.org/pdf/2505.17088.pdf' target='_blank'>https://arxiv.org/pdf/2505.17088.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmed Adel Attia, Dorottya Demszky, Jing Liu, Carol Espy-Wilson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17088">From Weak Labels to Strong Results: Utilizing 5,000 Hours of Noisy Classroom Transcripts with Minimal Accurate Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent progress in speech recognition has relied on models trained on vast amounts of labeled data. However, classroom Automatic Speech Recognition (ASR) faces the real-world challenge of abundant weak transcripts paired with only a small amount of accurate, gold-standard data. In such low-resource settings, high transcription costs make re-transcription impractical. To address this, we ask: what is the best approach when abundant inexpensive weak transcripts coexist with limited gold-standard data, as is the case for classroom speech data? We propose Weakly Supervised Pretraining (WSP), a two-step process where models are first pretrained on weak transcripts in a supervised manner, and then fine-tuned on accurate data. Our results, based on both synthetic and real weak transcripts, show that WSP outperforms alternative methods, establishing it as an effective training methodology for low-resource ASR in real-world scenarios.
<div id='section'>Paperid: <span id='pid'>783, <a href='https://arxiv.org/pdf/2412.12870.pdf' target='_blank'>https://arxiv.org/pdf/2412.12870.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenjiang Mao, Ivan Ruchkin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12870">Towards Physically Interpretable World Models: Meaningful Weakly Supervised Representations for Visual Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning models are increasingly employed for perception, prediction, and control in robotic systems. For for achieving realistic and consistent outputs, it is crucial to embed physical knowledge into their learned representations. However, doing so is difficult due to high-dimensional observation data, such as images, particularly under conditions of incomplete system knowledge and imprecise state sensing. To address this, we propose Physically Interpretable World Models, a novel architecture that aligns learned latent representations with real-world physical quantities. To this end, our architecture combines three key elements: (1) a vector-quantized image autoencoder, (2) a transformer-based physically interpretable autoencoder, and (3) a partially known dynamical model. The training incorporates weak interval-based supervision to eliminate the impractical reliance on ground-truth physical knowledge. Three case studies demonstrate that our approach achieves physical interpretability and accurate state predictions, thus advancing representation learning for robotics.
<div id='section'>Paperid: <span id='pid'>784, <a href='https://arxiv.org/pdf/2409.15801.pdf' target='_blank'>https://arxiv.org/pdf/2409.15801.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soojin Jang, Jungmin Yun, Junehyoung Kwon, Eunju Lee, Youngbin Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.15801">DIAL: Dense Image-text ALignment for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation (WSSS) approaches typically rely on class activation maps (CAMs) for initial seed generation, which often fail to capture global context due to limited supervision from image-level labels. To address this issue, we introduce DALNet, Dense Alignment Learning Network that leverages text embeddings to enhance the comprehensive understanding and precise localization of objects across different levels of granularity. Our key insight is to employ a dual-level alignment strategy: (1) Global Implicit Alignment (GIA) to capture global semantics by maximizing the similarity between the class token and the corresponding text embeddings while minimizing the similarity with background embeddings, and (2) Local Explicit Alignment (LEA) to improve object localization by utilizing spatial information from patch tokens. Moreover, we propose a cross-contrastive learning approach that aligns foreground features between image and text modalities while separating them from the background, encouraging activation in missing regions and suppressing distractions. Through extensive experiments on the PASCAL VOC and MS COCO datasets, we demonstrate that DALNet significantly outperforms state-of-the-art WSSS methods. Our approach, in particular, allows for more efficient end-to-end process as a single-stage method.
<div id='section'>Paperid: <span id='pid'>785, <a href='https://arxiv.org/pdf/2409.04011.pdf' target='_blank'>https://arxiv.org/pdf/2409.04011.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weijie He, Mushui Liu, Yunlong Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.04011">Hybrid Mask Generation for Infrared Small Target Detection with Single-Point Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single-frame infrared small target (SIRST) detection poses a significant challenge due to the requirement to discern minute targets amidst complex infrared background clutter. In this paper, we focus on a weakly-supervised paradigm to obtain high-quality pseudo masks from the point-level annotation by integrating a novel learning-free method with the hybrid of the learning-based method. The learning-free method adheres to a sequential process, progressing from a point annotation to the bounding box that encompasses the target, and subsequently to detailed pseudo masks, while the hybrid is achieved through filtering out false alarms and retrieving missed detections in the network's prediction to provide a reliable supplement for learning-free masks. The experimental results show that our learning-free method generates pseudo masks with an average Intersection over Union (IoU) that is 4.3% higher than the second-best learning-free competitor across three datasets, while the hybrid learning-based method further enhances the quality of pseudo masks, achieving an additional average IoU increase of 3.4%.
<div id='section'>Paperid: <span id='pid'>786, <a href='https://arxiv.org/pdf/2407.20818.pdf' target='_blank'>https://arxiv.org/pdf/2407.20818.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingcheng Zhou, Deyu Fu, Walter Zimmer, Mingyu Liu, Venkatnarayanan Lakshminarasimhan, Leah Strand, Alois C. Knoll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.20818">WARM-3D: A Weakly-Supervised Sim2Real Domain Adaptation Framework for Roadside Monocular 3D Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing roadside perception systems are limited by the absence of publicly available, large-scale, high-quality 3D datasets. Exploring the use of cost-effective, extensive synthetic datasets offers a viable solution to tackle this challenge and enhance the performance of roadside monocular 3D detection. In this study, we introduce the TUMTraf Synthetic Dataset, offering a diverse and substantial collection of high-quality 3D data to augment scarce real-world datasets. Besides, we present WARM-3D, a concise yet effective framework to aid the Sim2Real domain transfer for roadside monocular 3D detection. Our method leverages cheap synthetic datasets and 2D labels from an off-the-shelf 2D detector for weak supervision. We show that WARM-3D significantly enhances performance, achieving a +12.40% increase in mAP 3D over the baseline with only pseudo-2D supervision. With 2D GT as weak labels, WARM-3D even reaches performance close to the Oracle baseline. Moreover, WARM-3D improves the ability of 3D detectors to unseen sample recognition across various real-world environments, highlighting its potential for practical applications.
<div id='section'>Paperid: <span id='pid'>787, <a href='https://arxiv.org/pdf/2404.00257.pdf' target='_blank'>https://arxiv.org/pdf/2404.00257.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qian Wan, Xiang Xiang, Qinhao Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00257">YOLOOC: YOLO-based Open-Class Incremental Object Detection with Novel Class Discovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Because of its use in practice, open-world object detection (OWOD) has gotten a lot of attention recently. The challenge is how can a model detect novel classes and then incrementally learn them without forgetting previously known classes. Previous approaches hinge on strongly-supervised or weakly-supervised novel-class data for novel-class detection, which may not apply to real applications. We construct a new benchmark that novel classes are only encountered at the inference stage. And we propose a new OWOD detector YOLOOC, based on the YOLO architecture yet for the Open-Class setup. We introduce label smoothing to prevent the detector from over-confidently mapping novel classes to known classes and to discover novel classes. Extensive experiments conducted on our more realistic setup demonstrate the effectiveness of our method for discovering novel classes in our new benchmark.
<div id='section'>Paperid: <span id='pid'>788, <a href='https://arxiv.org/pdf/2401.09709.pdf' target='_blank'>https://arxiv.org/pdf/2401.09709.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zipeng Wang, Xuehui Yu, Xumeng Han, Wenwen Yu, Zhixun Huang, Jianbin Jiao, Zhenjun Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.09709">P2Seg: Pointly-supervised Segmentation via Mutual Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Point-level Supervised Instance Segmentation (PSIS) aims to enhance the applicability and scalability of instance segmentation by utilizing low-cost yet instance-informative annotations. Existing PSIS methods usually rely on positional information to distinguish objects, but predicting precise boundaries remains challenging due to the lack of contour annotations. Nevertheless, weakly supervised semantic segmentation methods are proficient in utilizing intra-class feature consistency to capture the boundary contours of the same semantic regions. In this paper, we design a Mutual Distillation Module (MDM) to leverage the complementary strengths of both instance position and semantic information and achieve accurate instance-level object perception. The MDM consists of Semantic to Instance (S2I) and Instance to Semantic (I2S). S2I is guided by the precise boundaries of semantic regions to learn the association between annotated points and instance contours. I2S leverages discriminative relationships between instances to facilitate the differentiation of various objects within the semantic map. Extensive experiments substantiate the efficacy of MDM in fostering the synergy between instance and semantic information, consequently improving the quality of instance-level object representations. Our method achieves 55.7 mAP$_{50}$ and 17.6 mAP on the PASCAL VOC and MS COCO datasets, significantly outperforming recent PSIS methods and several box-supervised instance segmentation competitors.
<div id='section'>Paperid: <span id='pid'>789, <a href='https://arxiv.org/pdf/2311.15605.pdf' target='_blank'>https://arxiv.org/pdf/2311.15605.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ozan Unal, Dengxin Dai, Lukas Hoyer, Yigit Baran Can, Luc Van Gool
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.15605">2D Feature Distillation for Weakly- and Semi-Supervised 3D Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As 3D perception problems grow in popularity and the need for large-scale labeled datasets for LiDAR semantic segmentation increase, new methods arise that aim to reduce the necessity for dense annotations by employing weakly-supervised training. However these methods continue to show weak boundary estimation and high false negative rates for small objects and distant sparse regions. We argue that such weaknesses can be compensated by using RGB images which provide a denser representation of the scene. We propose an image-guidance network (IGNet) which builds upon the idea of distilling high level feature information from a domain adapted synthetically trained 2D semantic segmentation network. We further utilize a one-way contrastive learning scheme alongside a novel mixing strategy called FOVMix, to combat the horizontal field-of-view mismatch between the two sensors and enhance the effects of image guidance. IGNet achieves state-of-the-art results for weakly-supervised LiDAR semantic segmentation on ScribbleKITTI, boasting up to 98% relative performance to fully supervised training with only 8% labeled points, while introducing no additional annotation burden or computational/memory cost during inference. Furthermore, we show that our contributions also prove effective for semi-supervised training, where IGNet claims state-of-the-art results on both ScribbleKITTI and SemanticKITTI.
<div id='section'>Paperid: <span id='pid'>790, <a href='https://arxiv.org/pdf/2311.01161.pdf' target='_blank'>https://arxiv.org/pdf/2311.01161.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kang-il Lee, Segwang Kim, Kyomin Jung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.01161">Weakly Supervised Semantic Parsing with Execution-based Spurious Program Filtering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The problem of spurious programs is a longstanding challenge when training a semantic parser from weak supervision. To eliminate such programs that have wrong semantics but correct denotation, existing methods focus on exploiting similarities between examples based on domain-specific knowledge. In this paper, we propose a domain-agnostic filtering mechanism based on program execution results. Specifically, for each program obtained through the search process, we first construct a representation that captures the program's semantics as execution results under various inputs. Then, we run a majority vote on these representations to identify and filter out programs with significantly different semantics from the other programs. In particular, our method is orthogonal to the program search process so that it can easily augment any of the existing weakly supervised semantic parsing frameworks. Empirical evaluations on the Natural Language Visual Reasoning and WikiTableQuestions demonstrate that applying our method to the existing semantic parsers induces significantly improved performances.
<div id='section'>Paperid: <span id='pid'>791, <a href='https://arxiv.org/pdf/2310.09265.pdf' target='_blank'>https://arxiv.org/pdf/2310.09265.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chufan Gao, Xulin Fan, Jimeng Sun, Xuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.09265">PromptRE: Weakly-Supervised Document-Level Relation Extraction via Prompting-Based Data Programming</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Relation extraction aims to classify the relationships between two entities into pre-defined categories. While previous research has mainly focused on sentence-level relation extraction, recent studies have expanded the scope to document-level relation extraction. Traditional relation extraction methods heavily rely on human-annotated training data, which is time-consuming and labor-intensive. To mitigate the need for manual annotation, recent weakly-supervised approaches have been developed for sentence-level relation extraction while limited work has been done on document-level relation extraction. Weakly-supervised document-level relation extraction faces significant challenges due to an imbalanced number "no relation" instances and the failure of directly probing pretrained large language models for document relation extraction. To address these challenges, we propose PromptRE, a novel weakly-supervised document-level relation extraction method that combines prompting-based techniques with data programming. Furthermore, PromptRE incorporates the label distribution and entity types as prior knowledge to improve the performance. By leveraging the strengths of both prompting and data programming, PromptRE achieves improved performance in relation classification and effectively handles the "no relation" problem. Experimental results on ReDocRED, a benchmark dataset for document-level relation extraction, demonstrate the superiority of PromptRE over baseline approaches.
<div id='section'>Paperid: <span id='pid'>792, <a href='https://arxiv.org/pdf/2310.08084.pdf' target='_blank'>https://arxiv.org/pdf/2310.08084.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiuhui Chen, Haiying Lyu, Xinyue Hu, Yong Lu, Yi Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.08084">Volumetric Medical Image Segmentation via Scribble Annotations and Shape Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, weakly-supervised image segmentation using weak annotations like scribbles has gained great attention in computer vision and medical image analysis, since such annotations are much easier to obtain compared to time-consuming and labor-intensive labeling at the pixel/voxel level. However, due to a lack of structure supervision on regions of interest (ROIs), existing scribble-based methods suffer from poor boundary localization. Furthermore, most current methods are designed for 2D image segmentation, which do not fully leverage the volumetric information if directly applied to each image slice. In this paper, we propose a scribble-based volumetric image segmentation, Scribble2D5, which tackles 3D anisotropic image segmentation and aims to its improve boundary prediction. To achieve this, we augment a 2.5D attention UNet with a proposed label propagation module to extend semantic information from scribbles and use a combination of static and active boundary prediction to learn ROI's boundary and regularize its shape. Also, we propose an optional add-on component, which incorporates the shape prior information from unpaired segmentation masks to further improve model accuracy. Extensive experiments on three public datasets and one private dataset demonstrate our Scribble2D5 achieves state-of-the-art performance on volumetric image segmentation using scribbles and shape prior if available.
<div id='section'>Paperid: <span id='pid'>793, <a href='https://arxiv.org/pdf/2308.11052.pdf' target='_blank'>https://arxiv.org/pdf/2308.11052.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>M. Maruf, Arka Daw, Amartya Dutta, Jie Bu, Anuj Karpatne
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.11052">Beyond Discriminative Regions: Saliency Maps as Alternatives to CAMs for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, several Weakly Supervised Semantic Segmentation (WS3) methods have been proposed that use class activation maps (CAMs) generated by a classifier to produce pseudo-ground truths for training segmentation models. While CAMs are good at highlighting discriminative regions (DR) of an image, they are known to disregard regions of the object that do not contribute to the classifier's prediction, termed non-discriminative regions (NDR). In contrast, attribution methods such as saliency maps provide an alternative approach for assigning a score to every pixel based on its contribution to the classification prediction. This paper provides a comprehensive comparison between saliencies and CAMs for WS3. Our study includes multiple perspectives on understanding their similarities and dissimilarities. Moreover, we provide new evaluation metrics that perform a comprehensive assessment of WS3 performance of alternative methods w.r.t. CAMs. We demonstrate the effectiveness of saliencies in addressing the limitation of CAMs through our empirical studies on benchmark datasets. Furthermore, we propose random cropping as a stochastic aggregation technique that improves the performance of saliency, making it a strong alternative to CAM for WS3.
<div id='section'>Paperid: <span id='pid'>794, <a href='https://arxiv.org/pdf/2306.01191.pdf' target='_blank'>https://arxiv.org/pdf/2306.01191.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alireza Javanmardi, Yusuf Sale, Paul Hofman, Eyke HÃ¼llermeier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.01191">Conformal Prediction with Partially Labeled Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While the predictions produced by conformal prediction are set-valued, the data used for training and calibration is supposed to be precise. In the setting of superset learning or learning from partial labels, a variant of weakly supervised learning, it is exactly the other way around: training data is possibly imprecise (set-valued), but the model induced from this data yields precise predictions. In this paper, we combine the two settings by making conformal prediction amenable to set-valued training data. We propose a generalization of the conformal prediction procedure that can be applied to set-valued training and calibration data. We prove the validity of the proposed method and present experimental studies in which it compares favorably to natural baselines.
<div id='section'>Paperid: <span id='pid'>795, <a href='https://arxiv.org/pdf/2305.17891.pdf' target='_blank'>https://arxiv.org/pdf/2305.17891.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Linhao Qu, Xiaoyuan Luo, Kexue Fu, Manning Wang, Zhijian Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.17891">The Rise of AI Language Pathologists: Exploring Two-level Prompt Learning for Few-shot Weakly-supervised Whole Slide Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces the novel concept of few-shot weakly supervised learning for pathology Whole Slide Image (WSI) classification, denoted as FSWC. A solution is proposed based on prompt learning and the utilization of a large language model, GPT-4. Since a WSI is too large and needs to be divided into patches for processing, WSI classification is commonly approached as a Multiple Instance Learning (MIL) problem. In this context, each WSI is considered a bag, and the obtained patches are treated as instances. The objective of FSWC is to classify both bags and instances with only a limited number of labeled bags. Unlike conventional few-shot learning problems, FSWC poses additional challenges due to its weak bag labels within the MIL framework. Drawing inspiration from the recent achievements of vision-language models (V-L models) in downstream few-shot classification tasks, we propose a two-level prompt learning MIL framework tailored for pathology, incorporating language prior knowledge. Specifically, we leverage CLIP to extract instance features for each patch, and introduce a prompt-guided pooling strategy to aggregate these instance features into a bag feature. Subsequently, we employ a small number of labeled bags to facilitate few-shot prompt learning based on the bag features. Our approach incorporates the utilization of GPT-4 in a question-and-answer mode to obtain language prior knowledge at both the instance and bag levels, which are then integrated into the instance and bag level language prompts. Additionally, a learnable component of the language prompts is trained using the available few-shot labeled data. We conduct extensive experiments on three real WSI datasets encompassing breast cancer, lung cancer, and cervical cancer, demonstrating the notable performance of the proposed method in bag and instance classification. All codes will be available.
<div id='section'>Paperid: <span id='pid'>796, <a href='https://arxiv.org/pdf/2304.07082.pdf' target='_blank'>https://arxiv.org/pdf/2304.07082.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zongheng Tang, Yifan Sun, Si Liu, Yi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.07082">DETR with Additional Global Aggregation for Cross-domain Weakly Supervised Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a DETR-based method for cross-domain weakly supervised object detection (CDWSOD), aiming at adapting the detector from source to target domain through weak supervision. We think DETR has strong potential for CDWSOD due to an insight: the encoder and the decoder in DETR are both based on the attention mechanism and are thus capable of aggregating semantics across the entire image. The aggregation results, i.e., image-level predictions, can naturally exploit the weak supervision for domain alignment. Such motivated, we propose DETR with additional Global Aggregation (DETR-GA), a CDWSOD detector that simultaneously makes "instance-level + image-level" predictions and utilizes "strong + weak" supervisions. The key point of DETR-GA is very simple: for the encoder / decoder, we respectively add multiple class queries / a foreground query to aggregate the semantics into image-level predictions. Our query-based aggregation has two advantages. First, in the encoder, the weakly-supervised class queries are capable of roughly locating the corresponding positions and excluding the distraction from non-relevant regions. Second, through our design, the object queries and the foreground query in the decoder share consensus on the class semantics, therefore making the strong and weak supervision mutually benefit each other for domain alignment. Extensive experiments on four popular cross-domain benchmarks show that DETR-GA significantly improves CSWSOD and advances the states of the art (e.g., 29.0% --> 79.4% mAP on PASCAL VOC --> Clipart_all dataset).
<div id='section'>Paperid: <span id='pid'>797, <a href='https://arxiv.org/pdf/2303.17688.pdf' target='_blank'>https://arxiv.org/pdf/2303.17688.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aiyu Cui, Sen He, Tao Xiang, Antoine Toisoul
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.17688">Learning Garment DensePose for Robust Warping in Virtual Try-On</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual try-on, i.e making people virtually try new garments, is an active research area in computer vision with great commercial applications. Current virtual try-on methods usually work in a two-stage pipeline. First, the garment image is warped on the person's pose using a flow estimation network. Then in the second stage, the warped garment is fused with the person image to render a new try-on image. Unfortunately, such methods are heavily dependent on the quality of the garment warping which often fails when dealing with hard poses (e.g., a person lifting or crossing arms). In this work, we propose a robust warping method for virtual try-on based on a learned garment DensePose which has a direct correspondence with the person's DensePose. Due to the lack of annotated data, we show how to leverage an off-the-shelf person DensePose model and a pretrained flow model to learn the garment DensePose in a weakly supervised manner. The garment DensePose allows a robust warping to any person's pose without any additional computation. Our method achieves the state-of-the-art equivalent on virtual try-on benchmarks and shows warping robustness on in-the-wild person images with hard poses, making it more suited for real-world virtual try-on applications.
<div id='section'>Paperid: <span id='pid'>798, <a href='https://arxiv.org/pdf/2303.11630.pdf' target='_blank'>https://arxiv.org/pdf/2303.11630.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Yang, Lin Song, Yixiao Ge, Xiu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.11630">BoxSnake: Polygonal Instance Segmentation with Box Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Box-supervised instance segmentation has gained much attention as it requires only simple box annotations instead of costly mask or polygon annotations. However, existing box-supervised instance segmentation models mainly focus on mask-based frameworks. We propose a new end-to-end training technique, termed BoxSnake, to achieve effective polygonal instance segmentation using only box annotations for the first time. Our method consists of two loss functions: (1) a point-based unary loss that constrains the bounding box of predicted polygons to achieve coarse-grained segmentation; and (2) a distance-aware pairwise loss that encourages the predicted polygons to fit the object boundaries. Compared with the mask-based weakly-supervised methods, BoxSnake further reduces the performance gap between the predicted segmentation and the bounding box, and shows significant superiority on the Cityscapes dataset. The code has been available publicly.
<div id='section'>Paperid: <span id='pid'>799, <a href='https://arxiv.org/pdf/2212.14615.pdf' target='_blank'>https://arxiv.org/pdf/2212.14615.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hasan Md Tusfiqur, Duy M. H. Nguyen, Mai T. N. Truong, Triet A. Nguyen, Binh T. Nguyen, Michael Barz, Hans-Juergen Profitlich, Ngoc T. T. Than, Ngan Le, Pengtao Xie, Daniel Sonntag
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.14615">DRG-Net: Interactive Joint Learning of Multi-lesion Segmentation and Classification for Diabetic Retinopathy Grading</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diabetic Retinopathy (DR) is a leading cause of vision loss in the world, and early DR detection is necessary to prevent vision loss and support an appropriate treatment. In this work, we leverage interactive machine learning and introduce a joint learning framework, termed DRG-Net, to effectively learn both disease grading and multi-lesion segmentation. Our DRG-Net consists of two modules: (i) DRG-AI-System to classify DR Grading, localize lesion areas, and provide visual explanations; (ii) DRG-Expert-Interaction to receive feedback from user-expert and improve the DRG-AI-System. To deal with sparse data, we utilize transfer learning mechanisms to extract invariant feature representations by using Wasserstein distance and adversarial learning-based entropy minimization. Besides, we propose a novel attention strategy at both low- and high-level features to automatically select the most significant lesion information and provide explainable properties. In terms of human interaction, we further develop DRG-Net as a tool that enables expert users to correct the system's predictions, which may then be used to update the system as a whole. Moreover, thanks to the attention mechanism and loss functions constraint between lesion features and classification features, our approach can be robust given a certain level of noise in the feedback of users. We have benchmarked DRG-Net on the two largest DR datasets, i.e., IDRID and FGADR, and compared it to various state-of-the-art deep learning networks. In addition to outperforming other SOTA approaches, DRG-Net is effectively updated using user feedback, even in a weakly-supervised manner.
<div id='section'>Paperid: <span id='pid'>800, <a href='https://arxiv.org/pdf/2212.06921.pdf' target='_blank'>https://arxiv.org/pdf/2212.06921.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dylan Sam, J. Zico Kolter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.06921">Losses over Labels: Weakly Supervised Learning via Direct Loss Construction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Owing to the prohibitive costs of generating large amounts of labeled data, programmatic weak supervision is a growing paradigm within machine learning. In this setting, users design heuristics that provide noisy labels for subsets of the data. These weak labels are combined (typically via a graphical model) to form pseudolabels, which are then used to train a downstream model. In this work, we question a foundational premise of the typical weakly supervised learning pipeline: given that the heuristic provides all ``label" information, why do we need to generate pseudolabels at all? Instead, we propose to directly transform the heuristics themselves into corresponding loss functions that penalize differences between our model and the heuristic. By constructing losses directly from the heuristics, we can incorporate more information than is used in the standard weakly supervised pipeline, such as how the heuristics make their decisions, which explicitly informs feature selection during training. We call our method Losses over Labels (LoL) as it creates losses directly from heuristics without going through the intermediate step of a label. We show that LoL improves upon existing weak supervision methods on several benchmark text and image classification tasks and further demonstrate that incorporating gradient information leads to better performance on almost every task.
<div id='section'>Paperid: <span id='pid'>801, <a href='https://arxiv.org/pdf/2212.06515.pdf' target='_blank'>https://arxiv.org/pdf/2212.06515.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pei Liu, Luping Ji, Feng Ye, Bo Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.06515">AdvMIL: Adversarial Multiple Instance Learning for the Survival Analysis on Whole-Slide Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The survival analysis on histological whole-slide images (WSIs) is one of the most important means to estimate patient prognosis. Although many weakly-supervised deep learning models have been developed for gigapixel WSIs, their potential is generally restricted by classical survival analysis rules and fully-supervised learning requirements. As a result, these models provide patients only with a completely-certain point estimation of time-to-event, and they could only learn from the labeled WSI data currently at a small scale. To tackle these problems, we propose a novel adversarial multiple instance learning (AdvMIL) framework. This framework is based on adversarial time-to-event modeling, and integrates the multiple instance learning (MIL) that is much necessary for WSI representation learning. It is a plug-and-play one, so that most existing MIL-based end-to-end methods can be easily upgraded by applying this framework, gaining the improved abilities of survival distribution estimation and semi-supervised learning. Our extensive experiments show that AdvMIL not only could often bring performance improvement to mainstream WSI survival analysis methods at a relatively low computational cost, but also enables these methods to effectively utilize unlabeled data via semi-supervised learning. Moreover, it is observed that AdvMIL could help improving the robustness of models against patch occlusion and two representative image noises. The proposed AdvMIL framework could promote the research of survival analysis in computational pathology with its novel adversarial MIL paradigm.
<div id='section'>Paperid: <span id='pid'>802, <a href='https://arxiv.org/pdf/2211.02499.pdf' target='_blank'>https://arxiv.org/pdf/2211.02499.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jian Xue, Peidong Wang, Jinyu Li, Eric Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.02499">A Weakly-Supervised Streaming Multilingual Speech Model with Truly Zero-Shot Capability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce our work of building a Streaming Multilingual Speech Model (SM2), which can transcribe or translate multiple spoken languages into texts of the target language. The backbone of SM2 is Transformer Transducer, which has high streaming capability. Instead of human labeled speech translation (ST) data, SM2 models are trained using weakly supervised data generated by converting the transcriptions in speech recognition corpora with a machine translation service. With 351 thousand hours of anonymized speech training data from 25 languages, SM2 models achieve comparable or even better ST quality than some recent popular large-scale non-streaming speech models. More importantly, we show that SM2 has the truly zero-shot capability when expanding to new target languages, yielding high quality ST results for {source-speech, target-text} pairs that are not seen during training.
<div id='section'>Paperid: <span id='pid'>803, <a href='https://arxiv.org/pdf/2210.10669.pdf' target='_blank'>https://arxiv.org/pdf/2210.10669.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tunazzina Islam, Shamik Roy, Dan Goldwasser
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.10669">Weakly Supervised Learning for Analyzing Political Campaigns on Facebook</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Social media platforms are currently the main channel for political messaging, allowing politicians to target specific demographics and adapt based on their reactions. However, making this communication transparent is challenging, as the messaging is tightly coupled with its intended audience and often echoed by multiple stakeholders interested in advancing specific policies. Our goal in this paper is to take a first step towards understanding these highly decentralized settings. We propose a weakly supervised approach to identify the stance and issue of political ads on Facebook and analyze how political campaigns use some kind of demographic targeting by location, gender, or age. Furthermore, we analyze the temporal dynamics of the political ads on election polls.
<div id='section'>Paperid: <span id='pid'>804, <a href='https://arxiv.org/pdf/2108.08988.pdf' target='_blank'>https://arxiv.org/pdf/2108.08988.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tunazzina Islam, Dan Goldwasser
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2108.08988">Twitter User Representation Using Weakly Supervised Graph Embedding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Social media platforms provide convenient means for users to participate in multiple online activities on various contents and create fast widespread interactions. However, this rapidly growing access has also increased the diverse information, and characterizing user types to understand people's lifestyle decisions shared in social media is challenging. In this paper, we propose a weakly supervised graph embedding based framework for understanding user types. We evaluate the user embedding learned using weak supervision over well-being related tweets from Twitter, focusing on 'Yoga', 'Keto diet'. Experiments on real-world datasets demonstrate that the proposed framework outperforms the baselines for detecting user types. Finally, we illustrate data analysis on different types of users (e.g., practitioner vs. promotional) from our dataset. While we focus on lifestyle-related tweets (i.e., yoga, keto), our method for constructing user representation readily generalizes to other domains.
<div id='section'>Paperid: <span id='pid'>805, <a href='https://arxiv.org/pdf/2511.17619.pdf' target='_blank'>https://arxiv.org/pdf/2511.17619.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qinghao Meng, Junbo Yin, Jianbing Shen, Yunde Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17619">Rethinking the Encoding and Annotating of 3D Bounding Box: Corner-Aware 3D Object Detection from Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Center-aligned regression remains dominant in LiDAR-based 3D object detection, yet it suffers from fundamental instability: object centers often fall in sparse or empty regions of the bird's-eye-view (BEV) due to the front-surface-biased nature of LiDAR point clouds, leading to noisy and inaccurate bounding box predictions. To circumvent this limitation, we revisit bounding box representation and propose corner-aligned regression, which shifts the prediction target from unstable centers to geometrically informative corners that reside in dense, observable regions. Leveraging the inherent geometric constraints among corners and image 2D boxes, partial parameters of 3D bounding boxes can be recovered from corner annotations, enabling a weakly supervised paradigm without requiring complete 3D labels. We design a simple yet effective corner-aware detection head that can be plugged into existing detectors. Experiments on KITTI show our method improves performance by 3.5% AP over center-based baseline, and achieves 83% of fully supervised accuracy using only BEV corner clicks, demonstrating the effectiveness of our corner-aware regression strategy.
<div id='section'>Paperid: <span id='pid'>806, <a href='https://arxiv.org/pdf/2511.14832.pdf' target='_blank'>https://arxiv.org/pdf/2511.14832.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marie Hein, Gregor Kasieczka, Michael Krämer, Louis Moureaux, Alexander Mück, David Shih
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.14832">How to pick the best anomaly detector?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anomaly detection has the potential to discover new physics in unexplored regions of the data. However, choosing the best anomaly detector for a given data set in a model-agnostic way is an important challenge which has hitherto largely been neglected. In this paper, we introduce the data-driven ARGOS metric, which has a sound theoretical foundation and is empirically shown to robustly select the most sensitive anomaly detection model given the data. Focusing on weakly-supervised, classifier-based anomaly detection methods, we show that the ARGOS metric outperforms other model selection metrics previously used in the literature, in particular the binary cross-entropy loss. We explore several realistic applications, including hyperparameter tuning as well as architecture and feature selection, and in all cases we demonstrate that ARGOS is robust to the noisy conditions of anomaly detection.
<div id='section'>Paperid: <span id='pid'>807, <a href='https://arxiv.org/pdf/2509.13116.pdf' target='_blank'>https://arxiv.org/pdf/2509.13116.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruibo Li, Hanyu Shi, Zhe Wang, Guosheng Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13116">Weakly and Self-Supervised Class-Agnostic Motion Prediction for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding motion in dynamic environments is critical for autonomous driving, thereby motivating research on class-agnostic motion prediction. In this work, we investigate weakly and self-supervised class-agnostic motion prediction from LiDAR point clouds. Outdoor scenes typically consist of mobile foregrounds and static backgrounds, allowing motion understanding to be associated with scene parsing. Based on this observation, we propose a novel weakly supervised paradigm that replaces motion annotations with fully or partially annotated (1%, 0.1%) foreground/background masks for supervision. To this end, we develop a weakly supervised approach utilizing foreground/background cues to guide the self-supervised learning of motion prediction models. Since foreground motion generally occurs in non-ground regions, non-ground/ground masks can serve as an alternative to foreground/background masks, further reducing annotation effort. Leveraging non-ground/ground cues, we propose two additional approaches: a weakly supervised method requiring fewer (0.01%) foreground/background annotations, and a self-supervised method without annotations. Furthermore, we design a Robust Consistency-aware Chamfer Distance loss that incorporates multi-frame information and robust penalty functions to suppress outliers in self-supervised learning. Experiments show that our weakly and self-supervised models outperform existing self-supervised counterparts, and our weakly supervised models even rival some supervised ones. This demonstrates that our approaches effectively balance annotation effort and performance.
<div id='section'>Paperid: <span id='pid'>808, <a href='https://arxiv.org/pdf/2508.07877.pdf' target='_blank'>https://arxiv.org/pdf/2508.07877.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>WonJun Moon, Hyun Seok Seong, Jae-Pil Heo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07877">Selective Contrastive Learning for Weakly Supervised Affordance Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Facilitating an entity's interaction with objects requires accurately identifying parts that afford specific actions. Weakly supervised affordance grounding (WSAG) seeks to imitate human learning from third-person demonstrations, where humans intuitively grasp functional parts without needing pixel-level annotations. To achieve this, grounding is typically learned using a shared classifier across images from different perspectives, along with distillation strategies incorporating part discovery process. However, since affordance-relevant parts are not always easily distinguishable, models primarily rely on classification, often focusing on common class-specific patterns that are unrelated to affordance. To address this limitation, we move beyond isolated part-level learning by introducing selective prototypical and pixel contrastive objectives that adaptively learn affordance-relevant cues at both the part and object levels, depending on the granularity of the available information. Initially, we find the action-associated objects in both egocentric (object-focused) and exocentric (third-person example) images by leveraging CLIP. Then, by cross-referencing the discovered objects of complementary views, we excavate the precise part-level affordance clues in each perspective. By consistently learning to distinguish affordance-relevant regions from affordance-irrelevant background context, our approach effectively shifts activation from irrelevant areas toward meaningful affordance cues. Experimental results demonstrate the effectiveness of our method. Codes are available at github.com/hynnsk/SelectiveCL.
<div id='section'>Paperid: <span id='pid'>809, <a href='https://arxiv.org/pdf/2506.18261.pdf' target='_blank'>https://arxiv.org/pdf/2506.18261.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Su, Dong Xu, Luping Zhou, Wanli Ouyang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18261">Improving Weakly Supervised Temporal Action Localization by Exploiting Multi-resolution Information in Temporal Domain</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised temporal action localization is a challenging task as only the video-level annotation is available during the training process. To address this problem, we propose a two-stage approach to fully exploit multi-resolution information in the temporal domain and generate high quality frame-level pseudo labels based on both appearance and motion streams. Specifically, in the first stage, we generate reliable initial frame-level pseudo labels, and in the second stage, we iteratively refine the pseudo labels and use a set of selected frames with highly confident pseudo labels to train neural networks and better predict action class scores at each frame. We fully exploit temporal information at multiple scales to improve temporal action localization performance. Specifically, in order to obtain reliable initial frame-level pseudo labels, in the first stage, we propose an Initial Label Generation (ILG) module, which leverages temporal multi-resolution consistency to generate high quality class activation sequences (CASs), which consist of a number of sequences with each sequence measuring how likely each video frame belongs to one specific action class. In the second stage, we propose a Progressive Temporal Label Refinement (PTLR) framework. In our PTLR framework, two networks called Network-OTS and Network-RTS, which are respectively used to generate CASs for the original temporal scale and the reduced temporal scales, are used as two streams (i.e., the OTS stream and the RTS stream) to refine the pseudo labels in turn. By this way, the multi-resolution information in the temporal domain is exchanged at the pseudo label level, and our work can help improve each stream (i.e., the OTS/RTS stream) by exploiting the refined pseudo labels from another stream (i.e., the RTS/OTS stream).
<div id='section'>Paperid: <span id='pid'>810, <a href='https://arxiv.org/pdf/2504.18866.pdf' target='_blank'>https://arxiv.org/pdf/2504.18866.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxu Leng, Zhanjie Wu, Mingpi Tan, Mengjingcheng Mo, Jiankang Zheng, Qingqing Li, Ji Gan, Xinbo Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.18866">PiercingEye: Dual-Space Video Violence Detection with Hyperbolic Vision-Language Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing weakly supervised video violence detection (VVD) methods primarily rely on Euclidean representation learning, which often struggles to distinguish visually similar yet semantically distinct events due to limited hierarchical modeling and insufficient ambiguous training samples. To address this challenge, we propose PiercingEye, a novel dual-space learning framework that synergizes Euclidean and hyperbolic geometries to enhance discriminative feature representation. Specifically, PiercingEye introduces a layer-sensitive hyperbolic aggregation strategy with hyperbolic Dirichlet energy constraints to progressively model event hierarchies, and a cross-space attention mechanism to facilitate complementary feature interactions between Euclidean and hyperbolic spaces. Furthermore, to mitigate the scarcity of ambiguous samples, we leverage large language models to generate logic-guided ambiguous event descriptions, enabling explicit supervision through a hyperbolic vision-language contrastive loss that prioritizes high-confusion samples via dynamic similarity-aware weighting. Extensive experiments on XD-Violence and UCF-Crime benchmarks demonstrate that PiercingEye achieves state-of-the-art performance, with particularly strong results on a newly curated ambiguous event subset, validating its superior capability in fine-grained violence detection.
<div id='section'>Paperid: <span id='pid'>811, <a href='https://arxiv.org/pdf/2502.03382.pdf' target='_blank'>https://arxiv.org/pdf/2502.03382.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tom Labiausse, Laurent MazarÃ©, Edouard Grave, Patrick PÃ©rez, Alexandre DÃ©fossez, Neil Zeghidour
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03382">High-Fidelity Simultaneous Speech-To-Speech Translation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Hibiki, a decoder-only model for simultaneous speech translation. Hibiki leverages a multistream language model to synchronously process source and target speech, and jointly produces text and audio tokens to perform speech-to-text and speech-to-speech translation. We furthermore address the fundamental challenge of simultaneous interpretation, which unlike its consecutive counterpart, where one waits for the end of the source utterance to start translating, adapts its flow to accumulate just enough context to produce a correct translation in real-time, chunk by chunk. To do so, we introduce a weakly-supervised method that leverages the perplexity of an off-the-shelf text translation system to identify optimal delays on a per-word basis and create aligned synthetic data. After supervised training, Hibiki performs adaptive, simultaneous speech translation with vanilla temperature sampling. On a French-English simultaneous speech translation task, Hibiki demonstrates state-of-the-art performance in translation quality, speaker fidelity and naturalness. Moreover, the simplicity of its inference process makes it compatible with batched translation and even real-time on-device deployment. We provide examples as well as models and inference code.
<div id='section'>Paperid: <span id='pid'>812, <a href='https://arxiv.org/pdf/2502.01455.pdf' target='_blank'>https://arxiv.org/pdf/2502.01455.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrea Marelli, Luca Magri, Federica Arrigoni, Giacomo Boracchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01455">Temporal-consistent CAMs for Weakly Supervised Video Segmentation in Waste Sorting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In industrial settings, weakly supervised (WS) methods are usually preferred over their fully supervised (FS) counterparts as they do not require costly manual annotations. Unfortunately, the segmentation masks obtained in the WS regime are typically poor in terms of accuracy. In this work, we present a WS method capable of producing accurate masks for semantic segmentation in the case of video streams. More specifically, we build saliency maps that exploit the temporal coherence between consecutive frames in a video, promoting consistency when objects appear in different frames. We apply our method in a waste-sorting scenario, where we perform weakly supervised video segmentation (WSVS) by training an auxiliary classifier that distinguishes between videos recorded before and after a human operator, who manually removes specific wastes from a conveyor belt. The saliency maps of this classifier identify materials to be removed, and we modify the classifier training to minimize differences between the saliency map of a central frame and those in adjacent frames, after having compensated object displacement. Experiments on a real-world dataset demonstrate the benefits of integrating temporal coherence directly during the training phase of the classifier. Code and dataset are available upon request.
<div id='section'>Paperid: <span id='pid'>813, <a href='https://arxiv.org/pdf/2412.18842.pdf' target='_blank'>https://arxiv.org/pdf/2412.18842.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Heng-Bo Fan, Ming-Kun Xie, Jia-Hao Xiao, Sheng-Jun Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.18842">Context-Based Semantic-Aware Alignment for Semi-Supervised Multi-Label Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the lack of extensive precisely-annotated multi-label data in real word, semi-supervised multi-label learning (SSMLL) has gradually gained attention. Abundant knowledge embedded in vision-language models (VLMs) pre-trained on large-scale image-text pairs could alleviate the challenge of limited labeled data under SSMLL setting.Despite existing methods based on fine-tuning VLMs have achieved advances in weakly-supervised multi-label learning, they failed to fully leverage the information from labeled data to enhance the learning of unlabeled data. In this paper, we propose a context-based semantic-aware alignment method to solve the SSMLL problem by leveraging the knowledge of VLMs. To address the challenge of handling multiple semantics within an image, we introduce a novel framework design to extract label-specific image features. This design allows us to achieve a more compact alignment between text features and label-specific image features, leading the model to generate high-quality pseudo-labels. To incorporate the model with comprehensive understanding of image, we design a semi-supervised context identification auxiliary task to enhance the feature representation by capturing co-occurrence information. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our proposed method.
<div id='section'>Paperid: <span id='pid'>814, <a href='https://arxiv.org/pdf/2409.19252.pdf' target='_blank'>https://arxiv.org/pdf/2409.19252.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxu Leng, Zhanjie Wu, Mingpi Tan, Yiran Liu, Ji Gan, Haosheng Chen, Xinbo Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19252">Beyond Euclidean: Dual-Space Representation Learning for Weakly Supervised Video Violence Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While numerous Video Violence Detection (VVD) methods have focused on representation learning in Euclidean space, they struggle to learn sufficiently discriminative features, leading to weaknesses in recognizing normal events that are visually similar to violent events (\emph{i.e.}, ambiguous violence). In contrast, hyperbolic representation learning, renowned for its ability to model hierarchical and complex relationships between events, has the potential to amplify the discrimination between visually similar events. Inspired by these, we develop a novel Dual-Space Representation Learning (DSRL) method for weakly supervised VVD to utilize the strength of both Euclidean and hyperbolic geometries, capturing the visual features of events while also exploring the intrinsic relations between events, thereby enhancing the discriminative capacity of the features. DSRL employs a novel information aggregation strategy to progressively learn event context in hyperbolic spaces, which selects aggregation nodes through layer-sensitive hyperbolic association degrees constrained by hyperbolic Dirichlet energy. Furthermore, DSRL attempts to break the cyber-balkanization of different spaces, utilizing cross-space attention to facilitate information interactions between Euclidean and hyperbolic space to capture better discriminative features for final violence detection. Comprehensive experiments demonstrate the effectiveness of our proposed DSRL.
<div id='section'>Paperid: <span id='pid'>815, <a href='https://arxiv.org/pdf/2409.06210.pdf' target='_blank'>https://arxiv.org/pdf/2409.06210.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ji Ha Jang, Hoigi Seo, Se Young Chun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.06210">INTRA: Interaction Relationship-aware Weakly Supervised Affordance Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Affordance denotes the potential interactions inherent in objects. The perception of affordance can enable intelligent agents to navigate and interact with new environments efficiently. Weakly supervised affordance grounding teaches agents the concept of affordance without costly pixel-level annotations, but with exocentric images. Although recent advances in weakly supervised affordance grounding yielded promising results, there remain challenges including the requirement for paired exocentric and egocentric image dataset, and the complexity in grounding diverse affordances for a single object. To address them, we propose INTeraction Relationship-aware weakly supervised Affordance grounding (INTRA). Unlike prior arts, INTRA recasts this problem as representation learning to identify unique features of interactions through contrastive learning with exocentric images only, eliminating the need for paired datasets. Moreover, we leverage vision-language model embeddings for performing affordance grounding flexibly with any text, designing text-conditioned affordance map generation to reflect interaction relationship for contrastive learning and enhancing robustness with our text synonym augmentation. Our method outperformed prior arts on diverse datasets such as AGD20K, IIT-AFF, CAD and UMD. Additionally, experimental results demonstrate that our method has remarkable domain scalability for synthesized images / illustrations and is capable of performing affordance grounding for novel interactions and objects.
<div id='section'>Paperid: <span id='pid'>816, <a href='https://arxiv.org/pdf/2407.01869.pdf' target='_blank'>https://arxiv.org/pdf/2407.01869.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenyi Lian, Joakim Lindblad, Christina Runow Stark, Jan-MichaÃ©l Hirsch, NataÅ¡a Sladoje
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.01869">Let it shine: Autofluorescence of Papanicolaou-stain improves AI-based cytological oral cancer detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Oral cancer is a global health challenge. It is treatable if detected early, but it is often fatal in late stages. There is a shift from the invasive and time-consuming tissue sampling and histological examination, toward non-invasive brush biopsies and cytological examination. Reliable computer-assisted methods are essential for cost-effective and accurate cytological analysis, but the lack of detailed cell-level annotations impairs model effectiveness. This study aims to improve AI-based oral cancer detection using multimodal imaging and deep fusion. We combine brightfield and fluorescence whole slide microscopy imaging to analyze Papanicolaou-stained liquid-based cytology slides of brush biopsies collected from both healthy and cancer patients. Due to limited cytological annotations, we utilize a weakly supervised deep learning approach using only patient-level labels. We evaluate various multimodal fusion strategies, including early, late, and three recent intermediate fusion methods. Our results show: (i) fluorescence imaging of Papanicolaou-stained samples provides substantial diagnostic information; (ii) multimodal fusion enhances classification and cancer detection accuracy over single-modality methods. Intermediate fusion is the leading method among the studied approaches. Specifically, the Co-Attention Fusion Network (CAFNet) model excels with an F1 score of 83.34% and accuracy of 91.79%, surpassing human performance on the task. Additional tests highlight the need for precise image registration to optimize multimodal analysis benefits. This study advances cytopathology by combining deep learning and multimodal imaging to enhance early, non-invasive detection of oral cancer, improving diagnostic accuracy and streamlining clinical workflows. The developed pipeline is also applicable in other cytological settings. Our codes and dataset are available online for further research.
<div id='section'>Paperid: <span id='pid'>817, <a href='https://arxiv.org/pdf/2403.01214.pdf' target='_blank'>https://arxiv.org/pdf/2403.01214.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyi Yu, Ling Yan, Pengtao Jiang, Hao Chen, Bo Li, Lin Yuanbo Wu, Linlin Ou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.01214">Boosting Box-supervised Instance Segmentation with Pseudo Depth</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The realm of Weakly Supervised Instance Segmentation (WSIS) under box supervision has garnered substantial attention, showcasing remarkable advancements in recent years. However, the limitations of box supervision become apparent in its inability to furnish effective information for distinguishing foreground from background within the specified target box. This research addresses this challenge by introducing pseudo-depth maps into the training process of the instance segmentation network, thereby boosting its performance by capturing depth differences between instances. These pseudo-depth maps are generated using a readily available depth predictor and are not necessary during the inference stage. To enable the network to discern depth features when predicting masks, we integrate a depth prediction layer into the mask prediction head. This innovative approach empowers the network to simultaneously predict masks and depth, enhancing its ability to capture nuanced depth-related information during the instance segmentation process. We further utilize the mask generated in the training process as supervision to distinguish the foreground from the background. When selecting the best mask for each box through the Hungarian algorithm, we use depth consistency as one calculation cost item. The proposed method achieves significant improvements on Cityscapes and COCO dataset.
<div id='section'>Paperid: <span id='pid'>818, <a href='https://arxiv.org/pdf/2401.13611.pdf' target='_blank'>https://arxiv.org/pdf/2401.13611.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rhiannon Mogridge, George Close, Robert Sutherland, Thomas Hain, Jon Barker, Stefan Goetze, Anton Ragni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.13611">Non-Intrusive Speech Intelligibility Prediction for Hearing-Impaired Users using Intermediate ASR Features and Human Memory Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural networks have been successfully used for non-intrusive speech intelligibility prediction. Recently, the use of feature representations sourced from intermediate layers of pre-trained self-supervised and weakly-supervised models has been found to be particularly useful for this task. This work combines the use of Whisper ASR decoder layer representations as neural network input features with an exemplar-based, psychologically motivated model of human memory to predict human intelligibility ratings for hearing-aid users. Substantial performance improvement over an established intrusive HASPI baseline system is found, including on enhancement systems and listeners unseen in the training data, with a root mean squared error of 25.3 compared with the baseline of 28.7.
<div id='section'>Paperid: <span id='pid'>819, <a href='https://arxiv.org/pdf/2312.15895.pdf' target='_blank'>https://arxiv.org/pdf/2312.15895.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoyang Wei, Pengfei Chen, Xuehui Yu, Guorong Li, Jianbin Jiao, Zhenjun Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.15895">Semantic-aware SAM for Point-Prompted Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single-point annotation in visual tasks, with the goal of minimizing labelling costs, is becoming increasingly prominent in research. Recently, visual foundation models, such as Segment Anything (SAM), have gained widespread usage due to their robust zero-shot capabilities and exceptional annotation performance. However, SAM's class-agnostic output and high confidence in local segmentation introduce 'semantic ambiguity', posing a challenge for precise category-specific segmentation. In this paper, we introduce a cost-effective category-specific segmenter using SAM. To tackle this challenge, we have devised a Semantic-Aware Instance Segmentation Network (SAPNet) that integrates Multiple Instance Learning (MIL) with matching capability and SAM with point prompts. SAPNet strategically selects the most representative mask proposals generated by SAM to supervise segmentation, with a specific focus on object category information. Moreover, we introduce the Point Distance Guidance and Box Mining Strategy to mitigate inherent challenges: 'group' and 'local' issues in weakly supervised segmentation. These strategies serve to further enhance the overall segmentation performance. The experimental results on Pascal VOC and COCO demonstrate the promising performance of our proposed SAPNet, emphasizing its semantic matching capabilities and its potential to advance point-prompted instance segmentation. The code will be made publicly available.
<div id='section'>Paperid: <span id='pid'>820, <a href='https://arxiv.org/pdf/2312.11629.pdf' target='_blank'>https://arxiv.org/pdf/2312.11629.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ranit Das, Gregor Kasieczka, David Shih
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.11629">Residual ANODE</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present R-ANODE, a new method for data-driven, model-agnostic resonant anomaly detection that raises the bar for both performance and interpretability. The key to R-ANODE is to enhance the inductive bias of the anomaly detection task by fitting a normalizing flow directly to the small and unknown signal component, while holding fixed a background model (also a normalizing flow) learned from sidebands. In doing so, R-ANODE is able to outperform all classifier-based, weakly-supervised approaches, as well as the previous ANODE method which fit a density estimator to all of the data in the signal region instead of just the signal. We show that the method works equally well whether the unknown signal fraction is learned or fixed, and is even robust to signal fraction misspecification. Finally, with the learned signal model we can sample and gain qualitative insights into the underlying anomaly, which greatly enhances the interpretability of resonant anomaly detection and offers the possibility of simultaneously discovering and characterizing the new physics that could be hiding in the data.
<div id='section'>Paperid: <span id='pid'>821, <a href='https://arxiv.org/pdf/2312.07384.pdf' target='_blank'>https://arxiv.org/pdf/2312.07384.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yupeng Hu, Han Jiang, Hao Liu, Kun Wang, Haoyu Tang, Liqiang Nie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.07384">Visual Self-paced Iterative Learning for Unsupervised Temporal Action Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, temporal action localization (TAL) has garnered significant interest in information retrieval community. However, existing supervised/weakly supervised methods are heavily dependent on extensive labeled temporal boundaries and action categories, which is labor-intensive and time-consuming. Although some unsupervised methods have utilized the ``iteratively clustering and localization'' paradigm for TAL, they still suffer from two pivotal impediments: 1) unsatisfactory video clustering confidence, and 2) unreliable video pseudolabels for model training. To address these limitations, we present a novel self-paced iterative learning model to enhance clustering and localization training simultaneously, thereby facilitating more effective unsupervised TAL. Concretely, we improve the clustering confidence through exploring the contextual feature-robust visual information. Thereafter, we design two (constant- and variable- speed) incremental instance learning strategies for easy-to-hard model training, thus ensuring the reliability of these video pseudolabels and further improving overall localization performance. Extensive experiments on two public datasets have substantiated the superiority of our model over several state-of-the-art competitors.
<div id='section'>Paperid: <span id='pid'>822, <a href='https://arxiv.org/pdf/2306.14269.pdf' target='_blank'>https://arxiv.org/pdf/2306.14269.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yangchen Xie, Xinyuan Chen, Hongjian Zhan, Palaiahankote Shivakum, Bing Yin, Cong Liu, Yue Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.14269">Weakly Supervised Scene Text Generation for Low-resource Languages</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A large number of annotated training images is crucial for training successful scene text recognition models. However, collecting sufficient datasets can be a labor-intensive and costly process, particularly for low-resource languages. To address this challenge, auto-generating text data has shown promise in alleviating the problem. Unfortunately, existing scene text generation methods typically rely on a large amount of paired data, which is difficult to obtain for low-resource languages. In this paper, we propose a novel weakly supervised scene text generation method that leverages a few recognition-level labels as weak supervision. The proposed method is able to generate a large amount of scene text images with diverse backgrounds and font styles through cross-language generation. Our method disentangles the content and style features of scene text images, with the former representing textual information and the latter representing characteristics such as font, alignment, and background. To preserve the complete content structure of generated images, we introduce an integrated attention module. Furthermore, to bridge the style gap in the style of different languages, we incorporate a pre-trained font classifier. We evaluate our method using state-of-the-art scene text recognition models. Experiments demonstrate that our generated scene text significantly improves the scene text recognition accuracy and help achieve higher accuracy when complemented with other generative methods.
<div id='section'>Paperid: <span id='pid'>823, <a href='https://arxiv.org/pdf/2306.10458.pdf' target='_blank'>https://arxiv.org/pdf/2306.10458.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Cheng, Yuzhou Cao, Ximing Li, Bo An, Lei Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.10458">Weakly Supervised Regression with Interval Targets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates an interesting weakly supervised regression setting called regression with interval targets (RIT). Although some of the previous methods on relevant regression settings can be adapted to RIT, they are not statistically consistent, and thus their empirical performance is not guaranteed. In this paper, we provide a thorough study on RIT. First, we proposed a novel statistical model to describe the data generation process for RIT and demonstrate its validity. Second, we analyze a simple selection method for RIT, which selects a particular value in the interval as the target value to train the model. Third, we propose a statistically consistent limiting method for RIT to train the model by limiting the predictions to the interval. We further derive an estimation error bound for our limiting method. Finally, extensive experiments on various datasets demonstrate the effectiveness of our proposed method.
<div id='section'>Paperid: <span id='pid'>824, <a href='https://arxiv.org/pdf/2306.08968.pdf' target='_blank'>https://arxiv.org/pdf/2306.08968.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Cheng, Deng-Bao Wang, Lei Feng, Min-Ling Zhang, Bo An
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.08968">Partial-Label Regression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partial-label learning is a popular weakly supervised learning setting that allows each training example to be annotated with a set of candidate labels. Previous studies on partial-label learning only focused on the classification setting where candidate labels are all discrete, which cannot handle continuous labels with real values. In this paper, we provide the first attempt to investigate partial-label regression, where each training example is annotated with a set of real-valued candidate labels. To solve this problem, we first propose a simple baseline method that takes the average loss incurred by candidate labels as the predictive loss. The drawback of this method lies in that the loss incurred by the true label may be overwhelmed by other false labels. To overcome this drawback, we propose an identification method that takes the least loss incurred by candidate labels as the predictive loss. We further improve it by proposing a progressive identification method to differentiate candidate labels using progressively updated weights for incurred losses. We prove that the latter two methods are model-consistent and provide convergence analyses. Our proposed methods are theoretically grounded and can be compatible with any models, optimizers, and losses. Experiments validate the effectiveness of our proposed methods.
<div id='section'>Paperid: <span id='pid'>825, <a href='https://arxiv.org/pdf/2305.16286.pdf' target='_blank'>https://arxiv.org/pdf/2305.16286.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wangyou Zhang, Yanmin Qian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.16286">Weakly-Supervised Speech Pre-training: A Case Study on Target Speech Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-supervised learning (SSL) based speech pre-training has attracted much attention for its capability of extracting rich representations learned from massive unlabeled data. On the other hand, the use of weakly-supervised data is less explored for speech pre-training. To fill this gap, we propose a weakly-supervised speech pre-training method based on speaker-aware speech data. It adopts a similar training procedure to the widely-used masked speech prediction based SSL framework, while incorporating additional target-speaker enrollment information as an auxiliary input. In this way, the learned representation is steered towards the target speaker even in the presence of highly overlapping interference, allowing potential applications to tasks such as target speech recognition. Our experiments on Libri2Mix and WSJ0-2mix datasets show that the proposed model achieves significantly better ASR performance compared to WavLM, the state-of-the-art SSL model with denoising capability.
<div id='section'>Paperid: <span id='pid'>826, <a href='https://arxiv.org/pdf/2305.03112.pdf' target='_blank'>https://arxiv.org/pdf/2305.03112.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lechao Cheng, Zerun Liu, Jingxuan He, Chaowei Fang, Dingwen Zhang, Meng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.03112">Calibrating Undisciplined Over-Smoothing in Transformer for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation (WSSS) has recently attracted considerable attention because it requires fewer annotations than fully supervised approaches, making it especially promising for large-scale image segmentation tasks. Although many vision transformer-based methods leverage self-attention affinity matrices to refine Class Activation Maps (CAMs), they often treat each layer's affinity equally and thus introduce considerable background noise at deeper layers, where attention tends to converge excessively on certain tokens (i.e., over-smoothing). We observe that this deep-level attention naturally converges on a subset of tokens, yet unregulated query-key affinity can generate unpredictable activation patterns (undisciplined over-smoothing), adversely affecting CAM accuracy. To address these limitations, we propose an Adaptive Re-Activation Mechanism (AReAM), which exploits shallow-level affinity to guide deeper-layer convergence in an entropy-aware manner, thereby suppressing background noise and re-activating crucial semantic regions in the CAMs. Experiments on two commonly used datasets demonstrate that AReAM substantially improves segmentation performance compared with existing WSSS methods, reducing noise while sharpening focus on relevant semantic regions. Overall, this work underscores the importance of controlling deep-level attention to mitigate undisciplined over-smoothing, introduces an entropy-aware mechanism that harmonizes shallow and deep-level affinities, and provides a refined approach to enhance transformer-based WSSS accuracy by re-activating CAMs.
<div id='section'>Paperid: <span id='pid'>827, <a href='https://arxiv.org/pdf/2303.03946.pdf' target='_blank'>https://arxiv.org/pdf/2303.03946.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingyu Xu, Zheng Lian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.03946">Pseudo Labels Regularization for Imbalanced Partial-Label Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partial-label learning (PLL) is an important branch of weakly supervised learning where the single ground truth resides in a set of candidate labels, while the research rarely considers the label imbalance. A recent study for imbalanced partial-Label learning proposed that the combinatorial challenge of partial-label learning and long-tail learning lies in matching between a decent marginal prior distribution with drawing the pseudo labels. However, we believe that even if the pseudo label matches the prior distribution, the tail classes will still be difficult to learn because the total weight is too small. Therefore, we propose a pseudo-label regularization technique specially designed for PLL. By punishing the pseudo labels of head classes, our method implements state-of-art under the standardized benchmarks compared to the previous PLL methods.
<div id='section'>Paperid: <span id='pid'>828, <a href='https://arxiv.org/pdf/2212.10057.pdf' target='_blank'>https://arxiv.org/pdf/2212.10057.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhao Wu, Wei Li, Xinyan Xiao, Jiachen Liu, Sujian Li, Yajuan Lv
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.10057">WeCheck: Strong Factual Consistency Checker via Weakly Supervised Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A crucial issue of current text generation models is that they often uncontrollably generate factually inconsistent text with respective of their inputs. Limited by the lack of annotated data, existing works in evaluating factual consistency directly transfer the reasoning ability of models trained on other data-rich upstream tasks like question answering (QA) and natural language inference (NLI) without any further adaptation. As a result, they perform poorly on the real generated text and are biased heavily by their single-source upstream tasks. To alleviate this problem, we propose a weakly supervised framework that aggregates multiple resources to train a precise and efficient factual metric, namely WeCheck. WeCheck first utilizes a generative model to accurately label a real generated sample by aggregating its weak labels, which are inferred from multiple resources. Then, we train the target metric model with the weak supervision while taking noises into consideration. Comprehensive experiments on a variety of tasks demonstrate the strong performance of WeCheck, which achieves a 3.4\% absolute improvement over previous state-of-the-art methods on TRUE benchmark on average.
<div id='section'>Paperid: <span id='pid'>829, <a href='https://arxiv.org/pdf/2208.12459.pdf' target='_blank'>https://arxiv.org/pdf/2208.12459.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bo-Shi Zou, Ming-Kun Xie, Sheng-Jun Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.12459">Meta Objective Guided Disambiguation for Partial Label Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partial label learning (PLL) is a typical weakly supervised learning framework, where each training instance is associated with a candidate label set, among which only one label is valid. To solve PLL problems, typically methods try to perform disambiguation for candidate sets by either using prior knowledge, such as structure information of training data, or refining model outputs in a self-training manner. Unfortunately, these methods often fail to obtain a favorable performance due to the lack of prior information or unreliable predictions in the early stage of model training. In this paper, we propose a novel framework for partial label learning with meta objective guided disambiguation (MoGD), which aims to recover the ground-truth label from candidate labels set by solving a meta objective on a small validation set. Specifically, to alleviate the negative impact of false positive labels, we re-weight each candidate label based on the meta loss on the validation set. Then, the classifier is trained by minimizing the weighted cross entropy loss. The proposed method can be easily implemented by using various deep networks with the ordinary SGD optimizer. Theoretically, we prove the convergence property of meta objective and derive the estimation error bounds of the proposed method. Extensive experiments on various benchmark datasets and real-world PLL datasets demonstrate that the proposed method can achieve competent performance when compared with the state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>830, <a href='https://arxiv.org/pdf/2207.07418.pdf' target='_blank'>https://arxiv.org/pdf/2207.07418.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin Alt, Christian Kunz, Darko Katic, Rayan Younis, Rainer JÃ¤kel, Beat Peter MÃ¼ller-Stich, Martin Wagner, Franziska Mathis-Ullrich
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.07418">LapSeg3D: Weakly Supervised Semantic Segmentation of Point Clouds Representing Laparoscopic Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The semantic segmentation of surgical scenes is a prerequisite for task automation in robot assisted interventions. We propose LapSeg3D, a novel DNN-based approach for the voxel-wise annotation of point clouds representing surgical scenes. As the manual annotation of training data is highly time consuming, we introduce a semi-autonomous clustering-based pipeline for the annotation of the gallbladder, which is used to generate segmented labels for the DNN. When evaluated against manually annotated data, LapSeg3D achieves an F1 score of 0.94 for gallbladder segmentation on various datasets of ex-vivo porcine livers. We show LapSeg3D to generalize accurately across different gallbladders and datasets recorded with different RGB-D camera systems.
<div id='section'>Paperid: <span id='pid'>831, <a href='https://arxiv.org/pdf/2206.07207.pdf' target='_blank'>https://arxiv.org/pdf/2206.07207.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hammad A. Ayyubi, Christopher Thomas, Lovish Chum, Rahul Lokesh, Long Chen, Yulei Niu, Xudong Lin, Xuande Feng, Jaywon Koo, Sounak Ray, Shih-Fu Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.07207">Beyond Grounding: Extracting Fine-Grained Event Hierarchies Across Modalities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Events describe happenings in our world that are of importance. Naturally, understanding events mentioned in multimedia content and how they are related forms an important way of comprehending our world. Existing literature can infer if events across textual and visual (video) domains are identical (via grounding) and thus, on the same semantic level. However, grounding fails to capture the intricate cross-event relations that exist due to the same events being referred to on many semantic levels. For example, in Figure 1, the abstract event of "war" manifests at a lower semantic level through subevents "tanks firing" (in video) and airplane "shot" (in text), leading to a hierarchical, multimodal relationship between the events.
  In this paper, we propose the task of extracting event hierarchies from multimodal (video and text) data to capture how the same event manifests itself in different modalities at different semantic levels. This reveals the structure of events and is critical to understanding them. To support research on this task, we introduce the Multimodal Hierarchical Events (MultiHiEve) dataset. Unlike prior video-language datasets, MultiHiEve is composed of news video-article pairs, which makes it rich in event hierarchies. We densely annotate a part of the dataset to construct the test benchmark. We show the limitations of state-of-the-art unimodal and multimodal baselines on this task. Further, we address these limitations via a new weakly supervised model, leveraging only unannotated video-article pairs from MultiHiEve. We perform a thorough evaluation of our proposed method which demonstrates improved performance on this task and highlight opportunities for future research.
<div id='section'>Paperid: <span id='pid'>832, <a href='https://arxiv.org/pdf/2206.05148.pdf' target='_blank'>https://arxiv.org/pdf/2206.05148.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soumick Chatterjee, Hadya Yassin, Florian Dubost, Andreas NÃ¼rnberger, Oliver Speck
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.05148">Weakly-supervised segmentation using inherently-explainable classification models and their application to brain tumour classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning models have shown their potential for several applications. However, most of the models are opaque and difficult to trust due to their complex reasoning - commonly known as the black-box problem. Some fields, such as medicine, require a high degree of transparency to accept and adopt such technologies. Consequently, creating explainable/interpretable models or applying post-hoc methods on classifiers to build trust in deep learning models are required. Moreover, deep learning methods can be used for segmentation tasks, which typically require hard-to-obtain, time-consuming manually-annotated segmentation labels for training. This paper introduces three inherently-explainable classifiers to tackle both of these problems as one. The localisation heatmaps provided by the networks -- representing the models' focus areas and being used in classification decision-making -- can be directly interpreted, without requiring any post-hoc methods to derive information for model explanation. The models are trained by using the input image and only the classification labels as ground-truth in a supervised fashion - without using any information about the location of the region of interest (i.e. the segmentation labels), making the segmentation training of the models weakly-supervised through classification labels. The final segmentation is obtained by thresholding these heatmaps. The models were employed for the task of multi-class brain tumour classification using two different datasets, resulting in the best F1-score of 0.93 for the supervised classification task while securing a median Dice score of 0.67$\pm$0.08 for the weakly-supervised segmentation task. Furthermore, the obtained accuracy on a subset of tumour-only images outperformed the state-of-the-art glioma tumour grading binary classifiers with the best model achieving 98.7\% accuracy.
<div id='section'>Paperid: <span id='pid'>833, <a href='https://arxiv.org/pdf/2509.22132.pdf' target='_blank'>https://arxiv.org/pdf/2509.22132.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingjing Lu, Huilong Pi, Yunchuan Qin, Zhuo Tang, Ruihui Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22132">Self-Supervised Point Cloud Completion based on Multi-View Augmentations of Single Partial Point Cloud</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Point cloud completion aims to reconstruct complete shapes from partial observations. Although current methods have achieved remarkable performance, they still have some limitations: Supervised methods heavily rely on ground truth, which limits their generalization to real-world datasets due to the synthetic-to-real domain gap. Unsupervised methods require complete point clouds to compose unpaired training data, and weakly-supervised methods need multi-view observations of the object. Existing self-supervised methods frequently produce unsatisfactory predictions due to the limited capabilities of their self-supervised signals. To overcome these challenges, we propose a novel self-supervised point cloud completion method. We design a set of novel self-supervised signals based on multi-view augmentations of the single partial point cloud. Additionally, to enhance the model's learning ability, we first incorporate Mamba into self-supervised point cloud completion task, encouraging the model to generate point clouds with better quality. Experiments on synthetic and real-world datasets demonstrate that our method achieves state-of-the-art results.
<div id='section'>Paperid: <span id='pid'>834, <a href='https://arxiv.org/pdf/2509.08129.pdf' target='_blank'>https://arxiv.org/pdf/2509.08129.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Francisco M. Castro-MacÃ­as, Francisco J. SÃ¡ez-Maldonado, Pablo Morales-Ãlvarez, Rafael Molina
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08129">torchmil: A PyTorch-based library for deep Multiple Instance Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple Instance Learning (MIL) is a powerful framework for weakly supervised learning, particularly useful when fine-grained annotations are unavailable. Despite growing interest in deep MIL methods, the field lacks standardized tools for model development, evaluation, and comparison, which hinders reproducibility and accessibility. To address this, we present torchmil, an open-source Python library built on PyTorch. torchmil offers a unified, modular, and extensible framework, featuring basic building blocks for MIL models, a standardized data format, and a curated collection of benchmark datasets and models. The library includes comprehensive documentation and tutorials to support both practitioners and researchers. torchmil aims to accelerate progress in MIL and lower the entry barrier for new users. Available at https://torchmil.readthedocs.io.
<div id='section'>Paperid: <span id='pid'>835, <a href='https://arxiv.org/pdf/2508.12322.pdf' target='_blank'>https://arxiv.org/pdf/2508.12322.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Deutges, Chen Yang, Raheleh Salehi, Nassir Navab, Carsten Marr, Ario Sadafi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12322">Neural Cellular Automata for Weakly Supervised Segmentation of White Blood Cells</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The detection and segmentation of white blood cells in blood smear images is a key step in medical diagnostics, supporting various downstream tasks such as automated blood cell counting, morphological analysis, cell classification, and disease diagnosis and monitoring. Training robust and accurate models requires large amounts of labeled data, which is both time-consuming and expensive to acquire. In this work, we propose a novel approach for weakly supervised segmentation using neural cellular automata (NCA-WSS). By leveraging the feature maps generated by NCA during classification, we can extract segmentation masks without the need for retraining with segmentation labels. We evaluate our method on three white blood cell microscopy datasets and demonstrate that NCA-WSS significantly outperforms existing weakly supervised approaches. Our work illustrates the potential of NCA for both classification and segmentation in a weakly supervised framework, providing a scalable and efficient solution for medical image analysis.
<div id='section'>Paperid: <span id='pid'>836, <a href='https://arxiv.org/pdf/2508.03997.pdf' target='_blank'>https://arxiv.org/pdf/2508.03997.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Zhang, Tianzhuzi Tan, Guanchun Yin, Bo Zhang, Xiuzhuang Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03997">JanusNet: Hierarchical Slice-Block Shuffle and Displacement for Semi-Supervised 3D Multi-Organ Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Limited by the scarcity of training samples and annotations, weakly supervised medical image segmentation often employs data augmentation to increase data diversity, while randomly mixing volumetric blocks has demonstrated strong performance. However, this approach disrupts the inherent anatomical continuity of 3D medical images along orthogonal axes, leading to severe structural inconsistencies and insufficient training in challenging regions, such as small-sized organs, etc. To better comply with and utilize human anatomical information, we propose JanusNet}, a data augmentation framework for 3D medical data that globally models anatomical continuity while locally focusing on hard-to-segment regions. Specifically, our Slice-Block Shuffle step performs aligned shuffling of same-index slice blocks across volumes along a random axis, while preserving the anatomical context on planes perpendicular to the perturbation axis. Concurrently, the Confidence-Guided Displacement step uses prediction reliability to replace blocks within each slice, amplifying signals from difficult areas. This dual-stage, axis-aligned framework is plug-and-play, requiring minimal code changes for most teacher-student schemes. Extensive experiments on the Synapse and AMOS datasets demonstrate that JanusNet significantly surpasses state-of-the-art methods, achieving, for instance, a 4% DSC gain on the Synapse dataset with only 20% labeled data.
<div id='section'>Paperid: <span id='pid'>837, <a href='https://arxiv.org/pdf/2508.01697.pdf' target='_blank'>https://arxiv.org/pdf/2508.01697.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiqi Huang, Tingfa Xu, Wen Yan, Dean Barratt, Yipeng Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01697">Register Anything: Estimating "Corresponding Prompts" for Segment Anything Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Establishing pixel/voxel-level or region-level correspondences is the core challenge in image registration. The latter, also known as region-based correspondence representation, leverages paired regions of interest (ROIs) to enable regional matching while preserving fine-grained capability at pixel/voxel level. Traditionally, this representation is implemented via two steps: segmenting ROIs in each image then matching them between the two images. In this paper, we simplify this into one step by directly "searching for corresponding prompts", using extensively pre-trained segmentation models (e.g., SAM) for a training-free registration approach, PromptReg. Firstly, we introduce the "corresponding prompt problem", which aims to identify a corresponding Prompt Y in Image Y for any given visual Prompt X in Image X, such that the two respectively prompt-conditioned segmentations are a pair of corresponding ROIs from the two images. Secondly, we present an "inverse prompt" solution that generates primary and optionally auxiliary prompts, inverting Prompt X into the prompt space of Image Y. Thirdly, we propose a novel registration algorithm that identifies multiple paired corresponding ROIs by marginalizing the inverted Prompt X across both prompt and spatial dimensions. Comprehensive experiments are conducted on five applications of registering 3D prostate MR, 3D abdomen MR, 3D lung CT, 2D histopathology and, as a non-medical example, 2D aerial images. Based on metrics including Dice and target registration errors on anatomical structures, the proposed registration outperforms both intensity-based iterative algorithms and learning-based DDF-predicting networks, even yielding competitive performance with weakly-supervised approaches that require fully-segmented training data.
<div id='section'>Paperid: <span id='pid'>838, <a href='https://arxiv.org/pdf/2506.23460.pdf' target='_blank'>https://arxiv.org/pdf/2506.23460.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dewen Zeng, Xinrong Hu, Yu-Jen Chen, Yawen Wu, Xiaowei Xu, Yiyu Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23460">Contrastive Learning with Diffusion Features for Weakly Supervised Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation (WSSS) methods using class labels often rely on class activation maps (CAMs) to localize objects. However, traditional CAM-based methods struggle with partial activations and imprecise object boundaries due to optimization discrepancies between classification and segmentation. Recently, the conditional diffusion model (CDM) has been used as an alternative for generating segmentation masks in WSSS, leveraging its strong image generation capabilities tailored to specific class distributions. By modifying or perturbing the condition during diffusion sampling, the related objects can be highlighted in the generated images. Yet, the saliency maps generated by CDMs are prone to noise from background alterations during reverse diffusion. To alleviate the problem, we introduce Contrastive Learning with Diffusion Features (CLDF), a novel method that uses contrastive learning to train a pixel decoder to map the diffusion features from a frozen CDM to a low-dimensional embedding space for segmentation. Specifically, we integrate gradient maps generated from CDM external classifier with CAMs to identify foreground and background pixels with fewer false positives/negatives for contrastive learning, enabling robust pixel embedding learning. Experimental results on four segmentation tasks from two public medical datasets demonstrate that our method significantly outperforms existing baselines.
<div id='section'>Paperid: <span id='pid'>839, <a href='https://arxiv.org/pdf/2504.13405.pdf' target='_blank'>https://arxiv.org/pdf/2504.13405.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengqin Jiang, Linfei Li, Haokui Zhang, Qingshan Liu, Amin Beheshti, Jian Yang, Anton van den Hengel, Quan Z. Sheng, Yuankai Qi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13405">ProgRoCC: A Progressive Approach to Rough Crowd Counting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As the number of individuals in a crowd grows, enumeration-based techniques become increasingly infeasible and their estimates increasingly unreliable. We propose instead an estimation-based version of the problem: we label Rough Crowd Counting that delivers better accuracy on the basis of training data that is easier to acquire. Rough crowd counting requires only rough annotations of the number of targets in an image, instead of the more traditional, and far more expensive, per-target annotations. We propose an approach to the rough crowd counting problem based on CLIP, termed ProgRoCC. Specifically, we introduce a progressive estimation learning strategy that determines the object count through a coarse-to-fine approach. This approach delivers answers quickly, outperforms the state-of-the-art in semi- and weakly-supervised crowd counting. In addition, we design a vision-language matching adapter that optimizes key-value pairs by mining effective matches of two modalities to refine the visual features, thereby improving the final performance. Extensive experimental results on three widely adopted crowd counting datasets demonstrate the effectiveness of our method.
<div id='section'>Paperid: <span id='pid'>840, <a href='https://arxiv.org/pdf/2504.01452.pdf' target='_blank'>https://arxiv.org/pdf/2504.01452.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Encheng Su, Hu Cao, Alois Knoll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01452">BiSeg-SAM: Weakly-Supervised Post-Processing Framework for Boosting Binary Segmentation in Segment Anything Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate segmentation of polyps and skin lesions is essential for diagnosing colorectal and skin cancers. While various segmentation methods for polyps and skin lesions using fully supervised deep learning techniques have been developed, the pixel-level annotation of medical images by doctors is both time-consuming and costly. Foundational vision models like the Segment Anything Model (SAM) have demonstrated superior performance; however, directly applying SAM to medical segmentation may not yield satisfactory results due to the lack of domain-specific medical knowledge. In this paper, we propose BiSeg-SAM, a SAM-guided weakly supervised prompting and boundary refinement network for the segmentation of polyps and skin lesions. Specifically, we fine-tune SAM combined with a CNN module to learn local features. We introduce a WeakBox with two functions: automatically generating box prompts for the SAM model and using our proposed Multi-choice Mask-to-Box (MM2B) transformation for rough mask-to-box conversion, addressing the mismatch between coarse labels and precise predictions. Additionally, we apply scale consistency (SC) loss for prediction scale alignment. Our DetailRefine module enhances boundary precision and segmentation accuracy by refining coarse predictions using a limited amount of ground truth labels. This comprehensive approach enables BiSeg-SAM to achieve excellent multi-task segmentation performance. Our method demonstrates significant superiority over state-of-the-art (SOTA) methods when tested on five polyp datasets and one skin cancer dataset.
<div id='section'>Paperid: <span id='pid'>841, <a href='https://arxiv.org/pdf/2503.20826.pdf' target='_blank'>https://arxiv.org/pdf/2503.20826.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiwei Yang, Yucong Meng, Kexue Fu, Feilong Tang, Shuo Wang, Zhijian Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20826">Exploring CLIP's Dense Knowledge for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Semantic Segmentation (WSSS) with image-level labels aims to achieve pixel-level predictions using Class Activation Maps (CAMs). Recently, Contrastive Language-Image Pre-training (CLIP) has been introduced in WSSS. However, recent methods primarily focus on image-text alignment for CAM generation, while CLIP's potential in patch-text alignment remains unexplored. In this work, we propose ExCEL to explore CLIP's dense knowledge via a novel patch-text alignment paradigm for WSSS. Specifically, we propose Text Semantic Enrichment (TSE) and Visual Calibration (VC) modules to improve the dense alignment across both text and vision modalities. To make text embeddings semantically informative, our TSE module applies Large Language Models (LLMs) to build a dataset-wide knowledge base and enriches the text representations with an implicit attribute-hunting process. To mine fine-grained knowledge from visual features, our VC module first proposes Static Visual Calibration (SVC) to propagate fine-grained knowledge in a non-parametric manner. Then Learnable Visual Calibration (LVC) is further proposed to dynamically shift the frozen features towards distributions with diverse semantics. With these enhancements, ExCEL not only retains CLIP's training-free advantages but also significantly outperforms other state-of-the-art methods with much less training cost on PASCAL VOC and MS COCO.
<div id='section'>Paperid: <span id='pid'>842, <a href='https://arxiv.org/pdf/2410.14790.pdf' target='_blank'>https://arxiv.org/pdf/2410.14790.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianchao Ci, Eldert J. van Henten, Xin Wang, Akshay K. Burusa, Gert Kootstra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14790">SSL-NBV: A Self-Supervised-Learning-Based Next-Best-View algorithm for Efficient 3D Plant Reconstruction by a Robot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The 3D reconstruction of plants is challenging due to their complex shape causing many occlusions. Next-Best-View (NBV) methods address this by iteratively selecting new viewpoints to maximize information gain (IG). Deep-learning-based NBV (DL-NBV) methods demonstrate higher computational efficiency over classic voxel-based NBV approaches but current methods require extensive training using ground-truth plant models, making them impractical for real-world plants. These methods, moreover, rely on offline training with pre-collected data, limiting adaptability in changing agricultural environments. This paper proposes a self-supervised learning-based NBV method (SSL-NBV) that uses a deep neural network to predict the IG for candidate viewpoints. The method allows the robot to gather its own training data during task execution by comparing new 3D sensor data to the earlier gathered data and by employing weakly-supervised learning and experience replay for efficient online learning. Comprehensive evaluations were conducted in simulation and real-world environments using cross-validation. The results showed that SSL-NBV required fewer views for plant reconstruction than non-NBV methods and was over 800 times faster than a voxel-based method. SSL-NBV reduced training annotations by over 90% compared to a baseline DL-NBV. Furthermore, SSL-NBV could adapt to novel scenarios through online fine-tuning. Also using real plants, the results showed that the proposed method can learn to effectively plan new viewpoints for 3D plant reconstruction. Most importantly, SSL-NBV automated the entire network training and uses continuous online learning, allowing it to operate in changing agricultural environments.
<div id='section'>Paperid: <span id='pid'>843, <a href='https://arxiv.org/pdf/2408.06743.pdf' target='_blank'>https://arxiv.org/pdf/2408.06743.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jialiang Wang, Ning Zhang, Shimin Di, Ruidong Wang, Lei Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.06743">Class-aware and Augmentation-free Contrastive Learning from Label Proportion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning from Label Proportion (LLP) is a weakly supervised learning scenario in which training data is organized into predefined bags of instances, disclosing only the class label proportions per bag. This paradigm is essential for user modeling and personalization, where user privacy is paramount, offering insights into user preferences without revealing individual data. LLP faces a unique difficulty: the misalignment between bag-level supervision and the objective of instance-level prediction, primarily due to the inherent ambiguity in label proportion matching. Previous studies have demonstrated deep representation learning can generate auxiliary signals to promote the supervision level in the image domain. However, applying these techniques to tabular data presents significant challenges: 1) they rely heavily on label-invariant augmentation to establish multi-view, which is not feasible with the heterogeneous nature of tabular datasets, and 2) tabular datasets often lack sufficient semantics for perfect class distinction, making them prone to suboptimality caused by the inherent ambiguity of label proportion matching.
  To address these challenges, we propose an augmentation-free contrastive framework TabLLP-BDC that introduces class-aware supervision (explicitly aware of class differences) at the instance level. Our solution features a two-stage Bag Difference Contrastive (BDC) learning mechanism that establishes robust class-aware instance-level supervision by disassembling the nuance between bag label proportions, without relying on augmentations. Concurrently, our model presents a pioneering multi-task pretraining pipeline tailored for tabular-based LLP, capturing intrinsic tabular feature correlations in alignment with label proportion distribution. Extensive experiments demonstrate that TabLLP-BDC achieves state-of-the-art performance for LLP in the tabular domain.
<div id='section'>Paperid: <span id='pid'>844, <a href='https://arxiv.org/pdf/2407.08971.pdf' target='_blank'>https://arxiv.org/pdf/2407.08971.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qianhan Feng, Wenshuo Li, Tong Lin, Xinghao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08971">Full-Stage Pseudo Label Quality Enhancement for Weakly-supervised Temporal Action Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised Temporal Action Localization (WSTAL) aims to localize actions in untrimmed videos using only video-level supervision. Latest WSTAL methods introduce pseudo label learning framework to bridge the gap between classification-based training and inferencing targets at localization, and achieve cutting-edge results. In these frameworks, a classification-based model is used to generate pseudo labels for a regression-based student model to learn from. However, the quality of pseudo labels in the framework, which is a key factor to the final result, is not carefully studied. In this paper, we propose a set of simple yet efficient pseudo label quality enhancement mechanisms to build our FuSTAL framework. FuSTAL enhances pseudo label quality at three stages: cross-video contrastive learning at proposal Generation-Stage, prior-based filtering at proposal Selection-Stage and EMA-based distillation at Training-Stage. These designs enhance pseudo label quality at different stages in the framework, and help produce more informative, less false and smoother action proposals. With the help of these comprehensive designs at all stages, FuSTAL achieves an average mAP of 50.8% on THUMOS'14, outperforming the previous best method by 1.2%, and becomes the first method to reach the milestone of 50%.
<div id='section'>Paperid: <span id='pid'>845, <a href='https://arxiv.org/pdf/2407.02920.pdf' target='_blank'>https://arxiv.org/pdf/2407.02920.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ramy Battrawy, RenÃ© Schuster, Didier Stricker
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.02920">EgoFlowNet: Non-Rigid Scene Flow from Point Clouds with Ego-Motion Support</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent weakly-supervised methods for scene flow estimation from LiDAR point clouds are limited to explicit reasoning on object-level. These methods perform multiple iterative optimizations for each rigid object, which makes them vulnerable to clustering robustness. In this paper, we propose our EgoFlowNet - a point-level scene flow estimation network trained in a weakly-supervised manner and without object-based abstraction. Our approach predicts a binary segmentation mask that implicitly drives two parallel branches for ego-motion and scene flow. Unlike previous methods, we provide both branches with all input points and carefully integrate the binary mask into the feature extraction and losses. We also use a shared cost volume with local refinement that is updated at multiple scales without explicit clustering or rigidity assumptions. On realistic KITTI scenes, we show that our EgoFlowNet performs better than state-of-the-art methods in the presence of ground surface points.
<div id='section'>Paperid: <span id='pid'>846, <a href='https://arxiv.org/pdf/2405.10879.pdf' target='_blank'>https://arxiv.org/pdf/2405.10879.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiqi Huang, Tingfa Xu, Ziyi Shen, Shaheer Ullah Saeed, Wen Yan, Dean Barratt, Yipeng Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.10879">One registration is worth two segmentations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The goal of image registration is to establish spatial correspondence between two or more images, traditionally through dense displacement fields (DDFs) or parametric transformations (e.g., rigid, affine, and splines). Rethinking the existing paradigms of achieving alignment via spatial transformations, we uncover an alternative but more intuitive correspondence representation: a set of corresponding regions-of-interest (ROI) pairs, which we demonstrate to have sufficient representational capability as other correspondence representation methods.Further, it is neither necessary nor sufficient for these ROIs to hold specific anatomical or semantic significance. In turn, we formulate image registration as searching for the same set of corresponding ROIs from both moving and fixed images - in other words, two multi-class segmentation tasks on a pair of images. For a general-purpose and practical implementation, we integrate the segment anything model (SAM) into our proposed algorithms, resulting in a SAM-enabled registration (SAMReg) that does not require any training data, gradient-based fine-tuning or engineered prompts. We experimentally show that the proposed SAMReg is capable of segmenting and matching multiple ROI pairs, which establish sufficiently accurate correspondences, in three clinical applications of registering prostate MR, cardiac MR and abdominal CT images. Based on metrics including Dice and target registration errors on anatomical structures, the proposed registration outperforms both intensity-based iterative algorithms and DDF-predicting learning-based networks, even yielding competitive performance with weakly-supervised registration which requires fully-segmented training data.
<div id='section'>Paperid: <span id='pid'>847, <a href='https://arxiv.org/pdf/2404.09475.pdf' target='_blank'>https://arxiv.org/pdf/2404.09475.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Byeongkeun Kang, Sinhae Cha, Yeejin Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.09475">Improving Weakly-Supervised Object Localization Using Adversarial Erasing and Pseudo Label</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised learning approaches have gained significant attention due to their ability to reduce the effort required for human annotations in training neural networks. This paper investigates a framework for weakly-supervised object localization, which aims to train a neural network capable of predicting both the object class and its location using only images and their image-level class labels. The proposed framework consists of a shared feature extractor, a classifier, and a localizer. The localizer predicts pixel-level class probabilities, while the classifier predicts the object class at the image level. Since image-level class labels are insufficient for training the localizer, weakly-supervised object localization methods often encounter challenges in accurately localizing the entire object region. To address this issue, the proposed method incorporates adversarial erasing and pseudo labels to improve localization accuracy. Specifically, novel losses are designed to utilize adversarially erased foreground features and adversarially erased feature maps, reducing dependence on the most discriminative region. Additionally, the proposed method employs pseudo labels to suppress activation values in the background while increasing them in the foreground. The proposed method is applied to two backbone networks (MobileNetV1 and InceptionV3) and is evaluated on three publicly available datasets (ILSVRC-2012, CUB-200-2011, and PASCAL VOC 2012). The experimental results demonstrate that the proposed method outperforms previous state-of-the-art methods across all evaluated metrics.
<div id='section'>Paperid: <span id='pid'>848, <a href='https://arxiv.org/pdf/2404.08195.pdf' target='_blank'>https://arxiv.org/pdf/2404.08195.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiwei Yang, Yucong Meng, Kexue Fu, Shuo Wang, Zhijian Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.08195">Tackling Ambiguity from Perspective of Uncertainty Inference and Affinity Diversification for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation (WSSS) with image-level labels intends to achieve dense tasks without laborious annotations. However, due to the ambiguous contexts and fuzzy regions, the performance of WSSS, especially the stages of generating Class Activation Maps (CAMs) and refining pseudo masks, widely suffers from ambiguity while being barely noticed by previous literature. In this work, we propose UniA, a unified single-staged WSSS framework, to efficiently tackle this issue from the perspective of uncertainty inference and affinity diversification, respectively. When activating class objects, we argue that the false activation stems from the bias to the ambiguous regions during the feature extraction. Therefore, we design a more robust feature representation with a probabilistic Gaussian distribution and introduce the uncertainty estimation to avoid the bias. A distribution loss is particularly proposed to supervise the process, which effectively captures the ambiguity and models the complex dependencies among features. When refining pseudo labels, we observe that the affinity from the prevailing refinement methods intends to be similar among ambiguities. To this end, an affinity diversification module is proposed to promote diversity among semantics. A mutual complementing refinement is proposed to initially rectify the ambiguous affinity with multiple inferred pseudo labels. More importantly, a contrastive affinity loss is further designed to diversify the relations among unrelated semantics, which reliably propagates the diversity into the whole feature representations and helps generate better pseudo masks. Extensive experiments are conducted on PASCAL VOC, MS COCO, and medical ACDC datasets, which validate the efficiency of UniA tackling ambiguity and the superiority over recent single-staged or even most multi-staged competitors.
<div id='section'>Paperid: <span id='pid'>849, <a href='https://arxiv.org/pdf/2310.19359.pdf' target='_blank'>https://arxiv.org/pdf/2310.19359.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pablo Morales-Ãlvarez, Arne Schmidt, JosÃ© Miguel HernÃ¡ndez-Lobato, Rafael Molina
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.19359">Introducing instance label correlation in multiple instance learning. Application to cancer detection on histopathological images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the last years, the weakly supervised paradigm of multiple instance learning (MIL) has become very popular in many different areas. A paradigmatic example is computational pathology, where the lack of patch-level labels for whole-slide images prevents the application of supervised models. Probabilistic MIL methods based on Gaussian Processes (GPs) have obtained promising results due to their excellent uncertainty estimation capabilities. However, these are general-purpose MIL methods that do not take into account one important fact: in (histopathological) images, the labels of neighboring patches are expected to be correlated. In this work, we extend a state-of-the-art GP-based MIL method, which is called VGPMIL-PR, to exploit such correlation. To do so, we develop a novel coupling term inspired by the statistical physics Ising model. We use variational inference to estimate all the model parameters. Interestingly, the VGPMIL-PR formulation is recovered when the weight that regulates the strength of the Ising term vanishes. The performance of the proposed method is assessed in two real-world problems of prostate cancer detection. We show that our model achieves better results than other state-of-the-art probabilistic MIL methods. We also provide different visualizations and analysis to gain insights into the influence of the novel Ising term. These insights are expected to facilitate the application of the proposed model to other research areas.
<div id='section'>Paperid: <span id='pid'>850, <a href='https://arxiv.org/pdf/2309.09721.pdf' target='_blank'>https://arxiv.org/pdf/2309.09721.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhipeng Xue, Zhipeng Gao, Xing Hu, Shanping Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.09721">ACWRecommender: A Tool for Validating Actionable Warnings with Weak Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Static analysis tools have gained popularity among developers for finding potential bugs, but their widespread adoption is hindered by the accomnpanying high false alarm rates (up to 90%). To address this challenge, previous studies proposed the concept of actionable warnings, and apply machine-learning methods to distinguish actionable warnings from false alarms. Despite these efforts, our preliminary study suggests that the current methods used to collect actionable warnings are rather shaky and unreliable, resulting in a large proportion of invalid actionable warnings. In this work, we mined 68,274 reversions from Top-500 Github C repositories to create a substantia actionable warning dataset and assigned weak labels to each warning's likelihood of being a real bug. To automatically identify actionable warnings and recommend those with a high probability of being real bugs (AWHB), we propose a two-stage framework called ACWRecommender. In the first stage, our tool use a pre-trained model, i.e., UniXcoder, to identify actionable warnings from a huge number of SA tool's reported warnings. In the second stage, we rerank valid actionable warnings to the top by using weakly supervised learning. Experimental results showed that our tool outperformed several baselines for actionable warning detection (in terms of F1-score) and performed better for AWHB recommendation (in terms of nDCG and MRR). Additionaly, we also performed an in-the-wild evaluation, we manually validated 24 warnings out of 2,197 reported warnings on 10 randomly selected projects, 22 of which were confirmed by developers as real bugs, demonstrating the practical usage of our tool.
<div id='section'>Paperid: <span id='pid'>851, <a href='https://arxiv.org/pdf/2309.09122.pdf' target='_blank'>https://arxiv.org/pdf/2309.09122.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sejin Park, Taehyung Lee, Yeejin Lee, Byeongkeun Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.09122">FDCNet: Feature Drift Compensation Network for Class-Incremental Weakly Supervised Object Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work addresses the task of class-incremental weakly supervised object localization (CI-WSOL). The goal is to incrementally learn object localization for novel classes using only image-level annotations while retaining the ability to localize previously learned classes. This task is important because annotating bounding boxes for every new incoming data is expensive, although object localization is crucial in various applications. To the best of our knowledge, we are the first to address this task. Thus, we first present a strong baseline method for CI-WSOL by adapting the strategies of class-incremental classifiers to mitigate catastrophic forgetting. These strategies include applying knowledge distillation, maintaining a small data set from previous tasks, and using cosine normalization. We then propose the feature drift compensation network to compensate for the effects of feature drifts on class scores and localization maps. Since updating network parameters to learn new tasks causes feature drifts, compensating for the final outputs is necessary. Finally, we evaluate our proposed method by conducting experiments on two publicly available datasets (ImageNet-100 and CUB-200). The experimental results demonstrate that the proposed method outperforms other baseline methods.
<div id='section'>Paperid: <span id='pid'>852, <a href='https://arxiv.org/pdf/2306.14505.pdf' target='_blank'>https://arxiv.org/pdf/2306.14505.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu-Jen Chen, Xinrong Hu, Yiyu Shi, Tsung-Yi Ho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.14505">AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Magnetic resonance imaging (MRI) is commonly used for brain tumor segmentation, which is critical for patient evaluation and treatment planning. To reduce the labor and expertise required for labeling, weakly-supervised semantic segmentation (WSSS) methods with class activation mapping (CAM) have been proposed. However, existing CAM methods suffer from low resolution due to strided convolution and pooling layers, resulting in inaccurate predictions. In this study, we propose a novel CAM method, Attentive Multiple-Exit CAM (AME-CAM), that extracts activation maps from multiple resolutions to hierarchically aggregate and improve prediction accuracy. We evaluate our method on the BraTS 2021 dataset and show that it outperforms state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>853, <a href='https://arxiv.org/pdf/2306.06982.pdf' target='_blank'>https://arxiv.org/pdf/2306.06982.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jian Wang, Liang Qiao, Shichong Zhou, Jin Zhou, Jun Wang, Juncheng Li, Shihui Ying, Cai Chang, Jun Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.06982">Weakly Supervised Lesion Detection and Diagnosis for Breast Cancers with Partially Annotated Ultrasound Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning (DL) has proven highly effective for ultrasound-based computer-aided diagnosis (CAD) of breast cancers. In an automaticCAD system, lesion detection is critical for the following diagnosis. However, existing DL-based methods generally require voluminous manually-annotated region of interest (ROI) labels and class labels to train both the lesion detection and diagnosis models. In clinical practice, the ROI labels, i.e. ground truths, may not always be optimal for the classification task due to individual experience of sonologists, resulting in the issue of coarse annotation that limits the diagnosis performance of a CAD model. To address this issue, a novel Two-Stage Detection and Diagnosis Network (TSDDNet) is proposed based on weakly supervised learning to enhance diagnostic accuracy of the ultrasound-based CAD for breast cancers. In particular, all the ROI-level labels are considered as coarse labels in the first training stage, and then a candidate selection mechanism is designed to identify optimallesion areas for both the fully and partially annotated samples. It refines the current ROI-level labels in the fully annotated images and the detected ROIs in the partially annotated samples with a weakly supervised manner under the guidance of class labels. In the second training stage, a self-distillation strategy further is further proposed to integrate the detection network and classification network into a unified framework as the final CAD model for joint optimization, which then further improves the diagnosis performance. The proposed TSDDNet is evaluated on a B-mode ultrasound dataset, and the experimental results show that it achieves the best performance on both lesion detection and diagnosis tasks, suggesting promising application potential.
<div id='section'>Paperid: <span id='pid'>854, <a href='https://arxiv.org/pdf/2306.05476.pdf' target='_blank'>https://arxiv.org/pdf/2306.05476.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu-Jen Chen, Yiyu Shi, Tsung-Yi Ho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.05476">A Novel Confidence Induced Class Activation Mapping for MRI Brain Tumor Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Magnetic resonance imaging (MRI) is a commonly used technique for brain tumor segmentation, which is critical for evaluating patients and planning treatment. To make the labeling process less laborious and dependent on expertise, weakly-supervised semantic segmentation (WSSS) methods using class activation mapping (CAM) have been proposed. However, current CAM-based WSSS methods generate the object localization map using internal neural network information, such as gradient or trainable parameters, which can lead to suboptimal solutions. To address these issues, we propose the confidence-induced CAM (Cfd-CAM), which calculates the weight of each feature map by using the confidence of the target class. Our experiments on two brain tumor datasets show that Cfd-CAM outperforms existing state-of-the-art methods under the same level of supervision. Overall, our proposed Cfd-CAM approach improves the accuracy of brain tumor segmentation and may provide valuable insights for developing better WSSS methods for other medical imaging tasks.
<div id='section'>Paperid: <span id='pid'>855, <a href='https://arxiv.org/pdf/2306.03878.pdf' target='_blank'>https://arxiv.org/pdf/2306.03878.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinrong Hu, Yu-Jen Chen, Tsung-Yi Ho, Yiyu Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.03878">Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in denoising diffusion probabilistic models have shown great success in image synthesis tasks. While there are already works exploring the potential of this powerful tool in image semantic segmentation, its application in weakly supervised semantic segmentation (WSSS) remains relatively under-explored. Observing that conditional diffusion models (CDM) is capable of generating images subject to specific distributions, in this work, we utilize category-aware semantic information underlied in CDM to get the prediction mask of the target object with only image-level annotations. More specifically, we locate the desired class by approximating the derivative of the output of CDM w.r.t the input condition. Our method is different from previous diffusion model methods with guidance from an external classifier, which accumulates noises in the background during the reconstruction process. Our method outperforms state-of-the-art CAM and diffusion model methods on two public medical image segmentation datasets, which demonstrates that CDM is a promising tool in WSSS. Also, experiment shows our method is more time-efficient than existing diffusion model methods, making it practical for wider applications.
<div id='section'>Paperid: <span id='pid'>856, <a href='https://arxiv.org/pdf/2304.09913.pdf' target='_blank'>https://arxiv.org/pdf/2304.09913.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sanghyun Jo, In-Jae Yu, Kyungsu Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.09913">MARS: Model-agnostic Biased Object Removal without Additional Supervision for Weakly-Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised semantic segmentation aims to reduce labeling costs by training semantic segmentation models using weak supervision, such as image-level class labels. However, most approaches struggle to produce accurate localization maps and suffer from false predictions in class-related backgrounds (i.e., biased objects), such as detecting a railroad with the train class. Recent methods that remove biased objects require additional supervision for manually identifying biased objects for each problematic class and collecting their datasets by reviewing predictions, limiting their applicability to the real-world dataset with multiple labels and complex relationships for biasing. Following the first observation that biased features can be separated and eliminated by matching biased objects with backgrounds in the same dataset, we propose a fully-automatic/model-agnostic biased removal framework called MARS (Model-Agnostic biased object Removal without additional Supervision), which utilizes semantically consistent features of an unsupervised technique to eliminate biased objects in pseudo labels. Surprisingly, we show that MARS achieves new state-of-the-art results on two popular benchmarks, PASCAL VOC 2012 (val: 77.7%, test: 77.2%) and MS COCO 2014 (val: 49.4%), by consistently improving the performance of various WSSS models by at least 30% without additional supervision.
<div id='section'>Paperid: <span id='pid'>857, <a href='https://arxiv.org/pdf/2304.07967.pdf' target='_blank'>https://arxiv.org/pdf/2304.07967.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zidong Cao, Hao Ai, Athanasios V. Vasilakos, Lin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.07967">360$^\circ$ High-Resolution Depth Estimation via Uncertainty-aware Structural Knowledge Transfer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To predict high-resolution (HR) omnidirectional depth map, existing methods typically leverage HR omnidirectional image (ODI) as the input via fully-supervised learning. However, in practice, taking HR ODI as input is undesired due to resource-constrained devices. In addition, depth maps are often with lower resolution than color images. Therefore, in this paper, we explore for the first time to estimate the HR omnidirectional depth directly from a low-resolution (LR) ODI, when no HR depth GT map is available. Our key idea is to transfer the scene structural knowledge from the HR image modality and the corresponding LR depth maps to achieve the goal of HR depth estimation without any extra inference cost. Specifically, we introduce ODI super-resolution (SR) as an auxiliary task and train both tasks collaboratively in a weakly supervised manner to boost the performance of HR depth estimation. The ODI SR task extracts the scene structural knowledge via uncertainty estimation. Buttressed by this, a scene structural knowledge transfer (SSKT) module is proposed with two key components. First, we employ a cylindrical implicit interpolation function (CIIF) to learn cylindrical neural interpolation weights for feature up-sampling and share the parameters of CIIFs between the two tasks. Then, we propose a feature distillation (FD) loss that provides extra structural regularization to help the HR depth estimation task learn more scene structural knowledge. Extensive experiments demonstrate that our weakly-supervised method outperforms baseline methods, and even achieves comparable performance with the fully-supervised methods.
<div id='section'>Paperid: <span id='pid'>858, <a href='https://arxiv.org/pdf/2303.01342.pdf' target='_blank'>https://arxiv.org/pdf/2303.01342.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ario Sadafi, Nassir Navab, Carsten Marr
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.01342">Active Learning Enhances Classification of Histopathology Whole Slide Images with Attention-based Multiple Instance Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In many histopathology tasks, sample classification depends on morphological details in tissue or single cells that are only visible at the highest magnification. For a pathologist, this implies tedious zooming in and out, while for a computational decision support algorithm, it leads to the analysis of a huge number of small image patches per whole slide image (WSI). Attention-based multiple instance learning (MIL), where attention estimation is learned in a weakly supervised manner, has been successfully applied in computational histopathology, but it is challenged by large numbers of irrelevant patches, reducing its accuracy. Here, we present an active learning approach to the problem. Querying the expert to annotate regions of interest in a WSI guides the formation of high-attention regions for MIL. We train an attention-based MIL and calculate a confidence metric for every image in the dataset to select the most uncertain WSIs for expert annotation. We test our approach on the CAMELYON17 dataset classifying metastatic lymph node sections in breast cancer. With a novel attention guiding loss, this leads to an accuracy boost of the trained models with few regions annotated for each class. Active learning thus improves WSIs classification accuracy, leads to faster and more robust convergence, and speeds up the annotation process. It may in the future serve as an important contribution to train MIL models in the clinically relevant context of cancer classification in histopathology.
<div id='section'>Paperid: <span id='pid'>859, <a href='https://arxiv.org/pdf/2302.04061.pdf' target='_blank'>https://arxiv.org/pdf/2302.04061.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arne Schmidt, Pablo Morales-Ãlvarez, Rafael Molina
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.04061">Probabilistic Attention based on Gaussian Processes for Deep Multiple Instance Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple Instance Learning (MIL) is a weakly supervised learning paradigm that is becoming increasingly popular because it requires less labeling effort than fully supervised methods. This is especially interesting for areas where the creation of large annotated datasets remains challenging, as in medicine. Although recent deep learning MIL approaches have obtained state-of-the-art results, they are fully deterministic and do not provide uncertainty estimations for the predictions. In this work, we introduce the Attention Gaussian Process (AGP) model, a novel probabilistic attention mechanism based on Gaussian Processes for deep MIL. AGP provides accurate bag-level predictions as well as instance-level explainability, and can be trained end-to-end. Moreover, its probabilistic nature guarantees robustness to overfitting on small datasets and uncertainty estimations for the predictions. The latter is especially important in medical applications, where decisions have a direct impact on the patient's health. The proposed model is validated experimentally as follows. First, its behavior is illustrated in two synthetic MIL experiments based on the well-known MNIST and CIFAR-10 datasets, respectively. Then, it is evaluated in three different real-world cancer detection experiments. AGP outperforms state-of-the-art MIL approaches, including deterministic deep learning ones. It shows a strong performance even on a small dataset with less than 100 labels and generalizes better than competing methods on an external test set. Moreover, we experimentally show that predictive uncertainty correlates with the risk of wrong predictions, and therefore it is a good indicator of reliability in practice. Our code is publicly available.
<div id='section'>Paperid: <span id='pid'>860, <a href='https://arxiv.org/pdf/2510.22209.pdf' target='_blank'>https://arxiv.org/pdf/2510.22209.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sofoklis Kitharidis, Cor J. Veenman, Thomas Bäck, Niki van Stein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.22209">Visual Model Selection using Feature Importance Clusters in Fairness-Performance Similarity Optimized Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the context of algorithmic decision-making, fair machine learning methods often yield multiple models that balance predictive fairness and performance in varying degrees. This diversity introduces a challenge for stakeholders who must select a model that aligns with their specific requirements and values. To address this, we propose an interactive framework that assists in navigating and interpreting the trade-offs across a portfolio of models. Our approach leverages weakly supervised metric learning to learn a Mahalanobis distance that reflects similarity in fairness and performance outcomes, effectively structuring the feature importance space of the models according to stakeholder-relevant criteria. We then apply clustering technique (k-means) to group models based on their transformed representations of feature importances, allowing users to explore clusters of models with similar predictive behaviors and fairness characteristics. This facilitates informed decision-making by helping users understand how models differ not only in their fairness-performance balance but also in the features that drive their predictions.
<div id='section'>Paperid: <span id='pid'>861, <a href='https://arxiv.org/pdf/2510.17484.pdf' target='_blank'>https://arxiv.org/pdf/2510.17484.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Umer Ramzan, Ali Zia, Abdelwahed Khamis, Noman Ali, Usman Ali, Wei Xiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17484">Split-Fuse-Transport: Annotation-Free Saliency via Dual Clustering and Optimal Transport Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Salient object detection (SOD) aims to segment visually prominent regions in images and serves as a foundational task for various computer vision applications. We posit that SOD can now reach near-supervised accuracy without a single pixel-level label, but only when reliable pseudo-masks are available. We revisit the prototype-based line of work and make two key observations. First, boundary pixels and interior pixels obey markedly different geometry; second, the global consistency enforced by optimal transport (OT) is underutilized if prototype quality is weak. To address this, we introduce POTNet, an adaptation of Prototypical Optimal Transport that replaces POT's single k-means step with an entropy-guided dual-clustering head: high-entropy pixels are organized by spectral clustering, low-entropy pixels by k-means, and the two prototype sets are subsequently aligned by OT. This split-fuse-transport design yields sharper, part-aware pseudo-masks in a single forward pass, without handcrafted priors. Those masks supervise a standard MaskFormer-style encoder-decoder, giving rise to AutoSOD, an end-to-end unsupervised SOD pipeline that eliminates SelfMask's offline voting yet improves both accuracy and training efficiency. Extensive experiments on five benchmarks show that AutoSOD outperforms unsupervised methods by up to 26% and weakly supervised methods by up to 36% in F-measure, further narrowing the gap to fully supervised models.
<div id='section'>Paperid: <span id='pid'>862, <a href='https://arxiv.org/pdf/2508.11472.pdf' target='_blank'>https://arxiv.org/pdf/2508.11472.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Wang, Yaxin Zhao, Xinyu Jiao, Sihan Xu, Xiangrui Cai, Ying Zhang, Xiaojie Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11472">RMSL: Weakly-Supervised Insider Threat Detection with Robust Multi-sphere Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Insider threat detection aims to identify malicious user behavior by analyzing logs that record user interactions. Due to the lack of fine-grained behavior-level annotations, detecting specific behavior-level anomalies within user behavior sequences is challenging. Unsupervised methods face high false positive rates and miss rates due to the inherent ambiguity between normal and anomalous behaviors. In this work, we instead introduce weak labels of behavior sequences, which have lower annotation costs, i.e., the training labels (anomalous or normal) are at sequence-level instead of behavior-level, to enhance the detection capability for behavior-level anomalies by learning discriminative features. To achieve this, we propose a novel framework called Robust Multi-sphere Learning (RMSL). RMSL uses multiple hyper-spheres to represent the normal patterns of behaviors. Initially, a one-class classifier is constructed as a good anomaly-supervision-free starting point. Building on this, using multiple instance learning and adaptive behavior-level self-training debiasing based on model prediction confidence, the framework further refines hyper-spheres and feature representations using weak sequence-level labels. This approach enhances the model's ability to distinguish between normal and anomalous behaviors. Extensive experiments demonstrate that RMSL significantly improves the performance of behavior-level insider threat detection.
<div id='section'>Paperid: <span id='pid'>863, <a href='https://arxiv.org/pdf/2507.15203.pdf' target='_blank'>https://arxiv.org/pdf/2507.15203.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyue Liu, Xicheng Sheng, Xiahai Zhuang, Vicente Grau, Mark YY Chan, Ching-Hui Sia, Lei Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15203">Personalized 4D Whole Heart Geometry Reconstruction from Cine MRI for Cardiac Digital Twins</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cardiac digital twins (CDTs) provide personalized in-silico cardiac representations and hold great potential for precision medicine in cardiology. However, whole-heart CDT models that simulate the full organ-scale electromechanics of all four heart chambers remain limited. In this work, we propose a weakly supervised learning model to reconstruct 4D (3D+t) heart mesh directly from multi-view 2D cardiac cine MRIs. This is achieved by learning a self-supervised mapping between cine MRIs and 4D cardiac meshes, enabling the generation of personalized heart models that closely correspond to input cine MRIs. The resulting 4D heart meshes can facilitate the automatic extraction of key cardiac variables, including ejection fraction and dynamic chamber volume changes with high temporal resolution. It demonstrates the feasibility of inferring personalized 4D heart models from cardiac MRIs, paving the way for an efficient CDT platform for precision medicine. The code will be publicly released once the manuscript is accepted.
<div id='section'>Paperid: <span id='pid'>864, <a href='https://arxiv.org/pdf/2506.06006.pdf' target='_blank'>https://arxiv.org/pdf/2506.06006.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifu Qiu, Yftah Ziser, Anna Korhonen, Shay B. Cohen, Edoardo M. Ponti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06006">Bootstrapping World Models from Dynamics Models in Multimodal Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To what extent do vision-and-language foundation models possess a realistic world model (observation $\times$ action $\rightarrow$ observation) and a dynamics model (observation $\times$ observation $\rightarrow$ action), when actions are expressed through language? While open-source foundation models struggle with both, we find that fine-tuning them to acquire a dynamics model through supervision is significantly easier than acquiring a world model. In turn, dynamics models can be used to bootstrap world models through two main strategies: 1) weakly supervised learning from synthetic data and 2) inference time verification. Firstly, the dynamics model can annotate actions for unlabelled pairs of video frame observations to expand the training data. We further propose a new objective, where image tokens in observation pairs are weighted by their importance, as predicted by a recognition model. Secondly, the dynamics models can assign rewards to multiple samples of the world model to score them, effectively guiding search at inference time. We evaluate the world models resulting from both strategies through the task of action-centric image editing on Aurora-Bench. Our best model achieves a performance competitive with state-of-the-art image editing models, improving on them by a margin of $15\%$ on real-world subsets according to GPT4o-as-judge, and achieving the best average human evaluation across all subsets of Aurora-Bench.
<div id='section'>Paperid: <span id='pid'>865, <a href='https://arxiv.org/pdf/2503.11376.pdf' target='_blank'>https://arxiv.org/pdf/2503.11376.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Panggih Kusuma Ningrum, Philipp Mayr, Nina Smirnova, Iana Atanassova
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11376">Annotating Scientific Uncertainty: A comprehensive model using linguistic patterns and comparison with existing approaches</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>UnScientify, a system designed to detect scientific uncertainty in scholarly full text. The system utilizes a weakly supervised technique to identify verbally expressed uncertainty in scientific texts and their authorial references. The core methodology of UnScientify is based on a multi-faceted pipeline that integrates span pattern matching, complex sentence analysis and author reference checking. This approach streamlines the labeling and annotation processes essential for identifying scientific uncertainty, covering a variety of uncertainty expression types to support diverse applications including information retrieval, text mining and scientific document processing. The evaluation results highlight the trade-offs between modern large language models (LLMs) and the UnScientify system. UnScientify, which employs more traditional techniques, achieved superior performance in the scientific uncertainty detection task, attaining an accuracy score of 0.808. This finding underscores the continued relevance and efficiency of UnScientify's simple rule-based and pattern matching strategy for this specific application. The results demonstrate that in scenarios where resource efficiency, interpretability, and domain-specific adaptability are critical, traditional methods can still offer significant advantages.
<div id='section'>Paperid: <span id='pid'>866, <a href='https://arxiv.org/pdf/2411.08753.pdf' target='_blank'>https://arxiv.org/pdf/2411.08753.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sagnik Majumder, Tushar Nagarajan, Ziad Al-Halah, Reina Pradhan, Kristen Grauman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08753">Which Viewpoint Shows it Best? Language for Weakly Supervising View Selection in Multi-view Instructional Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Given a multi-view video, which viewpoint is most informative for a human observer? Existing methods rely on heuristics or expensive "best-view" supervision to answer this question, limiting their applicability. We propose a weakly supervised approach that leverages language accompanying an instructional multi-view video as a means to recover its most informative viewpoint(s). Our key hypothesis is that the more accurately an individual view can predict a view-agnostic text summary, the more informative it is. To put this into action, we propose LangView, a framework that uses the relative accuracy of view-dependent caption predictions as a proxy for best view pseudo-labels. Then, those pseudo-labels are used to train a view selector, together with an auxiliary camera pose predictor that enhances view-sensitivity. During inference, our model takes as input only a multi-view video--no language or camera poses--and returns the best viewpoint to watch at each timestep. On two challenging datasets comprised of diverse multi-camera setups and how-to activities, our model consistently outperforms state-of-the-art baselines, both with quantitative metrics and human evaluation. Project page: https://vision.cs.utexas.edu/projects/which-view-shows-it-best.
<div id='section'>Paperid: <span id='pid'>867, <a href='https://arxiv.org/pdf/2410.19176.pdf' target='_blank'>https://arxiv.org/pdf/2410.19176.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dachun Sun, Ruijie Wang, Jinning Li, Ruipeng Han, Xinyi Liu, You Lyu, Tarek Abdelzaher
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.19176">Perturbation-based Graph Active Learning for Weakly-Supervised Belief Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the problem of optimizing the allocation of labeling resources for semi-supervised belief representation learning in social networks. The objective is to strategically identify valuable messages on social media graphs that are worth labeling within a constrained budget, ultimately maximizing the task's performance. Despite the progress in unsupervised or semi-supervised methods in advancing belief and ideology representation learning on social networks and the remarkable efficacy of graph learning techniques, the availability of high-quality curated labeled social data can greatly benefit and further improve performances. Consequently, allocating labeling efforts is a critical research problem in scenarios where labeling resources are limited. This paper proposes a graph data augmentation-inspired perturbation-based active learning strategy (PerbALGraph) that progressively selects messages for labeling according to an automatic estimator, obviating human guidance. This estimator is based on the principle that messages in the network that exhibit heightened sensitivity to structural features of the observational data indicate landmark quality that significantly influences semi-supervision processes. We design the estimator to be the prediction variance under a set of designed graph perturbations, which is model-agnostic and application-independent. Extensive experiment results demonstrate the effectiveness of the proposed strategy for belief representation learning tasks.
<div id='section'>Paperid: <span id='pid'>868, <a href='https://arxiv.org/pdf/2410.08091.pdf' target='_blank'>https://arxiv.org/pdf/2410.08091.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyi Pan, Wei Gao, Shan Liu, Ge Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08091">Distribution Guidance Network for Weakly Supervised Point Cloud Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite alleviating the dependence on dense annotations inherent to fully supervised methods, weakly supervised point cloud semantic segmentation suffers from inadequate supervision signals. In response to this challenge, we introduce a novel perspective that imparts auxiliary constraints by regulating the feature space under weak supervision. Our initial investigation identifies which distributions accurately characterize the feature space, subsequently leveraging this priori to guide the alignment of the weakly supervised embeddings. Specifically, we analyze the superiority of the mixture of von Mises-Fisher distributions (moVMF) among several common distribution candidates. Accordingly, we develop a Distribution Guidance Network (DGNet), which comprises a weakly supervised learning branch and a distribution alignment branch. Leveraging reliable clustering initialization derived from the weakly supervised learning branch, the distribution alignment branch alternately updates the parameters of the moVMF and the network, ensuring alignment with the moVMF-defined latent space. Extensive experiments validate the rationality and effectiveness of our distribution choice and network design. Consequently, DGNet achieves state-of-the-art performance under multiple datasets and various weakly supervised settings.
<div id='section'>Paperid: <span id='pid'>869, <a href='https://arxiv.org/pdf/2409.16566.pdf' target='_blank'>https://arxiv.org/pdf/2409.16566.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kartikeya Singh, Yash Turkar, Christo Aluckal, Charuvarahan Adhivarahan, Karthik Dantu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16566">PANOS: Payload-Aware Navigation in Offroad Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nature has evolved humans to walk on different terrains by developing a detailed understanding of their physical characteristics. Similarly, legged robots need to develop their capability to walk on complex terrains with a variety of task-dependent payloads to achieve their goals. However, conventional terrain adaptation methods are susceptible to failure with varying payloads. In this work, we introduce PANOS, a weakly supervised approach that integrates proprioception and exteroception from onboard sensing to achieve a stable gait while walking by a legged robot over various terrains. Our work also provides evidence of its adaptability over varying payloads. We evaluate our method on multiple terrains and payloads using a legged robot. PANOS improves the stability up to 44% without any payload and 53% with 15 lbs payload. We also notice a reduction in the vibration cost of 20% with the payload for various terrain types when compared to state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>870, <a href='https://arxiv.org/pdf/2408.02773.pdf' target='_blank'>https://arxiv.org/pdf/2408.02773.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinmiao Zhao, Zelin Shi, Chuang Yu, Yunpeng Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.02773">Refined Infrared Small Target Detection Scheme with Single-Point Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, infrared small target detection with single-point supervision has attracted extensive attention. However, the detection accuracy of existing methods has difficulty meeting actual needs. Therefore, we propose an innovative refined infrared small target detection scheme with single-point supervision, which has excellent segmentation accuracy and detection rate. Specifically, we introduce label evolution with single point supervision (LESPS) framework and explore the performance of various excellent infrared small target detection networks based on this framework. Meanwhile, to improve the comprehensive performance, we construct a complete post-processing strategy. On the one hand, to improve the segmentation accuracy, we use a combination of test-time augmentation (TTA) and conditional random field (CRF) for post-processing. On the other hand, to improve the detection rate, we introduce an adjustable sensitivity (AS) strategy for post-processing, which fully considers the advantages of multiple detection results and reasonably adds some areas with low confidence to the fine segmentation image in the form of centroid points. In addition, to further improve the performance and explore the characteristics of this task, on the one hand, we construct and find that a multi-stage loss is helpful for fine-grained detection. On the other hand, we find that a reasonable sliding window cropping strategy for test samples has better performance for actual multi-size samples. Extensive experimental results show that the proposed scheme achieves state-of-the-art (SOTA) performance. Notably, the proposed scheme won the third place in the "ICPR 2024 Resource-Limited Infrared Small Target Detection Challenge Track 1: Weakly Supervised Infrared Small Target Detection".
<div id='section'>Paperid: <span id='pid'>871, <a href='https://arxiv.org/pdf/2407.13157.pdf' target='_blank'>https://arxiv.org/pdf/2407.13157.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jin Zhang, Ruiheng Zhang, Yanjiao Shi, Zhe Cao, Nian Liu, Fahad Shahbaz Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.13157">Learning Camouflaged Object Detection from Noisy Pseudo Label</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing Camouflaged Object Detection (COD) methods rely heavily on large-scale pixel-annotated training sets, which are both time-consuming and labor-intensive. Although weakly supervised methods offer higher annotation efficiency, their performance is far behind due to the unclear visual demarcations between foreground and background in camouflaged images. In this paper, we explore the potential of using boxes as prompts in camouflaged scenes and introduce the first weakly semi-supervised COD method, aiming for budget-efficient and high-precision camouflaged object segmentation with an extremely limited number of fully labeled images. Critically, learning from such limited set inevitably generates pseudo labels with serious noisy pixels. To address this, we propose a noise correction loss that facilitates the model's learning of correct pixels in the early learning stage, and corrects the error risk gradients dominated by noisy pixels in the memorization stage, ultimately achieving accurate segmentation of camouflaged objects from noisy labels. When using only 20% of fully labeled data, our method shows superior performance over the state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>872, <a href='https://arxiv.org/pdf/2407.10814.pdf' target='_blank'>https://arxiv.org/pdf/2407.10814.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Linhao Qu, Dingkang Yang, Dan Huang, Qinhao Guo, Rongkui Luo, Shaoting Zhang, Xiaosong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10814">Pathology-knowledge Enhanced Multi-instance Prompt Learning for Few-shot Whole Slide Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current multi-instance learning algorithms for pathology image analysis often require a substantial number of Whole Slide Images for effective training but exhibit suboptimal performance in scenarios with limited learning data. In clinical settings, restricted access to pathology slides is inevitable due to patient privacy concerns and the prevalence of rare or emerging diseases. The emergence of the Few-shot Weakly Supervised WSI Classification accommodates the significant challenge of the limited slide data and sparse slide-level labels for diagnosis. Prompt learning based on the pre-trained models (\eg, CLIP) appears to be a promising scheme for this setting; however, current research in this area is limited, and existing algorithms often focus solely on patch-level prompts or confine themselves to language prompts. This paper proposes a multi-instance prompt learning framework enhanced with pathology knowledge, \ie, integrating visual and textual prior knowledge into prompts at both patch and slide levels. The training process employs a combination of static and learnable prompts, effectively guiding the activation of pre-trained models and further facilitating the diagnosis of key pathology patterns. Lightweight Messenger (self-attention) and Summary (attention-pooling) layers are introduced to model relationships between patches and slides within the same patient data. Additionally, alignment-wise contrastive losses ensure the feature-level alignment between visual and textual learnable prompts for both patches and slides. Our method demonstrates superior performance in three challenging clinical tasks, significantly outperforming comparative few-shot methods.
<div id='section'>Paperid: <span id='pid'>873, <a href='https://arxiv.org/pdf/2406.19638.pdf' target='_blank'>https://arxiv.org/pdf/2406.19638.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junsung Park, Hyunjung Shim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.19638">Precision matters: Precision-aware ensemble for weakly supervised semantic segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Semantic Segmentation (WSSS) employs weak supervision, such as image-level labels, to train the segmentation model. Despite the impressive achievement in recent WSSS methods, we identify that introducing weak labels with high mean Intersection of Union (mIoU) does not guarantee high segmentation performance. Existing studies have emphasized the importance of prioritizing precision and reducing noise to improve overall performance. In the same vein, we propose ORANDNet, an advanced ensemble approach tailored for WSSS. ORANDNet combines Class Activation Maps (CAMs) from two different classifiers to increase the precision of pseudo-masks (PMs). To further mitigate small noise in the PMs, we incorporate curriculum learning. This involves training the segmentation model initially with pairs of smaller-sized images and corresponding PMs, gradually transitioning to the original-sized pairs. By combining the original CAMs of ResNet-50 and ViT, we significantly improve the segmentation performance over the single-best model and the naive ensemble model, respectively. We further extend our ensemble method to CAMs from AMN (ResNet-like) and MCTformer (ViT-like) models, achieving performance benefits in advanced WSSS models. It highlights the potential of our ORANDNet as a final add-on module for WSSS models.
<div id='section'>Paperid: <span id='pid'>874, <a href='https://arxiv.org/pdf/2406.15284.pdf' target='_blank'>https://arxiv.org/pdf/2406.15284.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Georgios Paraskevopoulos, Chara Tsoukala, Athanasios Katsamanis, Vassilis Katsouros
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.15284">The Greek podcast corpus: Competitive speech models for low-resourced languages with weakly supervised data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of speech technologies for languages with limited digital representation poses significant challenges, primarily due to the scarcity of available data. This issue is exacerbated in the era of large, data-intensive models. Recent research has underscored the potential of leveraging weak supervision to augment the pool of available data. In this study, we compile an 800-hour corpus of Modern Greek from podcasts and employ Whisper large-v3 to generate silver transcriptions. This corpus is utilized to fine-tune our models, aiming to assess the efficacy of this approach in enhancing ASR performance. Our analysis spans 16 distinct podcast domains, alongside evaluations on established datasets for Modern Greek. The findings indicate consistent WER improvements, correlating with increases in both data volume and model size. Our study confirms that assembling large, weakly supervised corpora serves as a cost-effective strategy for advancing speech technologies in under-resourced languages.
<div id='section'>Paperid: <span id='pid'>875, <a href='https://arxiv.org/pdf/2406.01791.pdf' target='_blank'>https://arxiv.org/pdf/2406.01791.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weitong Cai, Jiabo Huang, Shaogang Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01791">Hybrid-Learning Video Moment Retrieval across Multi-Domain Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video moment retrieval (VMR) is to search for a visual temporal moment in an untrimmed raw video by a given text query description (sentence). Existing studies either start from collecting exhaustive frame-wise annotations on the temporal boundary of target moments (fully-supervised), or learn with only the video-level video-text pairing labels (weakly-supervised). The former is poor in generalisation to unknown concepts and/or novel scenes due to restricted dataset scale and diversity under expensive annotation costs; the latter is subject to visual-textual mis-correlations from incomplete labels. In this work, we introduce a new approach called hybrid-learning video moment retrieval to solve the problem by knowledge transfer through adapting the video-text matching relationships learned from a fully-supervised source domain to a weakly-labelled target domain when they do not share a common label space. Our aim is to explore shared universal knowledge between the two domains in order to improve model learning in the weakly-labelled target domain. Specifically, we introduce a multiplE branch Video-text Alignment model (EVA) that performs cross-modal (visual-textual) matching information sharing and multi-modal feature alignment to optimise domain-invariant visual and textual features as well as per-task discriminative joint video-text representations. Experiments show EVA's effectiveness in exploring temporal segment annotations in a source domain to help learn video moment retrieval without temporal labels in a target domain.
<div id='section'>Paperid: <span id='pid'>876, <a href='https://arxiv.org/pdf/2404.12784.pdf' target='_blank'>https://arxiv.org/pdf/2404.12784.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Myrna C. Silva, Mahtab Dahaghin, Matteo Toso, Alessio Del Bue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.12784">Contrastive Gaussian Clustering: Weakly Supervised 3D Scene Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Contrastive Gaussian Clustering, a novel approach capable of provide segmentation masks from any viewpoint and of enabling 3D segmentation of the scene. Recent works in novel-view synthesis have shown how to model the appearance of a scene via a cloud of 3D Gaussians, and how to generate accurate images from a given viewpoint by projecting on it the Gaussians before $Î±$ blending their color. Following this example, we train a model to include also a segmentation feature vector for each Gaussian. These can then be used for 3D scene segmentation, by clustering Gaussians according to their feature vectors; and to generate 2D segmentation masks, by projecting the Gaussians on a plane and $Î±$ blending over their segmentation features. Using a combination of contrastive learning and spatial regularization, our method can be trained on inconsistent 2D segmentation masks, and still learn to generate segmentation masks consistent across all views. Moreover, the resulting model is extremely accurate, improving the IoU accuracy of the predicted masks by $+8\%$ over the state of the art. Code and trained models will be released soon.
<div id='section'>Paperid: <span id='pid'>877, <a href='https://arxiv.org/pdf/2403.04558.pdf' target='_blank'>https://arxiv.org/pdf/2403.04558.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tim Lenz, Omar S. M. El Nahhas, Marta Ligero, Jakob Nikolas Kather
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.04558">Reducing self-supervised learning complexity improves weakly-supervised classification performance in computational pathology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep Learning models have been successfully utilized to extract clinically actionable insights from routinely available histology data. Generally, these models require annotations performed by clinicians, which are scarce and costly to generate. The emergence of self-supervised learning (SSL) methods remove this barrier, allowing for large-scale analyses on non-annotated data. However, recent SSL approaches apply increasingly expansive model architectures and larger datasets, causing the rapid escalation of data volumes, hardware prerequisites, and overall expenses, limiting access to these resources to few institutions. Therefore, we investigated the complexity of contrastive SSL in computational pathology in relation to classification performance with the utilization of consumer-grade hardware. Specifically, we analyzed the effects of adaptations in data volume, architecture, and algorithms on downstream classification tasks, emphasizing their impact on computational resources. We trained breast cancer foundation models on a large public patient cohort and validated them on various downstream classification tasks in a weakly supervised manner on two external public patient cohorts. Our experiments demonstrate that we can improve downstream classification performance whilst reducing SSL training duration by 90%. In summary, we propose a set of adaptations which enable the utilization of SSL in computational pathology in non-resource abundant environments.
<div id='section'>Paperid: <span id='pid'>878, <a href='https://arxiv.org/pdf/2402.17601.pdf' target='_blank'>https://arxiv.org/pdf/2402.17601.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matthias Boeker, Vajira Thambawita, Michael Riegler, PÃ¥l Halvorsen, Hugo L. Hammer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17601">Advancing sleep detection by modelling weak label sets: A novel weakly supervised learning approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding sleep and activity patterns plays a crucial role in physical and mental health. This study introduces a novel approach for sleep detection using weakly supervised learning for scenarios where reliable ground truth labels are unavailable. The proposed method relies on a set of weak labels, derived from the predictions generated by conventional sleep detection algorithms. Introducing a novel approach, we suggest a novel generalised non-linear statistical model in which the number of weak sleep labels is modelled as outcome of a binomial distribution. The probability of sleep in the binomial distribution is linked to the outcomes of neural networks trained to detect sleep based on actigraphy. We show that maximizing the likelihood function of the model, is equivalent to minimizing the soft cross-entropy loss. Additionally, we explored the use of the Brier score as a loss function for weak labels. The efficacy of the suggested modelling framework was demonstrated using the Multi-Ethnic Study of Atherosclerosis dataset. A \gls{lstm} trained on the soft cross-entropy outperformed conventional sleep detection algorithms, other neural network architectures and loss functions in accuracy and model calibration. This research not only advances sleep detection techniques in scenarios where ground truth data is scarce but also contributes to the broader field of weakly supervised learning by introducing innovative approach in modelling sets of weak labels.
<div id='section'>Paperid: <span id='pid'>879, <a href='https://arxiv.org/pdf/2402.08892.pdf' target='_blank'>https://arxiv.org/pdf/2402.08892.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiqi Peng, Bolin Lai, Guangyu Yao, Xiaoyun Zhang, Ya Zhang, Yan-Feng Wang, Hui Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.08892">Weakly Supervised Segmentation of Vertebral Bodies with Iterative Slice-propagation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vertebral body (VB) segmentation is an important preliminary step towards medical visual diagnosis for spinal diseases. However, most previous works require pixel/voxel-wise strong supervisions, which is expensive, tedious and time-consuming for experts to annotate. In this paper, we propose a Weakly supervised Iterative Spinal Segmentation (WISS) method leveraging only four corner landmark weak labels on a single sagittal slice to achieve automatic volumetric segmentation from CT images for VBs. WISS first segments VBs on an annotated sagittal slice in an iterative self-training manner. This self-training method alternates between training and refining labels in the training set. Then WISS proceeds to segment the whole VBs slice by slice with a slice-propagation method to obtain volumetric segmentations. We evaluate the performance of WISS on a private spinal metastases CT dataset and the public lumbar CT dataset. On the first dataset, WISS achieves distinct improvements with regard to two different backbones. For the second dataset, WISS achieves dice coefficients of $91.7\%$ and $83.7\%$ for mid-sagittal slices and 3D CT volumes, respectively, saving a lot of labeling costs and only sacrificing a little segmentation performance.
<div id='section'>Paperid: <span id='pid'>880, <a href='https://arxiv.org/pdf/2402.03345.pdf' target='_blank'>https://arxiv.org/pdf/2402.03345.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Antoine Collas, RÃ©mi Flamary, Alexandre Gramfort
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.03345">Weakly supervised covariance matrices alignment through Stiefel matrices estimation for MEG applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel domain adaptation technique for time series data, called Mixing model Stiefel Adaptation (MSA), specifically addressing the challenge of limited labeled signals in the target dataset. Leveraging a domain-dependent mixing model and the optimal transport domain adaptation assumption, we exploit abundant unlabeled data in the target domain to ensure effective prediction by establishing pairwise correspondence with equivalent signal variances between domains. Theoretical foundations are laid for identifying crucial Stiefel matrices, essential for recovering underlying signal variances from a Riemannian representation of observed signal covariances. We propose an integrated cost function that simultaneously learns these matrices, pairwise domain relationships, and a predictor, classifier, or regressor, depending on the task. Applied to neuroscience problems, MSA outperforms recent methods in brain-age regression with task variations using magnetoencephalography (MEG) signals from the Cam-CAN dataset.
<div id='section'>Paperid: <span id='pid'>881, <a href='https://arxiv.org/pdf/2402.00899.pdf' target='_blank'>https://arxiv.org/pdf/2402.00899.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ivan Y. Tyukin, Tatiana Tyukina, Daniel van Helden, Zedong Zheng, Evgeny M. Mirkes, Oliver J. Sutton, Qinghua Zhou, Alexander N. Gorban, Penelope Allison
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.00899">Weakly Supervised Learners for Correction of AI Errors with Provable Performance Guarantees</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a new methodology for handling AI errors by introducing weakly supervised AI error correctors with a priori performance guarantees. These AI correctors are auxiliary maps whose role is to moderate the decisions of some previously constructed underlying classifier by either approving or rejecting its decisions. The rejection of a decision can be used as a signal to suggest abstaining from making a decision. A key technical focus of the work is in providing performance guarantees for these new AI correctors through bounds on the probabilities of incorrect decisions. These bounds are distribution agnostic and do not rely on assumptions on the data dimension. Our empirical example illustrates how the framework can be applied to improve the performance of an image classifier in a challenging real-world task where training data are scarce.
<div id='section'>Paperid: <span id='pid'>882, <a href='https://arxiv.org/pdf/2312.06259.pdf' target='_blank'>https://arxiv.org/pdf/2312.06259.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyi Pan, Nan Zhang, Wei Gao, Shan Liu, Ge Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.06259">Point Cloud Semantic Segmentation with Sparse and Inhomogeneous Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Utilizing uniformly distributed sparse annotations, weakly supervised learning alleviates the heavy reliance on fine-grained annotations in point cloud semantic segmentation tasks. However, few works discuss the inhomogeneity of sparse annotations, albeit it is common in real-world scenarios. Therefore, this work introduces the probability density function into the gradient sampling approximation method to qualitatively analyze the impact of annotation sparsity and inhomogeneity under weakly supervised learning. Based on our analysis, we propose an Adaptive Annotation Distribution Network (AADNet) capable of robust learning on arbitrarily distributed sparse annotations. Specifically, we propose a label-aware point cloud downsampling strategy to increase the proportion of annotations involved in the training stage. Furthermore, we design the multiplicative dynamic entropy as the gradient calibration function to mitigate the gradient bias caused by non-uniformly distributed sparse annotations and explicitly reduce the epistemic uncertainty. Without any prior restrictions and additional information, our proposed method achieves comprehensive performance improvements at multiple label rates and different annotation distributions.
<div id='section'>Paperid: <span id='pid'>883, <a href='https://arxiv.org/pdf/2311.13718.pdf' target='_blank'>https://arxiv.org/pdf/2311.13718.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vinay Shukla, Zhe Zeng, Kareem Ahmed, Guy Van den Broeck
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.13718">A Unified Approach to Count-Based Weakly-Supervised Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-quality labels are often very scarce, whereas unlabeled data with inferred weak labels occurs more naturally. In many cases, these weak labels dictate the frequency of each respective class over a set of instances. In this paper, we develop a unified approach to learning from such weakly-labeled data, which we call count-based weakly-supervised learning. At the heart of our approach is the ability to compute the probability of exactly k out of n outputs being set to true. This computation is differentiable, exact, and efficient. Building upon the previous computation, we derive a count loss penalizing the model for deviations in its distribution from an arithmetic constraint defined over label counts. We evaluate our approach on three common weakly-supervised learning paradigms and observe that our proposed approach achieves state-of-the-art or highly competitive results across all three of the paradigms.
<div id='section'>Paperid: <span id='pid'>884, <a href='https://arxiv.org/pdf/2310.12565.pdf' target='_blank'>https://arxiv.org/pdf/2310.12565.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marcel Hoffmann, Lukas Galke, Ansgar Scherp
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.12565">Open-World Lifelong Graph Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study the problem of lifelong graph learning in an open-world scenario, where a model needs to deal with new tasks and potentially unknown classes. We utilize Out-of-Distribution (OOD) detection methods to recognize new classes and adapt existing non-graph OOD detection methods to graph data. Crucially, we suggest performing new class detection by combining OOD detection methods with information aggregated from the graph neighborhood. Most OOD detection methods avoid determining a crisp threshold for deciding whether a vertex is OOD. To tackle this problem, we propose a Weakly-supervised Relevance Feedback (Open-WRF) method, which decreases the sensitivity to thresholds in OOD detection. We evaluate our approach on six benchmark datasets. Our results show that the proposed neighborhood aggregation method for OOD scores outperforms existing methods independent of the underlying graph neural network. Furthermore, we demonstrate that our Open-WRF method is more robust to threshold selection and analyze the influence of graph neighborhood on OOD detection. The aggregation and threshold methods are compatible with arbitrary graph neural networks and OOD detection methods, making our approach versatile and applicable to many real-world applications.
<div id='section'>Paperid: <span id='pid'>885, <a href='https://arxiv.org/pdf/2309.00661.pdf' target='_blank'>https://arxiv.org/pdf/2309.00661.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dezhao Luo, Jiabo Huang, Shaogang Gong, Hailin Jin, Yang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.00661">Zero-Shot Video Moment Retrieval from Frozen Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate video moment retrieval (VMR) requires universal visual-textual correlations that can handle unknown vocabulary and unseen scenes. However, the learned correlations are likely either biased when derived from a limited amount of moment-text data which is hard to scale up because of the prohibitive annotation cost (fully-supervised), or unreliable when only the video-text pairwise relationships are available without fine-grained temporal annotations (weakly-supervised). Recently, the vision-language models (VLM) demonstrate a new transfer learning paradigm to benefit different vision tasks through the universal visual-textual correlations derived from large-scale vision-language pairwise web data, which has also shown benefits to VMR by fine-tuning in the target domains. In this work, we propose a zero-shot method for adapting generalisable visual-textual priors from arbitrary VLM to facilitate moment-text alignment, without the need for accessing the VMR data. To this end, we devise a conditional feature refinement module to generate boundary-aware visual features conditioned on text queries to enable better moment boundary understanding. Additionally, we design a bottom-up proposal generation strategy that mitigates the impact of domain discrepancies and breaks down complex-query retrieval tasks into individual action retrievals, thereby maximizing the benefits of VLM. Extensive experiments conducted on three VMR benchmark datasets demonstrate the notable performance advantages of our zero-shot algorithm, especially in the novel-word and novel-location out-of-distribution setups.
<div id='section'>Paperid: <span id='pid'>886, <a href='https://arxiv.org/pdf/2308.05707.pdf' target='_blank'>https://arxiv.org/pdf/2308.05707.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiageng Zhu, Hanchen Xie, Jianhua Wu, Jiazhi Li, Mahyar Khayatkhoei, Mohamed E. Hussein, Wael AbdAlmageed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.05707">Shadow Datasets, New challenging datasets for Causal Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Discovering causal relations among semantic factors is an emergent topic in representation learning. Most causal representation learning (CRL) methods are fully supervised, which is impractical due to costly labeling. To resolve this restriction, weakly supervised CRL methods were introduced. To evaluate CRL performance, four existing datasets, Pendulum, Flow, CelebA(BEARD) and CelebA(SMILE), are utilized. However, existing CRL datasets are limited to simple graphs with few generative factors. Thus we propose two new datasets with a larger number of diverse generative factors and more sophisticated causal graphs. In addition, current real datasets, CelebA(BEARD) and CelebA(SMILE), the originally proposed causal graphs are not aligned with the dataset distributions. Thus, we propose modifications to them.
<div id='section'>Paperid: <span id='pid'>887, <a href='https://arxiv.org/pdf/2307.13069.pdf' target='_blank'>https://arxiv.org/pdf/2307.13069.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Viet Duong, Qiong Wu, Zhengyi Zhou, Eric Zavesky, Jiahe Chen, Xiangzhou Liu, Wen-Ling Hsu, Huajie Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.13069">General-Purpose Multi-Modal OOD Detection Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Out-of-distribution (OOD) detection identifies test samples that differ from the training data, which is critical to ensuring the safety and reliability of machine learning (ML) systems. While a plethora of methods have been developed to detect uni-modal OOD samples, only a few have focused on multi-modal OOD detection. Current contrastive learning-based methods primarily study multi-modal OOD detection in a scenario where both a given image and its corresponding textual description come from a new domain. However, real-world deployments of ML systems may face more anomaly scenarios caused by multiple factors like sensor faults, bad weather, and environmental changes. Hence, the goal of this work is to simultaneously detect from multiple different OOD scenarios in a fine-grained manner. To reach this goal, we propose a general-purpose weakly-supervised OOD detection framework, called WOOD, that combines a binary classifier and a contrastive learning component to reap the benefits of both. In order to better distinguish the latent representations of in-distribution (ID) and OOD samples, we adopt the Hinge loss to constrain their similarity. Furthermore, we develop a new scoring metric to integrate the prediction results from both the binary classifier and contrastive learning for identifying OOD samples. We evaluate the proposed WOOD model on multiple real-world datasets, and the experimental results demonstrate that the WOOD model outperforms the state-of-the-art methods for multi-modal OOD detection. Importantly, our approach is able to achieve high accuracy in OOD detection in three different OOD scenarios simultaneously. The source code will be made publicly available upon publication.
<div id='section'>Paperid: <span id='pid'>888, <a href='https://arxiv.org/pdf/2306.07490.pdf' target='_blank'>https://arxiv.org/pdf/2306.07490.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Cai, Suchen Wang, Kim-hui Yap, Yi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.07490">Top-Down Framework for Weakly-supervised Grounded Image Captioning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised grounded image captioning (WSGIC) aims to generate the caption and ground (localize) predicted object words in the input image without using bounding box supervision. Recent two-stage solutions mostly apply a bottom-up pipeline: (1) encode the input image into multiple region features using an object detector; (2) leverage region features for captioning and grounding. However, utilizing independent proposals produced by object detectors tends to make the subsequent grounded captioner overfitted in finding the correct object words, overlooking the relation between objects, and selecting incompatible proposal regions for grounding. To address these issues, we propose a one-stage weakly-supervised grounded captioner that directly takes the RGB image as input to perform captioning and grounding at the top-down image level. Specifically, we encode the image into visual token representations and propose a Recurrent Grounding Module (RGM) in the decoder to obtain precise Visual Language Attention Maps (VLAMs), which recognize the spatial locations of the objects. In addition, we explicitly inject a relation module into our one-stage framework to encourage relation understanding through multi-label classification. This relation semantics served as contextual information facilitating the prediction of relation and object words in the caption. We observe that the relation semantic not only assists the grounded captioner in generating a more accurate caption but also improves the grounding performance. We validate the effectiveness of our proposed method on two challenging datasets (Flick30k Entities captioning and MSCOCO captioning). The experimental results demonstrate that our method achieves state-of-the-art grounding performance.
<div id='section'>Paperid: <span id='pid'>889, <a href='https://arxiv.org/pdf/2306.02902.pdf' target='_blank'>https://arxiv.org/pdf/2306.02902.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bashar Talafha, Abdul Waheed, Muhammad Abdul-Mageed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.02902">N-Shot Benchmarking of Whisper on Diverse Arabic Speech Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Whisper, the recently developed multilingual weakly supervised model, is reported to perform well on multiple speech recognition benchmarks in both monolingual and multilingual settings. However, it is not clear how Whisper would fare under diverse conditions even on languages it was evaluated on such as Arabic. In this work, we address this gap by comprehensively evaluating Whisper on several varieties of Arabic speech for the ASR task. Our evaluation covers most publicly available Arabic speech data and is performed under n-shot (zero-, few-, and full) finetuning. We also investigate the robustness of Whisper under completely novel conditions, such as in dialect-accented standard Arabic and in unseen dialects for which we develop evaluation data. Our experiments show that although Whisper zero-shot outperforms fully finetuned XLS-R models on all datasets, its performance deteriorates significantly in the zero-shot setting for five unseen dialects (i.e., Algeria, Jordan, Palestine, UAE, and Yemen).
<div id='section'>Paperid: <span id='pid'>890, <a href='https://arxiv.org/pdf/2306.00996.pdf' target='_blank'>https://arxiv.org/pdf/2306.00996.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Theodoros Kouzelis, Georgios Paraskevopoulos, Athanasios Katsamanis, Vassilis Katsouros
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.00996">Weakly-supervised forced alignment of disfluent speech using phoneme-level modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The study of speech disorders can benefit greatly from time-aligned data. However, audio-text mismatches in disfluent speech cause rapid performance degradation for modern speech aligners, hindering the use of automatic approaches. In this work, we propose a simple and effective modification of alignment graph construction of CTC-based models using Weighted Finite State Transducers. The proposed weakly-supervised approach alleviates the need for verbatim transcription of speech disfluencies for forced alignment. During the graph construction, we allow the modeling of common speech disfluencies, i.e. repetitions and omissions. Further, we show that by assessing the degree of audio-text mismatch through the use of Oracle Error Rate, our method can be effectively used in the wild. Our evaluation on a corrupted version of the TIMIT test set and the UCLASS dataset shows significant improvements, particularly for recall, achieving a 23-25% relative improvement over our baselines.
<div id='section'>Paperid: <span id='pid'>891, <a href='https://arxiv.org/pdf/2306.00595.pdf' target='_blank'>https://arxiv.org/pdf/2306.00595.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingying Fan, Yu Wu, Bo Du, Yutian Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.00595">Revisit Weakly-Supervised Audio-Visual Video Parsing from the Language Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We focus on the weakly-supervised audio-visual video parsing task (AVVP), which aims to identify and locate all the events in audio/visual modalities. Previous works only concentrate on video-level overall label denoising across modalities, but overlook the segment-level label noise, where adjacent video segments (i.e., 1-second video clips) may contain different events. However, recognizing events in the segment is challenging because its label could be any combination of events that occur in the video. To address this issue, we consider tackling AVVP from the language perspective, since language could freely describe how various events appear in each segment beyond fixed labels. Specifically, we design language prompts to describe all cases of event appearance for each video. Then, the similarity between language prompts and segments is calculated, where the event of the most similar prompt is regarded as the segment-level label. In addition, to deal with the mislabeled segments, we propose to perform dynamic re-weighting on the unreliable segments to adjust their labels. Experiments show that our simple yet effective approach outperforms state-of-the-art methods by a large margin.
<div id='section'>Paperid: <span id='pid'>892, <a href='https://arxiv.org/pdf/2304.05153.pdf' target='_blank'>https://arxiv.org/pdf/2304.05153.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Omar S. M. El Nahhas, Chiara M. L. Loeffler, Zunamys I. Carrero, Marko van Treeck, Fiona R. Kolbinger, Katherine J. Hewitt, Hannah S. Muti, Mara Graziani, Qinghe Zeng, Julien Calderaro, Nadina Ortiz-BrÃ¼chle, Tanwei Yuan, Michael Hoffmeister, Hermann Brenner, Alexander Brobeil, Jorge S. Reis-Filho, Jakob Nikolas Kather
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.05153">Regression-based Deep-Learning predicts molecular biomarkers from pathology slides</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep Learning (DL) can predict biomarkers from cancer histopathology. Several clinically approved applications use this technology. Most approaches, however, predict categorical labels, whereas biomarkers are often continuous measurements. We hypothesized that regression-based DL outperforms classification-based DL. Therefore, we developed and evaluated a new self-supervised attention-based weakly supervised regression method that predicts continuous biomarkers directly from images in 11,671 patients across nine cancer types. We tested our method for multiple clinically and biologically relevant biomarkers: homologous repair deficiency (HRD) score, a clinically used pan-cancer biomarker, as well as markers of key biological processes in the tumor microenvironment. Using regression significantly enhances the accuracy of biomarker prediction, while also improving the interpretability of the results over classification. In a large cohort of colorectal cancer patients, regression-based prediction scores provide a higher prognostic value than classification-based scores. Our open-source regression approach offers a promising alternative for continuous biomarker analysis in computational pathology.
<div id='section'>Paperid: <span id='pid'>893, <a href='https://arxiv.org/pdf/2304.03572.pdf' target='_blank'>https://arxiv.org/pdf/2304.03572.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongrun Zhang, Liam Burrows, Yanda Meng, Declan Sculthorpe, Abhik Mukherjee, Sarah E Coupland, Ke Chen, Yalin Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.03572">Weakly supervised segmentation with point annotations for histopathology images via contrast-based variational model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image segmentation is a fundamental task in the field of imaging and vision. Supervised deep learning for segmentation has achieved unparalleled success when sufficient training data with annotated labels are available. However, annotation is known to be expensive to obtain, especially for histopathology images where the target regions are usually with high morphology variations and irregular shapes. Thus, weakly supervised learning with sparse annotations of points is promising to reduce the annotation workload. In this work, we propose a contrast-based variational model to generate segmentation results, which serve as reliable complementary supervision to train a deep segmentation model for histopathology images. The proposed method considers the common characteristics of target regions in histopathology images and can be trained in an end-to-end manner. It can generate more regionally consistent and smoother boundary segmentation, and is more robust to unlabeled `novel' regions. Experiments on two different histology datasets demonstrate its effectiveness and efficiency in comparison to previous models.
<div id='section'>Paperid: <span id='pid'>894, <a href='https://arxiv.org/pdf/2303.07093.pdf' target='_blank'>https://arxiv.org/pdf/2303.07093.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shahad Hardan, Hussain Alasmawi, Xiangjian Hou, Mohammad Yaqub
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.07093">Weakly Unsupervised Domain Adaptation for Vestibular Schwannoma Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vestibular schwannoma (VS) is a non-cancerous tumor located next to the ear that can cause hearing loss. Most brain MRI images acquired from patients are contrast-enhanced T1 (ceT1), with a growing interest in high-resolution T2 images (hrT2) to replace ceT1, which involves the use of a contrast agent. As hrT2 images are currently scarce, it is less likely to train robust machine learning models to segment VS or other brain structures. In this work, we propose a weakly supervised machine learning approach that learns from only ceT1 scans and adapts to segment two structures from hrT2 scans: the VS and the cochlea from the crossMoDA dataset. Our model 1) generates fake hrT2 scans from ceT1 images and segmentation masks, 2) is trained using the fake hrT2 scans, 3) predicts the augmented real hrT2 scans, and 4) is retrained again using both the fake and real hrT2. The final result of this model has been computed on an unseen testing dataset provided by the 2022 crossMoDA challenge organizers. The mean dice score and average symmetric surface distance (ASSD) are 0.78 and 0.46, respectively. The predicted segmentation masks achieved a dice score of 0.83 and an ASSD of 0.56 on the VS, and a dice score of 0.74 and an ASSD of 0.35 on the cochleas.
<div id='section'>Paperid: <span id='pid'>895, <a href='https://arxiv.org/pdf/2303.06371.pdf' target='_blank'>https://arxiv.org/pdf/2303.06371.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuchen Shao, Liuxi Dai, Yifeng Wang, Haoqian Wang, Yongbing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.06371">AugDiff: Diffusion based Feature Augmentation for Multiple Instance Learning in Whole Slide Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple Instance Learning (MIL), a powerful strategy for weakly supervised learning, is able to perform various prediction tasks on gigapixel Whole Slide Images (WSIs). However, the tens of thousands of patches in WSIs usually incur a vast computational burden for image augmentation, limiting the MIL model's improvement in performance. Currently, the feature augmentation-based MIL framework is a promising solution, while existing methods such as Mixup often produce unrealistic features. To explore a more efficient and practical augmentation method, we introduce the Diffusion Model (DM) into MIL for the first time and propose a feature augmentation framework called AugDiff. Specifically, we employ the generation diversity of DM to improve the quality of feature augmentation and the step-by-step generation property to control the retention of semantic information. We conduct extensive experiments over three distinct cancer datasets, two different feature extractors, and three prevalent MIL algorithms to evaluate the performance of AugDiff. Ablation study and visualization further verify the effectiveness. Moreover, we highlight AugDiff's higher-quality augmented feature over image augmentation and its superiority over self-supervised learning. The generalization over external datasets indicates its broader applications.
<div id='section'>Paperid: <span id='pid'>896, <a href='https://arxiv.org/pdf/2302.08427.pdf' target='_blank'>https://arxiv.org/pdf/2302.08427.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Emma Sarfati, Alexandre Bone, Marc-Michel Rohe, Pietro Gori, Isabelle Bloch
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.08427">Learning to diagnose cirrhosis from radiological and histological labels with joint self and weakly-supervised pretraining strategies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Identifying cirrhosis is key to correctly assess the health of the liver. However, the gold standard diagnosis of the cirrhosis needs a medical intervention to obtain the histological confirmation, e.g. the METAVIR score, as the radiological presentation can be equivocal. In this work, we propose to leverage transfer learning from large datasets annotated by radiologists, which we consider as a weak annotation, to predict the histological score available on a small annex dataset. To this end, we propose to compare different pretraining methods, namely weakly-supervised and self-supervised ones, to improve the prediction of the cirrhosis. Finally, we introduce a loss function combining both supervised and self-supervised frameworks for pretraining. This method outperforms the baseline classification of the METAVIR score, reaching an AUC of 0.84 and a balanced accuracy of 0.75, compared to 0.77 and 0.72 for a baseline classifier.
<div id='section'>Paperid: <span id='pid'>897, <a href='https://arxiv.org/pdf/2206.04949.pdf' target='_blank'>https://arxiv.org/pdf/2206.04949.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Chen, Yongqiang Tang, Wensheng Zhang, Wenlong Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.04949">Deep Multi-View Semi-Supervised Clustering with Sample Pairwise Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-view clustering has attracted much attention thanks to the capacity of multi-source information integration. Although numerous advanced methods have been proposed in past decades, most of them generally overlook the significance of weakly-supervised information and fail to preserve the feature properties of multiple views, thus resulting in unsatisfactory clustering performance. To address these issues, in this paper, we propose a novel Deep Multi-view Semi-supervised Clustering (DMSC) method, which jointly optimizes three kinds of losses during networks finetuning, including multi-view clustering loss, semi-supervised pairwise constraint loss and multiple autoencoders reconstruction loss. Specifically, a KL divergence based multi-view clustering loss is imposed on the common representation of multi-view data to perform heterogeneous feature optimization, multi-view weighting and clustering prediction simultaneously. Then, we innovatively propose to integrate pairwise constraints into the process of multi-view clustering by enforcing the learned multi-view representation of must-link samples (cannot-link samples) to be similar (dissimilar), such that the formed clustering architecture can be more credible. Moreover, unlike existing rivals that only preserve the encoders for each heterogeneous branch during networks finetuning, we further propose to tune the intact autoencoders frame that contains both encoders and decoders. In this way, the issue of serious corruption of view-specific and view-shared feature space could be alleviated, making the whole training procedure more stable. Through comprehensive experiments on eight popular image datasets, we demonstrate that our proposed approach performs better than the state-of-the-art multi-view and single-view competitors.
<div id='section'>Paperid: <span id='pid'>898, <a href='https://arxiv.org/pdf/2205.05656.pdf' target='_blank'>https://arxiv.org/pdf/2205.05656.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hang Dong, VÃ­ctor SuÃ¡rez-Paniagua, Huayu Zhang, Minhong Wang, Arlene Casey, Emma Davidson, Jiaoyan Chen, Beatrice Alex, William Whiteley, Honghan Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2205.05656">Ontology-Driven and Weakly Supervised Rare Disease Identification from Clinical Notes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computational text phenotyping is the practice of identifying patients with certain disorders and traits from clinical notes. Rare diseases are challenging to be identified due to few cases available for machine learning and the need for data annotation from domain experts. We propose a method using ontologies and weak supervision, with recent pre-trained contextual representations from Bi-directional Transformers (e.g. BERT). The ontology-based framework includes two steps: (i) Text-to-UMLS, extracting phenotypes by contextually linking mentions to concepts in Unified Medical Language System (UMLS), with a Named Entity Recognition and Linking (NER+L) tool, SemEHR, and weak supervision with customised rules and contextual mention representation; (ii) UMLS-to-ORDO, matching UMLS concepts to rare diseases in Orphanet Rare Disease Ontology (ORDO). The weakly supervised approach is proposed to learn a phenotype confirmation model to improve Text-to-UMLS linking, without annotated data from domain experts. We evaluated the approach on three clinical datasets, MIMIC-III discharge summaries, MIMIC-III radiology reports, and NHS Tayside brain imaging reports from two institutions in the US and the UK, with annotations. The improvements in the precision were pronounced (by over 30% to 50% absolute score for Text-to-UMLS linking), with almost no loss of recall compared to the existing NER+L tool, SemEHR. Results on radiology reports from MIMIC-III and NHS Tayside were consistent with the discharge summaries. The overall pipeline processing clinical notes can extract rare disease cases, mostly uncaptured in structured data (manually assigned ICD codes). We discuss the usefulness of the weak supervision approach and propose directions for future studies.
<div id='section'>Paperid: <span id='pid'>899, <a href='https://arxiv.org/pdf/2203.04802.pdf' target='_blank'>https://arxiv.org/pdf/2203.04802.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fu Li, Hao Yu, Ivan Shugurov, Benjamin Busam, Shaowu Yang, Slobodan Ilic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.04802">NeRF-Pose: A First-Reconstruct-Then-Regress Approach for Weakly-supervised 6D Object Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pose estimation of 3D objects in monocular images is a fundamental and long-standing problem in computer vision. Existing deep learning approaches for 6D pose estimation typically rely on the assumption of availability of 3D object models and 6D pose annotations. However, precise annotation of 6D poses in real data is intricate, time-consuming and not scalable, while synthetic data scales well but lacks realism. To avoid these problems, we present a weakly-supervised reconstruction-based pipeline, named NeRF-Pose, which needs only 2D object segmentation and known relative camera poses during training. Following the first-reconstruct-then-regress idea, we first reconstruct the objects from multiple views in the form of an implicit neural representation. Then, we train a pose regression network to predict pixel-wise 2D-3D correspondences between images and the reconstructed model. At inference, the approach only needs a single image as input. A NeRF-enabled PnP+RANSAC algorithm is used to estimate stable and accurate pose from the predicted correspondences. Experiments on LineMod and LineMod-Occlusion show that the proposed method has state-of-the-art accuracy in comparison to the best 6D pose estimation methods in spite of being trained only with weak labels. Besides, we extend the Homebrewed DB dataset with more real training images to support the weakly supervised task and achieve compelling results on this dataset. The extended dataset and code will be released soon.
<div id='section'>Paperid: <span id='pid'>900, <a href='https://arxiv.org/pdf/2511.01131.pdf' target='_blank'>https://arxiv.org/pdf/2511.01131.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Nahiduzzaman, Steven Korevaar, Alireza Bab-Hadiashar, Ruwan Tennakoon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.01131">Weakly Supervised Concept Learning with Class-Level Priors for Interpretable Medical Diagnosis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-interpretable predictions are essential for deploying AI in medical imaging, yet most interpretable-by-design (IBD) frameworks require concept annotations for training data, which are costly and impractical to obtain in clinical contexts. Recent attempts to bypass annotation, such as zero-shot vision-language models or concept-generation frameworks, struggle to capture domain-specific medical features, leading to poor reliability. In this paper, we propose a novel Prior-guided Concept Predictor (PCP), a weakly supervised framework that enables concept answer prediction without explicit supervision or reliance on language models. PCP leverages class-level concept priors as weak supervision and incorporates a refinement mechanism with KL divergence and entropy regularization to align predictions with clinical reasoning. Experiments on PH2 (dermoscopy) and WBCatt (hematology) show that PCP improves concept-level F1-score by over 33% compared to zero-shot baselines, while delivering competitive classification performance on four medical datasets (PH2, WBCatt, HAM10000, and CXR4) relative to fully supervised concept bottleneck models (CBMs) and V-IP.
<div id='section'>Paperid: <span id='pid'>901, <a href='https://arxiv.org/pdf/2510.17875.pdf' target='_blank'>https://arxiv.org/pdf/2510.17875.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoxu Xu, Xuexun Liu, Jinlong Li, Yitian Yuan, Qiudan Zhang, Lin Ma, Nicu Sebe, Xu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17875">3D Weakly Supervised Semantic Segmentation via Class-Aware and Geometry-Guided Pseudo-Label Refinement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D weakly supervised semantic segmentation (3D WSSS) aims to achieve semantic segmentation by leveraging sparse or low-cost annotated data, significantly reducing reliance on dense point-wise annotations. Previous works mainly employ class activation maps or pre-trained vision-language models to address this challenge. However, the low quality of pseudo-labels and the insufficient exploitation of 3D geometric priors jointly create significant technical bottlenecks in developing high-performance 3D WSSS models. In this paper, we propose a simple yet effective 3D weakly supervised semantic segmentation method that integrates 3D geometric priors into a class-aware guidance mechanism to generate high-fidelity pseudo labels. Concretely, our designed methodology first employs Class-Aware Label Refinement module to generate more balanced and accurate pseudo labels for semantic categrories. This initial refinement stage focuses on enhancing label quality through category-specific optimization. Subsequently, the Geometry-Aware Label Refinement component is developed, which strategically integrates implicit 3D geometric constraints to effectively filter out low-confidence pseudo labels that fail to comply with geometric plausibility. Moreover, to address the challenge of extensive unlabeled regions, we propose a Label Update strategy that integrates Self-Training to propagate labels into these areas. This iterative process continuously enhances pseudo-label quality while expanding label coverage, ultimately fostering the development of high-performance 3D WSSS models. Comprehensive experimental validation reveals that our proposed methodology achieves state-of-the-art performance on both ScanNet and S3DIS benchmarks while demonstrating remarkable generalization capability in unsupervised settings, maintaining competitive accuracy through its robust design.
<div id='section'>Paperid: <span id='pid'>902, <a href='https://arxiv.org/pdf/2510.17566.pdf' target='_blank'>https://arxiv.org/pdf/2510.17566.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nachuan Ma, Zhengfei Song, Qiang Hu, Xiaoyu Tang, Chengxi Zhang, Rui Fan, Lihua Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17566">WP-CrackNet: A Collaborative Adversarial Learning Framework for End-to-End Weakly-Supervised Road Crack Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Road crack detection is essential for intelligent infrastructure maintenance in smart cities. To reduce reliance on costly pixel-level annotations, we propose WP-CrackNet, an end-to-end weakly-supervised method that trains with only image-level labels for pixel-wise crack detection. WP-CrackNet integrates three components: a classifier generating class activation maps (CAMs), a reconstructor measuring feature inferability, and a detector producing pixel-wise road crack detection results. During training, the classifier and reconstructor alternate in adversarial learning to encourage crack CAMs to cover complete crack regions, while the detector learns from pseudo labels derived from post-processed crack CAMs. This mutual feedback among the three components improves learning stability and detection accuracy. To further boost detection performance, we design a path-aware attention module (PAAM) that fuses high-level semantics from the classifier with low-level structural cues from the reconstructor by modeling spatial and channel-wise dependencies. Additionally, a center-enhanced CAM consistency module (CECCM) is proposed to refine crack CAMs using center Gaussian weighting and consistency constraints, enabling better pseudo-label generation. We create three image-level datasets and extensive experiments show that WP-CrackNet achieves comparable results to supervised methods and outperforms existing weakly-supervised methods, significantly advancing scalable road inspection. The source code package and datasets are available at https://mias.group/WP-CrackNet/.
<div id='section'>Paperid: <span id='pid'>903, <a href='https://arxiv.org/pdf/2509.14573.pdf' target='_blank'>https://arxiv.org/pdf/2509.14573.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Takamasa Yamaguchi, Brian Kenji Iwana, Ryoma Bise, Shota Harada, Takumi Okuo, Kiyohito Tanaka, Kaito Shiku
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14573">Domain Adaptation for Ulcerative Colitis Severity Estimation Using Patient-Level Diagnoses</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of methods to estimate the severity of Ulcerative Colitis (UC) is of significant importance. However, these methods often suffer from domain shifts caused by differences in imaging devices and clinical settings across hospitals. Although several domain adaptation methods have been proposed to address domain shift, they still struggle with the lack of supervision in the target domain or the high cost of annotation. To overcome these challenges, we propose a novel Weakly Supervised Domain Adaptation method that leverages patient-level diagnostic results, which are routinely recorded in UC diagnosis, as weak supervision in the target domain. The proposed method aligns class-wise distributions across domains using Shared Aggregation Tokens and a Max-Severity Triplet Loss, which leverages the characteristic that patient-level diagnoses are determined by the most severe region within each patient. Experimental results demonstrate that our method outperforms comparative DA approaches, improving UC severity estimation in a domain-shifted setting.
<div id='section'>Paperid: <span id='pid'>904, <a href='https://arxiv.org/pdf/2508.06318.pdf' target='_blank'>https://arxiv.org/pdf/2508.06318.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giacomo D'Amicantonio, Snehashis Majhi, Quan Kong, Lorenzo Garattoni, Gianpiero Francesca, FranÃ§ois Bremond, Egor Bondarev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06318">Mixture of Experts Guided by Gaussian Splatters Matters: A new Approach to Weakly-Supervised Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Anomaly Detection (VAD) is a challenging task due to the variability of anomalous events and the limited availability of labeled data. Under the Weakly-Supervised VAD (WSVAD) paradigm, only video-level labels are provided during training, while predictions are made at the frame level. Although state-of-the-art models perform well on simple anomalies (e.g., explosions), they struggle with complex real-world events (e.g., shoplifting). This difficulty stems from two key issues: (1) the inability of current models to address the diversity of anomaly types, as they process all categories with a shared model, overlooking category-specific features; and (2) the weak supervision signal, which lacks precise temporal information, limiting the ability to capture nuanced anomalous patterns blended with normal events. To address these challenges, we propose Gaussian Splatting-guided Mixture of Experts (GS-MoE), a novel framework that employs a set of expert models, each specialized in capturing specific anomaly types. These experts are guided by a temporal Gaussian splatting loss, enabling the model to leverage temporal consistency and enhance weak supervision. The Gaussian splatting approach encourages a more precise and comprehensive representation of anomalies by focusing on temporal segments most likely to contain abnormal events. The predictions from these specialized experts are integrated through a mixture-of-experts mechanism to model complex relationships across diverse anomaly patterns. Our approach achieves state-of-the-art performance, with a 91.58% AUC on the UCF-Crime dataset, and demonstrates superior results on XD-Violence and MSAD datasets. By leveraging category-specific expertise and temporal guidance, GS-MoE sets a new benchmark for VAD under weak supervision.
<div id='section'>Paperid: <span id='pid'>905, <a href='https://arxiv.org/pdf/2506.05912.pdf' target='_blank'>https://arxiv.org/pdf/2506.05912.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adrien Petralia, Paul Boniol, Philippe Charpentier, Themis Palpanas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05912">DeviceScope: An Interactive App to Detect and Localize Appliance Patterns in Electricity Consumption Time Series</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, electricity suppliers have installed millions of smart meters worldwide to improve the management of the smart grid system. These meters collect a large amount of electrical consumption data to produce valuable information to help consumers reduce their electricity footprint. However, having non-expert users (e.g., consumers or sales advisors) understand these data and derive usage patterns for different appliances has become a significant challenge for electricity suppliers because these data record the aggregated behavior of all appliances. At the same time, ground-truth labels (which could train appliance detection and localization models) are expensive to collect and extremely scarce in practice. This paper introduces DeviceScope, an interactive tool designed to facilitate understanding smart meter data by detecting and localizing individual appliance patterns within a given time period. Our system is based on CamAL (Class Activation Map-based Appliance Localization), a novel weakly supervised approach for appliance localization that only requires the knowledge of the existence of an appliance in a household to be trained. This paper appeared in ICDE 2025.
<div id='section'>Paperid: <span id='pid'>906, <a href='https://arxiv.org/pdf/2505.13123.pdf' target='_blank'>https://arxiv.org/pdf/2505.13123.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Snehashis Majhi, Giacomo D'Amicantonio, Antitza Dantcheva, Quan Kong, Lorenzo Garattoni, Gianpiero Francesca, Egor Bondarev, Francois Bremond
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13123">Just Dance with $Ï$! A Poly-modal Inductor for Weakly-supervised Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised methods for video anomaly detection (VAD) are conventionally based merely on RGB spatio-temporal features, which continues to limit their reliability in real-world scenarios. This is due to the fact that RGB-features are not sufficiently distinctive in setting apart categories such as shoplifting from visually similar events. Therefore, towards robust complex real-world VAD, it is essential to augment RGB spatio-temporal features by additional modalities. Motivated by this, we introduce the Poly-modal Induced framework for VAD: "PI-VAD", a novel approach that augments RGB representations by five additional modalities. Specifically, the modalities include sensitivity to fine-grained motion (Pose), three dimensional scene and entity representation (Depth), surrounding objects (Panoptic masks), global motion (optical flow), as well as language cues (VLM). Each modality represents an axis of a polygon, streamlined to add salient cues to RGB. PI-VAD includes two plug-in modules, namely Pseudo-modality Generation module and Cross Modal Induction module, which generate modality-specific prototypical representation and, thereby, induce multi-modal information into RGB cues. These modules operate by performing anomaly-aware auxiliary tasks and necessitate five modality backbones -- only during training. Notably, PI-VAD achieves state-of-the-art accuracy on three prominent VAD datasets encompassing real-world scenarios, without requiring the computational overhead of five modality backbones at inference.
<div id='section'>Paperid: <span id='pid'>907, <a href='https://arxiv.org/pdf/2505.08687.pdf' target='_blank'>https://arxiv.org/pdf/2505.08687.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hangwei Zhang, Zhimu Huang, Yan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08687">AC-PKAN: Attention-Enhanced and Chebyshev Polynomial-Based Physics-Informed Kolmogorov-Arnold Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Kolmogorov-Arnold Networks (KANs) have recently shown promise for solving partial differential equations (PDEs). Yet their original formulation is computationally and memory intensive, motivating the introduction of Chebyshev Type-I-based KANs (Chebyshev1KANs). Although Chebyshev1KANs have outperformed the vanilla KANs architecture, our rigorous theoretical analysis reveals that they still suffer from rank collapse, ultimately limiting their expressive capacity. To overcome these limitations, we enhance Chebyshev1KANs by integrating wavelet-activated MLPs with learnable parameters and an internal attention mechanism. We prove that this design preserves a full-rank Jacobian and is capable of approximating solutions to PDEs of arbitrary order. Furthermore, to alleviate the loss instability and imbalance introduced by the Chebyshev polynomial basis, we externally incorporate a Residual Gradient Attention (RGA) mechanism that dynamically re-weights individual loss terms according to their gradient norms and residual magnitudes. By jointly leveraging internal and external attention, we present AC-PKAN, a novel architecture that constitutes an enhancement to weakly supervised Physics-Informed Neural Networks (PINNs) and extends the expressive power of KANs. Experimental results from nine benchmark tasks across three domains show that AC-PKAN consistently outperforms or matches state-of-the-art models such as PINNsFormer, establishing it as a highly effective tool for solving complex real-world engineering problems in zero-data or data-sparse regimes. The code will be made publicly available upon acceptance.
<div id='section'>Paperid: <span id='pid'>908, <a href='https://arxiv.org/pdf/2505.01064.pdf' target='_blank'>https://arxiv.org/pdf/2505.01064.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hari Chandana Kuchibhotla, Sai Srinivas Kancheti, Abbavaram Gowtham Reddy, Vineeth N Balasubramanian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01064">Efficient Vocabulary-Free Fine-Grained Visual Recognition in the Age of Multimodal LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fine-grained Visual Recognition (FGVR) involves distinguishing between visually similar categories, which is inherently challenging due to subtle inter-class differences and the need for large, expert-annotated datasets. In domains like medical imaging, such curated datasets are unavailable due to issues like privacy concerns and high annotation costs. In such scenarios lacking labeled data, an FGVR model cannot rely on a predefined set of training labels, and hence has an unconstrained output space for predictions. We refer to this task as Vocabulary-Free FGVR (VF-FGVR), where a model must predict labels from an unconstrained output space without prior label information. While recent Multimodal Large Language Models (MLLMs) show potential for VF-FGVR, querying these models for each test input is impractical because of high costs and prohibitive inference times. To address these limitations, we introduce \textbf{Nea}rest-Neighbor Label \textbf{R}efinement (NeaR), a novel approach that fine-tunes a downstream CLIP model using labels generated by an MLLM. Our approach constructs a weakly supervised dataset from a small, unlabeled training set, leveraging MLLMs for label generation. NeaR is designed to handle the noise, stochasticity, and open-endedness inherent in labels generated by MLLMs, and establishes a new benchmark for efficient VF-FGVR.
<div id='section'>Paperid: <span id='pid'>909, <a href='https://arxiv.org/pdf/2411.10497.pdf' target='_blank'>https://arxiv.org/pdf/2411.10497.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xavier Bou, Gabriele Facciolo, Rafael Grompone von Gioi, Jean-Michel Morel, Thibaud Ehret
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10497">Structure Tensor Representation for Robust Oriented Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Oriented object detection predicts orientation in addition to object location and bounding box. Precisely predicting orientation remains challenging due to angular periodicity, which introduces boundary discontinuity issues and symmetry ambiguities. Inspired by classical works on edge and corner detection, this paper proposes to represent orientation in oriented bounding boxes as a structure tensor. This representation combines the strengths of Gaussian-based methods and angle-coder solutions, providing a simple yet efficient approach that is robust to angular periodicity issues without additional hyperparameters. Extensive evaluations across five datasets demonstrate that the proposed structure tensor representation outperforms previous methods in both fully-supervised and weakly supervised tasks, achieving high precision in angular prediction with minimal computational overhead. Thus, this work establishes structure tensors as a robust and modular alternative for encoding orientation in oriented object detection. We make our code publicly available, allowing for seamless integration into existing object detectors.
<div id='section'>Paperid: <span id='pid'>910, <a href='https://arxiv.org/pdf/2411.02466.pdf' target='_blank'>https://arxiv.org/pdf/2411.02466.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Robin Trombetta, Olivier RouviÃ¨re, Carole Lartizien
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.02466">Weakly supervised deep learning model with size constraint for prostate cancer detection in multiparametric MRI and generalization to unseen domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fully supervised deep models have shown promising performance for many medical segmentation tasks. Still, the deployment of these tools in clinics is limited by the very timeconsuming collection of manually expert-annotated data. Moreover, most of the state-ofthe-art models have been trained and validated on moderately homogeneous datasets. It is known that deep learning methods are often greatly degraded by domain or label shifts and are yet to be built in such a way as to be robust to unseen data or label distributions. In the clinical setting, this problematic is particularly relevant as the deployment institutions may have different scanners or acquisition protocols than those from which the data has been collected to train the model. In this work, we propose to address these two challenges on the detection of clinically significant prostate cancer (csPCa) from bi-parametric MRI. We evaluate the method proposed by (Kervadec et al., 2018), which introduces a size constaint loss to produce fine semantic cancer lesions segmentations from weak circle scribbles annotations. Performance of the model is based on two public (PI-CAI and Prostate158) and one private databases. First, we show that the model achieves on-par performance with strong fully supervised baseline models, both on in-distribution validation data and unseen test images. Second, we observe a performance decrease for both fully supervised and weakly supervised models when tested on unseen data domains. This confirms the crucial need for efficient domain adaptation methods if deep learning models are aimed to be deployed in a clinical environment. Finally, we show that ensemble predictions from multiple trainings increase generalization performance.
<div id='section'>Paperid: <span id='pid'>911, <a href='https://arxiv.org/pdf/2410.19836.pdf' target='_blank'>https://arxiv.org/pdf/2410.19836.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ronan Docherty, Antonis Vamvakeros, Samuel J. Cooper
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.19836">Upsampling DINOv2 features for unsupervised vision tasks and weakly supervised materials segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The features of self-supervised vision transformers (ViTs) contain strong semantic and positional information relevant to downstream tasks like object localization and segmentation. Recent works combine these features with traditional methods like clustering, graph partitioning or region correlations to achieve impressive baselines without finetuning or training additional networks. We leverage upsampled features from ViT networks (e.g DINOv2) in two workflows: in a clustering based approach for object localization and segmentation, and paired with standard classifiers in weakly supervised materials segmentation. Both show strong performance on benchmarks, especially in weakly supervised segmentation where the ViT features capture complex relationships inaccessible to classical approaches. We expect the flexibility and generalizability of these features will both speed up and strengthen materials characterization, from segmentation to property-prediction.
<div id='section'>Paperid: <span id='pid'>912, <a href='https://arxiv.org/pdf/2410.15657.pdf' target='_blank'>https://arxiv.org/pdf/2410.15657.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianjun Gao, Chen Cai, Ruoyu Wang, Wenyang Liu, Kim-Hui Yap, Kratika Garg, Boon-Siew Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15657">CL-HOI: Cross-Level Human-Object Interaction Distillation from Vision Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-object interaction (HOI) detection has seen advancements with Vision Language Models (VLMs), but these methods often depend on extensive manual annotations. Vision Large Language Models (VLLMs) can inherently recognize and reason about interactions at the image level but are computationally heavy and not designed for instance-level HOI detection. To overcome these limitations, we propose a Cross-Level HOI distillation (CL-HOI) framework, which distills instance-level HOIs from VLLMs image-level understanding without the need for manual annotations. Our approach involves two stages: context distillation, where a Visual Linguistic Translator (VLT) converts visual information into linguistic form, and interaction distillation, where an Interaction Cognition Network (ICN) reasons about spatial, visual, and context relations. We design contrastive distillation losses to transfer image-level context and interaction knowledge from the teacher to the student model, enabling instance-level HOI detection. Evaluations on HICO-DET and V-COCO datasets demonstrate that our CL-HOI surpasses existing weakly supervised methods and VLLM supervised methods, showing its efficacy in detecting HOIs without manual labels.
<div id='section'>Paperid: <span id='pid'>913, <a href='https://arxiv.org/pdf/2408.10641.pdf' target='_blank'>https://arxiv.org/pdf/2408.10641.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxiao Wang, Yu Lei, Li Cui, Weiying Xue, Qi Liu, Zhenao Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.10641">A Review of Human-Object Interaction Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-object interaction (HOI) detection plays a key role in high-level visual understanding, facilitating a deep comprehension of human activities. Specifically, HOI detection aims to locate the humans and objects involved in interactions within images or videos and classify the specific interactions between them. The success of this task is influenced by several key factors, including the accurate localization of human and object instances, as well as the correct classification of object categories and interaction relationships. This paper systematically summarizes and discusses the recent work in image-based HOI detection. First, the mainstream datasets involved in HOI relationship detection are introduced. Furthermore, starting with two-stage methods and end-to-end one-stage detection approaches, this paper comprehensively discusses the current developments in image-based HOI detection, analyzing the strengths and weaknesses of these two methods. Additionally, the advancements of zero-shot learning, weakly supervised learning, and the application of large-scale language models in HOI detection are discussed. Finally, the current challenges in HOI detection are outlined, and potential research directions and future trends are explored.
<div id='section'>Paperid: <span id='pid'>914, <a href='https://arxiv.org/pdf/2407.12342.pdf' target='_blank'>https://arxiv.org/pdf/2407.12342.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jintang Xue, Yun-Cheng Wang, Chengwei Wei, C. -C. Jay Kuo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.12342">Word Embedding Dimension Reduction via Weakly-Supervised Feature Selection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As a fundamental task in natural language processing, word embedding converts each word into a representation in a vector space. A challenge with word embedding is that as the vocabulary grows, the vector space's dimension increases, which can lead to a vast model size. Storing and processing word vectors are resource-demanding, especially for mobile edge-devices applications. This paper explores word embedding dimension reduction. To balance computational costs and performance, we propose an efficient and effective weakly-supervised feature selection method named WordFS. It has two variants, each utilizing novel criteria for feature selection. Experiments on various tasks (e.g., word and sentence similarity and binary and multi-class classification) indicate that the proposed WordFS model outperforms other dimension reduction methods at lower computational costs. We have released the code for reproducibility along with the paper.
<div id='section'>Paperid: <span id='pid'>915, <a href='https://arxiv.org/pdf/2407.07566.pdf' target='_blank'>https://arxiv.org/pdf/2407.07566.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arnon Turetzky, Or Tal, Yael Segal-Feldman, Yehoshua Dissen, Ella Zeldes, Amit Roth, Eyal Cohen, Yosi Shrem, Bronya R. Chernyak, Olga Seleznova, Joseph Keshet, Yossi Adi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.07566">HebDB: a Weakly Supervised Dataset for Hebrew Speech Processing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present HebDB, a weakly supervised dataset for spoken language processing in the Hebrew language. HebDB offers roughly 2500 hours of natural and spontaneous speech recordings in the Hebrew language, consisting of a large variety of speakers and topics. We provide raw recordings together with a pre-processed, weakly supervised, and filtered version. The goal of HebDB is to further enhance research and development of spoken language processing tools for the Hebrew language. Hence, we additionally provide two baseline systems for Automatic Speech Recognition (ASR): (i) a self-supervised model; and (ii) a fully supervised model. We present the performance of these two methods optimized on HebDB and compare them to current multi-lingual ASR alternatives. Results suggest the proposed method reaches better results than the evaluated baselines considering similar model sizes. Dataset, code, and models are publicly available under https://pages.cs.huji.ac.il/adiyoss-lab/HebDB/.
<div id='section'>Paperid: <span id='pid'>916, <a href='https://arxiv.org/pdf/2405.19638.pdf' target='_blank'>https://arxiv.org/pdf/2405.19638.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyang Huang, Chuang Zhu, Kebin Liu, Ruiying Ren, Shengjie Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19638">Learning Robust Correlation with Foundation Model for Weakly-Supervised Few-Shot Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing few-shot segmentation (FSS) only considers learning support-query correlation and segmenting unseen categories under the precise pixel masks. However, the cost of a large number of pixel masks during training is expensive. This paper considers a more challenging scenario, weakly-supervised few-shot segmentation (WS-FSS), which only provides category ($i.e.$ image-level) labels. It requires the model to learn robust support-query information when the generated mask is inaccurate. In this work, we design a Correlation Enhancement Network (CORENet) with foundation model, which utilizes multi-information guidance to learn robust correlation. Specifically, correlation-guided transformer (CGT) utilizes self-supervised ViT tokens to learn robust correlation from both local and global perspectives. From the perspective of semantic categories, the class-guided module (CGM) guides the model to locate valuable correlations through the pre-trained CLIP. Finally, the embedding-guided module (EGM) implicitly guides the model to supplement the inevitable information loss during the correlation learning by the original appearance embedding and finally generates the query mask. Extensive experiments on PASCAL-5$^i$ and COCO-20$^i$ have shown that CORENet exhibits excellent performance compared to existing methods.
<div id='section'>Paperid: <span id='pid'>917, <a href='https://arxiv.org/pdf/2405.19387.pdf' target='_blank'>https://arxiv.org/pdf/2405.19387.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Moshira Abdalla, Sajid Javed, Muaz Al Radi, Anwaar Ulhaq, Naoufel Werghi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19387">Video Anomaly Detection in 10 Years: A Survey and Outlook</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video anomaly detection (VAD) holds immense importance across diverse domains such as surveillance, healthcare, and environmental monitoring. While numerous surveys focus on conventional VAD methods, they often lack depth in exploring specific approaches and emerging trends. This survey explores deep learning-based VAD, expanding beyond traditional supervised training paradigms to encompass emerging weakly supervised, self-supervised, and unsupervised approaches. A prominent feature of this review is the investigation of core challenges within the VAD paradigms including large-scale datasets, features extraction, learning methods, loss functions, regularization, and anomaly score prediction. Moreover, this review also investigates the vision language models (VLMs) as potent feature extractors for VAD. VLMs integrate visual data with textual descriptions or spoken language from videos, enabling a nuanced understanding of scenes crucial for anomaly detection. By addressing these challenges and proposing future research directions, this review aims to foster the development of robust and efficient VAD systems leveraging the capabilities of VLMs for enhanced anomaly detection in complex real-world scenarios. This comprehensive analysis seeks to bridge existing knowledge gaps, provide researchers with valuable insights, and contribute to shaping the future of VAD research.
<div id='section'>Paperid: <span id='pid'>918, <a href='https://arxiv.org/pdf/2405.06586.pdf' target='_blank'>https://arxiv.org/pdf/2405.06586.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elham Ravanbakhsh, Cheng Niu, Yongqing Liang, J. Ramanujam, Xin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06586">Enhancing Weakly Supervised Semantic Segmentation with Multi-modal Foundation Models: An End-to-End Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semantic segmentation is a core computer vision problem, but the high costs of data annotation have hindered its wide application. Weakly-Supervised Semantic Segmentation (WSSS) offers a cost-efficient workaround to extensive labeling in comparison to fully-supervised methods by using partial or incomplete labels. Existing WSSS methods have difficulties in learning the boundaries of objects leading to poor segmentation results. We propose a novel and effective framework that addresses these issues by leveraging visual foundation models inside the bounding box. Adopting a two-stage WSSS framework, our proposed network consists of a pseudo-label generation module and a segmentation module. The first stage leverages Segment Anything Model (SAM) to generate high-quality pseudo-labels. To alleviate the problem of delineating precise boundaries, we adopt SAM inside the bounding box with the help of another pre-trained foundation model (e.g., Grounding-DINO). Furthermore, we eliminate the necessity of using the supervision of image labels, by employing CLIP in classification. Then in the second stage, the generated high-quality pseudo-labels are used to train an off-the-shelf segmenter that achieves the state-of-the-art performance on PASCAL VOC 2012 and MS COCO 2014.
<div id='section'>Paperid: <span id='pid'>919, <a href='https://arxiv.org/pdf/2405.04086.pdf' target='_blank'>https://arxiv.org/pdf/2405.04086.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongqi Tong, Sizhe Wang, Dawei Li, Yifan Wang, Simeng Han, Zi Lin, Chengsong Huang, Jiaxin Huang, Jingbo Shang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04086">Optimizing Language Model's Reasoning Abilities with Weak Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While Large Language Models (LLMs) have demonstrated proficiency in handling complex queries, much of the past work has depended on extensively annotated datasets by human experts. However, this reliance on fully-supervised annotations poses scalability challenges, particularly as models and data requirements grow. To mitigate this, we explore the potential of enhancing LLMs' reasoning abilities with minimal human supervision. In this work, we introduce self-reinforcement, which begins with Supervised Fine-Tuning (SFT) of the model using a small collection of annotated questions. Then it iteratively improves LLMs by learning from the differences in responses from the SFT and unfinetuned models on unlabeled questions. Our approach provides an efficient approach without relying heavily on extensive human-annotated explanations. However, current reasoning benchmarks typically only include golden-reference answers or rationales. Therefore, we present \textsc{PuzzleBen}, a weakly supervised benchmark that comprises 25,147 complex questions, answers, and human-generated rationales across various domains, such as brainteasers, puzzles, riddles, parajumbles, and critical reasoning tasks. A unique aspect of our dataset is the inclusion of 10,000 unannotated questions, enabling us to explore utilizing fewer supersized data to boost LLMs' inference capabilities. Our experiments underscore the significance of \textsc{PuzzleBen}, as well as the effectiveness of our methodology as a promising direction in future endeavors. Our dataset and code will be published soon on \texttt{Anonymity Link}.
<div id='section'>Paperid: <span id='pid'>920, <a href='https://arxiv.org/pdf/2404.09504.pdf' target='_blank'>https://arxiv.org/pdf/2404.09504.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiangqiang Wu, Antoni B. Chan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.09504">Learning Tracking Representations from Single Point Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing deep trackers are typically trained with largescale video frames with annotated bounding boxes. However, these bounding boxes are expensive and time-consuming to annotate, in particular for large scale datasets. In this paper, we propose to learn tracking representations from single point annotations (i.e., 4.5x faster to annotate than the traditional bounding box) in a weakly supervised manner. Specifically, we propose a soft contrastive learning (SoCL) framework that incorporates target objectness prior into end-to-end contrastive learning. Our SoCL consists of adaptive positive and negative sample generation, which is memory-efficient and effective for learning tracking representations. We apply the learned representation of SoCL to visual tracking and show that our method can 1) achieve better performance than the fully supervised baseline trained with box annotations under the same annotation time cost; 2) achieve comparable performance of the fully supervised baseline by using the same number of training frames and meanwhile reducing annotation time cost by 78% and total fees by 85%; 3) be robust to annotation noise.
<div id='section'>Paperid: <span id='pid'>921, <a href='https://arxiv.org/pdf/2403.15977.pdf' target='_blank'>https://arxiv.org/pdf/2403.15977.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Timur Ibrayev, Amitangshu Mukherjee, Sai Aparna Aketi, Kaushik Roy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15977">Towards Two-Stream Foveation-based Active Vision Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural network (DNN) based machine perception frameworks process the entire input in a one-shot manner to provide answers to both "what object is being observed" and "where it is located". In contrast, the "two-stream hypothesis" from neuroscience explains the neural processing in the human visual cortex as an active vision system that utilizes two separate regions of the brain to answer the what and the where questions. In this work, we propose a machine learning framework inspired by the "two-stream hypothesis" and explore the potential benefits that it offers. Specifically, the proposed framework models the following mechanisms: 1) ventral (what) stream focusing on the input regions perceived by the fovea part of an eye (foveation), 2) dorsal (where) stream providing visual guidance, and 3) iterative processing of the two streams to calibrate visual focus and process the sequence of focused image patches. The training of the proposed framework is accomplished by label-based DNN training for the ventral stream model and reinforcement learning for the dorsal stream model. We show that the two-stream foveation-based learning is applicable to the challenging task of weakly-supervised object localization (WSOL), where the training data is limited to the object class or its attributes. The framework is capable of both predicting the properties of an object and successfully localizing it by predicting its bounding box. We also show that, due to the independent nature of the two streams, the dorsal model can be applied on its own to unseen images to localize objects from different datasets.
<div id='section'>Paperid: <span id='pid'>922, <a href='https://arxiv.org/pdf/2403.01840.pdf' target='_blank'>https://arxiv.org/pdf/2403.01840.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Liu, Yuxiao Wang, Xinyu Jiang, Wolin Liang, Zhenao Wei, Yu Lei, Nan Zhuang, Weiying Xue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.01840">FreeA: Human-object Interaction Detection using Free Annotation Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent human-object interaction (HOI) detection methods depend on extensively annotated image datasets, which require a significant amount of manpower. In this paper, we propose a novel self-adaptive, language-driven HOI detection method, termed FreeA. This method leverages the adaptability of the text-image model to generate latent HOI labels without requiring manual annotation. Specifically, FreeA aligns image features of human-object pairs with HOI text templates and employs a knowledge-based masking technique to decrease improbable interactions. Furthermore, FreeA implements a proposed method for matching interaction correlations to increase the probability of actions associated with a particular action, thereby improving the generated HOI labels. Experiments on two benchmark datasets showcase that FreeA achieves state-of-the-art performance among weakly supervised HOI competitors. Our proposal gets +\textbf{13.29} (\textbf{159\%$\uparrow$}) mAP and +\textbf{17.30} (\textbf{98\%$\uparrow$}) mAP than the newest ``Weakly'' supervised model, and +\textbf{7.19} (\textbf{28\%$\uparrow$}) mAP and +\textbf{14.69} (\textbf{34\%$\uparrow$}) mAP than the latest ``Weakly+'' supervised model, respectively, on HICO-DET and V-COCO datasets, more accurate in localizing and classifying the interactive actions. The source code will be made public.
<div id='section'>Paperid: <span id='pid'>923, <a href='https://arxiv.org/pdf/2402.10851.pdf' target='_blank'>https://arxiv.org/pdf/2402.10851.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mobina Mansoori, Sajjad Shahabodini, Jamshid Abouei, Arash Mohammadi, Konstantinos N. Plataniotis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.10851">HistoSegCap: Capsules for Weakly-Supervised Semantic Segmentation of Histological Tissue Type in Whole Slide Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Digital pathology involves converting physical tissue slides into high-resolution Whole Slide Images (WSIs), which pathologists analyze for disease-affected tissues. However, large histology slides with numerous microscopic fields pose challenges for visual search. To aid pathologists, Computer Aided Diagnosis (CAD) systems offer visual assistance in efficiently examining WSIs and identifying diagnostically relevant regions. This paper presents a novel histopathological image analysis method employing Weakly Supervised Semantic Segmentation (WSSS) based on Capsule Networks, the first such application. The proposed model is evaluated using the Atlas of Digital Pathology (ADP) dataset and its performance is compared with other histopathological semantic segmentation methodologies. The findings underscore the potential of Capsule Networks in enhancing the precision and efficiency of histopathological image analysis. Experimental results show that the proposed model outperforms traditional methods in terms of accuracy and the mean Intersection-over-Union (mIoU) metric.
<div id='section'>Paperid: <span id='pid'>924, <a href='https://arxiv.org/pdf/2402.03783.pdf' target='_blank'>https://arxiv.org/pdf/2402.03783.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fudan Zheng, Jindong Cao, Weijiang Yu, Zhiguang Chen, Nong Xiao, Yutong Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.03783">Exploring Low-Resource Medical Image Classification with Weakly Supervised Prompt Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most advances in medical image recognition supporting clinical auxiliary diagnosis meet challenges due to the low-resource situation in the medical field, where annotations are highly expensive and professional. This low-resource problem can be alleviated by leveraging the transferable representations of large-scale pre-trained vision-language models via relevant medical text prompts. However, existing pre-trained vision-language models require domain experts to carefully design the medical prompts, which greatly increases the burden on clinicians. To address this problem, we propose a weakly supervised prompt learning method MedPrompt to automatically generate medical prompts, which includes an unsupervised pre-trained vision-language model and a weakly supervised prompt learning model. The unsupervised pre-trained vision-language model utilizes the natural correlation between medical images and corresponding medical texts for pre-training, without any manual annotations. The weakly supervised prompt learning model only utilizes the classes of images in the dataset to guide the learning of the specific class vector in the prompt, while the learning of other context vectors in the prompt requires no manual annotations for guidance. To the best of our knowledge, this is the first model to automatically generate medical prompts. With these prompts, the pre-trained vision-language model can be freed from the strong expert dependency of manual annotation and manual prompt design. Experimental results show that the model using our automatically generated prompts outperforms its full-shot learning hand-crafted prompts counterparts with only a minimal number of labeled samples for few-shot learning, and reaches superior or comparable accuracy on zero-shot image classification. The proposed prompt generator is lightweight and therefore can be embedded into any network architecture.
<div id='section'>Paperid: <span id='pid'>925, <a href='https://arxiv.org/pdf/2309.03744.pdf' target='_blank'>https://arxiv.org/pdf/2309.03744.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nazanin Moradinasab, Rebecca A. Deaton, Laura S. Shankman, Gary K. Owens, Donald E. Brown
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.03744">Label-efficient Contrastive Learning-based model for nuclei detection and classification in 3D Cardiovascular Immunofluorescent Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, deep learning-based methods achieved promising performance in nuclei detection and classification applications. However, training deep learning-based methods requires a large amount of pixel-wise annotated data, which is time-consuming and labor-intensive, especially in 3D images. An alternative approach is to adapt weak-annotation methods, such as labeling each nucleus with a point, but this method does not extend from 2D histopathology images (for which it was originally developed) to 3D immunofluorescent images. The reason is that 3D images contain multiple channels (z-axis) for nuclei and different markers separately, which makes training using point annotations difficult. To address this challenge, we propose the Label-efficient Contrastive learning-based (LECL) model to detect and classify various types of nuclei in 3D immunofluorescent images. Previous methods use Maximum Intensity Projection (MIP) to convert immunofluorescent images with multiple slices to 2D images, which can cause signals from different z-stacks to falsely appear associated with each other. To overcome this, we devised an Extended Maximum Intensity Projection (EMIP) approach that addresses issues using MIP. Furthermore, we performed a Supervised Contrastive Learning (SCL) approach for weakly supervised settings. We conducted experiments on cardiovascular datasets and found that our proposed framework is effective and efficient in detecting and classifying various types of nuclei in 3D immunofluorescent images.
<div id='section'>Paperid: <span id='pid'>926, <a href='https://arxiv.org/pdf/2308.09946.pdf' target='_blank'>https://arxiv.org/pdf/2308.09946.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guiqin Wang, Peng Zhao, Cong Zhao, Shusen Yang, Jie Cheng, Luziwei Leng, Jianxing Liao, Qinghai Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.09946">Weakly-Supervised Action Localization by Hierarchically-structured Latent Attention Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised action localization aims to recognize and localize action instancese in untrimmed videos with only video-level labels. Most existing models rely on multiple instance learning(MIL), where the predictions of unlabeled instances are supervised by classifying labeled bags. The MIL-based methods are relatively well studied with cogent performance achieved on classification but not on localization. Generally, they locate temporal regions by the video-level classification but overlook the temporal variations of feature semantics. To address this problem, we propose a novel attention-based hierarchically-structured latent model to learn the temporal variations of feature semantics. Specifically, our model entails two components, the first is an unsupervised change-points detection module that detects change-points by learning the latent representations of video features in a temporal hierarchy based on their rates of change, and the second is an attention-based classification model that selects the change-points of the foreground as the boundaries. To evaluate the effectiveness of our model, we conduct extensive experiments on two benchmark datasets, THUMOS-14 and ActivityNet-v1.3. The experiments show that our method outperforms current state-of-the-art methods, and even achieves comparable performance with fully-supervised methods.
<div id='section'>Paperid: <span id='pid'>927, <a href='https://arxiv.org/pdf/2307.04159.pdf' target='_blank'>https://arxiv.org/pdf/2307.04159.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xavier Bou, Aitor Artola, Thibaud Ehret, Gabriele Facciolo, Jean-Michel Morel, Rafael Grompone von Gioi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.04159">Reducing False Alarms in Video Surveillance by Deep Feature Statistical Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detecting relevant changes is a fundamental problem of video surveillance. Because of the high variability of data and the difficulty of properly annotating changes, unsupervised methods dominate the field. Arguably one of the most critical issues to make them practical is to reduce their false alarm rate. In this work, we develop a method-agnostic weakly supervised a-contrario validation process, based on high dimensional statistical modeling of deep features, to reduce the number of false alarms of any change detection algorithm. We also raise the insufficiency of the conventionally used pixel-wise evaluation, as it fails to precisely capture the performance needs of most real applications. For this reason, we complement pixel-wise metrics with object-wise metrics and evaluate the impact of our approach at both pixel and object levels, on six methods and several sequences from different datasets. Experimental results reveal that the proposed a-contrario validation is able to largely reduce the number of false alarms at both pixel and object levels.
<div id='section'>Paperid: <span id='pid'>928, <a href='https://arxiv.org/pdf/2306.02693.pdf' target='_blank'>https://arxiv.org/pdf/2306.02693.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyunsoo Cho, Youna Kim, Sang-goo Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.02693">CELDA: Leveraging Black-box Language Model as Enhanced Classifier without Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Utilizing language models (LMs) without internal access is becoming an attractive paradigm in the field of NLP as many cutting-edge LMs are released through APIs and boast a massive scale. The de-facto method in this type of black-box scenario is known as prompting, which has shown progressive performance enhancements in situations where data labels are scarce or unavailable. Despite their efficacy, they still fall short in comparison to fully supervised counterparts and are generally brittle to slight modifications. In this paper, we propose Clustering-enhanced Linear Discriminative Analysis, a novel approach that improves the text classification accuracy with a very weak-supervision signal (i.e., name of the labels). Our framework draws a precise decision boundary without accessing weights or gradients of the LM model or data labels. The core ideas of CELDA are twofold: (1) extracting a refined pseudo-labeled dataset from an unlabeled dataset, and (2) training a lightweight and robust model on the top of LM, which learns an accurate decision boundary from an extracted noisy dataset. Throughout in-depth investigations on various datasets, we demonstrated that CELDA reaches new state-of-the-art in weakly-supervised text classification and narrows the gap with a fully-supervised model. Additionally, our proposed methodology can be applied universally to any LM and has the potential to scale to larger models, making it a more viable option for utilizing large LMs.
<div id='section'>Paperid: <span id='pid'>929, <a href='https://arxiv.org/pdf/2305.02032.pdf' target='_blank'>https://arxiv.org/pdf/2305.02032.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sajid Javed, Arif Mahmood, Talha Qaiser, Naoufel Werghi, Nasir Rajpoot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.02032">Unsupervised Mutual Transformer Learning for Multi-Gigapixel Whole Slide Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Classification of gigapixel Whole Slide Images (WSIs) is an important prediction task in the emerging area of computational pathology. There has been a surge of research in deep learning models for WSI classification with clinical applications such as cancer detection or prediction of molecular mutations from WSIs. Most methods require expensive and labor-intensive manual annotations by expert pathologists. Weakly supervised Multiple Instance Learning (MIL) methods have recently demonstrated excellent performance; however, they still require large slide-level labeled training datasets that need a careful inspection of each slide by an expert pathologist. In this work, we propose a fully unsupervised WSI classification algorithm based on mutual transformer learning. Instances from gigapixel WSI (i.e., image patches) are transformed into a latent space and then inverse-transformed to the original space. Using the transformation loss, pseudo-labels are generated and cleaned using a transformer label-cleaner. The proposed transformer-based pseudo-label generation and cleaning modules mutually train each other iteratively in an unsupervised manner. A discriminative learning mechanism is introduced to improve normal versus cancerous instance labeling. In addition to unsupervised classification, we demonstrate the effectiveness of the proposed framework for weak supervision for cancer subtype classification as downstream analysis. Extensive experiments on four publicly available datasets show excellent performance compared to the state-of-the-art methods. We intend to make the source code of our algorithm publicly available soon.
<div id='section'>Paperid: <span id='pid'>930, <a href='https://arxiv.org/pdf/2303.13634.pdf' target='_blank'>https://arxiv.org/pdf/2303.13634.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Kashefi, Leonidas J. Guibas, Tapan Mukerji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.13634">Physics-informed PointNet: On how many irregular geometries can it solve an inverse problem simultaneously? Application to linear elasticity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Regular physics-informed neural networks (PINNs) predict the solution of partial differential equations using sparse labeled data but only over a single domain. On the other hand, fully supervised learning models are first trained usually over a few thousand domains with known solutions (i.e., labeled data) and then predict the solution over a few hundred unseen domains. Physics-informed PointNet (PIPN) is primarily designed to fill this gap between PINNs (as weakly supervised learning models) and fully supervised learning models. In this article, we demonstrate that PIPN predicts the solution of desired partial differential equations over a few hundred domains simultaneously, while it only uses sparse labeled data. This framework benefits fast geometric designs in the industry when only sparse labeled data are available. Particularly, we show that PIPN predicts the solution of a plane stress problem over more than 500 domains with different geometries, simultaneously. Moreover, we pioneer implementing the concept of remarkable batch size (i.e., the number of geometries fed into PIPN at each sub-epoch) into PIPN. Specifically, we try batch sizes of 7, 14, 19, 38, 76, and 133. Additionally, the effect of the PIPN size, symmetric function in the PIPN architecture, and static and dynamic weights for the component of the sparse labeled data in the loss function are investigated.
<div id='section'>Paperid: <span id='pid'>931, <a href='https://arxiv.org/pdf/2303.10365.pdf' target='_blank'>https://arxiv.org/pdf/2303.10365.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiyu Tian, Hongxin Wei, Yiqun Wang, Lei Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.10365">CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partial-label learning (PLL) is an important weakly supervised learning problem, which allows each training example to have a candidate label set instead of a single ground-truth label. Identification-based methods have been widely explored to tackle label ambiguity issues in PLL, which regard the true label as a latent variable to be identified. However, identifying the true labels accurately and completely remains challenging, causing noise in pseudo labels during model training. In this paper, we propose a new method called CroSel, which leverages historical predictions from the model to identify true labels for most training examples. First, we introduce a cross selection strategy, which enables two deep models to select true labels of partially labeled data for each other. Besides, we propose a novel consistency regularization term called co-mix to avoid sample waste and tiny noise caused by false selection. In this way, CroSel can pick out the true labels of most examples with high precision. Extensive experiments demonstrate the superiority of CroSel, which consistently outperforms previous state-of-the-art methods on benchmark datasets. Additionally, our method achieves over 90\% accuracy and quantity for selecting true labels on CIFAR-type datasets under various settings.
<div id='section'>Paperid: <span id='pid'>932, <a href='https://arxiv.org/pdf/2303.01283.pdf' target='_blank'>https://arxiv.org/pdf/2303.01283.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shota Harada, Ryoma Bise, Kengo Araki, Akihiko Yoshizawa, Kazuhiro Terada, Mariyo Kurata, Naoki Nakajima, Hiroyuki Abe, Tetsuo Ushiku, Seiichi Uchida
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.01283">Cluster-Guided Semi-Supervised Domain Adaptation for Imbalanced Medical Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semi-supervised domain adaptation is a technique to build a classifier for a target domain by modifying a classifier in another (source) domain using many unlabeled samples and a small number of labeled samples from the target domain. In this paper, we develop a semi-supervised domain adaptation method, which has robustness to class-imbalanced situations, which are common in medical image classification tasks. For robustness, we propose a weakly-supervised clustering pipeline to obtain high-purity clusters and utilize the clusters in representation learning for domain adaptation. The proposed method showed state-of-the-art performance in the experiment using severely class-imbalanced pathological image patches.
<div id='section'>Paperid: <span id='pid'>933, <a href='https://arxiv.org/pdf/2302.00669.pdf' target='_blank'>https://arxiv.org/pdf/2302.00669.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bhakti Baheti, Sunny Rai, Shubham Innani, Garv Mehdiratta, Sharath Chandra Guntuku, MacLean P. Nasrallah, Spyridon Bakas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.00669">Detecting Histologic & Clinical Glioblastoma Patterns of Prognostic Relevance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Glioblastoma is the most common and aggressive malignant adult tumor of the central nervous system, with a grim prognosis and heterogeneous morphologic and molecular profiles. Since adopting the current standard-of-care treatment 18 years ago, no substantial prognostic improvement has been noticed. Accurate prediction of patient overall survival (OS) from histopathology whole slide images (WSI) integrated with clinical data using advanced computational methods could optimize clinical decision-making and patient management. Here, we focus on identifying prognostically relevant glioblastoma characteristics from H&E stained WSI & clinical data relating to OS. The exact approach for WSI capitalizes on the comprehensive curation of apparent artifactual content and an interpretability mechanism via a weakly supervised attention-based multiple-instance learning algorithm that further utilizes clustering to constrain the search space. The automatically placed patterns of high diagnostic value classify each WSI as representative of short or long-survivors. Further assessment of the prognostic relevance of the associated clinical patient data is performed both in isolation and in an integrated manner, using XGBoost and SHapley Additive exPlanations (SHAP). Identifying tumor morphological & clinical patterns associated with short and long OS will enable the clinical neuropathologist to provide additional relevant prognostic information to the treating team and suggest avenues of biological investigation for understanding and potentially treating glioblastoma.
<div id='section'>Paperid: <span id='pid'>934, <a href='https://arxiv.org/pdf/2301.10772.pdf' target='_blank'>https://arxiv.org/pdf/2301.10772.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhijian Yang, Junhao Wen, Ahmed Abdulkadir, Yuhan Cui, Guray Erus, Elizabeth Mamourian, Randa Melhem, Dhivya Srinivasan, Sindhuja T. Govindarajan, Jiong Chen, Mohamad Habes, Colin L. Masters, Paul Maruff, Jurgen Fripp, Luigi Ferrucci, Marilyn S. Albert, Sterling C. Johnson, John C. Morris, Pamela LaMontagne, Daniel S. Marcus, Tammie L. S. Benzinger, David A. Wolk, Li Shen, Jingxuan Bao, Susan M. Resnick, Haochang Shou, Ilya M. Nasrallah, Christos Davatzikos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.10772">Gene-SGAN: a method for discovering disease subtypes with imaging and genetic signatures via multi-view weakly-supervised deep clustering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Disease heterogeneity has been a critical challenge for precision diagnosis and treatment, especially in neurologic and neuropsychiatric diseases. Many diseases can display multiple distinct brain phenotypes across individuals, potentially reflecting disease subtypes that can be captured using MRI and machine learning methods. However, biological interpretability and treatment relevance are limited if the derived subtypes are not associated with genetic drivers or susceptibility factors. Herein, we describe Gene-SGAN - a multi-view, weakly-supervised deep clustering method - which dissects disease heterogeneity by jointly considering phenotypic and genetic data, thereby conferring genetic correlations to the disease subtypes and associated endophenotypic signatures. We first validate the generalizability, interpretability, and robustness of Gene-SGAN in semi-synthetic experiments. We then demonstrate its application to real multi-site datasets from 28,858 individuals, deriving subtypes of Alzheimer's disease and brain endophenotypes associated with hypertension, from MRI and SNP data. Derived brain phenotypes displayed significant differences in neuroanatomical patterns, genetic determinants, biological and clinical biomarkers, indicating potentially distinct underlying neuropathologic processes, genetic drivers, and susceptibility factors. Overall, Gene-SGAN is broadly applicable to disease subtyping and endophenotype discovery, and is herein tested on disease-related, genetically-driven neuroimaging phenotypes.
<div id='section'>Paperid: <span id='pid'>935, <a href='https://arxiv.org/pdf/2301.07923.pdf' target='_blank'>https://arxiv.org/pdf/2301.07923.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Snehashis Majhi, Rui Dai, Quan Kong, Lorenzo Garattoni, Gianpiero Francesca, Francois Bremond
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.07923">Human-Scene Network: A Novel Baseline with Self-rectifying Loss for Weakly supervised Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video anomaly detection in surveillance systems with only video-level labels (i.e. weakly-supervised) is challenging. This is due to, (i) the complex integration of human and scene based anomalies comprising of subtle and sharp spatio-temporal cues in real-world scenarios, (ii) non-optimal optimization between normal and anomaly instances under weak supervision. In this paper, we propose a Human-Scene Network to learn discriminative representations by capturing both subtle and strong cues in a dissociative manner. In addition, a self-rectifying loss is also proposed that dynamically computes the pseudo temporal annotations from video-level labels for optimizing the Human-Scene Network effectively. The proposed Human-Scene Network optimized with self-rectifying loss is validated on three publicly available datasets i.e. UCF-Crime, ShanghaiTech and IITB-Corridor, outperforming recently reported state-of-the-art approaches on five out of the six scenarios considered.
<div id='section'>Paperid: <span id='pid'>936, <a href='https://arxiv.org/pdf/2212.09525.pdf' target='_blank'>https://arxiv.org/pdf/2212.09525.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yangyu Huang, Xi Chen, Jongyoo Kim, Hao Yang, Chong Li, Jiaolong Yang, Dong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.09525">FreeEnricher: Enriching Face Landmarks without Additional Cost</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent years have witnessed significant growth of face alignment. Though dense facial landmark is highly demanded in various scenarios, e.g., cosmetic medicine and facial beautification, most works only consider sparse face alignment. To address this problem, we present a framework that can enrich landmark density by existing sparse landmark datasets, e.g., 300W with 68 points and WFLW with 98 points. Firstly, we observe that the local patches along each semantic contour are highly similar in appearance. Then, we propose a weakly-supervised idea of learning the refinement ability on original sparse landmarks and adapting this ability to enriched dense landmarks. Meanwhile, several operators are devised and organized together to implement the idea. Finally, the trained model is applied as a plug-and-play module to the existing face alignment networks. To evaluate our method, we manually label the dense landmarks on 300W testset. Our method yields state-of-the-art accuracy not only in newly-constructed dense 300W testset but also in the original sparse 300W and WFLW testsets without additional cost.
<div id='section'>Paperid: <span id='pid'>937, <a href='https://arxiv.org/pdf/2210.15457.pdf' target='_blank'>https://arxiv.org/pdf/2210.15457.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hengwei Zhao, Yanfei Zhong, Xinyu Wang, Hong Shu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.15457">One-Class Risk Estimation for One-Class Hyperspectral Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hyperspectral imagery (HSI) one-class classification is aimed at identifying a single target class from the HSI by using only knowing positive data, which can significantly reduce the requirements for annotation. However, when one-class classification meets HSI, it is difficult for classifiers to find a balance between the overfitting and underfitting of positive data due to the problems of distribution overlap and distribution imbalance. Although deep learning-based methods are currently the mainstream to overcome distribution overlap in HSI multiclassification, few studies focus on deep learning-based HSI one-class classification. In this article, a weakly supervised deep HSI one-class classifier, namely, HOneCls, is proposed, where a risk estimator,the one-class risk estimator, is particularly introduced to make the fully convolutional neural network (FCN) with the ability of one class classification in the case of distribution imbalance. Extensive experiments (20 tasks in total) were conducted to demonstrate the superiority of the proposed classifier.
<div id='section'>Paperid: <span id='pid'>938, <a href='https://arxiv.org/pdf/2205.12420.pdf' target='_blank'>https://arxiv.org/pdf/2205.12420.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Te-Lin Wu, Caiqi Zhang, Qingyuan Hu, Alex Spangher, Nanyun Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2205.12420">Learning Action Conditions from Instructional Manuals for Instruction Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to infer pre- and postconditions of an action is vital for comprehending complex instructions, and is essential for applications such as autonomous instruction-guided agents and assistive AI that supports humans to perform physical tasks. In this work, we propose a task dubbed action condition inference, and collecting a high-quality, human annotated dataset of preconditions and postconditions of actions in instructional manuals. We propose a weakly supervised approach to automatically construct large-scale training instances from online instructional manuals, and curate a densely human-annotated and validated dataset to study how well the current NLP models can infer action-condition dependencies in the instruction texts. We design two types of models differ by whether contextualized and global information is leveraged, as well as various combinations of heuristics to construct the weak supervisions. Our experimental results show a >20% F1-score improvement with considering the entire instruction contexts and a >6% F1-score benefit with the proposed heuristics.
<div id='section'>Paperid: <span id='pid'>939, <a href='https://arxiv.org/pdf/2203.13704.pdf' target='_blank'>https://arxiv.org/pdf/2203.13704.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Zaigham Zaheer, Arif Mahmood, Marcella Astrid, Seung-Ik Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.13704">Clustering Aided Weakly Supervised Training to Detect Anomalous Events in Surveillance Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Formulating learning systems for the detection of real-world anomalous events using only video-level labels is a challenging task mainly due to the presence of noisy labels as well as the rare occurrence of anomalous events in the training data. We propose a weakly supervised anomaly detection system which has multiple contributions including a random batch selection mechanism to reduce inter-batch correlation and a normalcy suppression block which learns to minimize anomaly scores over normal regions of a video by utilizing the overall information available in a training batch. In addition, a clustering loss block is proposed to mitigate the label noise and to improve the representation learning for the anomalous and normal regions. This block encourages the backbone network to produce two distinct feature clusters representing normal and anomalous events. Extensive analysis of the proposed approach is provided using three popular anomaly detection datasets including UCF-Crime, ShanghaiTech, and UCSD Ped2. The experiments demonstrate a superior anomaly detection capability of our approach.
<div id='section'>Paperid: <span id='pid'>940, <a href='https://arxiv.org/pdf/2010.15675.pdf' target='_blank'>https://arxiv.org/pdf/2010.15675.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gnana Praveen R, Eric Granger, Patrick Cardinal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2010.15675">Deep DA for Ordinal Regression of Pain Intensity Estimation Using Weakly-Labeled Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic estimation of pain intensity from facial expressions in videos has an immense potential in health care applications. However, domain adaptation (DA) is needed to alleviate the problem of domain shifts that typically occurs between video data captured in source and target do-mains. Given the laborious task of collecting and annotating videos, and the subjective bias due to ambiguity among adjacent intensity levels, weakly-supervised learning (WSL)is gaining attention in such applications. Yet, most state-of-the-art WSL models are typically formulated as regression problems, and do not leverage the ordinal relation between intensity levels, nor the temporal coherence of multiple consecutive frames. This paper introduces a new deep learn-ing model for weakly-supervised DA with ordinal regression(WSDA-OR), where videos in target domain have coarse la-bels provided on a periodic basis. The WSDA-OR model enforces ordinal relationships among the intensity levels as-signed to the target sequences, and associates multiple relevant frames to sequence-level labels (instead of a single frame). In particular, it learns discriminant and domain-invariant feature representations by integrating multiple in-stance learning with deep adversarial DA, where soft Gaussian labels are used to efficiently represent the weak ordinal sequence-level labels from the target domain. The proposed approach was validated on the RECOLA video dataset as fully-labeled source domain, and UNBC-McMaster video data as weakly-labeled target domain. We have also validated WSDA-OR on BIOVID and Fatigue (private) datasets for sequence level estimation. Experimental results indicate that our approach can provide a significant improvement over the state-of-the-art models, allowing to achieve a greater localization accuracy.
<div id='section'>Paperid: <span id='pid'>941, <a href='https://arxiv.org/pdf/2008.06392.pdf' target='_blank'>https://arxiv.org/pdf/2008.06392.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>R. Gnana Praveen, Eric Granger, Patrick Cardinal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2008.06392">Deep Domain Adaptation for Ordinal Regression of Pain Intensity Estimation Using Weakly-Labelled Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Estimation of pain intensity from facial expressions captured in videos has an immense potential for health care applications. Given the challenges related to subjective variations of facial expressions, and operational capture conditions, the accuracy of state-of-the-art DL models for recognizing facial expressions may decline. Domain adaptation has been widely explored to alleviate the problem of domain shifts that typically occur between video data captured across various source and target domains. Moreover, given the laborious task of collecting and annotating videos, and subjective bias due to ambiguity among adjacent intensity levels, weakly-supervised learning is gaining attention in such applications. State-of-the-art WSL models are typically formulated as regression problems, and do not leverage the ordinal relationship among pain intensity levels, nor temporal coherence of multiple consecutive frames. This paper introduces a new DL model for weakly-supervised DA with ordinal regression that can be adapted using target domain videos with coarse labels provided on a periodic basis. The WSDA-OR model enforces ordinal relationships among intensity levels assigned to target sequences, and associates multiple relevant frames to sequence-level labels. In particular, it learns discriminant and domain-invariant feature representations by integrating multiple instance learning with deep adversarial DA, where soft Gaussian labels are used to efficiently represent the weak ordinal sequence-level labels from target domain. The proposed approach was validated using RECOLA video dataset as fully-labeled source domain data, and UNBC-McMaster shoulder pain video dataset as weakly-labeled target domain data. We have also validated WSDA-OR on BIOVID and Fatigue datasets for sequence level estimation.
<div id='section'>Paperid: <span id='pid'>942, <a href='https://arxiv.org/pdf/1910.08173.pdf' target='_blank'>https://arxiv.org/pdf/1910.08173.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>R. Gnana Praveen, Eric Granger, Patrick Cardinal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1910.08173">Deep Weakly-Supervised Domain Adaptation for Pain Localization in Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic pain assessment has an important potential diagnostic value for populations that are incapable of articulating their pain experiences. As one of the dominating nonverbal channels for eliciting pain expression events, facial expressions has been widely investigated for estimating the pain intensity of individual. However, using state-of-the-art deep learning (DL) models in real-world pain estimation applications poses several challenges related to the subjective variations of facial expressions, operational capture conditions, and lack of representative training videos with labels. Given the cost of annotating intensity levels for every video frame, we propose a weakly-supervised domain adaptation (WSDA) technique that allows for training 3D CNNs for spatio-temporal pain intensity estimation using weakly labeled videos, where labels are provided on a periodic basis. In particular, WSDA integrates multiple instance learning into an adversarial deep domain adaptation framework to train an Inflated 3D-CNN (I3D) model such that it can accurately estimate pain intensities in the target operational domain. The training process relies on weak target loss, along with domain loss and source loss for domain adaptation of the I3D model. Experimental results obtained using labeled source domain RECOLA videos and weakly-labeled target domain UNBC-McMaster videos indicate that the proposed deep WSDA approach can achieve significantly higher level of sequence (bag)-level and frame (instance)-level pain localization accuracy than related state-of-the-art approaches.
<div id='section'>Paperid: <span id='pid'>943, <a href='https://arxiv.org/pdf/1910.07655.pdf' target='_blank'>https://arxiv.org/pdf/1910.07655.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Saeid Asgari Taghanaki, Kumar Abhishek, Joseph Paul Cohen, Julien Cohen-Adad, Ghassan Hamarneh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1910.07655">Deep Semantic Segmentation of Natural and Medical Images: A Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The semantic image segmentation task consists of classifying each pixel of an image into an instance, where each instance corresponds to a class. This task is a part of the concept of scene understanding or better explaining the global context of an image. In the medical image analysis domain, image segmentation can be used for image-guided interventions, radiotherapy, or improved radiological diagnostics. In this review, we categorize the leading deep learning-based medical and non-medical image segmentation solutions into six main groups of deep architectural, data synthesis-based, loss function-based, sequenced models, weakly supervised, and multi-task methods and provide a comprehensive review of the contributions in each of these groups. Further, for each group, we analyze each variant of these groups and discuss the limitations of the current approaches and present potential future research directions for semantic image segmentation.
<div id='section'>Paperid: <span id='pid'>944, <a href='https://arxiv.org/pdf/2512.06845.pdf' target='_blank'>https://arxiv.org/pdf/2512.06845.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Satoshi Hashimoto, Hitoshi Nishimura, Yanan Wang, Mori Kurokawa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.06845">Pseudo Anomalies Are All You Need: Diffusion-Based Generation for Weakly-Supervised Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deploying video anomaly detection in practice is hampered by the scarcity and collection cost of real abnormal footage. We address this by training without any real abnormal videos while evaluating under the standard weakly supervised split, and we introduce PA-VAD, a generation-driven approach that learns a detector from synthesized pseudo-abnormal videos paired with real normal videos, using only a small set of real normal images to drive synthesis. For synthesis, we select class-relevant initial images with CLIP and refine textual prompts with a vision-language model to improve fidelity and scene consistency before invoking a video diffusion model. For training, we mitigate excessive spatiotemporal magnitude in synthesized anomalies by an domain-aligned regularized module that combines domain alignment and memory usage-aware updates. Extensive experiments show that our approach reaches 98.2% on ShanghaiTech and 82.5% on UCF-Crime, surpassing the strongest real-abnormal method on ShanghaiTech by +0.6% and outperforming the UVAD state-of-the-art on UCF-Crime by +1.9%. The results demonstrate that high-accuracy anomaly detection can be obtained without collecting real anomalies, providing a practical path toward scalable deployment.
<div id='section'>Paperid: <span id='pid'>945, <a href='https://arxiv.org/pdf/2512.01701.pdf' target='_blank'>https://arxiv.org/pdf/2512.01701.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiuli Bi, Die Xiao, Junchao Fan, Bin Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.01701">SSR: Semantic and Spatial Rectification for CLIP-based Weakly Supervised Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, Contrastive Language-Image Pretraining (CLIP) has been widely applied to Weakly Supervised Semantic Segmentation (WSSS) tasks due to its powerful cross-modal semantic understanding capabilities. This paper proposes a novel Semantic and Spatial Rectification (SSR) method to address the limitations of existing CLIP-based weakly supervised semantic segmentation approaches: over-activation in non-target foreground regions and background areas. Specifically, at the semantic level, the Cross-Modal Prototype Alignment (CMPA) establishes a contrastive learning mechanism to enforce feature space alignment across modalities, reducing inter-class overlap while enhancing semantic correlations, to rectify over-activation in non-target foreground regions effectively; at the spatial level, the Superpixel-Guided Correction (SGC) leverages superpixel-based spatial priors to precisely filter out interference from non-target regions during affinity propagation, significantly rectifying background over-activation. Extensive experiments on the PASCAL VOC and MS COCO datasets demonstrate that our method outperforms all single-stage approaches, as well as more complex multi-stage approaches, achieving mIoU scores of 79.5% and 50.6%, respectively.
<div id='section'>Paperid: <span id='pid'>946, <a href='https://arxiv.org/pdf/2510.16450.pdf' target='_blank'>https://arxiv.org/pdf/2510.16450.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shan Xiong, Jiabao Chen, Ye Wang, Jialin Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.16450">Instance-Aware Pseudo-Labeling and Class-Focused Contrastive Learning for Weakly Supervised Domain Adaptive Segmentation of Electron Microscopy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Annotation-efficient segmentation of the numerous mitochondria instances from various electron microscopy (EM) images is highly valuable for biological and neuroscience research. Although unsupervised domain adaptation (UDA) methods can help mitigate domain shifts and reduce the high costs of annotating each domain, they typically have relatively low performance in practical applications. Thus, we investigate weakly supervised domain adaptation (WDA) that utilizes additional sparse point labels on the target domain, which require minimal annotation effort and minimal expert knowledge. To take full use of the incomplete and imprecise point annotations, we introduce a multitask learning framework that jointly conducts segmentation and center detection with a novel cross-teaching mechanism and class-focused cross-domain contrastive learning. While leveraging unlabeled image regions is essential, we introduce segmentation self-training with a novel instance-aware pseudo-label (IPL) selection strategy. Unlike existing methods that typically rely on pixel-wise pseudo-label filtering, the IPL semantically selects reliable and diverse pseudo-labels with the help of the detection task. Comprehensive validations and comparisons on challenging datasets demonstrate that our method outperforms existing UDA and WDA methods, significantly narrowing the performance gap with the supervised upper bound. Furthermore, under the UDA setting, our method also achieves substantial improvements over other UDA techniques.
<div id='section'>Paperid: <span id='pid'>947, <a href='https://arxiv.org/pdf/2509.18973.pdf' target='_blank'>https://arxiv.org/pdf/2509.18973.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiabao Chen, Shan Xiong, Jialin Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18973">Prompt-DAS: Annotation-Efficient Prompt Learning for Domain Adaptive Semantic Segmentation of Electron Microscopy Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain adaptive segmentation (DAS) of numerous organelle instances from large-scale electron microscopy (EM) is a promising way to enable annotation-efficient learning. Inspired by SAM, we propose a promptable multitask framework, namely Prompt-DAS, which is flexible enough to utilize any number of point prompts during the adaptation training stage and testing stage. Thus, with varying prompt configurations, Prompt-DAS can perform unsupervised domain adaptation (UDA) and weakly supervised domain adaptation (WDA), as well as interactive segmentation during testing. Unlike the foundation model SAM, which necessitates a prompt for each individual object instance, Prompt-DAS is only trained on a small dataset and can utilize full points on all instances, sparse points on partial instances, or even no points at all, facilitated by the incorporation of an auxiliary center-point detection task. Moreover, a novel prompt-guided contrastive learning is proposed to enhance discriminative feature learning. Comprehensive experiments conducted on challenging benchmarks demonstrate the effectiveness of the proposed approach over existing UDA, WDA, and SAM-based approaches.
<div id='section'>Paperid: <span id='pid'>948, <a href='https://arxiv.org/pdf/2509.01209.pdf' target='_blank'>https://arxiv.org/pdf/2509.01209.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>MaÃ«lic Neau, Zoe Falomir, CÃ©dric Buche, Akihiro Sugimoto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01209">Measuring Image-Relation Alignment: Reference-Free Evaluation of VLMs and Synthetic Pre-training for Open-Vocabulary Scene Graph Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene Graph Generation (SGG) encodes visual relationships between objects in images as graph structures. Thanks to the advances of Vision-Language Models (VLMs), the task of Open-Vocabulary SGG has been recently proposed where models are evaluated on their functionality to learn a wide and diverse range of relations. Current benchmarks in SGG, however, possess a very limited vocabulary, making the evaluation of open-source models inefficient. In this paper, we propose a new reference-free metric to fairly evaluate the open-vocabulary capabilities of VLMs for relation prediction. Another limitation of Open-Vocabulary SGG is the reliance on weakly supervised data of poor quality for pre-training. We also propose a new solution for quickly generating high-quality synthetic data through region-specific prompt tuning of VLMs. Experimental results show that pre-training with this new data split can benefit the generalization capabilities of Open-Voc SGG models.
<div id='section'>Paperid: <span id='pid'>949, <a href='https://arxiv.org/pdf/2508.19909.pdf' target='_blank'>https://arxiv.org/pdf/2508.19909.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lechun You, Zhonghua Wu, Weide Liu, Xulei Yang, Jun Cheng, Wei Zhou, Bharadwaj Veeravalli, Guosheng Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19909">Integrating SAM Supervision for 3D Weakly Supervised Point Cloud Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current methods for 3D semantic segmentation propose training models with limited annotations to address the difficulty of annotating large, irregular, and unordered 3D point cloud data. They usually focus on the 3D domain only, without leveraging the complementary nature of 2D and 3D data. Besides, some methods extend original labels or generate pseudo labels to guide the training, but they often fail to fully use these labels or address the noise within them. Meanwhile, the emergence of comprehensive and adaptable foundation models has offered effective solutions for segmenting 2D data. Leveraging this advancement, we present a novel approach that maximizes the utility of sparsely available 3D annotations by incorporating segmentation masks generated by 2D foundation models. We further propagate the 2D segmentation masks into the 3D space by establishing geometric correspondences between 3D scenes and 2D views. We extend the highly sparse annotations to encompass the areas delineated by 3D masks, thereby substantially augmenting the pool of available labels. Furthermore, we apply confidence- and uncertainty-based consistency regularization on augmentations of the 3D point cloud and select the reliable pseudo labels, which are further spread on the 3D masks to generate more labels. This innovative strategy bridges the gap between limited 3D annotations and the powerful capabilities of 2D foundation models, ultimately improving the performance of 3D weakly supervised segmentation.
<div id='section'>Paperid: <span id='pid'>950, <a href='https://arxiv.org/pdf/2507.02399.pdf' target='_blank'>https://arxiv.org/pdf/2507.02399.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peilin Zhang, Shaouxan Wua, Jun Feng, Zhuo Jin, Zhizezhang Gao, Jingkun Chen, Yaqiong Xing, Xiao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02399">TABNet: A Triplet Augmentation Self-Recovery Framework with Boundary-Aware Pseudo-Labels for Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Background and objective: Medical image segmentation is a core task in various clinical applications. However, acquiring large-scale, fully annotated medical image datasets is both time-consuming and costly. Scribble annotations, as a form of sparse labeling, provide an efficient and cost-effective alternative for medical image segmentation. However, the sparsity of scribble annotations limits the feature learning of the target region and lacks sufficient boundary supervision, which poses significant challenges for training segmentation networks. Methods: We propose TAB Net, a novel weakly-supervised medical image segmentation framework, consisting of two key components: the triplet augmentation self-recovery (TAS) module and the boundary-aware pseudo-label supervision (BAP) module. The TAS module enhances feature learning through three complementary augmentation strategies: intensity transformation improves the model's sensitivity to texture and contrast variations, cutout forces the network to capture local anatomical structures by masking key regions, and jigsaw augmentation strengthens the modeling of global anatomical layout by disrupting spatial continuity. By guiding the network to recover complete masks from diverse augmented inputs, TAS promotes a deeper semantic understanding of medical images under sparse supervision. The BAP module enhances pseudo-supervision accuracy and boundary modeling by fusing dual-branch predictions into a loss-weighted pseudo-label and introducing a boundary-aware loss for fine-grained contour refinement. Results: Experimental evaluations on two public datasets, ACDC and MSCMR seg, demonstrate that TAB Net significantly outperforms state-of-the-art methods for scribble-based weakly supervised segmentation. Moreover, it achieves performance comparable to that of fully supervised methods.
<div id='section'>Paperid: <span id='pid'>951, <a href='https://arxiv.org/pdf/2506.19222.pdf' target='_blank'>https://arxiv.org/pdf/2506.19222.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinke Ma, Yongsheng Pan, Qingjie Zeng, Mengkang Lu, Bolysbek Murat Yerzhanuly, Bazargul Matkerim, Yong Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19222">Deformable Medical Image Registration with Effective Anatomical Structure Representation and Divide-and-Conquer Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective representation of Regions of Interest (ROI) and independent alignment of these ROIs can significantly enhance the performance of deformable medical image registration (DMIR). However, current learning-based DMIR methods have limitations. Unsupervised techniques disregard ROI representation and proceed directly with aligning pairs of images, while weakly-supervised methods heavily depend on label constraints to facilitate registration. To address these issues, we introduce a novel ROI-based registration approach named EASR-DCN. Our method represents medical images through effective ROIs and achieves independent alignment of these ROIs without requiring labels. Specifically, we first used a Gaussian mixture model for intensity analysis to represent images using multiple effective ROIs with distinct intensities. Furthermore, we propose a novel Divide-and-Conquer Network (DCN) to process these ROIs through separate channels to learn feature alignments for each ROI. The resultant correspondences are seamlessly integrated to generate a comprehensive displacement vector field. Extensive experiments were performed on three MRI and one CT datasets to showcase the superior accuracy and deformation reduction efficacy of our EASR-DCN. Compared to VoxelMorph, our EASR-DCN achieved improvements of 10.31\% in the Dice score for brain MRI, 13.01\% for cardiac MRI, and 5.75\% for hippocampus MRI, highlighting its promising potential for clinical applications. The code for this work will be released upon acceptance of the paper.
<div id='section'>Paperid: <span id='pid'>952, <a href='https://arxiv.org/pdf/2506.10633.pdf' target='_blank'>https://arxiv.org/pdf/2506.10633.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Konstantinos Vilouras, Ilias Stogiannidis, Junyu Yan, Alison Q. O'Neil, Sotirios A. Tsaftaris
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10633">Anatomy-Grounded Weakly Supervised Prompt Tuning for Chest X-ray Latent Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Latent Diffusion Models have shown remarkable results in text-guided image synthesis in recent years. In the domain of natural (RGB) images, recent works have shown that such models can be adapted to various vision-language downstream tasks with little to no supervision involved. On the contrary, text-to-image Latent Diffusion Models remain relatively underexplored in the field of medical imaging, primarily due to limited data availability (e.g., due to privacy concerns). In this work, focusing on the chest X-ray modality, we first demonstrate that a standard text-conditioned Latent Diffusion Model has not learned to align clinically relevant information in free-text radiology reports with the corresponding areas of the given scan. Then, to alleviate this issue, we propose a fine-tuning framework to improve multi-modal alignment in a pre-trained model such that it can be efficiently repurposed for downstream tasks such as phrase grounding. Our method sets a new state-of-the-art on a standard benchmark dataset (MS-CXR), while also exhibiting robust performance on out-of-distribution data (VinDr-CXR). Our code will be made publicly available.
<div id='section'>Paperid: <span id='pid'>953, <a href='https://arxiv.org/pdf/2506.04351.pdf' target='_blank'>https://arxiv.org/pdf/2506.04351.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maksym Ivashechkin, Oscar Mendez, Richard Bowden
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.04351">HuGeDiff: 3D Human Generation via Diffusion with Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D human generation is an important problem with a wide range of applications in computer vision and graphics. Despite recent progress in generative AI such as diffusion models or rendering methods like Neural Radiance Fields or Gaussian Splatting, controlling the generation of accurate 3D humans from text prompts remains an open challenge. Current methods struggle with fine detail, accurate rendering of hands and faces, human realism, and controlability over appearance. The lack of diversity, realism, and annotation in human image data also remains a challenge, hindering the development of a foundational 3D human model. We present a weakly supervised pipeline that tries to address these challenges. In the first step, we generate a photorealistic human image dataset with controllable attributes such as appearance, race, gender, etc using a state-of-the-art image diffusion model. Next, we propose an efficient mapping approach from image features to 3D point clouds using a transformer-based architecture. Finally, we close the loop by training a point-cloud diffusion model that is conditioned on the same text prompts used to generate the original samples. We demonstrate orders-of-magnitude speed-ups in 3D human generation compared to the state-of-the-art approaches, along with significantly improved text-prompt alignment, realism, and rendering quality. We will make the code and dataset available.
<div id='section'>Paperid: <span id='pid'>954, <a href='https://arxiv.org/pdf/2502.06591.pdf' target='_blank'>https://arxiv.org/pdf/2502.06591.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ron Shapira Weber, Oren Freifeld
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.06591">Diffeomorphic Temporal Alignment Nets for Time-series Joint Alignment and Averaging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In time-series analysis, nonlinear temporal misalignment remains a pivotal challenge that forestalls even simple averaging. Since its introduction, the Diffeomorphic Temporal Alignment Net (DTAN), which we first introduced (Weber et al., 2019) and further developed in (Weber & Freifeld, 2023), has proven itself as an effective solution for this problem (these conference papers are earlier partial versions of the current manuscript). DTAN predicts and applies diffeomorphic transformations in an input-dependent manner, thus facilitating the joint alignment (JA) and averaging of time-series ensembles in an unsupervised or a weakly-supervised manner. The inherent challenges of the weakly/unsupervised setting, particularly the risk of trivial solutions through excessive signal distortion, are mitigated using either one of two distinct strategies: 1) a regularization term for warps; 2) using the Inverse Consistency Averaging Error (ICAE). The latter is a novel, regularization-free approach which also facilitates the JA of variable-length signals. We also further extend our framework to incorporate multi-task learning (MT-DTAN), enabling simultaneous time-series alignment and classification. Additionally, we conduct a comprehensive evaluation of different backbone architectures, demonstrating their efficacy in time-series alignment tasks. Finally, we showcase the utility of our approach in enabling Principal Component Analysis (PCA) for misaligned time-series data. Extensive experiments across 128 UCR datasets validate the superiority of our approach over contemporary averaging methods, including both traditional and learning-based approaches, marking a significant advancement in the field of time-series analysis.
<div id='section'>Paperid: <span id='pid'>955, <a href='https://arxiv.org/pdf/2501.19345.pdf' target='_blank'>https://arxiv.org/pdf/2501.19345.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Masahiro Kato, Fumiaki Kozai, Ryo Inokuchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.19345">PUATE: Efficient Average Treatment Effect Estimation from Treated (Positive) and Unlabeled Units</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The estimation of average treatment effects (ATEs), defined as the difference in expected outcomes between treatment and control groups, is a central topic in causal inference. This study develops semiparametric efficient estimators for ATE in a setting where only a treatment group and an unlabeled group, consisting of units whose treatment status is unknown, are observed. This scenario constitutes a variant of learning from positive and unlabeled data (PU learning) and can be viewed as a special case of ATE estimation with missing data. For this setting, we derive the semiparametric efficiency bounds, which characterize the lowest achievable asymptotic variance for regular estimators. We then construct semiparametric efficient ATE estimators that attain these bounds. Our results contribute to the literature on causal inference with missing data and weakly supervised learning.
<div id='section'>Paperid: <span id='pid'>956, <a href='https://arxiv.org/pdf/2501.01845.pdf' target='_blank'>https://arxiv.org/pdf/2501.01845.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunshuang Yuan, Frank Thiemann, Monika Sester
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01845">Semantic Segmentation for Sequential Historical Maps by Learning from Only One Map</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Historical maps are valuable resources that capture detailed geographical information from the past. However, these maps are typically available in printed formats, which are not conducive to modern computer-based analyses. Digitizing these maps into a machine-readable format enables efficient computational analysis. In this paper, we propose an automated approach to digitization using deep-learning-based semantic segmentation, which assigns a semantic label to each pixel in scanned historical maps. A key challenge in this process is the lack of ground-truth annotations required for training deep neural networks, as manual labeling is time-consuming and labor-intensive. To address this issue, we introduce a weakly-supervised age-tracing strategy for model fine-tuning. This approach exploits the similarity in appearance and land-use patterns between historical maps from neighboring time periods to guide the training process. Specifically, model predictions for one map are utilized as pseudo-labels for training on maps from adjacent time periods. Experiments conducted on our newly curated \textit{Hameln} dataset demonstrate that the proposed age-tracing strategy significantly enhances segmentation performance compared to baseline models. In the best-case scenario, the mean Intersection over Union (mIoU) achieved 77.3\%, reflecting an improvement of approximately 20\% over baseline methods. Additionally, the fine-tuned model achieved an average overall accuracy of 97\%, highlighting the effectiveness of our approach for digitizing historical maps.
<div id='section'>Paperid: <span id='pid'>957, <a href='https://arxiv.org/pdf/2410.05900.pdf' target='_blank'>https://arxiv.org/pdf/2410.05900.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiling Zhang, Erkut Akdag, Egor Bondarev, Peter H. N. De With
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05900">MTFL: Multi-Timescale Feature Learning for Weakly-Supervised Anomaly Detection in Surveillance Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detection of anomaly events is relevant for public safety and requires a combination of fine-grained motion information and contextual events at variable time-scales. To this end, we propose a Multi-Timescale Feature Learning (MTFL) method to enhance the representation of anomaly features. Short, medium, and long temporal tubelets are employed to extract spatio-temporal video features using a Video Swin Transformer. Experimental results demonstrate that MTFL outperforms state-of-the-art methods on the UCF-Crime dataset, achieving an anomaly detection performance 89.78% AUC. Moreover, it performs complementary to SotA with 95.32% AUC on the ShanghaiTech and 84.57% AP on the XD-Violence dataset. Furthermore, we generate an extended dataset of the UCF-Crime for development and evaluation on a wider range of anomalies, namely Video Anomaly Detection Dataset (VADD), involving 2,591 videos in 18 classes with extensive coverage of realistic anomalies.
<div id='section'>Paperid: <span id='pid'>958, <a href='https://arxiv.org/pdf/2409.20253.pdf' target='_blank'>https://arxiv.org/pdf/2409.20253.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Iira HÃ¤kkinen, Iaroslav Melekhov, Erik Englesson, Hossein Azizpour, Juho Kannala
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.20253">Medical Image Segmentation with SAM-generated Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The field of medical image segmentation is hindered by the scarcity of large, publicly available annotated datasets. Not all datasets are made public for privacy reasons, and creating annotations for a large dataset is time-consuming and expensive, as it requires specialized expertise to accurately identify regions of interest (ROIs) within the images. To address these challenges, we evaluate the performance of the Segment Anything Model (SAM) as an annotation tool for medical data by using it to produce so-called "pseudo labels" on the Medical Segmentation Decathlon (MSD) computed tomography (CT) tasks. The pseudo labels are then used in place of ground truth labels to train a UNet model in a weakly-supervised manner. We experiment with different prompt types on SAM and find that the bounding box prompt is a simple yet effective method for generating pseudo labels. This method allows us to develop a weakly-supervised model that performs comparably to a fully supervised model.
<div id='section'>Paperid: <span id='pid'>959, <a href='https://arxiv.org/pdf/2409.19587.pdf' target='_blank'>https://arxiv.org/pdf/2409.19587.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abhijeet Patil, Harsh Diwakar, Jay Sawant, Nikhil Cherian Kurian, Subhash Yadav, Swapnil Rane, Tripti Bameta, Amit Sethi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19587">Efficient Quality Control of Whole Slide Pathology Images with Human-in-the-loop Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Histopathology whole slide images (WSIs) are being widely used to develop deep learning-based diagnostic solutions, especially for precision oncology. Most of these diagnostic softwares are vulnerable to biases and impurities in the training and test data which can lead to inaccurate diagnoses. For instance, WSIs contain multiple types of tissue regions, at least some of which might not be relevant to the diagnosis. We introduce HistoROI, a robust yet lightweight deep learning-based classifier to segregate WSI into six broad tissue regions -- epithelium, stroma, lymphocytes, adipose, artifacts, and miscellaneous. HistoROI is trained using a novel human-in-the-loop and active learning paradigm that ensures variations in training data for labeling-efficient generalization. HistoROI consistently performs well across multiple organs, despite being trained on only a single dataset, demonstrating strong generalization. Further, we have examined the utility of HistoROI in improving the performance of downstream deep learning-based tasks using the CAMELYON breast cancer lymph node and TCGA lung cancer datasets. For the former dataset, the area under the receiver operating characteristic curve (AUC) for metastasis versus normal tissue of a neural network trained using weakly supervised learning increased from 0.88 to 0.92 by filtering the data using HistoROI. Similarly, the AUC increased from 0.88 to 0.93 for the classification between adenocarcinoma and squamous cell carcinoma on the lung cancer dataset. We also found that the performance of the HistoROI improves upon HistoQC for artifact detection on a test dataset of 93 annotated WSIs. The limitations of the proposed model are analyzed, and potential extensions are also discussed.
<div id='section'>Paperid: <span id='pid'>960, <a href='https://arxiv.org/pdf/2407.20461.pdf' target='_blank'>https://arxiv.org/pdf/2407.20461.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pascal Spiegler, Amirhossein Rasoulian, Yiming Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.20461">Weakly Supervised Intracranial Hemorrhage Segmentation with YOLO and an Uncertainty Rectified Segment Anything Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intracranial hemorrhage (ICH) is a life-threatening condition that requires rapid and accurate diagnosis to improve treatment outcomes and patient survival rates. Recent advancements in supervised deep learning have greatly improved the analysis of medical images, but often rely on extensive datasets with high-quality annotations, which are costly, time-consuming, and require medical expertise to prepare. To mitigate the need for large amounts of expert-prepared segmentation data, we have developed a novel weakly supervised ICH segmentation method that utilizes the YOLO object detection model and an uncertainty-rectified Segment Anything Model (SAM). In addition, we have proposed a novel point prompt generator for this model to further improve segmentation results with YOLO-predicted bounding box prompts. Our approach achieved a high accuracy of 0.933 and an AUC of 0.796 in ICH detection, along with a mean Dice score of 0.629 for ICH segmentation, outperforming existing weakly supervised and popular supervised (UNet and Swin-UNETR) approaches. Overall, the proposed method provides a robust and accurate alternative to the more commonly used supervised techniques for ICH quantification without requiring refined segmentation ground truths during model training.
<div id='section'>Paperid: <span id='pid'>961, <a href='https://arxiv.org/pdf/2407.15794.pdf' target='_blank'>https://arxiv.org/pdf/2407.15794.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guiqiu Liao, Matjaz Jogan, Sai Koushik, Eric Eaton, Daniel A. Hashimoto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.15794">Disentangling spatio-temporal knowledge for weakly supervised object detection and segmentation in surgical video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised video object segmentation (WSVOS) enables the identification of segmentation maps without requiring an extensive training dataset of object masks, relying instead on coarse video labels indicating object presence. Current state-of-the-art methods either require multiple independent stages of processing that employ motion cues or, in the case of end-to-end trainable networks, lack in segmentation accuracy, in part due to the difficulty of learning segmentation maps from videos with transient object presence. This limits the application of WSVOS for semantic annotation of surgical videos where multiple surgical tools frequently move in and out of the field of view, a problem that is more difficult than typically encountered in WSVOS. This paper introduces Video Spatio-Temporal Disentanglement Networks (VDST-Net), a framework to disentangle spatiotemporal information using semi-decoupled knowledge distillation to predict high-quality class activation maps (CAMs). A teacher network designed to resolve temporal conflicts when specifics about object location and timing in the video are not provided works with a student network that integrates information over time by leveraging temporal dependencies. We demonstrate the efficacy of our framework on a public reference dataset and on a more challenging surgical video dataset where objects are, on average, present in less than 60\% of annotated frames. Our method outperforms state-of-the-art techniques and generates superior segmentation masks under video-level weak supervision.
<div id='section'>Paperid: <span id='pid'>962, <a href='https://arxiv.org/pdf/2406.05308.pdf' target='_blank'>https://arxiv.org/pdf/2406.05308.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Heming Yao, Phil Hanslovsky, Jan-Christian Huetter, Burkhard Hoeckendorf, David Richmond
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.05308">Weakly Supervised Set-Consistency Learning Improves Morphological Profiling of Single-Cell Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Optical Pooled Screening (OPS) is a powerful tool combining high-content microscopy with genetic engineering to investigate gene function in disease. The characterization of high-content images remains an active area of research and is currently undergoing rapid innovation through the application of self-supervised learning and vision transformers. In this study, we propose a set-level consistency learning algorithm, Set-DINO, that combines self-supervised learning with weak supervision to improve learned representations of perturbation effects in single-cell images. Our method leverages the replicate structure of OPS experiments (i.e., cells undergoing the same genetic perturbation, both within and across batches) as a form of weak supervision. We conduct extensive experiments on a large-scale OPS dataset with more than 5000 genetic perturbations, and demonstrate that Set-DINO helps mitigate the impact of confounders and encodes more biologically meaningful information. In particular, Set-DINO recalls known biological relationships with higher accuracy compared to commonly used methods for morphological profiling, suggesting that it can generate more reliable insights from drug target discovery campaigns leveraging OPS.
<div id='section'>Paperid: <span id='pid'>963, <a href='https://arxiv.org/pdf/2405.10690.pdf' target='_blank'>https://arxiv.org/pdf/2405.10690.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Faegheh Sardari, Armin Mustafa, Philip J. B. Jackson, Adrian Hilton
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.10690">CoLeaF: A Contrastive-Collaborative Learning Framework for Weakly Supervised Audio-Visual Video Parsing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised audio-visual video parsing (AVVP) methods aim to detect audible-only, visible-only, and audible-visible events using only video-level labels. Existing approaches tackle this by leveraging unimodal and cross-modal contexts. However, we argue that while cross-modal learning is beneficial for detecting audible-visible events, in the weakly supervised scenario, it negatively impacts unaligned audible or visible events by introducing irrelevant modality information. In this paper, we propose CoLeaF, a novel learning framework that optimizes the integration of cross-modal context in the embedding space such that the network explicitly learns to combine cross-modal information for audible-visible events while filtering them out for unaligned events. Additionally, as videos often involve complex class relationships, modelling them improves performance. However, this introduces extra computational costs into the network. Our framework is designed to leverage cross-class relationships during training without incurring additional computations at inference. Furthermore, we propose new metrics to better evaluate a method's capabilities in performing AVVP. Our extensive experiments demonstrate that CoLeaF significantly improves the state-of-the-art results by an average of 1.9% and 2.4% F-score on the LLP and UnAV-100 datasets, respectively.
<div id='section'>Paperid: <span id='pid'>964, <a href='https://arxiv.org/pdf/2405.08699.pdf' target='_blank'>https://arxiv.org/pdf/2405.08699.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenrui Li, Wei Zhang, Qinghao Zhang, Xuegong Zhang, Xiaowo Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.08699">Weakly-supervised causal discovery based on fuzzy knowledge and complex data complementarity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Causal discovery based on observational data is important for deciphering the causal mechanism behind complex systems. However, the effectiveness of existing causal discovery methods is limited due to inferior prior knowledge, domain inconsistencies, and the challenges of high-dimensional datasets with small sample sizes. To address this gap, we propose a novel weakly-supervised fuzzy knowledge and data co-driven causal discovery method named KEEL. KEEL adopts a fuzzy causal knowledge schema to encapsulate diverse types of fuzzy knowledge, and forms corresponding weakened constraints. This schema not only lessens the dependency on expertise but also allows various types of limited and error-prone fuzzy knowledge to guide causal discovery. It can enhance the generalization and robustness of causal discovery, especially in high-dimensional and small-sample scenarios. In addition, we integrate the extended linear causal model (ELCM) into KEEL for dealing with the multi-distribution and incomplete data. Extensive experiments with different datasets demonstrate the superiority of KEEL over several state-of-the-art methods in accuracy, robustness and computational efficiency. For causal discovery in real protein signal transduction processes, KEEL outperforms the benchmark method with limited data. In summary, KEEL is effective to tackle the causal discovery tasks with higher accuracy while alleviating the requirement for extensive domain expertise.
<div id='section'>Paperid: <span id='pid'>965, <a href='https://arxiv.org/pdf/2404.17324.pdf' target='_blank'>https://arxiv.org/pdf/2404.17324.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jyri MaanpÃ¤Ã¤, Julius Pesonen, Heikki Hyyti, Iaroslav Melekhov, Juho Kannala, Petri Manninen, Antero Kukko, Juha HyyppÃ¤
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.17324">Dense Road Surface Grip Map Prediction from Multimodal Image Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Slippery road weather conditions are prevalent in many regions and cause a regular risk for traffic. Still, there has been less research on how autonomous vehicles could detect slippery driving conditions on the road to drive safely. In this work, we propose a method to predict a dense grip map from the area in front of the car, based on postprocessed multimodal sensor data. We trained a convolutional neural network to predict pixelwise grip values from fused RGB camera, thermal camera, and LiDAR reflectance images, based on weakly supervised ground truth from an optical road weather sensor.
  The experiments show that it is possible to predict dense grip values with good accuracy from the used data modalities as the produced grip map follows both ground truth measurements and local weather conditions, such as snowy areas on the road. The model using only the RGB camera or LiDAR reflectance modality provided good baseline results for grip prediction accuracy while using models fusing the RGB camera, thermal camera, and LiDAR modalities improved the grip predictions significantly.
<div id='section'>Paperid: <span id='pid'>966, <a href='https://arxiv.org/pdf/2404.13311.pdf' target='_blank'>https://arxiv.org/pdf/2404.13311.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yangcen Liu, Ziyi Liu, Yuanhao Zhai, Wen Li, David Doerman, Junsong Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.13311">STAT: Towards Generalizable Temporal Action Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised temporal action localization (WTAL) aims to recognize and localize action instances with only video-level labels. Despite the significant progress, existing methods suffer from severe performance degradation when transferring to different distributions and thus may hardly adapt to real-world scenarios . To address this problem, we propose the Generalizable Temporal Action Localization task (GTAL), which focuses on improving the generalizability of action localization methods. We observed that the performance decline can be primarily attributed to the lack of generalizability to different action scales. To address this problem, we propose STAT (Self-supervised Temporal Adaptive Teacher), which leverages a teacher-student structure for iterative refinement. Our STAT features a refinement module and an alignment module. The former iteratively refines the model's output by leveraging contextual information and helps adapt to the target scale. The latter improves the refinement process by promoting a consensus between student and teacher models. We conduct extensive experiments on three datasets, THUMOS14, ActivityNet1.2, and HACS, and the results show that our method significantly improves the Baseline methods under the cross-distribution evaluation setting, even approaching the same-distribution evaluation performance.
<div id='section'>Paperid: <span id='pid'>967, <a href='https://arxiv.org/pdf/2404.12015.pdf' target='_blank'>https://arxiv.org/pdf/2404.12015.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Claudia Cuttano, Gabriele Rosi, Gabriele Trivigno, Giuseppe Averta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.12015">What does CLIP know about peeling a banana?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans show an innate capability to identify tools to support specific actions. The association between objects parts and the actions they facilitate is usually named affordance. Being able to segment objects parts depending on the tasks they afford is crucial to enable intelligent robots to use objects of daily living. Traditional supervised learning methods for affordance segmentation require costly pixel-level annotations, while weakly supervised approaches, though less demanding, still rely on object-interaction examples and support a closed set of actions. These limitations hinder scalability, may introduce biases, and usually restrict models to a limited set of predefined actions. This paper proposes AffordanceCLIP, to overcome these limitations by leveraging the implicit affordance knowledge embedded within large pre-trained Vision-Language models like CLIP. We experimentally demonstrate that CLIP, although not explicitly trained for affordances detection, retains valuable information for the task. Our AffordanceCLIP achieves competitive zero-shot performance compared to methods with specialized training, while offering several advantages: i) it works with any action prompt, not just a predefined set; ii) it requires training only a small number of additional parameters compared to existing solutions and iii) eliminates the need for direct supervision on action-object pairs, opening new perspectives for functionality-based reasoning of models.
<div id='section'>Paperid: <span id='pid'>968, <a href='https://arxiv.org/pdf/2404.02527.pdf' target='_blank'>https://arxiv.org/pdf/2404.02527.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xu Wang, Yifan Li, Qiudan Zhang, Wenhui Wu, Mark Junjie Li, Jianmin Jinag
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.02527">Weakly-Supervised 3D Scene Graph Generation via Visual-Linguistic Assisted Pseudo-labeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning to build 3D scene graphs is essential for real-world perception in a structured and rich fashion. However, previous 3D scene graph generation methods utilize a fully supervised learning manner and require a large amount of entity-level annotation data of objects and relations, which is extremely resource-consuming and tedious to obtain. To tackle this problem, we propose 3D-VLAP, a weakly-supervised 3D scene graph generation method via Visual-Linguistic Assisted Pseudo-labeling. Specifically, our 3D-VLAP exploits the superior ability of current large-scale visual-linguistic models to align the semantics between texts and 2D images, as well as the naturally existing correspondences between 2D images and 3D point clouds, and thus implicitly constructs correspondences between texts and 3D point clouds. First, we establish the positional correspondence from 3D point clouds to 2D images via camera intrinsic and extrinsic parameters, thereby achieving alignment of 3D point clouds and 2D images. Subsequently, a large-scale cross-modal visual-linguistic model is employed to indirectly align 3D instances with the textual category labels of objects by matching 2D images with object category labels. The pseudo labels for objects and relations are then produced for 3D-VLAP model training by calculating the similarity between visual embeddings and textual category embeddings of objects and relations encoded by the visual-linguistic model, respectively. Ultimately, we design an edge self-attention based graph neural network to generate scene graphs of 3D point cloud scenes. Extensive experiments demonstrate that our 3D-VLAP achieves comparable results with current advanced fully supervised methods, meanwhile significantly alleviating the pressure of data annotation.
<div id='section'>Paperid: <span id='pid'>969, <a href='https://arxiv.org/pdf/2404.00667.pdf' target='_blank'>https://arxiv.org/pdf/2404.00667.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dafei Qiu, Shan Xiong, Jiajin Yi, Jialin Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00667">Weakly-Supervised Cross-Domain Segmentation of Electron Microscopy with Sparse Point Annotation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate segmentation of organelle instances from electron microscopy (EM) images plays an essential role in many neuroscience researches. However, practical scenarios usually suffer from high annotation costs, label scarcity, and large domain diversity. While unsupervised domain adaptation (UDA) that assumes no annotation effort on the target data is promising to alleviate these challenges, its performance on complicated segmentation tasks is still far from practical usage. To address these issues, we investigate a highly annotation-efficient weak supervision, which assumes only sparse center-points on a small subset of object instances in the target training images. To achieve accurate segmentation with partial point annotations, we introduce instance counting and center detection as auxiliary tasks and design a multitask learning framework to leverage correlations among the counting, detection, and segmentation, which are all tasks with partial or no supervision. Building upon the different domain-invariances of the three tasks, we enforce counting estimation with a novel soft consistency loss as a global prior for center detection, which further guides the per-pixel segmentation. To further compensate for annotation sparsity, we develop a cross-position cut-and-paste for label augmentation and an entropy-based pseudo-label selection. The experimental results highlight that, by simply using extremely weak annotation, e.g., 15\% sparse points, for model training, the proposed model is capable of significantly outperforming UDA methods and produces comparable performance as the supervised counterpart. The high robustness of our model shown in the validations and the low requirement of expert knowledge for sparse point annotation further improve the potential application value of our model.
<div id='section'>Paperid: <span id='pid'>970, <a href='https://arxiv.org/pdf/2403.14240.pdf' target='_blank'>https://arxiv.org/pdf/2403.14240.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wang-Wang Yu, Xian-Shi Zhang, Fu-Ya Luo, Yijun Cao, Kai-Fu Yang, Hong-Mei Yan, Yong-Jie Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.14240">Weak Supervision with Arbitrary Single Frame for Micro- and Macro-expression Spotting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Frame-level micro- and macro-expression spotting methods require time-consuming frame-by-frame observation during annotation. Meanwhile, video-level spotting lacks sufficient information about the location and number of expressions during training, resulting in significantly inferior performance compared with fully-supervised spotting. To bridge this gap, we propose a point-level weakly-supervised expression spotting (PWES) framework, where each expression requires to be annotated with only one random frame (i.e., a point). To mitigate the issue of sparse label distribution, the prevailing solution is pseudo-label mining, which, however, introduces new problems: localizing contextual background snippets results in inaccurate boundaries and discarding foreground snippets leads to fragmentary predictions. Therefore, we design the strategies of multi-refined pseudo label generation (MPLG) and distribution-guided feature contrastive learning (DFCL) to address these problems. Specifically, MPLG generates more reliable pseudo labels by merging class-specific probabilities, attention scores, fused features, and point-level labels. DFCL is utilized to enhance feature similarity for the same categories and feature variability for different categories while capturing global representations across the entire datasets. Extensive experiments on the CAS(ME)^2, CAS(ME)^3, and SAMM-LV datasets demonstrate PWES achieves promising performance comparable to that of recent fully-supervised methods.
<div id='section'>Paperid: <span id='pid'>971, <a href='https://arxiv.org/pdf/2403.10298.pdf' target='_blank'>https://arxiv.org/pdf/2403.10298.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qin Xu, Sitong Li, Jiahui Wang, Bo Jiang, Jinhui Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10298">Context-Semantic Quality Awareness Network for Fine-Grained Visual Categorization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Exploring and mining subtle yet distinctive features between sub-categories with similar appearances is crucial for fine-grained visual categorization (FGVC). However, less effort has been devoted to assessing the quality of extracted visual representations. Intuitively, the network may struggle to capture discriminative features from low-quality samples, which leads to a significant decline in FGVC performance. To tackle this challenge, we propose a weakly supervised Context-Semantic Quality Awareness Network (CSQA-Net) for FGVC. In this network, to model the spatial contextual relationship between rich part descriptors and global semantics for capturing more discriminative details within the object, we design a novel multi-part and multi-scale cross-attention (MPMSCA) module. Before feeding to the MPMSCA module, the part navigator is developed to address the scale confusion problems and accurately identify the local distinctive regions. Furthermore, we propose a generic multi-level semantic quality evaluation module (MLSQE) to progressively supervise and enhance hierarchical semantics from different levels of the backbone network. Finally, context-aware features from MPMSCA and semantically enhanced features from MLSQE are fed into the corresponding quality probing classifiers to evaluate their quality in real-time, thus boosting the discriminability of feature representations. Comprehensive experiments on four popular and highly competitive FGVC datasets demonstrate the superiority of the proposed CSQA-Net in comparison with the state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>972, <a href='https://arxiv.org/pdf/2402.13079.pdf' target='_blank'>https://arxiv.org/pdf/2402.13079.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Charles Arnal, Vivien Cabannes, Vianney Perchet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.13079">Mode Estimation with Partial Feedback</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The combination of lightly supervised pre-training and online fine-tuning has played a key role in recent AI developments. These new learning pipelines call for new theoretical frameworks. In this paper, we formalize core aspects of weakly supervised and active learning with a simple problem: the estimation of the mode of a distribution using partial feedback. We show how entropy coding allows for optimal information acquisition from partial feedback, develop coarse sufficient statistics for mode identification, and adapt bandit algorithms to our new setting. Finally, we combine those contributions into a statistically and computationally efficient solution to our problem.
<div id='section'>Paperid: <span id='pid'>973, <a href='https://arxiv.org/pdf/2402.04419.pdf' target='_blank'>https://arxiv.org/pdf/2402.04419.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fakrul Islam Tushar, Vincent M. D'Anniballe, Geoffrey D. Rubin, Joseph Y. Lo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04419">What limits performance of weakly supervised deep learning for chest CT classification?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised learning with noisy data has drawn attention in the medical imaging community due to the sparsity of high-quality disease labels. However, little is known about the limitations of such weakly supervised learning and the effect of these constraints on disease classification performance. In this paper, we test the effects of such weak supervision by examining model tolerance for three conditions. First, we examined model tolerance for noisy data by incrementally increasing error in the labels within the training data. Second, we assessed the impact of dataset size by varying the amount of training data. Third, we compared performance differences between binary and multi-label classification. Results demonstrated that the model could endure up to 10% added label error before experiencing a decline in disease classification performance. Disease classification performance steadily rose as the amount of training data was increased for all disease classes, before experiencing a plateau in performance at 75% of training data. Last, the binary model outperformed the multilabel model in every disease category. However, such interpretations may be misleading, as the binary model was heavily influenced by co-occurring diseases and may not have learned the specific features of the disease in the image. In conclusion, this study may help the medical imaging community understand the benefits and risks of weak supervision with noisy labels. Such studies demonstrate the need to build diverse, large-scale datasets and to develop explainable and responsible AI.
<div id='section'>Paperid: <span id='pid'>974, <a href='https://arxiv.org/pdf/2311.07398.pdf' target='_blank'>https://arxiv.org/pdf/2311.07398.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>TomÃ¡Å¡ Kunzo, Viktor Kocur, LukÃ¡Å¡ GajdoÅ¡ech, Martin Madaras
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.07398">Processing and Segmentation of Human Teeth from 2D Images using Weakly Supervised Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Teeth segmentation is an essential task in dental image analysis for accurate diagnosis and treatment planning. While supervised deep learning methods can be utilized for teeth segmentation, they often require extensive manual annotation of segmentation masks, which is time-consuming and costly. In this research, we propose a weakly supervised approach for teeth segmentation that reduces the need for manual annotation. Our method utilizes the output heatmaps and intermediate feature maps from a keypoint detection network to guide the segmentation process. We introduce the TriDental dataset, consisting of 3000 oral cavity images annotated with teeth keypoints, to train a teeth keypoint detection network. We combine feature maps from different layers of the keypoint detection network, enabling accurate teeth segmentation without explicit segmentation annotations. The detected keypoints are also used for further refinement of the segmentation masks. Experimental results on the TriDental dataset demonstrate the superiority of our approach in terms of accuracy and robustness compared to state-of-the-art segmentation methods. Our method offers a cost-effective and efficient solution for teeth segmentation in real-world dental applications, eliminating the need for extensive manual annotation efforts.
<div id='section'>Paperid: <span id='pid'>975, <a href='https://arxiv.org/pdf/2309.08216.pdf' target='_blank'>https://arxiv.org/pdf/2309.08216.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao-Kai Chiang, Masashi Sugiyama
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.08216">Unified Risk Analysis for Weakly Supervised Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Among the flourishing research of weakly supervised learning (WSL), we recognize the lack of a unified interpretation of the mechanism behind the weakly supervised scenarios, let alone a systematic treatment of the risk rewrite problem, a crucial step in the empirical risk minimization approach. In this paper, we introduce a framework providing a comprehensive understanding and a unified methodology for WSL. The formulation component of the framework, leveraging a contamination perspective, provides a unified interpretation of how weak supervision is formed and subsumes fifteen existing WSL settings. The induced reduction graphs offer comprehensive connections over WSLs. The analysis component of the framework, viewed as a decontamination process, provides a systematic method of conducting risk rewrite. In addition to the conventional inverse matrix approach, we devise a novel strategy called marginal chain aiming to decontaminate distributions. We justify the feasibility of the proposed framework by recovering existing rewrites reported in the literature.
<div id='section'>Paperid: <span id='pid'>976, <a href='https://arxiv.org/pdf/2309.00871.pdf' target='_blank'>https://arxiv.org/pdf/2309.00871.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunyan Wang, Dong Zhang, Rui Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.00871">Boosting Weakly-Supervised Image Segmentation via Representation, Transform, and Compensator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised image segmentation (WSIS) is a critical task in computer vision that relies on image-level class labels. Multi-stage training procedures have been widely used in existing WSIS approaches to obtain high-quality pseudo-masks as ground-truth, resulting in significant progress. However, single-stage WSIS methods have recently gained attention due to their potential for simplifying training procedures, despite often suffering from low-quality pseudo-masks that limit their practical applications. To address this issue, we propose a novel single-stage WSIS method that utilizes a siamese network with contrastive learning to improve the quality of class activation maps (CAMs) and achieve a self-refinement process. Our approach employs a cross-representation refinement method that expands reliable object regions by utilizing different feature representations from the backbone. Additionally, we introduce a cross-transform regularization module that learns robust class prototypes for contrastive learning and captures global context information to feed back rough CAMs, thereby improving the quality of CAMs. Our final high-quality CAMs are used as pseudo-masks to supervise the segmentation result. Experimental results on the PASCAL VOC 2012 dataset demonstrate that our method significantly outperforms other state-of-the-art methods, achieving 67.2% and 68.76% mIoU on PASCAL VOC 2012 val set and test set, respectively. Furthermore, our method has been extended to weakly supervised object localization task, and experimental results demonstrate that our method continues to achieve very competitive results.
<div id='section'>Paperid: <span id='pid'>977, <a href='https://arxiv.org/pdf/2308.03001.pdf' target='_blank'>https://arxiv.org/pdf/2308.03001.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amirhossein Rasoulian, Arash Harirpoush, Soorena Salari, Yiming Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.03001">Weakly supervised segmentation of intracranial aneurysms using a novel 3D focal modulation UNet</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate identification and quantification of unruptured intracranial aneurysms (UIAs) is crucial for the risk assessment and treatment of this cerebrovascular disorder. Current 2D manual assessment on 3D magnetic resonance angiography (MRA) is suboptimal and time-consuming. In addition, one major issue in medical image segmentation is the need for large well-annotated data, which can be expensive to obtain. Techniques that mitigate this requirement, such as weakly supervised learning with coarse labels are highly desirable. In the paper, we propose FocalSegNet, a novel 3D focal modulation UNet, to detect an aneurysm and offer an initial, coarse segmentation of it from time-of-flight MRA image patches, which is further refined with a dense conditional random field (CRF) post-processing layer to produce a final segmentation map. We trained and evaluated our model on a public dataset, and in terms of UIA detection, our model showed a low false-positive rate of 0.21 and a high sensitivity of 0.80. For voxel-wise aneurysm segmentation, we achieved a Dice score of 0.68 and a 95% Hausdorff distance of ~0.95 mm, demonstrating its strong performance. We evaluated our algorithms against the state-of-the-art 3D Residual-UNet and Swin-UNETR, and illustrated the superior performance of our proposed FocalSegNet, highlighting the advantages of employing focal modulation for this task.
<div id='section'>Paperid: <span id='pid'>978, <a href='https://arxiv.org/pdf/2307.16634.pdf' target='_blank'>https://arxiv.org/pdf/2307.16634.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rabab Abdelfattah, Qing Guo, Xiaoguang Li, Xiaofeng Wang, Song Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.16634">CDUL: CLIP-Driven Unsupervised Learning for Multi-Label Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a CLIP-based unsupervised learning method for annotation-free multi-label image classification, including three stages: initialization, training, and inference. At the initialization stage, we take full advantage of the powerful CLIP model and propose a novel approach to extend CLIP for multi-label predictions based on global-local image-text similarity aggregation. To be more specific, we split each image into snippets and leverage CLIP to generate the similarity vector for the whole image (global) as well as each snippet (local). Then a similarity aggregator is introduced to leverage the global and local similarity vectors. Using the aggregated similarity scores as the initial pseudo labels at the training stage, we propose an optimization framework to train the parameters of the classification network and refine pseudo labels for unobserved labels. During inference, only the classification network is used to predict the labels of the input image. Extensive experiments show that our method outperforms state-of-the-art unsupervised methods on MS-COCO, PASCAL VOC 2007, PASCAL VOC 2012, and NUS datasets and even achieves comparable results to weakly supervised classification methods.
<div id='section'>Paperid: <span id='pid'>979, <a href='https://arxiv.org/pdf/2307.06720.pdf' target='_blank'>https://arxiv.org/pdf/2307.06720.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minh-Tan Pham, Hugo Gangloff, SÃ©bastien LefÃ¨vre
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.06720">Weakly supervised marine animal detection from remote sensing images using vector-quantized variational autoencoder</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper studies a reconstruction-based approach for weakly-supervised animal detection from aerial images in marine environments. Such an approach leverages an anomaly detection framework that computes metrics directly on the input space, enhancing interpretability and anomaly localization compared to feature embedding methods. Building upon the success of Vector-Quantized Variational Autoencoders in anomaly detection on computer vision datasets, we adapt them to the marine animal detection domain and address the challenge of handling noisy data. To evaluate our approach, we compare it with existing methods in the context of marine animal detection from aerial image data. Experiments conducted on two dedicated datasets demonstrate the superior performance of the proposed method over recent studies in the literature. Our framework offers improved interpretability and localization of anomalies, providing valuable insights for monitoring marine ecosystems and mitigating the impact of human activities on marine animals.
<div id='section'>Paperid: <span id='pid'>980, <a href='https://arxiv.org/pdf/2307.04472.pdf' target='_blank'>https://arxiv.org/pdf/2307.04472.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Zhang, Xiaolei Zhang, Yaolei Qi, Guanyu Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.04472">Partial Vessels Annotation-based Coronary Artery Segmentation with Self-training and Prototype Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Coronary artery segmentation on coronary-computed tomography angiography (CCTA) images is crucial for clinical use. Due to the expertise-required and labor-intensive annotation process, there is a growing demand for the relevant label-efficient learning algorithms. To this end, we propose partial vessels annotation (PVA) based on the challenges of coronary artery segmentation and clinical diagnostic characteristics. Further, we propose a progressive weakly supervised learning framework to achieve accurate segmentation under PVA. First, our proposed framework learns the local features of vessels to propagate the knowledge to unlabeled regions. Subsequently, it learns the global structure by utilizing the propagated knowledge, and corrects the errors introduced in the propagation process. Finally, it leverages the similarity between feature embeddings and the feature prototype to enhance testing outputs. Experiments on clinical data reveals that our proposed framework outperforms the competing methods under PVA (24.29% vessels), and achieves comparable performance in trunk continuity with the baseline model using full annotation (100% vessels).
<div id='section'>Paperid: <span id='pid'>981, <a href='https://arxiv.org/pdf/2306.17290.pdf' target='_blank'>https://arxiv.org/pdf/2306.17290.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hantian Ding, Jinrui Yang, Yuqian Deng, Hongming Zhang, Dan Roth
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.17290">Towards Open-Domain Topic Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce an open-domain topic classification system that accepts user-defined taxonomy in real time. Users will be able to classify a text snippet with respect to any candidate labels they want, and get instant response from our web interface. To obtain such flexibility, we build the backend model in a zero-shot way. By training on a new dataset constructed from Wikipedia, our label-aware text classifier can effectively utilize implicit knowledge in the pretrained language model to handle labels it has never seen before. We evaluate our model across four datasets from various domains with different label sets. Experiments show that the model significantly improves over existing zero-shot baselines in open-domain scenarios, and performs competitively with weakly-supervised models trained on in-domain data.
<div id='section'>Paperid: <span id='pid'>982, <a href='https://arxiv.org/pdf/2306.01610.pdf' target='_blank'>https://arxiv.org/pdf/2306.01610.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ameen Ali, Tomer Galanti, Lior Wolf
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.01610">Centered Self-Attention Layers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The self-attention mechanism in transformers and the message-passing mechanism in graph neural networks are repeatedly applied within deep learning architectures. We show that this application inevitably leads to oversmoothing, i.e., to similar representations at the deeper layers for different tokens in transformers and different nodes in graph neural networks. Based on our analysis, we present a correction term to the aggregating operator of these mechanisms. Empirically, this simple term eliminates much of the oversmoothing problem in visual transformers, obtaining performance in weakly supervised segmentation that surpasses elaborate baseline methods that introduce multiple auxiliary networks and training phrases. In graph neural networks, the correction term enables the training of very deep architectures more effectively than many recent solutions to the same problem.
<div id='section'>Paperid: <span id='pid'>983, <a href='https://arxiv.org/pdf/2305.14281.pdf' target='_blank'>https://arxiv.org/pdf/2305.14281.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Emanuele Bugliarello, Aida Nematzadeh, Lisa Anne Hendricks
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.14281">Weakly-Supervised Learning of Visual Relations in Multimodal Pretraining</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent work in vision-and-language pretraining has investigated supervised signals from object detection data to learn better, fine-grained multimodal representations. In this work, we take a step further and explore how we can tap into supervision from small-scale visual relation data. In particular, we propose two pretraining approaches to contextualise visual entities in a multimodal setup. With verbalised scene graphs, we transform visual relation triplets into structured captions, and treat them as additional image descriptions. With masked relation prediction, we further encourage relating entities from image regions with visually masked contexts. When applied to strong baselines pretrained on large amounts of Web data, zero-shot evaluations on both coarse-grained and fine-grained tasks show the efficacy of our methods in learning multimodal representations from weakly-supervised relations data.
<div id='section'>Paperid: <span id='pid'>984, <a href='https://arxiv.org/pdf/2305.06973.pdf' target='_blank'>https://arxiv.org/pdf/2305.06973.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhikai Zhang, Jian Ding, Li Jiang, Dengxin Dai, Gui-Song Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.06973">FreePoint: Unsupervised Point Cloud Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Instance segmentation of point clouds is a crucial task in 3D field with numerous applications that involve localizing and segmenting objects in a scene. However, achieving satisfactory results requires a large number of manual annotations, which is a time-consuming and expensive process. To alleviate dependency on annotations, we propose a novel framework, FreePoint, for underexplored unsupervised class-agnostic instance segmentation on point clouds. In detail, we represent the point features by combining coordinates, colors, and self-supervised deep features. Based on the point features, we perform a bottom-up multicut algorithm to segment point clouds into coarse instance masks as pseudo labels, which are used to train a point cloud instance segmentation model. We propose an id-as-feature strategy at this stage to alleviate the randomness of the multicut algorithm and improve the pseudo labels' quality. During training, we propose a weakly-supervised two-step training strategy and corresponding losses to overcome the inaccuracy of coarse masks. FreePoint has achieved breakthroughs in unsupervised class-agnostic instance segmentation on point clouds and outperformed previous traditional methods by over 18.2% and a competitive concurrent work UnScene3D by 5.5% in AP. Additionally, when used as a pretext task and fine-tuned on S3DIS, FreePoint performs significantly better than existing self-supervised pre-training methods with limited annotations and surpasses CSC by 6.0% in AP with 10% annotation masks.
<div id='section'>Paperid: <span id='pid'>985, <a href='https://arxiv.org/pdf/2305.02734.pdf' target='_blank'>https://arxiv.org/pdf/2305.02734.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wang-Wang Yu, Kai-Fu Yang, Hong-Mei Yan, Yong-Jie Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.02734">Weakly-supervised Micro- and Macro-expression Spotting Based on Multi-level Consistency</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most micro- and macro-expression spotting methods in untrimmed videos suffer from the burden of video-wise collection and frame-wise annotation. Weakly-supervised expression spotting (WES) based on video-level labels can potentially mitigate the complexity of frame-level annotation while achieving fine-grained frame-level spotting. However, we argue that existing weakly-supervised methods are based on multiple instance learning (MIL) involving inter-modality, inter-sample, and inter-task gaps. The inter-sample gap is primarily from the sample distribution and duration. Therefore, we propose a novel and simple WES framework, MC-WES, using multi-consistency collaborative mechanisms that include modal-level saliency, video-level distribution, label-level duration and segment-level feature consistency strategies to implement fine frame-level spotting with only video-level labels to alleviate the above gaps and merge prior knowledge. The modal-level saliency consistency strategy focuses on capturing key correlations between raw images and optical flow. The video-level distribution consistency strategy utilizes the difference of sparsity in temporal distribution. The label-level duration consistency strategy exploits the difference in the duration of facial muscles. The segment-level feature consistency strategy emphasizes that features under the same labels maintain similarity. Experimental results on three challenging datasets -- CAS(ME)$^2$, CAS(ME)$^3$, and SAMM-LV -- demonstrate that MC-WES is comparable to state-of-the-art fully-supervised methods.
<div id='section'>Paperid: <span id='pid'>986, <a href='https://arxiv.org/pdf/2304.02080.pdf' target='_blank'>https://arxiv.org/pdf/2304.02080.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vladislav Lialin, Stephen Rawls, David Chan, Shalini Ghosh, Anna Rumshisky, Wael Hamza
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.02080">Scalable and Accurate Self-supervised Multimodal Representation Learning without Aligned Video and Text Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scaling up weakly-supervised datasets has shown to be highly effective in the image-text domain and has contributed to most of the recent state-of-the-art computer vision and multimodal neural networks. However, existing large-scale video-text datasets and mining techniques suffer from several limitations, such as the scarcity of aligned data, the lack of diversity in the data, and the difficulty of collecting aligned data. Currently popular video-text data mining approach via automatic speech recognition (ASR) used in HowTo100M provides low-quality captions that often do not refer to the video content. Other mining approaches do not provide proper language descriptions (video tags) and are biased toward short clips (alt text). In this work, we show how recent advances in image captioning allow us to pre-train high-quality video models without any parallel video-text data. We pre-train several video captioning models that are based on an OPT language model and a TimeSformer visual backbone. We fine-tune these networks on several video captioning datasets. First, we demonstrate that image captioning pseudolabels work better for pre-training than the existing HowTo100M ASR captions. Second, we show that pre-training on both images and videos produces a significantly better network (+4 CIDER on MSR-VTT) than pre-training on a single modality. Our methods are complementary to the existing pre-training or data mining approaches and can be used in a variety of settings. Given the efficacy of the pseudolabeling method, we are planning to publicly release the generated captions.
<div id='section'>Paperid: <span id='pid'>987, <a href='https://arxiv.org/pdf/2303.14999.pdf' target='_blank'>https://arxiv.org/pdf/2303.14999.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaofei Wang, Weijia Zhang, Min-Ling Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.14999">Transformer-based Multi-Instance Learning for Weakly Supervised Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Object Detection (WSOD) enables the training of object detection models using only image-level annotations. State-of-the-art WSOD detectors commonly rely on multi-instance learning (MIL) as the backbone of their detectors and assume that the bounding box proposals of an image are independent of each other. However, since such approaches only utilize the highest score proposal and discard the potentially useful information from other proposals, their independent MIL backbone often limits models to salient parts of an object or causes them to detect only one object per class. To solve the above problems, we propose a novel backbone for WSOD based on our tailored Vision Transformer named Weakly Supervised Transformer Detection Network (WSTDN). Our algorithm is not only the first to demonstrate that self-attention modules that consider inter-instance relationships are effective backbones for WSOD, but also we introduce a novel bounding box mining method (BBM) integrated with a memory transfer refinement (MTR) procedure to utilize the instance dependencies for facilitating instance refinements. Experimental results on PASCAL VOC2007 and VOC2012 benchmarks demonstrate the effectiveness of our proposed WSTDN and modified instance refinement modules.
<div id='section'>Paperid: <span id='pid'>988, <a href='https://arxiv.org/pdf/2303.04737.pdf' target='_blank'>https://arxiv.org/pdf/2303.04737.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqun Yang, Xu Tang, Xiangrong Zhang, Jingjing Ma, Licheng Jiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.04737">SoftMatch Distance: A Novel Distance for Weakly-Supervised Trend Change Detection in Bi-Temporal Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>General change detection (GCD) and semantic change detection (SCD) are common methods for identifying changes and distinguishing object categories involved in those changes, respectively. However, the binary changes provided by GCD is often not practical enough, while annotating semantic labels for training SCD models is very expensive. Therefore, there is a novel solution that intuitively dividing changes into three trends (``appear'', ``disappear'' and ``transform'') instead of semantic categories, named it trend change detection (TCD) in this paper. It offers more detailed change information than GCD, while requiring less manual annotation cost than SCD. However, there are limited public data sets with specific trend labels to support TCD application. To address this issue, we propose a softmatch distance which is used to construct a weakly-supervised TCD branch in a simple GCD model, using GCD labels instead of TCD label for training. Furthermore, a strategic approach is presented to successfully explore and extract background information, which is crucial for the weakly-supervised TCD task. The experiment results on four public data sets are highly encouraging, which demonstrates the effectiveness of our proposed model.
<div id='section'>Paperid: <span id='pid'>989, <a href='https://arxiv.org/pdf/2303.03670.pdf' target='_blank'>https://arxiv.org/pdf/2303.03670.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Boxiao Yu, Reagan Tibbetts, Titon Barua, Ailani Morales, Ioannis Rekleitis, Md Jahidul Islam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.03670">Weakly Supervised Caveline Detection For AUV Navigation Inside Underwater Caves</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Underwater caves are challenging environments that are crucial for water resource management, and for our understanding of hydro-geology and history. Mapping underwater caves is a time-consuming, labor-intensive, and hazardous operation. For autonomous cave mapping by underwater robots, the major challenge lies in vision-based estimation in the complete absence of ambient light, which results in constantly moving shadows due to the motion of the camera-light setup. Thus, detecting and following the caveline as navigation guidance is paramount for robots in autonomous cave mapping missions. In this paper, we present a computationally light caveline detection model based on a novel Vision Transformer (ViT)-based learning pipeline. We address the problem of scarce annotated training data by a weakly supervised formulation where the learning is reinforced through a series of noisy predictions from intermediate sub-optimal models. We validate the utility and effectiveness of such weak supervision for caveline detection and tracking in three different cave locations: USA, Mexico, and Spain. Experimental results demonstrate that our proposed model, CL-ViT, balances the robustness-efficiency trade-off, ensuring good generalization performance while offering 10+ FPS on single-board (Jetson TX2) devices.
<div id='section'>Paperid: <span id='pid'>990, <a href='https://arxiv.org/pdf/2302.08867.pdf' target='_blank'>https://arxiv.org/pdf/2302.08867.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jack Breen, Katie Allen, Kieran Zucker, Geoff Hall, Nicolas M. Orsi, Nishant Ravikumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.08867">Efficient subtyping of ovarian cancer histopathology whole slide images using active sampling in multiple instance learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised classification of histopathology slides is a computationally intensive task, with a typical whole slide image (WSI) containing billions of pixels to process. We propose Discriminative Region Active Sampling for Multiple Instance Learning (DRAS-MIL), a computationally efficient slide classification method using attention scores to focus sampling on highly discriminative regions. We apply this to the diagnosis of ovarian cancer histological subtypes, which is an essential part of the patient care pathway as different subtypes have different genetic and molecular profiles, treatment options, and patient outcomes. We use a dataset of 714 WSIs acquired from 147 epithelial ovarian cancer patients at Leeds Teaching Hospitals NHS Trust to distinguish the most common subtype, high-grade serous carcinoma, from the other four subtypes (low-grade serous, endometrioid, clear cell, and mucinous carcinomas) combined. We demonstrate that DRAS-MIL can achieve similar classification performance to exhaustive slide analysis, with a 3-fold cross-validated AUC of 0.8679 compared to 0.8781 with standard attention-based MIL classification. Our approach uses at most 18% as much memory as the standard approach, while taking 33% of the time when evaluating on a GPU and only 14% on a CPU alone. Reducing prediction time and memory requirements may benefit clinical deployment and the democratisation of AI, reducing the extent to which computational hardware limits end-user adoption.
<div id='section'>Paperid: <span id='pid'>991, <a href='https://arxiv.org/pdf/2212.04575.pdf' target='_blank'>https://arxiv.org/pdf/2212.04575.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangyu Xu, Li Guan, Enrique Dunn, Haoxiang Li, Gang Hua
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.04575">DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose an end-to-end framework that jointly learns keypoint detection, descriptor representation and cross-frame matching for the task of image-based 3D localization. Prior art has tackled each of these components individually, purportedly aiming to alleviate difficulties in effectively train a holistic network. We design a self-supervised image warping correspondence loss for both feature detection and matching, a weakly-supervised epipolar constraints loss on relative camera pose learning, and a directional matching scheme that detects key-point features in a source image and performs coarse-to-fine correspondence search on the target image. We leverage this framework to enforce cycle consistency in our matching module. In addition, we propose a new loss to robustly handle both definite inlier/outlier matches and less-certain matches. The integration of these learning mechanisms enables end-to-end training of a single network performing all three localization components. Bench-marking our approach on public data-sets, exemplifies how such an end-to-end framework is able to yield more accurate localization that out-performs both traditional methods as well as state-of-the-art weakly supervised methods.
<div id='section'>Paperid: <span id='pid'>992, <a href='https://arxiv.org/pdf/2211.05269.pdf' target='_blank'>https://arxiv.org/pdf/2211.05269.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jay J. Yoo, Khashayar Namdar, Matthias W. Wagner, Liana Nobre, Uri Tabori, Cynthia Hawkins, Birgit B. Ertl-Wagner, Farzad Khalvati
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.05269">Generative Adversarial Networks for Weakly Supervised Generation and Evaluation of Brain Tumor Segmentations on MR Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Segmentation of regions of interest (ROIs) for identifying abnormalities is a leading problem in medical imaging. Using machine learning for this problem generally requires manually annotated ground-truth segmentations, demanding extensive time and resources from radiologists. This work presents a weakly supervised approach that utilizes binary image-level labels, which are much simpler to acquire, to effectively segment anomalies in 2D magnetic resonance images without ground truth annotations. We train a generative adversarial network (GAN) that converts cancerous images to healthy variants, which are used along with localization seeds as priors to generate improved weakly supervised segmentations. The non-cancerous variants can also be used to evaluate the segmentations in a weakly supervised fashion, which allows for the most effective segmentations to be identified and then applied to downstream clinical classification tasks. On the Multimodal Brain Tumor Segmentation (BraTS) 2020 dataset, our proposed method generates and identifies segmentations that achieve test Dice coefficients of 83.91%. Using these segmentations for pathology classification results with a test AUC of 93.32% which is comparable to the test AUC of 95.80% achieved when using true segmentations.
<div id='section'>Paperid: <span id='pid'>993, <a href='https://arxiv.org/pdf/2209.09930.pdf' target='_blank'>https://arxiv.org/pdf/2209.09930.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jay J. Yoo, Khashayar Namdar, Farzad Khalvati
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.09930">Deep Superpixel Generation and Clustering for Weakly Supervised Segmentation of Brain Tumors in MR Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training machine learning models to segment tumors and other anomalies in medical images is an important step for developing diagnostic tools but generally requires manually annotated ground truth segmentations, which necessitates significant time and resources. This work proposes the use of a superpixel generation model and a superpixel clustering model to enable weakly supervised brain tumor segmentations. The proposed method utilizes binary image-level classification labels, which are readily accessible, to significantly improve the initial region of interest segmentations generated by standard weakly supervised methods without requiring ground truth annotations. We used 2D slices of magnetic resonance brain scans from the Multimodal Brain Tumor Segmentation Challenge 2020 dataset and labels indicating the presence of tumors to train the pipeline. On the test cohort, our method achieved a mean Dice coefficient of 0.691 and a mean 95% Hausdorff distance of 18.1, outperforming existing superpixel-based weakly supervised segmentation methods.
<div id='section'>Paperid: <span id='pid'>994, <a href='https://arxiv.org/pdf/2204.10068.pdf' target='_blank'>https://arxiv.org/pdf/2204.10068.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanchun Wang, Xiangrong Zhang, Zelin Peng, Xu Tang, Huiyu Zhou, Licheng Jiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2204.10068">Absolute Wrong Makes Better: Boosting Weakly Supervised Object Detection via Negative Deterministic Information</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised object detection (WSOD) is a challenging task, in which image-level labels (e.g., categories of the instances in the whole image) are used to train an object detector. Many existing methods follow the standard multiple instance learning (MIL) paradigm and have achieved promising performance. However, the lack of deterministic information leads to part domination and missing instances. To address these issues, this paper focuses on identifying and fully exploiting the deterministic information in WSOD. We discover that negative instances (i.e. absolutely wrong instances), ignored in most of the previous studies, normally contain valuable deterministic information. Based on this observation, we here propose a negative deterministic information (NDI) based method for improving WSOD, namely NDI-WSOD. Specifically, our method consists of two stages: NDI collecting and exploiting. In the collecting stage, we design several processes to identify and distill the NDI from negative instances online. In the exploiting stage, we utilize the extracted NDI to construct a novel negative contrastive learning mechanism and a negative guided instance selection strategy for dealing with the issues of part domination and missing instances, respectively. Experimental results on several public benchmarks including VOC 2007, VOC 2012 and MS COCO show that our method achieves satisfactory performance.
<div id='section'>Paperid: <span id='pid'>995, <a href='https://arxiv.org/pdf/2109.08927.pdf' target='_blank'>https://arxiv.org/pdf/2109.08927.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijun Wu, Zi Xuan Zhang, Atharva Naik, Zhijian Mei, Mauajama Firdaus, Lili Mou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2109.08927">Weakly Supervised Explainable Phrasal Reasoning with Neural Fuzzy Logic</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Natural language inference (NLI) aims to determine the logical relationship between two sentences, such as Entailment, Contradiction, and Neutral. In recent years, deep learning models have become a prevailing approach to NLI, but they lack interpretability and explainability. In this work, we address the explainability of NLI by weakly supervised logical reasoning, and propose an Explainable Phrasal Reasoning (EPR) approach. Our model first detects phrases as the semantic unit and aligns corresponding phrases in the two sentences. Then, the model predicts the NLI label for the aligned phrases, and induces the sentence label by fuzzy logic formulas. Our EPR is almost everywhere differentiable and thus the system can be trained end to end. In this way, we are able to provide explicit explanations of phrasal logical relationships in a weakly supervised manner. We further show that such reasoning results help textual explanation generation.
<div id='section'>Paperid: <span id='pid'>996, <a href='https://arxiv.org/pdf/2510.15666.pdf' target='_blank'>https://arxiv.org/pdf/2510.15666.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Shi, Gang Li, Junxing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.15666">Uncertainty-Aware Extreme Point Tracing for Weakly Supervised Ultrasound Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic medical image segmentation is a fundamental step in computer-aided diagnosis, yet fully supervised approaches demand extensive pixel-level annotations that are costly and time-consuming. To alleviate this burden, we propose a weakly supervised segmentation framework that leverages only four extreme points as annotation. Specifically, bounding boxes derived from the extreme points are used as prompts for the Segment Anything Model 2 (SAM2) to generate reliable initial pseudo labels. These pseudo labels are progressively refined by an enhanced Feature-Guided Extreme Point Masking (FGEPM) algorithm, which incorporates Monte Carlo dropout-based uncertainty estimation to construct a unified gradient uncertainty cost map for boundary tracing. Furthermore, a dual-branch Uncertainty-aware Scale Consistency (USC) loss and a box alignment loss are introduced to ensure spatial consistency and precise boundary alignment during training. Extensive experiments on two public ultrasound datasets, BUSI and UNS, demonstrate that our method achieves performance comparable to, and even surpassing fully supervised counterparts while significantly reducing annotation cost. These results validate the effectiveness and practicality of the proposed weakly supervised framework for ultrasound image segmentation.
<div id='section'>Paperid: <span id='pid'>997, <a href='https://arxiv.org/pdf/2509.23376.pdf' target='_blank'>https://arxiv.org/pdf/2509.23376.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinghong Zheng, Changlong Jiang, Jiaqi Li, Haohong Kuang, Hang Xu, Tingbing Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23376">UniPose: Unified Cross-modality Pose Prior Propagation towards RGB-D data for Weakly Supervised 3D Human Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present UniPose, a unified cross-modality pose prior propagation method for weakly supervised 3D human pose estimation (HPE) using unannotated single-view RGB-D sequences (RGB, depth, and point cloud data). UniPose transfers 2D HPE annotations from large-scale RGB datasets (e.g., MS COCO) to the 3D domain via self-supervised learning on easily acquired RGB-D sequences, eliminating the need for labor-intensive 3D keypoint annotations. This approach bridges the gap between 2D and 3D domains without suffering from issues related to multi-view camera calibration or synthetic-to-real data shifts. During training, UniPose leverages off-the-shelf 2D pose estimations as weak supervision for point cloud networks, incorporating spatial-temporal constraints like body symmetry and joint motion. The 2D-to-3D back-projection loss and cross-modality interaction further enhance this process. By treating the point cloud network's 3D HPE results as pseudo ground truth, our anchor-to-joint prediction method performs 3D lifting on RGB and depth networks, making it more robust against inaccuracies in 2D HPE results compared to state-of-the-art methods. Experiments on CMU Panoptic and ITOP datasets show that UniPose achieves comparable performance to fully supervised methods. Incorporating large-scale unlabeled data (e.g., NTU RGB+D 60) enhances its performance under challenging conditions, demonstrating its potential for practical applications. Our proposed 3D lifting method also achieves state-of-the-art results.
<div id='section'>Paperid: <span id='pid'>998, <a href='https://arxiv.org/pdf/2509.19028.pdf' target='_blank'>https://arxiv.org/pdf/2509.19028.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ioannis Sarafis, Alexandros Papadopoulos, Anastasios Delopoulos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19028">Weakly Supervised Food Image Segmentation using Vision Transformers and Segment Anything Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a weakly supervised semantic segmentation approach for food images which takes advantage of the zero-shot capabilities and promptability of the Segment Anything Model (SAM) along with the attention mechanisms of Vision Transformers (ViTs). Specifically, we use class activation maps (CAMs) from ViTs to generate prompts for SAM, resulting in masks suitable for food image segmentation. The ViT model, a Swin Transformer, is trained exclusively using image-level annotations, eliminating the need for pixel-level annotations during training. Additionally, to enhance the quality of the SAM-generated masks, we examine the use of image preprocessing techniques in combination with single-mask and multi-mask SAM generation strategies. The methodology is evaluated on the FoodSeg103 dataset, generating an average of 2.4 masks per image (excluding background), and achieving an mIoU of 0.54 for the multi-mask scenario. We envision the proposed approach as a tool to accelerate food image annotation tasks or as an integrated component in food and nutrition tracking applications.
<div id='section'>Paperid: <span id='pid'>999, <a href='https://arxiv.org/pdf/2509.17971.pdf' target='_blank'>https://arxiv.org/pdf/2509.17971.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tan-Ha Mai, Hsuan-Tien Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17971">Intra-Cluster Mixup: An Effective Data Augmentation Technique for Complementary-Label Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we investigate the challenges of complementary-label learning (CLL), a specialized form of weakly-supervised learning (WSL) where models are trained with labels indicating classes to which instances do not belong, rather than standard ordinary labels. This alternative supervision is appealing because collecting complementary labels is generally cheaper and less labor-intensive. Although most existing research in CLL emphasizes the development of novel loss functions, the potential of data augmentation in this domain remains largely underexplored. In this work, we uncover that the widely-used Mixup data augmentation technique is ineffective when directly applied to CLL. Through in-depth analysis, we identify that the complementary-label noise generated by Mixup negatively impacts the performance of CLL models. We then propose an improved technique called Intra-Cluster Mixup (ICM), which only synthesizes augmented data from nearby examples, to mitigate the noise effect. ICM carries the benefits of encouraging complementary label sharing of nearby examples, and leads to substantial performance improvements across synthetic and real-world labeled datasets. In particular, our wide spectrum of experimental results on both balanced and imbalanced CLL settings justifies the potential of ICM in allying with state-of-the-art CLL algorithms, achieving significant accuracy increases of 30% and 10% on MNIST and CIFAR datasets, respectively.
<div id='section'>Paperid: <span id='pid'>1000, <a href='https://arxiv.org/pdf/2508.18790.pdf' target='_blank'>https://arxiv.org/pdf/2508.18790.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhui Tao, Yizhe Zhang, Qiang Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18790">A Closer Look at Edema Area Segmentation in SD-OCT Images Using Adversarial Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of artificial intelligence models for macular edema (ME) analy-sis always relies on expert-annotated pixel-level image datasets which are expen-sive to collect prospectively. While anomaly-detection-based weakly-supervised methods have shown promise in edema area (EA) segmentation task, their per-formance still lags behind fully-supervised approaches. In this paper, we leverage the strong correlation between EA and retinal layers in spectral-domain optical coherence tomography (SD-OCT) images, along with the update characteristics of weakly-supervised learning, to enhance an off-the-shelf adversarial framework for EA segmentation with a novel layer-structure-guided post-processing step and a test-time-adaptation (TTA) strategy. By incorporating additional retinal lay-er information, our framework reframes the dense EA prediction task as one of confirming intersection points between the EA contour and retinal layers, result-ing in predictions that better align with the shape prior of EA. Besides, the TTA framework further helps address discrepancies in the manifestations and presen-tations of EA between training and test sets. Extensive experiments on two pub-licly available datasets demonstrate that these two proposed ingredients can im-prove the accuracy and robustness of EA segmentation, bridging the gap between weakly-supervised and fully-supervised models.
<div id='section'>Paperid: <span id='pid'>1001, <a href='https://arxiv.org/pdf/2507.14237.pdf' target='_blank'>https://arxiv.org/pdf/2507.14237.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Louis Bahrman, Mathieu Fontaine, GaÃ«l Richard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14237">U-DREAM: Unsupervised Dereverberation guided by a Reverberation Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper explores the outcome of training state-ofthe-art dereverberation models with supervision settings ranging from weakly-supervised to fully unsupervised, relying solely on reverberant signals and an acoustic model for training. Most of the existing deep learning approaches typically require paired dry and reverberant data, which are difficult to obtain in practice. We develop instead a sequential learning strategy motivated by a bayesian formulation of the dereverberation problem, wherein acoustic parameters and dry signals are estimated from reverberant inputs using deep neural networks, guided by a reverberation matching loss. Our most data-efficient variant requires only 100 reverberation-parameter-labelled samples to outperform an unsupervised baseline, demonstrating the effectiveness and practicality of the proposed method in low-resource scenarios.
<div id='section'>Paperid: <span id='pid'>1002, <a href='https://arxiv.org/pdf/2507.02454.pdf' target='_blank'>https://arxiv.org/pdf/2507.02454.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiwei Duan, Luping Ji, Shengjia Chen, Sicheng Zhu, Jianghong Huang, Mao Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02454">Weakly-supervised Contrastive Learning with Quantity Prompts for Moving Infrared Small Target Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Different from general object detection, moving infrared small target detection faces huge challenges due to tiny target size and weak background contrast.Currently, most existing methods are fully-supervised, heavily relying on a large number of manual target-wise annotations. However, manually annotating video sequences is often expensive and time-consuming, especially for low-quality infrared frame images. Inspired by general object detection, non-fully supervised strategies ($e.g.$, weakly supervised) are believed to be potential in reducing annotation requirements. To break through traditional fully-supervised frameworks, as the first exploration work, this paper proposes a new weakly-supervised contrastive learning (WeCoL) scheme, only requires simple target quantity prompts during model training.Specifically, in our scheme, based on the pretrained segment anything model (SAM), a potential target mining strategy is designed to integrate target activation maps and multi-frame energy accumulation.Besides, contrastive learning is adopted to further improve the reliability of pseudo-labels, by calculating the similarity between positive and negative samples in feature subspace.Moreover, we propose a long-short term motion-aware learning scheme to simultaneously model the local motion patterns and global motion trajectory of small targets.The extensive experiments on two public datasets (DAUB and ITSDT-15K) verify that our weakly-supervised scheme could often outperform early fully-supervised methods. Even, its performance could reach over 90\% of state-of-the-art (SOTA) fully-supervised ones.
<div id='section'>Paperid: <span id='pid'>1003, <a href='https://arxiv.org/pdf/2507.02393.pdf' target='_blank'>https://arxiv.org/pdf/2507.02393.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seokyeong Lee, Sithu Aung, Junyong Choi, Seungryong Kim, Ig-Jae Kim, Junghyun Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02393">PLOT: Pseudo-Labeling via Video Object Tracking for Scalable Monocular 3D Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monocular 3D object detection (M3OD) has long faced challenges due to data scarcity caused by high annotation costs and inherent 2D-to-3D ambiguity. Although various weakly supervised methods and pseudo-labeling methods have been proposed to address these issues, they are mostly limited by domain-specific learning or rely solely on shape information from a single observation. In this paper, we propose a novel pseudo-labeling framework that uses only video data and is more robust to occlusion, without requiring a multi-view setup, additional sensors, camera poses, or domain-specific training. Specifically, we explore a technique for aggregating the pseudo-LiDARs of both static and dynamic objects across temporally adjacent frames using object point tracking, enabling 3D attribute extraction in scenarios where 3D data acquisition is infeasible. Extensive experiments demonstrate that our method ensures reliable accuracy and strong scalability, making it a practical and effective solution for M3OD.
<div id='section'>Paperid: <span id='pid'>1004, <a href='https://arxiv.org/pdf/2506.21883.pdf' target='_blank'>https://arxiv.org/pdf/2506.21883.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Basudha Pal, Sharif Amit Kamran, Brendon Lutnick, Molly Lucas, Chaitanya Parmar, Asha Patel Shah, David Apfel, Steven Fakharzadeh, Lloyd Miller, Gabriela Cula, Kristopher Standish
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21883">GRASP-PsONet: Gradient-based Removal of Spurious Patterns for PsOriasis Severity Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Psoriasis (PsO) severity scoring is important for clinical trials but is hindered by inter-rater variability and the burden of in person clinical evaluation. Remote imaging using patient captured mobile photos offers scalability but introduces challenges, such as variation in lighting, background, and device quality that are often imperceptible to humans but can impact model performance. These factors, along with inconsistencies in dermatologist annotations, reduce the reliability of automated severity scoring. We propose a framework to automatically flag problematic training images that introduce spurious correlations which degrade model generalization, using a gradient based interpretability approach. By tracing the gradients of misclassified validation images, we detect training samples where model errors align with inconsistently rated examples or are affected by subtle, nonclinical artifacts. We apply this method to a ConvNeXT based weakly supervised model designed to classify PsO severity from phone images. Removing 8.2% of flagged images improves model AUC-ROC by 5% (85% to 90%) on a held out test set. Commonly, multiple annotators and an adjudication process ensure annotation accuracy, which is expensive and time consuming. Our method detects training images with annotation inconsistencies, potentially removing the need for manual review. When applied to a subset of training data rated by two dermatologists, the method identifies over 90% of cases with inter-rater disagreement by reviewing only the top 30% of samples. This improves automated scoring for remote assessments, ensuring robustness despite data collection variability.
<div id='section'>Paperid: <span id='pid'>1005, <a href='https://arxiv.org/pdf/2506.07652.pdf' target='_blank'>https://arxiv.org/pdf/2506.07652.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hangbei Cheng, Xiaorong Dong, Xueyu Liu, Jianan Zhang, Xuetao Ma, Mingqiang Wei, Liansheng Wang, Junxin Chen, Yongfei Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07652">FMaMIL: Frequency-Driven Mamba Multi-Instance Learning for Weakly Supervised Lesion Segmentation in Medical Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate lesion segmentation in histopathology images is essential for diagnostic interpretation and quantitative analysis, yet it remains challenging due to the limited availability of costly pixel-level annotations. To address this, we propose FMaMIL, a novel two-stage framework for weakly supervised lesion segmentation based solely on image-level labels. In the first stage, a lightweight Mamba-based encoder is introduced to capture long-range dependencies across image patches under the MIL paradigm. To enhance spatial sensitivity and structural awareness, we design a learnable frequency-domain encoding module that supplements spatial-domain features with spectrum-based information. CAMs generated in this stage are used to guide segmentation training. In the second stage, we refine the initial pseudo labels via a CAM-guided soft-label supervision and a self-correction mechanism, enabling robust training even under label noise. Extensive experiments on both public and private histopathology datasets demonstrate that FMaMIL outperforms state-of-the-art weakly supervised methods without relying on pixel-level annotations, validating its effectiveness and potential for digital pathology applications.
<div id='section'>Paperid: <span id='pid'>1006, <a href='https://arxiv.org/pdf/2506.03229.pdf' target='_blank'>https://arxiv.org/pdf/2506.03229.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qian-Wei Wang, Yuqiu Xie, Letian Zhang, Zimo Liu, Shu-Tao Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03229">Pre-trained Vision-Language Models Assisted Noisy Partial Label Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the context of noisy partial label learning (NPLL), each training sample is associated with a set of candidate labels annotated by multiple noisy annotators. With the emergence of high-performance pre-trained vision-language models (VLMs) such as CLIP, LLaVa and GPT-4V, the direction of using these models to replace time-consuming manual annotation workflows and achieve "manual-annotation-free" training for downstream tasks has become a highly promising research avenue. This paper focuses on learning from noisy partial labels annotated by pre-trained VLMs and proposes an innovative collaborative consistency regularization (Co-Reg) method. Unlike the symmetric noise primarily addressed in traditional noisy label learning, the noise generated by pre-trained models is instance-dependent, embodying the underlying patterns of the pre-trained models themselves, which significantly increases the learning difficulty for the model. To address this, we simultaneously train two neural networks that implement collaborative purification of training labels through a "Co-Pseudo-Labeling" mechanism, while enforcing consistency regularization constraints in both the label space and feature representation space. Our method can also leverage few-shot manually annotated valid labels to further enhance its performances. Comparative experiments with different denoising and disambiguation algorithms, annotation manners, and pre-trained model application schemes fully validate the effectiveness of the proposed method, while revealing the broad prospects of integrating weakly-supervised learning techniques into the knowledge distillation process of pre-trained models.
<div id='section'>Paperid: <span id='pid'>1007, <a href='https://arxiv.org/pdf/2505.23341.pdf' target='_blank'>https://arxiv.org/pdf/2505.23341.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daoxi Cao, Hangbei Cheng, Yijin Li, Ruolin Zhou, Xuehan Zhang, Xinyi Li, Binwei Li, Xuancheng Gu, Jianan Zhang, Xueyu Liu, Yongfei Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23341">DSAGL: Dual-Stream Attention-Guided Learning for Weakly Supervised Whole Slide Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Whole-slide images (WSIs) are critical for cancer diagnosis due to their ultra-high resolution and rich semantic content. However, their massive size and the limited availability of fine-grained annotations pose substantial challenges for conventional supervised learning. We propose DSAGL (Dual-Stream Attention-Guided Learning), a novel weakly supervised classification framework that combines a teacher-student architecture with a dual-stream design. DSAGL explicitly addresses instance-level ambiguity and bag-level semantic consistency by generating multi-scale attention-based pseudo labels and guiding instance-level learning. A shared lightweight encoder (VSSMamba) enables efficient long-range dependency modeling, while a fusion-attentive module (FASA) enhances focus on sparse but diagnostically relevant regions. We further introduce a hybrid loss to enforce mutual consistency between the two streams. Experiments on CIFAR-10, NCT-CRC, and TCGA-Lung datasets demonstrate that DSAGL consistently outperforms state-of-the-art MIL baselines, achieving superior discriminative performance and robustness under weak supervision.
<div id='section'>Paperid: <span id='pid'>1008, <a href='https://arxiv.org/pdf/2505.06710.pdf' target='_blank'>https://arxiv.org/pdf/2505.06710.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yicheng Song, Tiancheng Lin, Die Peng, Su Yang, Yi Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06710">SimMIL: A Universal Weakly Supervised Pre-Training Framework for Multi-Instance Learning in Whole Slide Pathology Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Various multi-instance learning (MIL) based approaches have been developed and successfully applied to whole-slide pathological images (WSI). Existing MIL methods emphasize the importance of feature aggregators, but largely neglect the instance-level representation learning. They assume that the availability of a pre-trained feature extractor can be directly utilized or fine-tuned, which is not always the case. This paper proposes to pre-train feature extractor for MIL via a weakly-supervised scheme, i.e., propagating the weak bag-level labels to the corresponding instances for supervised learning. To learn effective features for MIL, we further delve into several key components, including strong data augmentation, a non-linear prediction head and the robust loss function. We conduct experiments on common large-scale WSI datasets and find it achieves better performance than other pre-training schemes (e.g., ImageNet pre-training and self-supervised learning) in different downstream tasks. We further show the compatibility and scalability of the proposed scheme by deploying it in fine-tuning the pathological-specific models and pre-training on merged multiple datasets. To our knowledge, this is the first work focusing on the representation learning for MIL.
<div id='section'>Paperid: <span id='pid'>1009, <a href='https://arxiv.org/pdf/2504.05403.pdf' target='_blank'>https://arxiv.org/pdf/2504.05403.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Manahil Raza, Muhammad Dawood, Talha Qaiser, Nasir M. Rajpoot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.05403">A Novel Approach to Linking Histology Images with DNA Methylation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>DNA methylation is an epigenetic mechanism that regulates gene expression by adding methyl groups to DNA. Abnormal methylation patterns can disrupt gene expression and have been linked to cancer development. To quantify DNA methylation, specialized assays are typically used. However, these assays are often costly and have lengthy processing times, which limits their widespread availability in routine clinical practice. In contrast, whole slide images (WSIs) for the majority of cancer patients can be more readily available. As such, given the ready availability of WSIs, there is a compelling need to explore the potential relationship between WSIs and DNA methylation patterns. To address this, we propose an end-to-end graph neural network based weakly supervised learning framework to predict the methylation state of gene groups exhibiting coherent patterns across samples. Using data from three cohorts from The Cancer Genome Atlas (TCGA) - TCGA-LGG (Brain Lower Grade Glioma), TCGA-GBM (Glioblastoma Multiforme) ($n$=729) and TCGA-KIRC (Kidney Renal Clear Cell Carcinoma) ($n$=511) - we demonstrate that the proposed approach achieves significantly higher AUROC scores than the state-of-the-art (SOTA) methods, by more than $20\%$. We conduct gene set enrichment analyses on the gene groups and show that majority of the gene groups are significantly enriched in important hallmarks and pathways. We also generate spatially enriched heatmaps to further investigate links between histological patterns and DNA methylation states. To the best of our knowledge, this is the first study that explores association of spatially resolved histological patterns with gene group methylation states across multiple cancer types using weakly supervised deep learning.
<div id='section'>Paperid: <span id='pid'>1010, <a href='https://arxiv.org/pdf/2503.18082.pdf' target='_blank'>https://arxiv.org/pdf/2503.18082.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nachuan Ma, Zhengfei Song, Qiang Hu, Chuang-Wei Liu, Yu Han, Yanting Zhang, Rui Fan, Lihua Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18082">Vehicular Road Crack Detection with Deep Learning: A New Online Benchmark for Comprehensive Evaluation of Existing Algorithms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the emerging field of urban digital twins (UDTs), advancing intelligent road inspection (IRI) vehicles with automatic road crack detection systems is essential for maintaining civil infrastructure. Over the past decade, deep learning-based road crack detection methods have been developed to detect cracks more efficiently, accurately, and objectively, with the goal of replacing manual visual inspection. Nonetheless, there is a lack of systematic reviews on state-of-the-art (SoTA) deep learning techniques, especially data-fusion and label-efficient algorithms for this task. This paper thoroughly reviews the SoTA deep learning-based algorithms, including (1) supervised, (2) unsupervised, (3) semi-supervised, and (4) weakly-supervised methods developed for road crack detection. Also, we create a dataset called UDTIRI-Crack, comprising $2,500$ high-quality images from seven public annotated sources, as the first extensive online benchmark in this field. Comprehensive experiments are conducted to compare the detection performance, computational efficiency, and generalizability of public SoTA deep learning-based algorithms for road crack detection. In addition, the feasibility of foundation models and large language models (LLMs) for road crack detection is explored. Afterwards, the existing challenges and future development trends of deep learning-based road crack detection algorithms are discussed. We believe this review can serve as practical guidance for developing intelligent road detection vehicles with the next-generation road condition assessment systems. The released benchmark UDTIRI-Crack is available at https://udtiri.com/submission/.
<div id='section'>Paperid: <span id='pid'>1011, <a href='https://arxiv.org/pdf/2503.15260.pdf' target='_blank'>https://arxiv.org/pdf/2503.15260.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Shi, Xi Fang, Naiyu Wang, Junxing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15260">DEPT: Deep Extreme Point Tracing for Ultrasound Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic medical image segmentation plays a crucial role in computer aided diagnosis. However, fully supervised learning approaches often require extensive and labor-intensive annotation efforts. To address this challenge, weakly supervised learning methods, particularly those using extreme points as supervisory signals, have the potential to offer an effective solution. In this paper, we introduce Deep Extreme Point Tracing (DEPT) integrated with Feature-Guided Extreme Point Masking (FGEPM) algorithm for ultrasound image segmentation. Notably, our method generates pseudo labels by identifying the lowest-cost path that connects all extreme points on the feature map-based cost matrix. Additionally, an iterative training strategy is proposed to refine pseudo labels progressively, enabling continuous network improvement. Experimental results on two public datasets demonstrate the effectiveness of our proposed method. The performance of our method approaches that of the fully supervised method and outperforms several existing weakly supervised methods.
<div id='section'>Paperid: <span id='pid'>1012, <a href='https://arxiv.org/pdf/2503.10287.pdf' target='_blank'>https://arxiv.org/pdf/2503.10287.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Zhou, Xiaobao Guo, Yuzhe Zhu, Adams Wai-Kin Kong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10287">MACS: Multi-source Audio-to-image Generation with Contextual Significance and Semantic Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Propelled by the breakthrough in deep generative models, audio-to-image generation has emerged as a pivotal cross-model task that converts complex auditory signals into rich visual representations. However, previous works only focus on single-source audio inputs for image generation, ignoring the multi-source characteristic in natural auditory scenes, thus limiting the performance in generating comprehensive visual content. To bridge this gap, a method called MACS is proposed to conduct multi-source audio-to-image generation. This is the first work that explicitly separates multi-source audio to capture the rich audio components before image generation. MACS is a two-stage method. In the first stage, multi-source audio inputs are separated by a weakly supervised method, where the audio and text labels are semantically aligned by casting into a common space using the large pre-trained CLAP model. We introduce a ranking loss to consider the contextual significance of the separated audio signals. In the second stage, efficient image generation is achieved by mapping the separated audio signals to the generation condition using only a trainable adapter and a MLP layer. We preprocess the LLP dataset as the first full multi-source audio-to-image generation benchmark. The experiments are conducted on multi-source, mixed-source, and single-source audio-to-image generation tasks. The proposed MACS outperforms the current state-of-the-art methods in 17 of the 21 evaluation indexes on all tasks and delivers superior visual quality. The code will be publicly available.
<div id='section'>Paperid: <span id='pid'>1013, <a href='https://arxiv.org/pdf/2502.20249.pdf' target='_blank'>https://arxiv.org/pdf/2502.20249.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pierre Vuillecard, Jean-Marc Odobez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20249">Enhancing 3D Gaze Estimation in the Wild using Weak Supervision with Gaze Following Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate 3D gaze estimation in unconstrained real-world environments remains a significant challenge due to variations in appearance, head pose, occlusion, and the limited availability of in-the-wild 3D gaze datasets. To address these challenges, we introduce a novel Self-Training Weakly-Supervised Gaze Estimation framework (ST-WSGE). This two-stage learning framework leverages diverse 2D gaze datasets, such as gaze-following data, which offer rich variations in appearances, natural scenes, and gaze distributions, and proposes an approach to generate 3D pseudo-labels and enhance model generalization. Furthermore, traditional modality-specific models, designed separately for images or videos, limit the effective use of available training data. To overcome this, we propose the Gaze Transformer (GaT), a modality-agnostic architecture capable of simultaneously learning static and dynamic gaze information from both image and video datasets. By combining 3D video datasets with 2D gaze target labels from gaze following tasks, our approach achieves the following key contributions: (i) Significant state-of-the-art improvements in within-domain and cross-domain generalization on unconstrained benchmarks like Gaze360 and GFIE, with notable cross-modal gains in video gaze estimation; (ii) Superior cross-domain performance on datasets such as MPIIFaceGaze and Gaze360 compared to frontal face methods. Code and pre-trained models will be released to the community.
<div id='section'>Paperid: <span id='pid'>1014, <a href='https://arxiv.org/pdf/2502.11743.pdf' target='_blank'>https://arxiv.org/pdf/2502.11743.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tobias Fuchs, Florian Kalinke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.11743">Robust Partial-Label Learning by Leveraging Class Activation Values</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-world training data is often noisy; for example, human annotators assign conflicting class labels to the same instances. Partial-label learning (PLL) is a weakly supervised learning paradigm that allows training classifiers in this context without manual data cleaning. While state-of-the-art methods have good predictive performance, their predictions are sensitive to high noise levels, out-of-distribution data, and adversarial perturbations. We propose a novel PLL method based on subjective logic, which explicitly represents uncertainty by leveraging the magnitudes of the underlying neural network's class activation values. Thereby, we effectively incorporate prior knowledge about the class labels by using a novel label weight re-distribution strategy that we prove to be optimal. We empirically show that our method yields more robust predictions in terms of predictive performance under high PLL noise levels, handling out-of-distribution examples, and handling adversarial perturbations on the test instances.
<div id='section'>Paperid: <span id='pid'>1015, <a href='https://arxiv.org/pdf/2502.06839.pdf' target='_blank'>https://arxiv.org/pdf/2502.06839.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Louis Bahrman, Mathieu Fontaine, Gael Richard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.06839">A Hybrid Model for Weakly-Supervised Speech Dereverberation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a new training strategy to improve speech dereverberation systems using minimal acoustic information and reverberant (wet) speech. Most existing algorithms rely on paired dry/wet data, which is difficult to obtain, or on target metrics that may not adequately capture reverberation characteristics and can lead to poor results on non-target metrics. Our approach uses limited acoustic information, like the reverberation time (RT60), to train a dereverberation system. The system's output is resynthesized using a generated room impulse response and compared with the original reverberant speech, providing a novel reverberation matching loss replacing the standard target metrics. During inference, only the trained dereverberation model is used. Experimental results demonstrate that our method achieves more consistent performance across various objective metrics used in speech dereverberation than the state-of-the-art.
<div id='section'>Paperid: <span id='pid'>1016, <a href='https://arxiv.org/pdf/2412.20455.pdf' target='_blank'>https://arxiv.org/pdf/2412.20455.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ayush Ghadiya, Purbayan Kar, Vishal Chudasama, Pankaj Wasnik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20455">Cross-Modal Fusion and Attention Mechanism for Weakly Supervised Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, weakly supervised video anomaly detection (WS-VAD) has emerged as a contemporary research direction to identify anomaly events like violence and nudity in videos using only video-level labels. However, this task has substantial challenges, including addressing imbalanced modality information and consistently distinguishing between normal and abnormal features. In this paper, we address these challenges and propose a multi-modal WS-VAD framework to accurately detect anomalies such as violence and nudity. Within the proposed framework, we introduce a new fusion mechanism known as the Cross-modal Fusion Adapter (CFA), which dynamically selects and enhances highly relevant audio-visual features in relation to the visual modality. Additionally, we introduce a Hyperbolic Lorentzian Graph Attention (HLGAtt) to effectively capture the hierarchical relationships between normal and abnormal representations, thereby enhancing feature separation accuracy. Through extensive experiments, we demonstrate that the proposed model achieves state-of-the-art results on benchmark datasets of violence and nudity detection.
<div id='section'>Paperid: <span id='pid'>1017, <a href='https://arxiv.org/pdf/2412.20201.pdf' target='_blank'>https://arxiv.org/pdf/2412.20201.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wen-Dong Jiang, Chih-Yung Chang, Hsiang-Chuan Chang, Ji-Yuan Chen, Diptendu Sinha Roy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20201">Injecting Explainability and Lightweight Design into Weakly Supervised Video Anomaly Detection Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Monitoring Anomaly Detection (WSMAD) utilizes weak supervision learning to identify anomalies, a critical task for smart city monitoring. However, existing multimodal approaches often fail to meet the real-time and interpretability requirements of edge devices due to their complexity. This paper presents TCVADS (Two-stage Cross-modal Video Anomaly Detection System), which leverages knowledge distillation and cross-modal contrastive learning to enable efficient, accurate, and interpretable anomaly detection on edge devices.TCVADS operates in two stages: coarse-grained rapid classification and fine-grained detailed analysis. In the first stage, TCVADS extracts features from video frames and inputs them into a time series analysis module, which acts as the teacher model. Insights are then transferred via knowledge distillation to a simplified convolutional network (student model) for binary classification. Upon detecting an anomaly, the second stage is triggered, employing a fine-grained multi-class classification model. This stage uses CLIP for cross-modal contrastive learning with text and images, enhancing interpretability and achieving refined classification through specially designed triplet textual relationships. Experimental results demonstrate that TCVADS significantly outperforms existing methods in model performance, detection efficiency, and interpretability, offering valuable contributions to smart city monitoring applications.
<div id='section'>Paperid: <span id='pid'>1018, <a href='https://arxiv.org/pdf/2411.12276.pdf' target='_blank'>https://arxiv.org/pdf/2411.12276.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nai-Xuan Ye, Tan-Ha Mai, Hsiu-Hsuan Wang, Wei-I Lin, Hsuan-Tien Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.12276">libcll: an Extendable Python Toolkit for Complementary-Label Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Complementary-label learning (CLL) is a weakly supervised learning paradigm for multiclass classification, where only complementary labels -- indicating classes an instance does not belong to -- are provided to the learning algorithm. Despite CLL's increasing popularity, previous studies highlight two main challenges: (1) inconsistent results arising from varied assumptions on complementary label generation, and (2) high barriers to entry due to the lack of a standardized evaluation platform across datasets and algorithms. To address these challenges, we introduce \texttt{libcll}, an extensible Python toolkit for CLL research. \texttt{libcll} provides a universal interface that supports a wide range of generation assumptions, both synthetic and real-world datasets, and key CLL algorithms. The toolkit is designed to mitigate inconsistencies and streamline the research process, with easy installation, comprehensive usage guides, and quickstart tutorials that facilitate efficient adoption and implementation of CLL techniques. Extensive ablation studies conducted with \texttt{libcll} demonstrate its utility in generating valuable insights to advance future CLL research.
<div id='section'>Paperid: <span id='pid'>1019, <a href='https://arxiv.org/pdf/2410.21991.pdf' target='_blank'>https://arxiv.org/pdf/2410.21991.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wen-Dong Jiang, Chih-Yung Chang, Ssu-Chi Kuai, Diptendu Sinha Roy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.21991">A Lightweight Dual-Branch System for Weakly-Supervised Video Anomaly Detection on Consumer Edge Devices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing demand for intelligent security in consumer electronics, such as smart home cameras and personal monitoring systems, is often hindered by the high computational cost and large model sizes of advanced AI. These limitations prevent the effective deployment of real-time Video Anomaly Detection (VAD) on resource-constrained edge devices. To bridge this gap, this paper introduces Rule-based Video Anomaly Detection (RuleVAD), a novel, lightweight system engineered for high-efficiency and low-complexity threat detection directly on consumer hardware. RuleVAD features an innovative decoupled dual-branch architecture to minimize computational load. An implicit branch uses visual features for rapid, coarse-grained binary classification, efficiently filtering out normal activity to avoid unnecessary processing. For potentially anomalous or complex events, a multimodal explicit branch takes over. This branch leverages YOLO-World to detect objects and applies data mining to generate interpretable, text-based association rules from the scene. By aligning these rules with visual data, RuleVAD achieves a more nuanced, fine-grained classification, significantly reducing the false alarms common in vision-only systems. Extensive experiments on the XD-Violence and UCF-Crime benchmark datasets show that RuleVAD achieves superior performance, surpassing existing state-of-the-art methods in both accuracy and speed. Crucially, the entire system is optimized for low-power operation and is fully deployable on an NVIDIA Jetson Nano board, demonstrating its practical feasibility for bringing advanced, real-time security monitoring to everyday consumer electronic devices.
<div id='section'>Paperid: <span id='pid'>1020, <a href='https://arxiv.org/pdf/2410.00536.pdf' target='_blank'>https://arxiv.org/pdf/2410.00536.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Krishna Chaitanya, Pablo F. Damasceno, Shreyas Fadnavis, Pooya Mobadersany, Chaitanya Parmar, Emily Scherer, Natalia Zemlianskaia, Lindsey Surace, Louis R. Ghanem, Oana Gabriela Cula, Tommaso Mansi, Kristopher Standish
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.00536">Arges: Spatio-Temporal Transformer for Ulcerative Colitis Severity Assessment in Endoscopy Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate assessment of disease severity from endoscopy videos in ulcerative colitis (UC) is crucial for evaluating drug efficacy in clinical trials. Severity is often measured by the Mayo Endoscopic Subscore (MES) and Ulcerative Colitis Endoscopic Index of Severity (UCEIS) score. However, expert MES/UCEIS annotation is time-consuming and susceptible to inter-rater variability, factors addressable by automation. Automation attempts with frame-level labels face challenges in fully-supervised solutions due to the prevalence of video-level labels in clinical trials. CNN-based weakly-supervised models (WSL) with end-to-end (e2e) training lack generalization to new disease scores and ignore spatio-temporal information crucial for accurate scoring. To address these limitations, we propose "Arges", a deep learning framework that utilizes a transformer with positional encoding to incorporate spatio-temporal information from frame features to estimate disease severity scores in endoscopy video. Extracted features are derived from a foundation model (ArgesFM), pre-trained on a large diverse dataset from multiple clinical trials (61M frames, 3927 videos). We evaluate four UC disease severity scores, including MES and three UCEIS component scores. Test set evaluation indicates significant improvements, with F1 scores increasing by 4.1% for MES and 18.8%, 6.6%, 3.8% for the three UCEIS component scores compared to state-of-the-art methods. Prospective validation on previously unseen clinical trial data further demonstrates the model's successful generalization.
<div id='section'>Paperid: <span id='pid'>1021, <a href='https://arxiv.org/pdf/2409.02145.pdf' target='_blank'>https://arxiv.org/pdf/2409.02145.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zekang Yang, Hong Liu, Xiangdong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.02145">A Multimodal Object-level Contrast Learning Method for Cancer Survival Risk Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computer-aided cancer survival risk prediction plays an important role in the timely treatment of patients. This is a challenging weakly supervised ordinal regression task associated with multiple clinical factors involved such as pathological images, genomic data and etc. In this paper, we propose a new training method, multimodal object-level contrast learning, for cancer survival risk prediction. First, we construct contrast learning pairs based on the survival risk relationship among the samples in the training sample set. Then we introduce the object-level contrast learning method to train the survival risk predictor. We further extend it to the multimodal scenario by applying cross-modal constrast. Considering the heterogeneity of pathological images and genomics data, we construct a multimodal survival risk predictor employing attention-based and self-normalizing based nerural network respectively. Finally, the survival risk predictor trained by our proposed method outperforms state-of-the-art methods on two public multimodal cancer datasets for survival risk prediction.
<div id='section'>Paperid: <span id='pid'>1022, <a href='https://arxiv.org/pdf/2408.08444.pdf' target='_blank'>https://arxiv.org/pdf/2408.08444.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinming Nian, Zhiyuan Peng, Qifan Wang, Yi Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.08444">W-RAG: Weakly Supervised Dense Retrieval in RAG for Open-domain Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In knowledge-intensive tasks such as open-domain question answering (OpenQA), large language models (LLMs) often struggle to generate factual answers, relying solely on their internal (parametric) knowledge. To address this limitation, Retrieval-Augmented Generation (RAG) systems enhance LLMs by retrieving relevant information from external sources, thereby positioning the retriever as a pivotal component. Although dense retrieval demonstrates state-of-the-art performance, its training poses challenges due to the scarcity of ground-truth evidence, largely attributed to the high costs of human annotation. In this paper, we propose W-RAG, a method that draws weak training signals from the downstream task (such as OpenQA) of an LLM, and fine-tunes the retriever to prioritize passages that most benefit the task. Specifically, we rerank the top-$k$ passages retrieved via BM25 by assessing the probability that the LLM will generate the correct answer for a question given each passage. The highest-ranking passages are then used as positive fine-tuning examples for dense retrieval. We conduct comprehensive experiments across four publicly available OpenQA datasets to demonstrate that our approach enhances both retrieval and OpenQA performance compared to baseline models, achieving results comparable to models fine-tuned with human-labeled data.
<div id='section'>Paperid: <span id='pid'>1023, <a href='https://arxiv.org/pdf/2407.21384.pdf' target='_blank'>https://arxiv.org/pdf/2407.21384.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanxu Mao, Xiaohui Chen, Peipei Liu, Tiehan Cui, Zuhui Yue, Zheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.21384">GEGA: Graph Convolutional Networks and Evidence Retrieval Guided Attention for Enhanced Document-level Relation Extraction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Document-level relation extraction (DocRE) aims to extract relations between entities from unstructured document text. Compared to sentence-level relation extraction, it requires more complex semantic understanding from a broader text context. Currently, some studies are utilizing logical rules within evidence sentences to enhance the performance of DocRE. However, in the data without provided evidence sentences, researchers often obtain a list of evidence sentences for the entire document through evidence retrieval (ER). Therefore, DocRE suffers from two challenges: firstly, the relevance between evidence and entity pairs is weak; secondly, there is insufficient extraction of complex cross-relations between long-distance multi-entities. To overcome these challenges, we propose GEGA, a novel model for DocRE. The model leverages graph neural networks to construct multiple weight matrices, guiding attention allocation to evidence sentences. It also employs multi-scale representation aggregation to enhance ER. Subsequently, we integrate the most efficient evidence information to implement both fully supervised and weakly supervised training processes for the model. We evaluate the GEGA model on three widely used benchmark datasets: DocRED, Re-DocRED, and Revisit-DocRED. The experimental results indicate that our model has achieved comprehensive improvements compared to the existing SOTA model.
<div id='section'>Paperid: <span id='pid'>1024, <a href='https://arxiv.org/pdf/2407.13553.pdf' target='_blank'>https://arxiv.org/pdf/2407.13553.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingyue Zhao, Peiqi Li, Xiangde Luo, Meng Yang, Shi Chang, Zhongyu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.13553">SAM-Driven Weakly Supervised Nodule Segmentation with Uncertainty-Aware Cross Teaching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automated nodule segmentation is essential for computer-assisted diagnosis in ultrasound images. Nevertheless, most existing methods depend on precise pixel-level annotations by medical professionals, a process that is both costly and labor-intensive. Recently, segmentation foundation models like SAM have shown impressive generalizability on natural images, suggesting their potential as pseudo-labelers. However, accurate prompts remain crucial for their success in medical images. In this work, we devise a novel weakly supervised framework that effectively utilizes the segmentation foundation model to generate pseudo-labels from aspect ration annotations for automatic nodule segmentation. Specifically, we develop three types of bounding box prompts based on scalable shape priors, followed by an adaptive pseudo-label selection module to fully exploit the prediction capabilities of the foundation model for nodules. We also present a SAM-driven uncertainty-aware cross-teaching strategy. This approach integrates SAM-based uncertainty estimation and label-space perturbations into cross-teaching to mitigate the impact of pseudo-label inaccuracies on model training. Extensive experiments on two clinically collected ultrasound datasets demonstrate the superior performance of our proposed method.
<div id='section'>Paperid: <span id='pid'>1025, <a href='https://arxiv.org/pdf/2407.13363.pdf' target='_blank'>https://arxiv.org/pdf/2407.13363.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chang Liu, Giulia Rizzoli, Pietro Zanuttigh, Fu Li, Yi Niu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.13363">Learning from the Web: Language Drives Weakly-Supervised Incremental Learning for Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current weakly-supervised incremental learning for semantic segmentation (WILSS) approaches only consider replacing pixel-level annotations with image-level labels, while the training images are still from well-designed datasets. In this work, we argue that widely available web images can also be considered for the learning of new classes. To achieve this, firstly we introduce a strategy to select web images which are similar to previously seen examples in the latent space using a Fourier-based domain discriminator. Then, an effective caption-driven reharsal strategy is proposed to preserve previously learnt classes. To our knowledge, this is the first work to rely solely on web images for both the learning of new concepts and the preservation of the already learned ones in WILSS. Experimental results show that the proposed approach can reach state-of-the-art performances without using manually selected and annotated data in the incremental steps.
<div id='section'>Paperid: <span id='pid'>1026, <a href='https://arxiv.org/pdf/2407.10131.pdf' target='_blank'>https://arxiv.org/pdf/2407.10131.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinjian Wu, Ruisong Zhang, Jie Qin, Shijie Ma, Cheng-Lin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10131">WPS-SAM: Towards Weakly-Supervised Part Segmentation with Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Segmenting and recognizing diverse object parts is crucial in computer vision and robotics. Despite significant progress in object segmentation, part-level segmentation remains underexplored due to complex boundaries and scarce annotated data. To address this, we propose a novel Weakly-supervised Part Segmentation (WPS) setting and an approach called WPS-SAM, built on the large-scale pre-trained vision foundation model, Segment Anything Model (SAM). WPS-SAM is an end-to-end framework designed to extract prompt tokens directly from images and perform pixel-level segmentation of part regions. During its training phase, it only uses weakly supervised labels in the form of bounding boxes or points. Extensive experiments demonstrate that, through exploiting the rich knowledge embedded in pre-trained foundation models, WPS-SAM outperforms other segmentation models trained with pixel-level strong annotations. Specifically, WPS-SAM achieves 68.93% mIOU and 79.53% mACC on the PartImageNet dataset, surpassing state-of-the-art fully supervised methods by approximately 4% in terms of mIOU.
<div id='section'>Paperid: <span id='pid'>1027, <a href='https://arxiv.org/pdf/2406.18550.pdf' target='_blank'>https://arxiv.org/pdf/2406.18550.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qian-Wei Wang, Yuqiu Xie, Letian Zhang, Zimo Liu, Shu-Tao Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.18550">Pre-Trained Vision-Language Models as Partial Annotators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pre-trained vision-language models learn massive data to model unified representations of images and natural languages, which can be widely applied to downstream machine learning tasks. In addition to zero-shot inference, in order to better adapt pre-trained models to the requirements of downstream tasks, people usually use methods such as few-shot or parameter-efficient fine-tuning and knowledge distillation. However, annotating samples is laborious, while a large number of unlabeled samples can be easily obtained. In this paper, we investigate a novel "pre-trained annotating - weakly-supervised learning" paradigm for pre-trained model application and experiment on image classification tasks. Specifically, based on CLIP, we annotate image samples with multiple prompt templates to obtain multiple candidate labels to form the noisy partial label dataset, and design a collaborative consistency regularization algorithm to solve this problem. Our method simultaneously trains two neural networks, which collaboratively purify training labels for each other and obtain pseudo-labels for self-training, while adopting prototypical similarity alignment and noisy supervised contrastive learning to optimize model representation. In experiments, our method achieves performances far beyond zero-shot inference without introducing additional label information, and outperforms other weakly supervised learning and few-shot fine-tuning methods, and obtains smaller deployed models. Our code is available at: \url{https://anonymous.4open.science/r/Co-Reg-8CF9}.
<div id='section'>Paperid: <span id='pid'>1028, <a href='https://arxiv.org/pdf/2404.15683.pdf' target='_blank'>https://arxiv.org/pdf/2404.15683.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Che, Fazle Rafsani, Jay Shah, Md Mahfuzur Rahman Siddiquee, Teresa Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.15683">AnoFPDM: Anomaly Segmentation with Forward Process of Diffusion Models for Brain MRI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised diffusion models (DMs) in anomaly segmentation, leveraging image-level labels, have attracted significant attention for their superior performance compared to unsupervised methods. It eliminates the need for pixel-level labels in training, offering a more cost-effective alternative to supervised methods. However, existing methods are not fully weakly-supervised because they heavily rely on costly pixel-level labels for hyperparameter tuning in inference. To tackle this challenge, we introduce Anomaly Segmentation with Forward Process of Diffusion Models (AnoFPDM), a fully weakly-supervised framework that operates without the need of pixel-level labels. Leveraging the unguided forward process as a reference for the guided forward process, we select hyperparameters such as the noise scale, the threshold for segmentation and the guidance strength. We aggregate anomaly maps from guided forward process, enhancing the signal strength of anomalous regions. Remarkably, our proposed method outperforms recent state-of-the-art weakly-supervised approaches, even without utilizing pixel-level labels.
<div id='section'>Paperid: <span id='pid'>1029, <a href='https://arxiv.org/pdf/2404.12861.pdf' target='_blank'>https://arxiv.org/pdf/2404.12861.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yilong Chen, Zongyi Xu, xiaoshui Huang, Ruicheng Zhang, Xinqi Jiang, Xinbo Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.12861">Weakly Supervised LiDAR Semantic Segmentation via Scatter Image Annotation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised LiDAR semantic segmentation has made significant strides with limited labeled data. However, most existing methods focus on the network training under weak supervision, while efficient annotation strategies remain largely unexplored. To tackle this gap, we implement LiDAR semantic segmentation using scatter image annotation, effectively integrating an efficient annotation strategy with network training. Specifically, we propose employing scatter images to annotate LiDAR point clouds, combining a pre-trained optical flow estimation network with a foundation image segmentation model to rapidly propagate manual annotations into dense labels for both images and point clouds. Moreover, we propose ScatterNet, a network that includes three pivotal strategies to reduce the performance gap caused by such annotations. Firstly, it utilizes dense semantic labels as supervision for the image branch, alleviating the modality imbalance between point clouds and images. Secondly, an intermediate fusion branch is proposed to obtain multimodal texture and structural features. Lastly, a perception consistency loss is introduced to determine which information needs to be fused and which needs to be discarded during the fusion process. Extensive experiments on the nuScenes and SemanticKITTI datasets have demonstrated that our method requires less than 0.02% of the labeled points to achieve over 95% of the performance of fully-supervised methods. Notably, our labeled points are only 5% of those used in the most advanced weakly supervised methods.
<div id='section'>Paperid: <span id='pid'>1030, <a href='https://arxiv.org/pdf/2404.05022.pdf' target='_blank'>https://arxiv.org/pdf/2404.05022.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Valentin Koch, Sophia J. Wagner, Salome Kazeminia, Ece Sancar, Matthias Hehr, Julia Schnabel, Tingying Peng, Carsten Marr
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.05022">DinoBloom: A Foundation Model for Generalizable Cell Embeddings in Hematology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In hematology, computational models offer significant potential to improve diagnostic accuracy, streamline workflows, and reduce the tedious work of analyzing single cells in peripheral blood or bone marrow smears. However, clinical adoption of computational models has been hampered by the lack of generalization due to large batch effects, small dataset sizes, and poor performance in transfer learning from natural images. To address these challenges, we introduce DinoBloom, the first foundation model for single cell images in hematology, utilizing a tailored DINOv2 pipeline. Our model is built upon an extensive collection of 13 diverse, publicly available datasets of peripheral blood and bone marrow smears, the most substantial open-source cohort in hematology so far, comprising over 380,000 white blood cell images. To assess its generalization capability, we evaluate it on an external dataset with a challenging domain shift. We show that our model outperforms existing medical and non-medical vision models in (i) linear probing and k-nearest neighbor evaluations for cell-type classification on blood and bone marrow smears and (ii) weakly supervised multiple instance learning for acute myeloid leukemia subtyping by a large margin. A family of four DinoBloom models (small, base, large, and giant) can be adapted for a wide range of downstream applications, be a strong baseline for classification problems, and facilitate the assessment of batch effects in new datasets. All models are available at github.com/marrlab/DinoBloom.
<div id='section'>Paperid: <span id='pid'>1031, <a href='https://arxiv.org/pdf/2404.01740.pdf' target='_blank'>https://arxiv.org/pdf/2404.01740.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tanvir Mahmud, Saeed Amizadeh, Kazuhito Koishida, Diana Marculescu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01740">Weakly-supervised Audio Separation via Bi-modal Semantic Similarity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conditional sound separation in multi-source audio mixtures without having access to single source sound data during training is a long standing challenge. Existing mix-and-separate based methods suffer from significant performance drop with multi-source training mixtures due to the lack of supervision signal for single source separation cases during training. However, in the case of language-conditional audio separation, we do have access to corresponding text descriptions for each audio mixture in our training data, which can be seen as (rough) representations of the audio samples in the language modality. To this end, in this paper, we propose a generic bi-modal separation framework which can enhance the existing unsupervised frameworks to separate single-source signals in a target modality (i.e., audio) using the easily separable corresponding signals in the conditioning modality (i.e., language), without having access to single-source samples in the target modality during training. We empirically show that this is well within reach if we have access to a pretrained joint embedding model between the two modalities (i.e., CLAP). Furthermore, we propose to incorporate our framework into two fundamental scenarios to enhance separation performance. First, we show that our proposed methodology significantly improves the performance of purely unsupervised baselines by reducing the distribution shift between training and test samples. In particular, we show that our framework can achieve 71% boost in terms of Signal-to-Distortion Ratio (SDR) over the baseline, reaching 97.5% of the supervised learning performance. Second, we show that we can further improve the performance of the supervised learning itself by 17% if we augment it by our proposed weakly-supervised framework, that enables a powerful semi-supervised framework for audio separation.
<div id='section'>Paperid: <span id='pid'>1032, <a href='https://arxiv.org/pdf/2403.19786.pdf' target='_blank'>https://arxiv.org/pdf/2403.19786.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingxing Rao, Yinhong Qin, Soheil Kolouri, Jie Ying Wu, Daniel Moyer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.19786">Zero-shot Prompt-based Video Encoder for Surgical Gesture Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Purpose: In order to produce a surgical gesture recognition system that can support a wide variety of procedures, either a very large annotated dataset must be acquired, or fitted models must generalize to new labels (so called "zero-shot" capability). In this paper we investigate the feasibility of latter option. Methods: Leveraging the Bridge-Prompt framework, we prompt-tune a pre-trained vision-text model (CLIP) for gesture recognition in surgical videos. This can utilize extensive outside video data such as text, but also make use of label meta-data and weakly supervised contrastive losses. Results: Our experiments show that prompt-based video encoder outperforms standard encoders in surgical gesture recognition tasks. Notably, it displays strong performance in zero-shot scenarios, where gestures/tasks that were not provided during the encoder training phase are included in the prediction phase. Additionally, we measure the benefit of inclusion text descriptions in the feature extractor training schema. Conclusion Bridge-Prompt and similar pre-trained+prompt-tuned video encoder models present significant visual representation for surgical robotics, especially in gesture recognition tasks. Given the diverse range of surgical tasks (gestures), the ability of these models to zero-shot transfer without the need for any task (gesture) specific retraining makes them invaluable.
<div id='section'>Paperid: <span id='pid'>1033, <a href='https://arxiv.org/pdf/2403.14390.pdf' target='_blank'>https://arxiv.org/pdf/2403.14390.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingwen Lin, Boyan Xu, Zhengting Huang, Ruichu Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.14390">From Large to Tiny: Distilling and Refining Mathematical Expertise for Math Word Problems with Weakly Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Addressing the challenge of high annotation costs in solving Math Word Problems (MWPs) through full supervision with intermediate equations, recent works have proposed weakly supervised task settings that rely solely on the final answer as a supervised signal. Existing leading approaches typically employ various search techniques to infer intermediate equations, but cannot ensure their semantic consistency with natural language descriptions. The rise of Large Language Models (LLMs) like ChatGPT has opened up new possibilities for addressing MWPs directly. However, the computational demands of LLMs make them less than ideal for use in settings where resources are tight. In light of these challenges, we introduce an innovative two-stage framework that adeptly transfers mathematical Expertise from large to tiny language models. In \emph{Distillation Stage}, we propose a series of extraction processes that satisfy the properties of MWPs to distill mathematical knowledge from LLMs to construct problem-equation pairs required for supervised training. In \emph{Refinement Stage}, Due to Knowledge distilling method cannot guarantee the full utilization of all data, we further utilize the unsuccessfully searched data effectively by Knowledge Refine method. Finally, We train a small model using distilled data generated through two-stage methods. As our method fully leverages the semantic understanding capabilities during the searching 'problem-equation' pair, it demonstrates significantly improved performance on the Math23K and Weak12K datasets compared to existing small model methods, while maintaining a much lower computational cost than ChatGPT.
<div id='section'>Paperid: <span id='pid'>1034, <a href='https://arxiv.org/pdf/2403.14366.pdf' target='_blank'>https://arxiv.org/pdf/2403.14366.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lizhe Liu, Bohua Wang, Hongwei Xie, Daqi Liu, Li Liu, Zhiqiang Tian, Kuiyuan Yang, Bing Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.14366">SurroundSDF: Implicit 3D Scene Understanding Based on Signed Distance Field</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-centric 3D environment understanding is both vital and challenging for autonomous driving systems. Recently, object-free methods have attracted considerable attention. Such methods perceive the world by predicting the semantics of discrete voxel grids but fail to construct continuous and accurate obstacle surfaces. To this end, in this paper, we propose SurroundSDF to implicitly predict the signed distance field (SDF) and semantic field for the continuous perception from surround images. Specifically, we introduce a query-based approach and utilize SDF constrained by the Eikonal formulation to accurately describe the surfaces of obstacles. Furthermore, considering the absence of precise SDF ground truth, we propose a novel weakly supervised paradigm for SDF, referred to as the Sandwich Eikonal formulation, which emphasizes applying correct and dense constraints on both sides of the surface, thereby enhancing the perceptual accuracy of the surface. Experiments suggest that our method achieves SOTA for both occupancy prediction and 3D scene reconstruction tasks on the nuScenes dataset.
<div id='section'>Paperid: <span id='pid'>1035, <a href='https://arxiv.org/pdf/2403.09315.pdf' target='_blank'>https://arxiv.org/pdf/2403.09315.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Xiong, Churan Wang, Wenxue Li, Guanbin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09315">Semi- and Weakly-Supervised Learning for Mammogram Mass Segmentation with Limited Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate identification of breast masses is crucial in diagnosing breast cancer; however, it can be challenging due to their small size and being camouflaged in surrounding normal glands. Worse still, it is also expensive in clinical practice to obtain adequate pixel-wise annotations for training deep neural networks. To overcome these two difficulties with one stone, we propose a semi- and weakly-supervised learning framework for mass segmentation that utilizes limited strongly-labeled samples and sufficient weakly-labeled samples to achieve satisfactory performance. The framework consists of an auxiliary branch to exclude lesion-irrelevant background areas, a segmentation branch for final prediction, and a spatial prompting module to integrate the complementary information of the two branches. We further disentangle encoded obscure features into lesion-related and others to boost performance. Experiments on CBIS-DDSM and INbreast datasets demonstrate the effectiveness of our method.
<div id='section'>Paperid: <span id='pid'>1036, <a href='https://arxiv.org/pdf/2402.00592.pdf' target='_blank'>https://arxiv.org/pdf/2402.00592.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tobias Fuchs, Florian Kalinke, Klemens BÃ¶hm
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.00592">Partial-Label Learning with a Reject Option</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In real-world applications, one often encounters ambiguously labeled data, where different annotators assign conflicting class labels. Partial-label learning allows training classifiers in this weakly supervised setting, where state-of-the-art methods already show good predictive performance. However, even the best algorithms give incorrect predictions, which can have severe consequences when they impact actions or decisions. We propose a novel risk-consistent nearest-neighbor-based partial-label learning algorithm with a reject option, that is, the algorithm can reject unsure predictions. Extensive experiments on artificial and real-world datasets show that our method provides the best trade-off between the number and accuracy of non-rejected predictions when compared to our competitors, which use confidence thresholds for rejecting unsure predictions. When evaluated without the reject option, our nearest-neighbor-based approach also achieves competitive prediction performance.
<div id='section'>Paperid: <span id='pid'>1037, <a href='https://arxiv.org/pdf/2401.13551.pdf' target='_blank'>https://arxiv.org/pdf/2401.13551.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongwei Nie, Hao Huang, Chengjiang Long, Qing Zhang, Pradipta Maji, Hongmin Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.13551">Interleaving One-Class and Weakly-Supervised Models with Adaptive Thresholding for Unsupervised Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Anomaly Detection (VAD) has been extensively studied under the settings of One-Class Classification (OCC) and Weakly-Supervised learning (WS), which however both require laborious human-annotated normal/abnormal labels. In this paper, we study Unsupervised VAD (UVAD) that does not depend on any label by combining OCC and WS into a unified training framework. Specifically, we extend OCC to weighted OCC (wOCC) and propose a wOCC-WS interleaving training module, where the two models automatically generate pseudo-labels for each other. We face two challenges to make the combination effective: (1) Models' performance fluctuates occasionally during the training process due to the inevitable randomness of the pseudo labels. (2) Thresholds are needed to divide pseudo labels, making the training depend on the accuracy of user intervention. For the first problem, we propose to use wOCC requiring soft labels instead of OCC trained with hard zero/one labels, as soft labels exhibit high consistency throughout different training cycles while hard labels are prone to sudden changes. For the second problem, we repeat the interleaving training module multiple times, during which we propose an adaptive thresholding strategy that can progressively refine a rough threshold to a relatively optimal threshold, which reduces the influence of user interaction. A benefit of employing OCC and WS methods to compose a UVAD method is that we can incorporate the most recent OCC or WS model into our framework. Experiments demonstrate the effectiveness of the proposed UVAD framework.
<div id='section'>Paperid: <span id='pid'>1038, <a href='https://arxiv.org/pdf/2401.10711.pdf' target='_blank'>https://arxiv.org/pdf/2401.10711.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haibo Wang, Chenghang Lai, Yixuan Sun, Weifeng Ge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.10711">Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal Models for Video Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Question Answering (VideoQA) aims to answer natural language questions based on the information observed in videos. Despite the recent success of Large Multimodal Models (LMMs) in image-language understanding and reasoning, they deal with VideoQA insufficiently, by simply taking uniformly sampled frames as visual inputs, which ignores question-relevant visual clues. Moreover, there are no human annotations for question-critical timestamps in existing VideoQA datasets. In light of this, we propose a novel weakly supervised framework to enforce the LMMs to reason out the answers with question-critical moments as visual inputs. Specifically, we first fuse the question and answer pairs as event descriptions to find multiple keyframes as target moments and pseudo-labels, with the visual-language alignment capability of the CLIP models. With these pseudo-labeled keyframes as additionally weak supervision, we devise a lightweight Gaussian-based Contrastive Grounding (GCG) module. GCG learns multiple Gaussian functions to characterize the temporal structure of the video, and sample question-critical frames as positive moments to be the visual inputs of LMMs. Extensive experiments on several benchmarks verify the effectiveness of our framework, and we achieve substantial improvements compared to previous state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1039, <a href='https://arxiv.org/pdf/2401.04720.pdf' target='_blank'>https://arxiv.org/pdf/2401.04720.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benedikt Roth, Valentin Koch, Sophia J. Wagner, Julia A. Schnabel, Carsten Marr, Tingying Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.04720">Low-resource finetuning of foundation models beats state-of-the-art in histopathology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To handle the large scale of whole slide images in computational pathology, most approaches first tessellate the images into smaller patches, extract features from these patches, and finally aggregate the feature vectors with weakly-supervised learning. The performance of this workflow strongly depends on the quality of the extracted features. Recently, foundation models in computer vision showed that leveraging huge amounts of data through supervised or self-supervised learning improves feature quality and generalizability for a variety of tasks. In this study, we benchmark the most popular vision foundation models as feature extractors for histopathology data. We evaluate the models in two settings: slide-level classification and patch-level classification. We show that foundation models are a strong baseline. Our experiments demonstrate that by finetuning a foundation model on a single GPU for only two hours or three days depending on the dataset, we can match or outperform state-of-the-art feature extractors for computational pathology. These findings imply that even with little resources one can finetune a feature extractor tailored towards a specific downstream task and dataset. This is a considerable shift from the current state, where only few institutions with large amounts of resources and datasets are able to train a feature extractor. We publish all code used for training and evaluation as well as the finetuned models.
<div id='section'>Paperid: <span id='pid'>1040, <a href='https://arxiv.org/pdf/2312.02798.pdf' target='_blank'>https://arxiv.org/pdf/2312.02798.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Miriam Rateike, Celia Cintas, John Wamburu, Tanya Akumu, Skyler Speakman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02798">Weakly Supervised Detection of Hallucinations in LLM Activations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose an auditing method to identify whether a large language model (LLM) encodes patterns such as hallucinations in its internal states, which may propagate to downstream tasks. We introduce a weakly supervised auditing technique using a subset scanning approach to detect anomalous patterns in LLM activations from pre-trained models. Importantly, our method does not need knowledge of the type of patterns a-priori. Instead, it relies on a reference dataset devoid of anomalies during testing. Further, our approach enables the identification of pivotal nodes responsible for encoding these patterns, which may offer crucial insights for fine-tuning specific sub-networks for bias mitigation. We introduce two new scanning methods to handle LLM activations for anomalous sentences that may deviate from the expected distribution in either direction. Our results confirm prior findings of BERT's limited internal capacity for encoding hallucinations, while OPT appears capable of encoding hallucination information internally. Importantly, our scanning approach, without prior exposure to false statements, performs comparably to a fully supervised out-of-distribution classifier.
<div id='section'>Paperid: <span id='pid'>1041, <a href='https://arxiv.org/pdf/2311.02061.pdf' target='_blank'>https://arxiv.org/pdf/2311.02061.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christian Lange, Elijah Cole, Grant Van Horn, Oisin Mac Aodha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.02061">Active Learning-Based Species Range Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a new active learning approach for efficiently estimating the geographic range of a species from a limited number of on the ground observations. We model the range of an unmapped species of interest as the weighted combination of estimated ranges obtained from a set of different species. We show that it is possible to generate this candidate set of ranges by using models that have been trained on large weakly supervised community collected observation data. From this, we develop a new active querying approach that sequentially selects geographic locations to visit that best reduce our uncertainty over an unmapped species' range. We conduct a detailed evaluation of our approach and compare it to existing active learning methods using an evaluation dataset containing expert-derived ranges for one thousand species. Our results demonstrate that our method outperforms alternative active learning methods and approaches the performance of end-to-end trained models, even when only using a fraction of the data. This highlights the utility of active learning via transfer learned spatial representations for species range estimation. It also emphasizes the value of leveraging emerging large-scale crowdsourced datasets, not only for modeling a species' range, but also for actively discovering them.
<div id='section'>Paperid: <span id='pid'>1042, <a href='https://arxiv.org/pdf/2310.17209.pdf' target='_blank'>https://arxiv.org/pdf/2310.17209.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Roy Hirsch, Regev Cohen, Mathilde Caron, Tomer Golany, Daniel Freedman, Ehud Rivlin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.17209">Weakly-Supervised Surgical Phase Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A key element of computer-assisted surgery systems is phase recognition of surgical videos. Existing phase recognition algorithms require frame-wise annotation of a large number of videos, which is time and money consuming. In this work we join concepts of graph segmentation with self-supervised learning to derive a random-walk solution for per-frame phase prediction. Furthermore, we utilize within our method two forms of weak supervision: sparse timestamps or few-shot learning. The proposed algorithm enjoys low complexity and can operate in lowdata regimes. We validate our method by running experiments with the public Cholec80 dataset of laparoscopic cholecystectomy videos, demonstrating promising performance in multiple setups.
<div id='section'>Paperid: <span id='pid'>1043, <a href='https://arxiv.org/pdf/2310.05330.pdf' target='_blank'>https://arxiv.org/pdf/2310.05330.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Wang, Jiaogen Zhou, Jihong Guan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.05330">A Lightweight Video Anomaly Detection Model with Weak Supervision and Adaptive Instance Selection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video anomaly detection is to determine whether there are any abnormal events, behaviors or objects in a given video, which enables effective and intelligent public safety management. As video anomaly labeling is both time-consuming and expensive, most existing works employ unsupervised or weakly supervised learning methods. This paper focuses on weakly supervised video anomaly detection, in which the training videos are labeled whether or not they contain any anomalies, but there is no information about which frames the anomalies are located. However, the uncertainty of weakly labeled data and the large model size prevent existing methods from wide deployment in real scenarios, especially the resource-limit situations such as edge-computing. In this paper, we develop a lightweight video anomaly detection model. On the one hand, we propose an adaptive instance selection strategy, which is based on the model's current status to select confident instances, thereby mitigating the uncertainty of weakly labeled data and subsequently promoting the model's performance. On the other hand, we design a lightweight multi-level temporal correlation attention module and an hourglass-shaped fully connected layer to construct the model, which can reduce the model parameters to only 0.56\% of the existing methods (e.g. RTFM). Our extensive experiments on two public datasets UCF-Crime and ShanghaiTech show that our model can achieve comparable or even superior AUC score compared to the state-of-the-art methods, with a significantly reduced number of model parameters.
<div id='section'>Paperid: <span id='pid'>1044, <a href='https://arxiv.org/pdf/2309.05086.pdf' target='_blank'>https://arxiv.org/pdf/2309.05086.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhijun Chen, Hailong Sun, Wanhao Zhang, Chunyi Xu, Qianren Mao, Pengpeng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.05086">Neural-Hidden-CRF: A Robust Weakly-Supervised Sequence Labeler</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a neuralized undirected graphical model called Neural-Hidden-CRF to solve the weakly-supervised sequence labeling problem. Under the umbrella of probabilistic undirected graph theory, the proposed Neural-Hidden-CRF embedded with a hidden CRF layer models the variables of word sequence, latent ground truth sequence, and weak label sequence with the global perspective that undirected graphical models particularly enjoy. In Neural-Hidden-CRF, we can capitalize on the powerful language model BERT or other deep models to provide rich contextual semantic knowledge to the latent ground truth sequence, and use the hidden CRF layer to capture the internal label dependencies. Neural-Hidden-CRF is conceptually simple and empirically powerful. It obtains new state-of-the-art results on one crowdsourcing benchmark and three weak-supervision benchmarks, including outperforming the recent advanced model CHMM by 2.80 F1 points and 2.23 F1 points in average generalization and inference performance, respectively.
<div id='section'>Paperid: <span id='pid'>1045, <a href='https://arxiv.org/pdf/2309.05069.pdf' target='_blank'>https://arxiv.org/pdf/2309.05069.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bo Wan, Tinne Tuytelaars
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.05069">Exploiting CLIP for Zero-shot HOI Detection Requires Knowledge Distillation at Multiple Levels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we investigate the task of zero-shot human-object interaction (HOI) detection, a novel paradigm for identifying HOIs without the need for task-specific annotations. To address this challenging task, we employ CLIP, a large-scale pre-trained vision-language model (VLM), for knowledge distillation on multiple levels. Specifically, we design a multi-branch neural network that leverages CLIP for learning HOI representations at various levels, including global images, local union regions encompassing human-object pairs, and individual instances of humans or objects. To train our model, CLIP is utilized to generate HOI scores for both global images and local union regions that serve as supervision signals. The extensive experiments demonstrate the effectiveness of our novel multi-level CLIP knowledge integration strategy. Notably, the model achieves strong performance, which is even comparable with some fully-supervised and weakly-supervised methods on the public HICO-DET benchmark.
<div id='section'>Paperid: <span id='pid'>1046, <a href='https://arxiv.org/pdf/2309.03925.pdf' target='_blank'>https://arxiv.org/pdf/2309.03925.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Willem BonnaffÃ©, CRUK ICGC Prostate Group, Freddie Hamdy, Yang Hu, Ian Mills, Jens Rittscher, Clare Verrill, Dan J. Woodcock
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.03925">Beyond attention: deriving biologically interpretable insights from weakly-supervised multiple-instance learning models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in attention-based multiple instance learning (MIL) have improved our insights into the tissue regions that models rely on to make predictions in digital pathology. However, the interpretability of these approaches is still limited. In particular, they do not report whether high-attention regions are positively or negatively associated with the class labels or how well these regions correspond to previously established clinical and biological knowledge. We address this by introducing a post-training methodology to analyse MIL models. Firstly, we introduce prediction-attention-weighted (PAW) maps by combining tile-level attention and prediction scores produced by a refined encoder, allowing us to quantify the predictive contribution of high-attention regions. Secondly, we introduce a biological feature instantiation technique by integrating PAW maps with nuclei segmentation masks. This further improves interpretability by providing biologically meaningful features related to the cellular organisation of the tissue and facilitates comparisons with known clinical features. We illustrate the utility of our approach by comparing PAW maps obtained for prostate cancer diagnosis (i.e. samples containing malignant tissue, 381/516 tissue samples) and prognosis (i.e. samples from patients with biochemical recurrence following surgery, 98/663 tissue samples) in a cohort of patients from the international cancer genome consortium (ICGC UK Prostate Group). Our approach reveals that regions that are predictive of adverse prognosis do not tend to co-locate with the tumour regions, indicating that non-cancer cells should also be studied when evaluating prognosis.
<div id='section'>Paperid: <span id='pid'>1047, <a href='https://arxiv.org/pdf/2309.00197.pdf' target='_blank'>https://arxiv.org/pdf/2309.00197.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bruno Machado Pacheco, Laio Oriel Seman, Eduardo Camponogara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.00197">Deep-learning-based Early Fixing for Gas-lifted Oil Production Optimization: Supervised and Weakly-supervised Approaches</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Maximizing oil production from gas-lifted oil wells entails solving Mixed-Integer Linear Programs (MILPs). As the parameters of the wells, such as the basic-sediment-to-water ratio and the gas-oil ratio, are updated, the problems must be repeatedly solved. Instead of relying on costly exact methods or the accuracy of general approximate methods, in this paper, we propose a tailor-made heuristic solution based on deep learning models trained to provide values to all integer variables given varying well parameters, early-fixing the integer variables and, thus, reducing the original problem to a linear program (LP). We propose two approaches for developing the learning-based heuristic: a supervised learning approach, which requires the optimal integer values for several instances of the original problem in the training set, and a weakly-supervised learning approach, which requires only solutions for the early-fixed linear problems with random assignments for the integer variables. Our results show a runtime reduction of 71.11% Furthermore, the weakly-supervised learning model provided significant values for early fixing, despite never seeing the optimal values during training.
<div id='section'>Paperid: <span id='pid'>1048, <a href='https://arxiv.org/pdf/2308.11452.pdf' target='_blank'>https://arxiv.org/pdf/2308.11452.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Valasia Vlachopoulou, Ioannis Sarafis, Alexandros Papadopoulos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.11452">Food Image Classification and Segmentation with Attention-based Multiple Instance Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The demand for accurate food quantification has increased in the recent years, driven by the needs of applications in dietary monitoring. At the same time, computer vision approaches have exhibited great potential in automating tasks within the food domain. Traditionally, the development of machine learning models for these problems relies on training data sets with pixel-level class annotations. However, this approach introduces challenges arising from data collection and ground truth generation that quickly become costly and error-prone since they must be performed in multiple settings and for thousands of classes. To overcome these challenges, the paper presents a weakly supervised methodology for training food image classification and semantic segmentation models without relying on pixel-level annotations. The proposed methodology is based on a multiple instance learning approach in combination with an attention-based mechanism. At test time, the models are used for classification and, concurrently, the attention mechanism generates semantic heat maps which are used for food class segmentation. In the paper, we conduct experiments on two meta-classes within the FoodSeg103 data set to verify the feasibility of the proposed approach and we explore the functioning properties of the attention mechanism.
<div id='section'>Paperid: <span id='pid'>1049, <a href='https://arxiv.org/pdf/2307.02041.pdf' target='_blank'>https://arxiv.org/pdf/2307.02041.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Fu, Junyu Gao, Changsheng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.02041">Multimodal Imbalance-Aware Gradient Modulation for Weakly-supervised Audio-Visual Video Parsing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised audio-visual video parsing (WS-AVVP) aims to localize the temporal extents of audio, visual and audio-visual event instances as well as identify the corresponding event categories with only video-level category labels for training. Most previous methods pay much attention to refining the supervision for each modality or extracting fruitful cross-modality information for more reliable feature learning. None of them have noticed the imbalanced feature learning between different modalities in the task. In this paper, to balance the feature learning processes of different modalities, a dynamic gradient modulation (DGM) mechanism is explored, where a novel and effective metric function is designed to measure the imbalanced feature learning between audio and visual modalities. Furthermore, principle analysis indicates that the multimodal confusing calculation will hamper the precise measurement of multimodal imbalanced feature learning, which further weakens the effectiveness of our DGM mechanism. To cope with this issue, a modality-separated decision unit (MSDU) is designed for more precise measurement of imbalanced feature learning between audio and visual modalities. Comprehensive experiments are conducted on public benchmarks and the corresponding experimental results demonstrate the effectiveness of our proposed method.
<div id='section'>Paperid: <span id='pid'>1050, <a href='https://arxiv.org/pdf/2306.06979.pdf' target='_blank'>https://arxiv.org/pdf/2306.06979.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soujanya Narayana, Ibrahim Radwan, Ravikiran Parameshwara, Iman Abbasnejad, Akshay Asthana, Ramanathan Subramanian, Roland Goecke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.06979">A Weakly Supervised Approach to Emotion-change Prediction and Improved Mood Inference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Whilst a majority of affective computing research focuses on inferring emotions, examining mood or understanding the \textit{mood-emotion interplay} has received significantly less attention. Building on prior work, we (a) deduce and incorporate emotion-change ($Î$) information for inferring mood, without resorting to annotated labels, and (b) attempt mood prediction for long duration video clips, in alignment with the characterisation of mood. We generate the emotion-change ($Î$) labels via metric learning from a pre-trained Siamese Network, and use these in addition to mood labels for mood classification. Experiments evaluating \textit{unimodal} (training only using mood labels) vs \textit{multimodal} (training using mood plus $Î$ labels) models show that mood prediction benefits from the incorporation of emotion-change information, emphasising the importance of modelling the mood-emotion interplay for effective mood inference.
<div id='section'>Paperid: <span id='pid'>1051, <a href='https://arxiv.org/pdf/2306.01736.pdf' target='_blank'>https://arxiv.org/pdf/2306.01736.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiuye Gu, Yin Cui, Jonathan Huang, Abdullah Rashwan, Xuan Yang, Xingyi Zhou, Golnaz Ghiasi, Weicheng Kuo, Huizhong Chen, Liang-Chieh Chen, David A Ross
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.01736">DaTaSeg: Taming a Universal Multi-Dataset Multi-Task Segmentation Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Observing the close relationship among panoptic, semantic and instance segmentation tasks, we propose to train a universal multi-dataset multi-task segmentation model: DaTaSeg.We use a shared representation (mask proposals with class predictions) for all tasks. To tackle task discrepancy, we adopt different merge operations and post-processing for different tasks. We also leverage weak-supervision, allowing our segmentation model to benefit from cheaper bounding box annotations. To share knowledge across datasets, we use text embeddings from the same semantic embedding space as classifiers and share all network parameters among datasets. We train DaTaSeg on ADE semantic, COCO panoptic, and Objects365 detection datasets. DaTaSeg improves performance on all datasets, especially small-scale datasets, achieving 54.0 mIoU on ADE semantic and 53.5 PQ on COCO panoptic. DaTaSeg also enables weakly-supervised knowledge transfer on ADE panoptic and Objects365 instance segmentation. Experiments show DaTaSeg scales with the number of training datasets and enables open-vocabulary segmentation through direct transfer. In addition, we annotate an Objects365 instance segmentation set of 1,000 images and will release it as a public benchmark.
<div id='section'>Paperid: <span id='pid'>1052, <a href='https://arxiv.org/pdf/2305.10913.pdf' target='_blank'>https://arxiv.org/pdf/2305.10913.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Davide Rigoni, Luca Parolari, Luciano Serafini, Alessandro Sperduti, Lamberto Ballan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.10913">Weakly-Supervised Visual-Textual Grounding with Semantic Prior Refinement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Using only image-sentence pairs, weakly-supervised visual-textual grounding aims to learn region-phrase correspondences of the respective entity mentions. Compared to the supervised approach, learning is more difficult since bounding boxes and textual phrases correspondences are unavailable. In light of this, we propose the Semantic Prior Refinement Model (SPRM), whose predictions are obtained by combining the output of two main modules. The first untrained module aims to return a rough alignment between textual phrases and bounding boxes. The second trained module is composed of two sub-components that refine the rough alignment to improve the accuracy of the final phrase-bounding box alignments. The model is trained to maximize the multimodal similarity between an image and a sentence, while minimizing the multimodal similarity of the same sentence and a new unrelated image, carefully selected to help the most during training. Our approach shows state-of-the-art results on two popular datasets, Flickr30k Entities and ReferIt, shining especially on ReferIt with a 9.6% absolute improvement. Moreover, thanks to the untrained component, it reaches competitive performances just using a small fraction of training examples.
<div id='section'>Paperid: <span id='pid'>1053, <a href='https://arxiv.org/pdf/2303.07517.pdf' target='_blank'>https://arxiv.org/pdf/2303.07517.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanxue Gu, Hongyu He, Roy Colglazier, Jordan Axelrod, Robert French, Maciej A Mazurowski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.07517">SuperMask: Generating High-resolution object masks from multi-view, unaligned low-resolution MRIs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Three-dimensional segmentation in magnetic resonance images (MRI), which reflects the true shape of the objects, is challenging since high-resolution isotropic MRIs are rare and typical MRIs are anisotropic, with the out-of-plane dimension having a much lower resolution. A potential remedy to this issue lies in the fact that often multiple sequences are acquired on different planes. However, in practice, these sequences are not orthogonal to each other, limiting the applicability of many previous solutions to reconstruct higher-resolution images from multiple lower-resolution ones. We propose a weakly-supervised deep learning-based solution to generating high-resolution masks from multiple low-resolution images. Our method combines segmentation and unsupervised registration networks by introducing two new regularizations to make registration and segmentation reinforce each other. Finally, we introduce a multi-view fusion method to generate high-resolution target object masks. The experimental results on two datasets show the superiority of our methods. Importantly, the advantage of not using high-resolution images in the training process makes our method applicable to a wide variety of MRI segmentation tasks.
<div id='section'>Paperid: <span id='pid'>1054, <a href='https://arxiv.org/pdf/2303.05678.pdf' target='_blank'>https://arxiv.org/pdf/2303.05678.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifei Xin, Dongchao Yang, Fan Cui, Yujun Wang, Yuexian Zou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.05678">Improving Weakly Supervised Sound Event Detection with Causal Intervention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing weakly supervised sound event detection (WSSED) work has not explored both types of co-occurrences simultaneously, i.e., some sound events often co-occur, and their occurrences are usually accompanied by specific background sounds, so they would be inevitably entangled, causing misclassification and biased localization results with only clip-level supervision. To tackle this issue, we first establish a structural causal model (SCM) to reveal that the context is the main cause of co-occurrence confounders that mislead the model to learn spurious correlations between frames and clip-level labels. Based on the causal analysis, we propose a causal intervention (CI) method for WSSED to remove the negative impact of co-occurrence confounders by iteratively accumulating every possible context of each class and then re-projecting the contexts to the frame-level features for making the event boundary clearer. Experiments show that our method effectively improves the performance on multiple datasets and can generalize to various baseline models.
<div id='section'>Paperid: <span id='pid'>1055, <a href='https://arxiv.org/pdf/2303.01313.pdf' target='_blank'>https://arxiv.org/pdf/2303.01313.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bo Wan, Yongfei Liu, Desen Zhou, Tinne Tuytelaars, Xuming He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.01313">Weakly-supervised HOI Detection via Prior-guided Bi-level Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human object interaction (HOI) detection plays a crucial role in human-centric scene understanding and serves as a fundamental building-block for many vision tasks. One generalizable and scalable strategy for HOI detection is to use weak supervision, learning from image-level annotations only. This is inherently challenging due to ambiguous human-object associations, large search space of detecting HOIs and highly noisy training signal. A promising strategy to address those challenges is to exploit knowledge from large-scale pretrained models (e.g., CLIP), but a direct knowledge distillation strategy~\citep{liao2022gen} does not perform well on the weakly-supervised setting. In contrast, we develop a CLIP-guided HOI representation capable of incorporating the prior knowledge at both image level and HOI instance level, and adopt a self-taught mechanism to prune incorrect human-object associations. Experimental results on HICO-DET and V-COCO show that our method outperforms the previous works by a sizable margin, showing the efficacy of our HOI representation.
<div id='section'>Paperid: <span id='pid'>1056, <a href='https://arxiv.org/pdf/2302.03115.pdf' target='_blank'>https://arxiv.org/pdf/2302.03115.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Robert Istvan Busa-Fekete, Heejin Choi, Travis Dick, Claudio Gentile, Andres Munoz medina
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.03115">Easy Learning from Label Proportions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider the problem of Learning from Label Proportions (LLP), a weakly supervised classification setup where instances are grouped into "bags", and only the frequency of class labels at each bag is available. Albeit, the objective of the learner is to achieve low task loss at an individual instance level. Here we propose Easyllp: a flexible and simple-to-implement debiasing approach based on aggregate labels, which operates on arbitrary loss functions. Our technique allows us to accurately estimate the expected loss of an arbitrary model at an individual level. We showcase the flexibility of our approach by applying it to popular learning frameworks, like Empirical Risk Minimization (ERM) and Stochastic Gradient Descent (SGD) with provable guarantees on instance level performance. More concretely, we exhibit a variance reduction technique that makes the quality of LLP learning deteriorate only by a factor of k (k being bag size) in both ERM and SGD setups, as compared to full supervision. Finally, we validate our theoretical results on multiple datasets demonstrating our algorithm performs as well or better than previous LLP approaches in spite of its simplicity.
<div id='section'>Paperid: <span id='pid'>1057, <a href='https://arxiv.org/pdf/2302.01653.pdf' target='_blank'>https://arxiv.org/pdf/2302.01653.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marco Bertolini, Van-Khoa Le, Jake Pencharz, Andreas Poehlmann, Djork-ArnÃ© Clevert, Santiago Villalba, Floriane Montanari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.01653">From slides (through tiles) to pixels: an explainability framework for weakly supervised models in pre-clinical pathology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In pre-clinical pathology, there is a paradox between the abundance of raw data (whole slide images from many organs of many individual animals) and the lack of pixel-level slide annotations done by pathologists. Due to time constraints and requirements from regulatory authorities, diagnoses are instead stored as slide labels. Weakly supervised training is designed to take advantage of those data, and the trained models can be used by pathologists to rank slides by their probability of containing a given lesion of interest. In this work, we propose a novel contextualized eXplainable AI (XAI) framework and its application to deep learning models trained on Whole Slide Images (WSIs) in Digital Pathology. Specifically, we apply our methods to a multi-instance-learning (MIL) model, which is trained solely on slide-level labels, without the need for pixel-level annotations. We validate quantitatively our methods by quantifying the agreements of our explanations' heatmaps with pathologists' annotations, as well as with predictions from a segmentation model trained on such annotations. We demonstrate the stability of the explanations with respect to input shifts, and the fidelity with respect to increased model performance. We quantitatively evaluate the correlation between available pixel-wise annotations and explainability heatmaps. We show that the explanations on important tiles of the whole slide correlate with tissue changes between healthy regions and lesions, but do not exactly behave like a human annotator. This result is coherent with the model training strategy.
<div id='section'>Paperid: <span id='pid'>1058, <a href='https://arxiv.org/pdf/2212.02090.pdf' target='_blank'>https://arxiv.org/pdf/2212.02090.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junhyun Nam, Sangwoo Mo, Jaeho Lee, Jinwoo Shin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.02090">Breaking the Spurious Causality of Conditional Generation via Fairness Intervention with Corrective Sampling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To capture the relationship between samples and labels, conditional generative models often inherit spurious correlations from the training dataset. This can result in label-conditional distributions that are imbalanced with respect to another latent attribute. To mitigate this issue, which we call spurious causality of conditional generation, we propose a general two-step strategy. (a) Fairness Intervention (FI): emphasize the minority samples that are hard to generate due to the spurious correlation in the training dataset. (b) Corrective Sampling (CS): explicitly filter the generated samples and ensure that they follow the desired latent attribute distribution. We have designed the fairness intervention to work for various degrees of supervision on the spurious attribute, including unsupervised, weakly-supervised, and semi-supervised scenarios. Our experimental results demonstrate that FICS can effectively resolve spurious causality of conditional generation across various datasets.
<div id='section'>Paperid: <span id='pid'>1059, <a href='https://arxiv.org/pdf/2210.05663.pdf' target='_blank'>https://arxiv.org/pdf/2210.05663.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nur Muhammad Mahi Shafiullah, Chris Paxton, Lerrel Pinto, Soumith Chintala, Arthur Szlam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.05663">CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose CLIP-Fields, an implicit scene model that can be used for a variety of tasks, such as segmentation, instance identification, semantic search over space, and view localization. CLIP-Fields learns a mapping from spatial locations to semantic embedding vectors. Importantly, we show that this mapping can be trained with supervision coming only from web-image and web-text trained models such as CLIP, Detic, and Sentence-BERT; and thus uses no direct human supervision. When compared to baselines like Mask-RCNN, our method outperforms on few-shot instance identification or semantic segmentation on the HM3D dataset with only a fraction of the examples. Finally, we show that using CLIP-Fields as a scene memory, robots can perform semantic navigation in real-world environments. Our code and demonstration videos are available here: https://mahis.life/clip-fields
<div id='section'>Paperid: <span id='pid'>1060, <a href='https://arxiv.org/pdf/2210.04135.pdf' target='_blank'>https://arxiv.org/pdf/2210.04135.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shraman Pramanick, Li Jing, Sayan Nag, Jiachen Zhu, Hardik Shah, Yann LeCun, Rama Chellappa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.04135">VoLTA: Vision-Language Transformer with Weakly-Supervised Local-Feature Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language pre-training (VLP) has recently proven highly effective for various uni- and multi-modal downstream applications. However, most existing end-to-end VLP methods use high-resolution image-text box data to perform well on fine-grained region-level tasks, such as object detection, segmentation, and referring expression comprehension. Unfortunately, such high-resolution images with accurate bounding box annotations are expensive to collect and use for supervision at scale. In this work, we propose VoLTA (Vision-Language Transformer with weakly-supervised local-feature Alignment), a new VLP paradigm that only utilizes image-caption data but achieves fine-grained region-level image understanding, eliminating the use of expensive box annotations. VoLTA adopts graph optimal transport-based weakly-supervised alignment on local image patches and text tokens to germinate an explicit, self-normalized, and interpretable low-level matching criterion. In addition, VoLTA pushes multi-modal fusion deep into the uni-modal backbones during pre-training and removes fusion-specific transformer layers, further reducing memory requirements. Extensive experiments on a wide range of vision- and vision-language downstream tasks demonstrate the effectiveness of VoLTA on fine-grained applications without compromising the coarse-grained downstream performance, often outperforming methods using significantly more caption and box annotations.
<div id='section'>Paperid: <span id='pid'>1061, <a href='https://arxiv.org/pdf/2206.01646.pdf' target='_blank'>https://arxiv.org/pdf/2206.01646.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benoit Dufumier, Carlo Alberto Barbano, Robin Louiset, Edouard Duchesnay, Pietro Gori
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.01646">Integrating Prior Knowledge in Contrastive Learning with Kernel</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data augmentation is a crucial component in unsupervised contrastive learning (CL). It determines how positive samples are defined and, ultimately, the quality of the learned representation. In this work, we open the door to new perspectives for CL by integrating prior knowledge, given either by generative models -- viewed as prior representations -- or weak attributes in the positive and negative sampling. To this end, we use kernel theory to propose a novel loss, called decoupled uniformity, that i) allows the integration of prior knowledge and ii) removes the negative-positive coupling in the original InfoNCE loss. We draw a connection between contrastive learning and conditional mean embedding theory to derive tight bounds on the downstream classification loss. In an unsupervised setting, we empirically demonstrate that CL benefits from generative models to improve its representation both on natural and medical images. In a weakly supervised scenario, our framework outperforms other unconditional and conditional CL approaches.
<div id='section'>Paperid: <span id='pid'>1062, <a href='https://arxiv.org/pdf/2512.06840.pdf' target='_blank'>https://arxiv.org/pdf/2512.06840.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Satoshi Hashimoto, Tatsuya Konishi, Tomoya Kaichi, Kazunori Matsumoto, Mori Kurokawa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.06840">CADE: Continual Weakly-supervised Video Anomaly Detection with Ensembles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video anomaly detection (VAD) has long been studied as a crucial problem in public security and crime prevention. In recent years, weakly-supervised VAD (WVAD) have attracted considerable attention due to their easy annotation process and promising research results. While existing WVAD methods tackle mainly on static datasets, the possibility that the domain of data can vary has been neglected. To adapt such domain-shift, the continual learning (CL) perspective is required because otherwise additional training only with new coming data could easily cause performance degradation for previous data, i.e., forgetting. Therefore, we propose a brand-new approach, called Continual Anomaly Detection with Ensembles (CADE) that is the first work combining CL and WVAD viewpoints. Specifically, CADE uses the Dual-Generator(DG) to address data imbalance and label uncertainty in WVAD. We also found that forgetting exacerbates the "incompleteness'' where the model becomes biased towards certain anomaly modes, leading to missed detections of various anomalies. To address this, we propose to ensemble Multi-Discriminator (MD) that capture missed anomalies in past scenes due to forgetting, using multiple models. Extensive experiments show that CADE significantly outperforms existing VAD methods on the common multi-scene VAD datasets, such as ShanghaiTech and Charlotte Anomaly datasets.
<div id='section'>Paperid: <span id='pid'>1063, <a href='https://arxiv.org/pdf/2511.19765.pdf' target='_blank'>https://arxiv.org/pdf/2511.19765.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Torabi, Sanjog Gaihre, Yaqoob Majeed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19765">Lightweight Transformer Framework for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation (WSSS) must learn dense masks from noisy, under-specified cues. We revisit the SegFormer decoder and show that three small, synergistic changes make weak supervision markedly more effective-without altering the MiT backbone or relying on heavy post-processing. Our method, CrispFormer, augments the decoder with: (1) a boundary branch that supervises thin object contours using a lightweight edge head and a boundary-aware loss; (2) an uncertainty-guided refiner that predicts per-pixel aleatoric uncertainty and uses it to weight losses and gate a residual correction of the segmentation logits; and (3) a dynamic multi-scale fusion layer that replaces static concatenation with spatial softmax gating over multi-resolution features, optionally modulated by uncertainty. The result is a single-pass model that preserves crisp boundaries, selects appropriate scales per location, and resists label noise from weak cues. Integrated into a standard WSSS pipeline (seed, student, and EMA relabeling), CrispFormer consistently improves boundary F-score, small-object recall, and mIoU over SegFormer baselines trained on the same seeds, while adding minimal compute. Our decoder-centric formulation is simple to implement, broadly compatible with existing SegFormer variants, and offers a reproducible path to higher-fidelity masks from image-level supervision.
<div id='section'>Paperid: <span id='pid'>1064, <a href='https://arxiv.org/pdf/2511.10958.pdf' target='_blank'>https://arxiv.org/pdf/2511.10958.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gunho Jung, Heejo Kong, Seong-Whan Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.10958">Text-guided Weakly Supervised Framework for Dynamic Facial Expression Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic facial expression recognition (DFER) aims to identify emotional states by modeling the temporal changes in facial movements across video sequences. A key challenge in DFER is the many-to-one labeling problem, where a video composed of numerous frames is assigned a single emotion label. A common strategy to mitigate this issue is to formulate DFER as a Multiple Instance Learning (MIL) problem. However, MIL-based approaches inherently suffer from the visual diversity of emotional expressions and the complexity of temporal dynamics. To address this challenge, we propose TG-DFER, a text-guided weakly supervised framework that enhances MIL-based DFER by incorporating semantic guidance and coherent temporal modeling. We incorporate a vision-language pre-trained (VLP) model is integrated to provide semantic guidance through fine-grained textual descriptions of emotional context. Furthermore, we introduce visual prompts, which align enriched textual emotion labels with visual instance features, enabling fine-grained reasoning and frame-level relevance estimation. In addition, a multi-grained temporal network is designed to jointly capture short-term facial dynamics and long-range emotional flow, ensuring coherent affective understanding across time. Extensive results demonstrate that TG-DFER achieves improved generalization, interpretability, and temporal sensitivity under weak supervision.
<div id='section'>Paperid: <span id='pid'>1065, <a href='https://arxiv.org/pdf/2510.11047.pdf' target='_blank'>https://arxiv.org/pdf/2510.11047.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nivea Roy, Son Tran, Atul Sajjanhar, K. Devaraja, Prakashini Koteshwara, Yong Xiang, Divya Rao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.11047">Benchmarking Deep Learning Models for Laryngeal Cancer Staging Using the LaryngealCT Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Laryngeal cancer imaging research lacks standardised datasets to enable reproducible deep learning (DL) model development. We present LaryngealCT, a curated benchmark of 1,029 computed tomography (CT) scans aggregated from six collections from The Cancer Imaging Archive (TCIA). Uniform 1 mm isotropic volumes of interest encompassing the larynx were extracted using a weakly supervised parameter search framework validated by clinical experts. 3D DL architectures (3D CNN, ResNet18,50,101, DenseNet121) were benchmarked on (i) early (Tis,T1,T2) vs. advanced (T3,T4) and (ii) T4 vs. non-T4 classification tasks. 3D CNN (AUC-0.881, F1-macro-0.821) and ResNet18 (AUC-0.892, F1-macro-0.646) respectively outperformed the other models in the two tasks. Model explainability assessed using 3D GradCAMs with thyroid cartilage overlays revealed greater peri-cartilage attention in non-T4 cases and focal activations in T4 predictions. Through open-source data, pretrained models, and integrated explainability tools, LaryngealCT offers a reproducible foundation for AI-driven research to support clinical decisions in laryngeal oncology.
<div id='section'>Paperid: <span id='pid'>1066, <a href='https://arxiv.org/pdf/2509.12496.pdf' target='_blank'>https://arxiv.org/pdf/2509.12496.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Torabi, Sanjog Gaihre, MD Mahbubur Rahman, Yaqoob Majeed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12496">Instance-Guided Class Activation Mapping for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Semantic Segmentation (WSSS) addresses the challenge of training segmentation models using only image-level annotations, eliminating the need for expensive pixel-level labeling. While existing methods struggle with precise object boundary localization and often focus only on the most discriminative regions, we propose IG-CAM (Instance-Guided Class Activation Mapping), a novel approach that leverages instance-level cues and influence functions to generate high-quality, boundary-aware localization maps. Our method introduces three key innovations: (1) Instance-Guided Refinement that uses ground truth segmentation masks to guide CAM generation, ensuring complete object coverage rather than just discriminative parts; (2) Influence Function Integration that captures the relationship between training samples and model predictions, leading to more robust feature representations; and (3) Multi-Scale Boundary Enhancement that employs progressive refinement strategies to achieve sharp, precise object boundaries. IG-CAM achieves state-of-the-art performance on the PASCAL VOC 2012 dataset with an mIoU of 82.3% before post-processing, which further improves to 86.6% after applying Conditional Random Field (CRF) refinement, significantly outperforming previous WSSS methods. Our approach demonstrates superior localization accuracy, with complete object coverage and precise boundary delineation, while maintaining computational efficiency. Extensive ablation studies validate the contribution of each component, and qualitative comparisons across 600 diverse images showcase the method's robustness and generalization capability. The results establish IG-CAM as a new benchmark for weakly supervised semantic segmentation, offering a practical solution for scenarios where pixel-level annotations are unavailable or prohibitively expensive.
<div id='section'>Paperid: <span id='pid'>1067, <a href='https://arxiv.org/pdf/2509.10641.pdf' target='_blank'>https://arxiv.org/pdf/2509.10641.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikita Rajaneesh, Thomas Zollo, Richard Zemel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10641">Test-Time Warmup for Multimodal Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal Large Language Models (MLLMs) hold great promise for advanced reasoning at the intersection of text and images, yet they have not fully realized this potential. MLLMs typically integrate an LLM, a vision encoder, and a connector that maps the vision encoder's embeddings into the LLM's text embedding space. Although each component is pretrained on massive datasets with billions of samples, the entire multimodal model is typically trained on only thousands (or a few million) samples, which can result in weak performance on complex reasoning tasks. To address these shortcomings, instead of relying on extensive labeled datasets for fine-tuning, we propose a Test-Time Warmup method that adapts the MLLM per test instance by leveraging data from weakly supervised auxiliary tasks. With our approach, we observe a relative performance improvement of 4.03% on MMMU, 5.28% on VQA-Rad, and 1.63% on GQA on the Llama-Vision-Instruct model. Our method demonstrates that 'warming up' before inference can enhance MLLMs' robustness across diverse reasoning tasks.
<div id='section'>Paperid: <span id='pid'>1068, <a href='https://arxiv.org/pdf/2508.19647.pdf' target='_blank'>https://arxiv.org/pdf/2508.19647.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bikash Kumar Badatya, Vipul Baghel, Ravi Hegde
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19647">UTAL-GNN: Unsupervised Temporal Action Localization using Graph Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fine-grained action localization in untrimmed sports videos presents a significant challenge due to rapid and subtle motion transitions over short durations. Existing supervised and weakly supervised solutions often rely on extensive annotated datasets and high-capacity models, making them computationally intensive and less adaptable to real-world scenarios. In this work, we introduce a lightweight and unsupervised skeleton-based action localization pipeline that leverages spatio-temporal graph neural representations. Our approach pre-trains an Attention-based Spatio-Temporal Graph Convolutional Network (ASTGCN) on a pose-sequence denoising task with blockwise partitions, enabling it to learn intrinsic motion dynamics without any manual labeling. At inference, we define a novel Action Dynamics Metric (ADM), computed directly from low-dimensional ASTGCN embeddings, which detects motion boundaries by identifying inflection points in its curvature profile. Our method achieves a mean Average Precision (mAP) of 82.66% and average localization latency of 29.09 ms on the DSV Diving dataset, matching state-of-the-art supervised performance while maintaining computational efficiency. Furthermore, it generalizes robustly to unseen, in-the-wild diving footage without retraining, demonstrating its practical applicability for lightweight, real-time action analysis systems in embedded or dynamic environments.
<div id='section'>Paperid: <span id='pid'>1069, <a href='https://arxiv.org/pdf/2508.08876.pdf' target='_blank'>https://arxiv.org/pdf/2508.08876.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaiyu Wang, Lin Mu, Zhiyao Yang, Ximing Li, Xiaotang Zhou Wanfu Gao, Huimao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08876">Weakly Supervised Fine-grained Span-Level Framework for Chinese Radiology Report Quality Assurance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quality Assurance (QA) for radiology reports refers to judging whether the junior reports (written by junior doctors) are qualified. The QA scores of one junior report are given by the senior doctor(s) after reviewing the image and junior report. This process requires intensive labor costs for senior doctors. Additionally, the QA scores may be inaccurate for reasons like diagnosis bias, the ability of senior doctors, and so on. To address this issue, we propose a Span-level Quality Assurance EvaluaTOR (Sqator) to mark QA scores automatically. Unlike the common document-level semantic comparison method, we try to analyze the semantic difference by exploring more fine-grained text spans. Specifically, Sqator measures QA scores by measuring the importance of revised spans between junior and senior reports, and outputs the final QA scores by merging all revised span scores. We evaluate Sqator using a collection of 12,013 radiology reports. Experimental results show that Sqator can achieve competitive QA scores. Moreover, the importance scores of revised spans can be also consistent with the judgments of senior doctors.
<div id='section'>Paperid: <span id='pid'>1070, <a href='https://arxiv.org/pdf/2508.06115.pdf' target='_blank'>https://arxiv.org/pdf/2508.06115.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weichen Zhang, Kebin Liu, Fan Dang, Zhui Zhu, Xikai Sun, Yunhao Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06115">SynSeg: Feature Synergy for Multi-Category Contrastive Learning in Open-Vocabulary Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semantic segmentation in open-vocabulary scenarios presents significant challenges due to the wide range and granularity of semantic categories. Existing weakly-supervised methods often rely on category-specific supervision and ill-suited feature construction methods for contrastive learning, leading to semantic misalignment and poor performance. In this work, we propose a novel weakly-supervised approach, SynSeg, to address the challenges. SynSeg performs Multi-Category Contrastive Learning (MCCL) as a stronger training signal with a new feature reconstruction framework named Feature Synergy Structure (FSS). Specifically, MCCL strategy robustly combines both intra- and inter-category alignment and separation in order to make the model learn the knowledge of correlations from different categories within the same image. Moreover, FSS reconstructs discriminative features for contrastive learning through prior fusion and semantic-activation-map enhancement, effectively avoiding the foreground bias introduced by the visual encoder. In general, SynSeg effectively improves the abilities in semantic localization and discrimination under weak supervision. Extensive experiments on benchmarks demonstrate that our method outperforms state-of-the-art (SOTA) performance. For instance, SynSeg achieves higher accuracy than SOTA baselines by 4.5\% on VOC, 8.9\% on Context, 2.6\% on Object and 2.0\% on City.
<div id='section'>Paperid: <span id='pid'>1071, <a href='https://arxiv.org/pdf/2508.05108.pdf' target='_blank'>https://arxiv.org/pdf/2508.05108.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tomoya Tate, Kosuke Sugiyama, Masato Uchida
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05108">Learning from Similarity-Confidence and Confidence-Difference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In practical machine learning applications, it is often challenging to assign accurate labels to data, and increasing the number of labeled instances is often limited. In such cases, Weakly Supervised Learning (WSL), which enables training with incomplete or imprecise supervision, provides a practical and effective solution. However, most existing WSL methods focus on leveraging a single type of weak supervision. In this paper, we propose a novel WSL framework that leverages complementary weak supervision signals from multiple relational perspectives, which can be especially valuable when labeled data is limited. Specifically, we introduce SconfConfDiff Classification, a method that integrates two distinct forms of weaklabels: similarity-confidence and confidence-difference, which are assigned to unlabeled data pairs. To implement this method, we derive two types of unbiased risk estimators for classification: one based on a convex combination of existing estimators, and another newly designed by modeling the interaction between two weak labels. We prove that both estimators achieve optimal convergence rates with respect to estimation error bounds. Furthermore, we introduce a risk correction approach to mitigate overfitting caused by negative empirical risk, and provide theoretical analysis on the robustness of the proposed method against inaccurate class prior probability and label noise. Experimental results demonstrate that the proposed method consistently outperforms existing baselines across a variety of settings.
<div id='section'>Paperid: <span id='pid'>1072, <a href='https://arxiv.org/pdf/2508.00563.pdf' target='_blank'>https://arxiv.org/pdf/2508.00563.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hannah Kniesel, Leon Sick, Tristan Payer, Tim Bergner, Kavitha Shaga Devan, Clarissa Read, Paul Walther, Timo Ropinski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00563">Weakly Supervised Virus Capsid Detection with Image-Level Annotations in Electron Microscopy Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current state-of-the-art methods for object detection rely on annotated bounding boxes of large data sets for training. However, obtaining such annotations is expensive and can require up to hundreds of hours of manual labor. This poses a challenge, especially since such annotations can only be provided by experts, as they require knowledge about the scientific domain. To tackle this challenge, we propose a domain-specific weakly supervised object detection algorithm that only relies on image-level annotations, which are significantly easier to acquire. Our method distills the knowledge of a pre-trained model, on the task of predicting the presence or absence of a virus in an image, to obtain a set of pseudo-labels that can be used to later train a state-of-the-art object detection model. To do so, we use an optimization approach with a shrinking receptive field to extract virus particles directly without specific network architectures. Through a set of extensive studies, we show how the proposed pseudo-labels are easier to obtain, and, more importantly, are able to outperform other existing weak labeling methods, and even ground truth labels, in cases where the time to obtain the annotation is limited.
<div id='section'>Paperid: <span id='pid'>1073, <a href='https://arxiv.org/pdf/2507.02998.pdf' target='_blank'>https://arxiv.org/pdf/2507.02998.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kimberly F. Greco, Zongxin Yang, Mengyan Li, Han Tong, Sara Morini Sweet, Alon Geva, Kenneth D. Mandl, Benjamin A. Raby, Tianxi Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02998">A Weakly Supervised Transformer to Support Rare Disease Diagnosis from Electronic Health Records: Methods and Applications in Rare Pulmonary Disease</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rare diseases affect an estimated 300-400 million people worldwide, yet individual conditions often remain poorly characterized and difficult to diagnose due to their low prevalence and limited clinician familiarity. While computational phenotyping algorithms show promise for automating rare disease detection, their development is hindered by the scarcity of labeled data and biases in existing label sources. Gold-standard labels from registries and expert chart reviews are highly accurate but constrained by selection bias and the cost of manual review. In contrast, labels derived from electronic health records (EHRs) cover a broader range of patients but can introduce substantial noise. To address these challenges, we propose a weakly supervised, transformer-based framework that combines a small set of gold-standard labels with a large volume of iteratively updated silver-standard labels derived from EHR data. This hybrid approach enables the training of a highly accurate and generalizable phenotyping model that scales rare disease detection beyond the scope of individual clinical expertise. Our method is initialized by learning embeddings of medical concepts based on their semantic meaning or co-occurrence patterns in EHRs, which are then refined and aggregated into patient-level representations via a multi-layer transformer architecture. Using two rare pulmonary diseases as a case study, we validate our model on EHR data from Boston Children's Hospital. Our framework demonstrates notable improvements in phenotype classification, identification of clinically meaningful subphenotypes through patient clustering, and prediction of disease progression compared to baseline methods. These results highlight the potential of our approach to enable scalable identification and stratification of rare disease patients for clinical care and research applications.
<div id='section'>Paperid: <span id='pid'>1074, <a href='https://arxiv.org/pdf/2504.09582.pdf' target='_blank'>https://arxiv.org/pdf/2504.09582.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christos Theodoropoulos, Andrei Catalin Coman, James Henderson, Marie-Francine Moens
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09582">Reduction of Supervision for Biomedical Knowledge Discovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Knowledge discovery is hindered by the increasing volume of publications and the scarcity of extensive annotated data. To tackle the challenge of information overload, it is essential to employ automated methods for knowledge extraction and processing. Finding the right balance between the level of supervision and the effectiveness of models poses a significant challenge. While supervised techniques generally result in better performance, they have the major drawback of demanding labeled data. This requirement is labor-intensive and time-consuming and hinders scalability when exploring new domains. In this context, our study addresses the challenge of identifying semantic relationships between biomedical entities (e.g., diseases, proteins) in unstructured text while minimizing dependency on supervision. We introduce a suite of unsupervised algorithms based on dependency trees and attention mechanisms and employ a range of pointwise binary classification methods. Transitioning from weakly supervised to fully unsupervised settings, we assess the methods' ability to learn from data with noisy labels. The evaluation on biomedical benchmark datasets explores the effectiveness of the methods. Our approach tackles a central issue in knowledge discovery: balancing performance with minimal supervision. By gradually decreasing supervision, we assess the robustness of pointwise binary classification techniques in handling noisy labels, revealing their capability to shift from weakly supervised to entirely unsupervised scenarios. Comprehensive benchmarking offers insights into the effectiveness of these techniques, suggesting an encouraging direction toward adaptable knowledge discovery systems, representing progress in creating data-efficient methodologies for extracting useful insights when annotated data is limited.
<div id='section'>Paperid: <span id='pid'>1075, <a href='https://arxiv.org/pdf/2503.18725.pdf' target='_blank'>https://arxiv.org/pdf/2503.18725.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zimin Xia, Alexandre Alahi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18725">FG$^2$: Fine-Grained Cross-View Localization by Fine-Grained Feature Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel fine-grained cross-view localization method that estimates the 3 Degrees of Freedom pose of a ground-level image in an aerial image of the surroundings by matching fine-grained features between the two images. The pose is estimated by aligning a point plane generated from the ground image with a point plane sampled from the aerial image. To generate the ground points, we first map ground image features to a 3D point cloud. Our method then learns to select features along the height dimension to pool the 3D points to a Bird's-Eye-View (BEV) plane. This selection enables us to trace which feature in the ground image contributes to the BEV representation. Next, we sample a set of sparse matches from computed point correspondences between the two point planes and compute their relative pose using Procrustes alignment. Compared to the previous state-of-the-art, our method reduces the mean localization error by 28% on the VIGOR cross-area test set. Qualitative results show that our method learns semantically consistent matches across ground and aerial views through weakly supervised learning from the camera pose.
<div id='section'>Paperid: <span id='pid'>1076, <a href='https://arxiv.org/pdf/2503.03042.pdf' target='_blank'>https://arxiv.org/pdf/2503.03042.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Han, Soumava Kumar Roy, Mehrtash Harandi, Lars Petersson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.03042">Learning from Noisy Labels with Contrastive Co-Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning with noisy labels is an interesting challenge in weakly supervised learning. Despite their significant learning capacity, CNNs have a tendency to overfit in the presence of samples with noisy labels. Alleviating this issue, the well known Co-Training framework is used as a fundamental basis for our work. In this paper, we introduce a Contrastive Co-Transformer framework, which is simple and fast, yet able to improve the performance by a large margin compared to the state-of-the-art approaches. We argue the robustness of transformers when dealing with label noise. Our Contrastive Co-Transformer approach is able to utilize all samples in the dataset, irrespective of whether they are clean or noisy. Transformers are trained by a combination of contrastive loss and classification loss. Extensive experimental results on corrupted data from six standard benchmark datasets including Clothing1M, demonstrate that our Contrastive Co-Transformer is superior to existing state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1077, <a href='https://arxiv.org/pdf/2412.19504.pdf' target='_blank'>https://arxiv.org/pdf/2412.19504.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Li, Bo Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19504">Hear the Scene: Audio-Enhanced Text Spotting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in scene text spotting have focused on end-to-end methodologies that heavily rely on precise location annotations, which are often costly and labor-intensive to procure. In this study, we introduce an innovative approach that leverages only transcription annotations for training text spotting models, substantially reducing the dependency on elaborate annotation processes. Our methodology employs a query-based paradigm that facilitates the learning of implicit location features through the interaction between text queries and image embeddings. These features are later refined during the text recognition phase using an attention activation map. Addressing the challenges associated with training a weakly-supervised model from scratch, we implement a circular curriculum learning strategy to enhance model convergence. Additionally, we introduce a coarse-to-fine cross-attention localization mechanism for more accurate text instance localization. Notably, our framework supports audio-based annotation, which significantly diminishes annotation time and provides an inclusive alternative for individuals with disabilities. Our approach achieves competitive performance against existing benchmarks, demonstrating that high accuracy in text spotting can be attained without extensive location annotations.
<div id='section'>Paperid: <span id='pid'>1078, <a href='https://arxiv.org/pdf/2410.15051.pdf' target='_blank'>https://arxiv.org/pdf/2410.15051.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vittorio Torri, Elisa Barbieri, Anna Cantarutti, Carlo Giaquinto, Francesca Ieva
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15051">Weakly-supervised diagnosis identification from Italian discharge letters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Objective: Recognizing diseases from discharge letters is crucial for cohort selection and epidemiological analyses, as this is the only type of data consistently produced across hospitals. This is a classic document classification problem, typically requiring supervised learning. However, manual annotation of large datasets of discharge letters is uncommon since it is extremely time-consuming. We propose a novel weakly-supervised pipeline to recognize diseases from Italian discharge letters. Methods: Our Natural Language Processing pipeline is based on a fine-tuned version of the Italian Umberto model. The pipeline extracts diagnosis-related sentences from a subset of letters and applies a two-level clustering using the embeddings generated by the fine-tuned Umberto model. These clusters are summarized and those mapped to the diseases of interest are selected as weak labels. Finally, the same BERT-based model is trained using these weak labels to detect the targeted diseases. Results: A case study related to the identification of bronchiolitis with 33'176 Italian discharge letters from 44 hospitals in the Veneto Region shows the potential of our method, with an AUC of 77.7 % and an F1-Score of 75.1 % on manually annotated labels, improving compared to other non-supervised methods and with a limited loss compared to fully supervised methods. Results are robust to the cluster selection and the identified clusters highlight the potential to recognize a variety of diseases. Conclusions: This study demonstrates the feasibility of diagnosis identification from Italian discharge letters in the absence of labelled data. Our pipeline showed strong performance and robustness, and its flexibility allows for easy adaptation to various diseases. This approach offers a scalable solution for clinical text classification, reducing the need for manual annotation while maintaining good accuracy.
<div id='section'>Paperid: <span id='pid'>1079, <a href='https://arxiv.org/pdf/2409.06471.pdf' target='_blank'>https://arxiv.org/pdf/2409.06471.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujiao Shi, Hongdong Li, Akhil Perincherry, Ankit Vora
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.06471">Weakly-supervised Camera Localization by Ground-to-satellite Image Registration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ground-to-satellite image matching/retrieval was initially proposed for city-scale ground camera localization. This work addresses the problem of improving camera pose accuracy by ground-to-satellite image matching after a coarse location and orientation have been obtained, either from the city-scale retrieval or from consumer-level GPS and compass sensors. Existing learning-based methods for solving this task require accurate GPS labels of ground images for network training. However, obtaining such accurate GPS labels is difficult, often requiring an expensive {\color{black}Real Time Kinematics (RTK)} setup and suffering from signal occlusion, multi-path signal disruptions, \etc. To alleviate this issue, this paper proposes a weakly supervised learning strategy for ground-to-satellite image registration when only noisy pose labels for ground images are available for network training. It derives positive and negative satellite images for each ground image and leverages contrastive learning to learn feature representations for ground and satellite images useful for translation estimation. We also propose a self-supervision strategy for cross-view image relative rotation estimation, which trains the network by creating pseudo query and reference image pairs. Experimental results show that our weakly supervised learning strategy achieves the best performance on cross-area evaluation compared to recent state-of-the-art methods that are reliant on accurate pose labels for supervision.
<div id='section'>Paperid: <span id='pid'>1080, <a href='https://arxiv.org/pdf/2408.04813.pdf' target='_blank'>https://arxiv.org/pdf/2408.04813.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingfan Ma, Xiaoyuan Luo, Mingzhi Yuan, Xinrong Chen, Manning Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.04813">Rethinking Multiple Instance Learning: Developing an Instance-Level Classifier via Weakly-Supervised Self-Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple instance learning (MIL) problem is currently solved from either bag-classification or instance-classification perspective, both of which ignore important information contained in some instances and result in limited performance. For example, existing methods often face difficulty in learning hard positive instances. In this paper, we formulate MIL as a semi-supervised instance classification problem, so that all the labeled and unlabeled instances can be fully utilized to train a better classifier. The difficulty in this formulation is that all the labeled instances are negative in MIL, and traditional self-training techniques used in semi-supervised learning tend to degenerate in generating pseudo labels for the unlabeled instances in this scenario. To resolve this problem, we propose a weakly-supervised self-training method, in which we utilize the positive bag labels to construct a global constraint and a local constraint on the pseudo labels to prevent them from degenerating and force the classifier to learn hard positive instances. It is worth noting that easy positive instances are instances are far from the decision boundary in the classification process, while hard positive instances are those close to the decision boundary. Through iterative optimization, the pseudo labels can gradually approach the true labels. Extensive experiments on two MNIST synthetic datasets, five traditional MIL benchmark datasets and two histopathology whole slide image datasets show that our method achieved new SOTA performance on all of them. The code will be publicly available.
<div id='section'>Paperid: <span id='pid'>1081, <a href='https://arxiv.org/pdf/2407.12814.pdf' target='_blank'>https://arxiv.org/pdf/2407.12814.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Priyanshu Priya, Mauajama Firdaus, Asif Ekbal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.12814">Computational Politeness in Natural Language Processing: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computational approach to politeness is the task of automatically predicting and generating politeness in text. This is a pivotal task for conversational analysis, given the ubiquity and challenges of politeness in interactions. The computational approach to politeness has witnessed great interest from the conversational analysis community. This article is a compilation of past works in computational politeness in natural language processing. We view four milestones in the research so far, viz. supervised and weakly-supervised feature extraction to identify and induce politeness in a given text, incorporation of context beyond the target text, study of politeness across different social factors, and study the relationship between politeness and various sociolinguistic cues. In this article, we describe the datasets, approaches, trends, and issues in computational politeness research. We also discuss representative performance values and provide pointers to future works, as given in the prior works. In terms of resources to understand the state-of-the-art, this survey presents several valuable illustrations, most prominently, a table summarizing the past papers along different dimensions, such as the types of features, annotation techniques, and datasets used.
<div id='section'>Paperid: <span id='pid'>1082, <a href='https://arxiv.org/pdf/2406.17469.pdf' target='_blank'>https://arxiv.org/pdf/2406.17469.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaichen Chi, Wei Jing, Junjie Li, Qiang Li, Qi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.17469">Cross-Modal Spherical Aggregation for Weakly Supervised Remote Sensing Shadow Removal</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Remote sensing shadow removal, which aims to recover contaminated surface information, is tricky since shadows typically display overwhelmingly low illumination intensities. In contrast, the infrared image is robust toward significant light changes, providing visual clues complementary to the visible image. Nevertheless, the existing methods ignore the collaboration between heterogeneous modalities, leading to undesired quality degradation. To fill this gap, we propose a weakly supervised shadow removal network with a spherical feature space, dubbed S2-ShadowNet, to explore the best of both worlds for visible and infrared modalities. Specifically, we employ a modal translation (visible-to-infrared) model to learn the cross-domain mapping, thus generating realistic infrared samples. Then, Swin Transformer is utilized to extract strong representational visible/infrared features. Simultaneously, the extracted features are mapped to the smooth spherical manifold, which alleviates the domain shift through regularization. Well-designed similarity loss and orthogonality loss are embedded into the spherical space, prompting the separation of private visible/infrared features and the alignment of shared visible/infrared features through constraints on both representation content and orientation. Such a manner encourages implicit reciprocity between modalities, thus providing a novel insight into shadow removal. Notably, ground truth is not available in practice, thus S2-ShadowNet is trained by cropping shadow and shadow-free patches from the shadow image itself, avoiding stereotypical and strict pair data acquisition. More importantly, we contribute a large-scale weakly supervised shadow removal benchmark, including 4000 shadow images with corresponding shadow masks.
<div id='section'>Paperid: <span id='pid'>1083, <a href='https://arxiv.org/pdf/2406.15755.pdf' target='_blank'>https://arxiv.org/pdf/2406.15755.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xu Yin, Woobin Im, Dongbo Min, Yuchi Huo, Fei Pan, Sung-Eui Yoon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.15755">Fine-grained Background Representation for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating reliable pseudo masks from image-level labels is challenging in the weakly supervised semantic segmentation (WSSS) task due to the lack of spatial information. Prevalent class activation map (CAM)-based solutions are challenged to discriminate the foreground (FG) objects from the suspicious background (BG) pixels (a.k.a. co-occurring) and learn the integral object regions. This paper proposes a simple fine-grained background representation (FBR) method to discover and represent diverse BG semantics and address the co-occurring problems. We abandon using the class prototype or pixel-level features for BG representation. Instead, we develop a novel primitive, negative region of interest (NROI), to capture the fine-grained BG semantic information and conduct the pixel-to-NROI contrast to distinguish the confusing BG pixels. We also present an active sampling strategy to mine the FG negatives on-the-fly, enabling efficient pixel-to-pixel intra-foreground contrastive learning to activate the entire object region. Thanks to the simplicity of design and convenience in use, our proposed method can be seamlessly plugged into various models, yielding new state-of-the-art results under various WSSS settings across benchmarks. Leveraging solely image-level (I) labels as supervision, our method achieves 73.2 mIoU and 45.6 mIoU segmentation results on Pascal Voc and MS COCO test sets, respectively. Furthermore, by incorporating saliency maps as an additional supervision signal (I+S), we attain 74.9 mIoU on Pascal Voc test set. Concurrently, our FBR approach demonstrates meaningful performance gains in weakly-supervised instance segmentation (WSIS) tasks, showcasing its robustness and strong generalization capabilities across diverse domains.
<div id='section'>Paperid: <span id='pid'>1084, <a href='https://arxiv.org/pdf/2406.14745.pdf' target='_blank'>https://arxiv.org/pdf/2406.14745.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sefika Efeoglu, Adrian Paschke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.14745">Relation Extraction with Fine-Tuned Large Language Models in Retrieval Augmented Generation Frameworks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Information Extraction (IE) is crucial for converting unstructured data into structured formats like Knowledge Graphs (KGs). A key task within IE is Relation Extraction (RE), which identifies relationships between entities in text. Various RE methods exist, including supervised, unsupervised, weakly supervised, and rule-based approaches. Recent studies leveraging pre-trained language models (PLMs) have shown significant success in this area. In the current era dominated by Large Language Models (LLMs), fine-tuning these models can overcome limitations associated with zero-shot LLM prompting-based RE methods, especially regarding domain adaptation challenges and identifying implicit relations between entities in sentences. These implicit relations, which cannot be easily extracted from a sentence's dependency tree, require logical inference for accurate identification. This work explores the performance of fine-tuned LLMs and their integration into the Retrieval Augmented-based (RAG) RE approach to address the challenges of identifying implicit relations at the sentence level, particularly when LLMs act as generators within the RAG framework. Empirical evaluations on the TACRED, TACRED-Revisited (TACREV), Re-TACRED, and SemEVAL datasets show significant performance improvements with fine-tuned LLMs, including Llama2-7B, Mistral-7B, and T5 (Large). Notably, our approach achieves substantial gains on SemEVAL, where implicit relations are common, surpassing previous results on this dataset. Additionally, our method outperforms previous works on TACRED, TACREV, and Re-TACRED, demonstrating exceptional performance across diverse evaluation scenarios.
<div id='section'>Paperid: <span id='pid'>1085, <a href='https://arxiv.org/pdf/2405.14239.pdf' target='_blank'>https://arxiv.org/pdf/2405.14239.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammed Baharoon, Jonathan Klein, Dominik L. Michels
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14239">Harmony: A Joint Self-Supervised and Weakly-Supervised Framework for Learning General Purpose Visual Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language contrastive learning frameworks such as CLIP enable learning representations from natural language supervision and provide strong zero-shot classification capabilities. However, due to the nature of the supervisory signal in these paradigms, they lack the ability to learn localized features, leading to degraded performance on dense prediction tasks such as segmentation and detection. On the other hand, self-supervised learning methods have shown the ability to learn granular representations, complementing the high-level features in vision-language training. In this work, we present Harmony, a framework that combines vision-language training with discriminative and generative self-supervision to learn visual features that can be generalized across different downstream vision tasks. Our framework is specifically designed to work on web-scraped data by not relying on negative examples in the self-supervised learning path and addressing the one-to-one correspondence issue using soft CLIP targets generated by an EMA model. Moreover, Harmony optimizes for five different objectives simultaneously, efficiently utilizing the supervision in each data example, making it even more suited in data-constrained settings. We comprehensively evaluate Harmony across various vision downstream tasks and find that it significantly outperforms the baseline CLIP and outperforms the previously leading joint self- and weakly supervised methods, SLIP, MaskCLIP, and DetailCLIP.
<div id='section'>Paperid: <span id='pid'>1086, <a href='https://arxiv.org/pdf/2405.09697.pdf' target='_blank'>https://arxiv.org/pdf/2405.09697.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jadie Adams, Krithika Iyer, Shireen Elhabian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.09697">Weakly Supervised Bayesian Shape Modeling from Unsegmented Medical Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anatomical shape analysis plays a pivotal role in clinical research and hypothesis testing, where the relationship between form and function is paramount. Correspondence-based statistical shape modeling (SSM) facilitates population-level morphometrics but requires a cumbersome, potentially bias-inducing construction pipeline. Recent advancements in deep learning have streamlined this process in inference by providing SSM prediction directly from unsegmented medical images. However, the proposed approaches are fully supervised and require utilizing a traditional SSM construction pipeline to create training data, thus inheriting the associated burdens and limitations. To address these challenges, we introduce a weakly supervised deep learning approach to predict SSM from images using point cloud supervision. Specifically, we propose reducing the supervision associated with the state-of-the-art fully Bayesian variational information bottleneck DeepSSM (BVIB-DeepSSM) model. BVIB-DeepSSM is an effective, principled framework for predicting probabilistic anatomical shapes from images with quantification of both aleatoric and epistemic uncertainties. Whereas the original BVIB-DeepSSM method requires strong supervision in the form of ground truth correspondence points, the proposed approach utilizes weak supervision via point cloud surface representations, which are more readily obtainable. Furthermore, the proposed approach learns correspondence in a completely data-driven manner without prior assumptions about the expected variability in shape cohort. Our experiments demonstrate that this approach yields similar accuracy and uncertainty estimation to the fully supervised scenario while substantially enhancing the feasibility of model training for SSM construction.
<div id='section'>Paperid: <span id='pid'>1087, <a href='https://arxiv.org/pdf/2405.07655.pdf' target='_blank'>https://arxiv.org/pdf/2405.07655.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liuxin Bao, Xiaofei Zhou, Xiankai Lu, Yaoqi Sun, Haibing Yin, Zhenghui Hu, Jiyong Zhang, Chenggang Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.07655">Quality-aware Selective Fusion Network for V-D-T Salient Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Depth images and thermal images contain the spatial geometry information and surface temperature information, which can act as complementary information for the RGB modality. However, the quality of the depth and thermal images is often unreliable in some challenging scenarios, which will result in the performance degradation of the two-modal based salient object detection (SOD). Meanwhile, some researchers pay attention to the triple-modal SOD task, where they attempt to explore the complementarity of the RGB image, the depth image, and the thermal image. However, existing triple-modal SOD methods fail to perceive the quality of depth maps and thermal images, which leads to performance degradation when dealing with scenes with low-quality depth and thermal images. Therefore, we propose a quality-aware selective fusion network (QSF-Net) to conduct VDT salient object detection, which contains three subnets including the initial feature extraction subnet, the quality-aware region selection subnet, and the region-guided selective fusion subnet. Firstly, except for extracting features, the initial feature extraction subnet can generate a preliminary prediction map from each modality via a shrinkage pyramid architecture. Then, we design the weakly-supervised quality-aware region selection subnet to generate the quality-aware maps. Concretely, we first find the high-quality and low-quality regions by using the preliminary predictions, which further constitute the pseudo label that can be used to train this subnet. Finally, the region-guided selective fusion subnet purifies the initial features under the guidance of the quality-aware maps, and then fuses the triple-modal features and refines the edge details of prediction maps through the intra-modality and inter-modality attention (IIA) module and the edge refinement (ER) module, respectively. Extensive experiments are performed on VDT-2048
<div id='section'>Paperid: <span id='pid'>1088, <a href='https://arxiv.org/pdf/2403.17254.pdf' target='_blank'>https://arxiv.org/pdf/2403.17254.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gaurav Negi, Rajdeep Sarkar, Omnia Zayed, Paul Buitelaar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17254">A Hybrid Approach To Aspect Based Sentiment Analysis Using Transfer Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Aspect-Based Sentiment Analysis (ABSA) aims to identify terms or multiword expressions (MWEs) on which sentiments are expressed and the sentiment polarities associated with them. The development of supervised models has been at the forefront of research in this area. However, training these models requires the availability of manually annotated datasets which is both expensive and time-consuming. Furthermore, the available annotated datasets are tailored to a specific domain, language, and text type. In this work, we address this notable challenge in current state-of-the-art ABSA research. We propose a hybrid approach for Aspect Based Sentiment Analysis using transfer learning. The approach focuses on generating weakly-supervised annotations by exploiting the strengths of both large language models (LLM) and traditional syntactic dependencies. We utilise syntactic dependency structures of sentences to complement the annotations generated by LLMs, as they may overlook domain-specific aspect terms. Extensive experimentation on multiple datasets is performed to demonstrate the efficacy of our hybrid method for the tasks of aspect term extraction and aspect sentiment classification.
  Keywords: Aspect Based Sentiment Analysis, Syntactic Parsing, large language model (LLM)
<div id='section'>Paperid: <span id='pid'>1089, <a href='https://arxiv.org/pdf/2403.09551.pdf' target='_blank'>https://arxiv.org/pdf/2403.09551.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiyuan Wang, Yanzhe Liu, Shang Zhao, Rong Liu, S. Kevin Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09551">WeakSurg: Weakly supervised surgical instrument segmentation using temporal equivariance and semantic continuity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For robotic surgical videos, instrument presence annotations are typically recorded with video streams, which offering the potential to reduce the manually annotated costs for segmentation. However, weakly supervised surgical instrument segmentation with only instrument presence labels has been rarely explored in surgical domain due to the highly under-constrained challenges. Temporal properties can enhance representation learning by capturing sequential dependencies and patterns over time even in incomplete supervision situations. From this, we take the inherent temporal attributes of surgical video into account and extend a two-stage weakly supervised segmentation paradigm from different perspectives. Firstly, we make temporal equivariance constraint to enhance pixel-wise temporal consistency between adjacent features. Secondly, we constrain class-aware semantic continuity between global and local regions across temporal dimension. Finally, we generate temporal-enhanced pseudo masks from consecutive frames to suppress irrelevant regions. Extensive experiments are validated on two surgical video datasets, including one cholecystectomy surgery benchmark and one real robotic left lateral segment liver surgery dataset. We annotate instance-wise instrument labels with fixed time-steps which are double checked by a clinician with 3-years experience to evaluate segmentation results. Experimental results demonstrate the promising performances of our method, which consistently achieves comparable or favorable results with previous state-of-the-art approaches.
<div id='section'>Paperid: <span id='pid'>1090, <a href='https://arxiv.org/pdf/2402.15477.pdf' target='_blank'>https://arxiv.org/pdf/2402.15477.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Renan D. B. Brotto, Jean-Michel Loubes, Laurent Risser, Jean-Pierre Florens, Kenji Nose-Filho, JoÃ£o M. T. Romano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.15477">Debiasing Machine Learning Models by Using Weakly Supervised Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We tackle the problem of bias mitigation of algorithmic decisions in a setting where both the output of the algorithm and the sensitive variable are continuous. Most of prior work deals with discrete sensitive variables, meaning that the biases are measured for subgroups of persons defined by a label, leaving out important algorithmic bias cases, where the sensitive variable is continuous. Typical examples are unfair decisions made with respect to the age or the financial status. In our work, we then propose a bias mitigation strategy for continuous sensitive variables, based on the notion of endogeneity which comes from the field of econometrics. In addition to solve this new problem, our bias mitigation strategy is a weakly supervised learning method which requires that a small portion of the data can be measured in a fair manner. It is model agnostic, in the sense that it does not make any hypothesis on the prediction model. It also makes use of a reasonably large amount of input observations and their corresponding predictions. Only a small fraction of the true output predictions should be known. This therefore limits the need for expert interventions. Results obtained on synthetic data show the effectiveness of our approach for examples as close as possible to real-life applications in econometrics.
<div id='section'>Paperid: <span id='pid'>1091, <a href='https://arxiv.org/pdf/2402.12913.pdf' target='_blank'>https://arxiv.org/pdf/2402.12913.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengcheng Wei, Ze Chen, Songtan Fang, Jiarong He, Max Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.12913">OPDAI at SemEval-2024 Task 6: Small LLMs can Accelerate Hallucination Detection with Weakly Supervised Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper mainly describes a unified system for hallucination detection of LLMs, which wins the second prize in the model-agnostic track of the SemEval-2024 Task 6, and also achieves considerable results in the model-aware track. This task aims to detect hallucination with LLMs for three different text-generation tasks without labeled training data. We utilize prompt engineering and few-shot learning to verify the performance of different LLMs on the validation data. Then we select the LLMs with better performance to generate high-quality weakly supervised training data, which not only satisfies the consistency of different LLMs, but also satisfies the consistency of the optimal LLM with different sampling parameters. Furthermore, we finetune different LLMs by using the constructed training data, and finding that a relatively small LLM can achieve a competitive level of performance in hallucination detection, when compared to the large LLMs and the prompt-based approaches using GPT-4.
<div id='section'>Paperid: <span id='pid'>1092, <a href='https://arxiv.org/pdf/2401.00128.pdf' target='_blank'>https://arxiv.org/pdf/2401.00128.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lujia Wang, Hairong Wang, Fulvio D'Angelo, Lee Curtin, Christopher P. Sereduk, Gustavo De Leon, Kyle W. Singleton, Javier Urcuyo, Andrea Hawkins-Daarud, Pamela R. Jackson, Chandan Krishna, Richard S. Zimmerman, Devi P. Patra, Bernard R. Bendok, Kris A. Smith, Peter Nakaji, Kliment Donev, Leslie C. Baxter, Maciej M. MrugaÅa, Michele Ceccarelli, Antonio Iavarone, Kristin R. Swanson, Nhan L. Tran, Leland S. Hu, Jing Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.00128">Quantifying intra-tumoral genetic heterogeneity of glioblastoma toward precision medicine using MRI and a data-inclusive machine learning algorithm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Glioblastoma (GBM) is one of the most aggressive and lethal human cancers. Intra-tumoral genetic heterogeneity poses a significant challenge for treatment. Biopsy is invasive, which motivates the development of non-invasive, MRI-based machine learning (ML) models to quantify intra-tumoral genetic heterogeneity for each patient. This capability holds great promise for enabling better therapeutic selection to improve patient outcomes. We proposed a novel Weakly Supervised Ordinal Support Vector Machine (WSO-SVM) to predict regional genetic alteration status within each GBM tumor using MRI. WSO-SVM was applied to a unique dataset of 318 image-localized biopsies with spatially matched multiparametric MRI from 74 GBM patients. The model was trained to predict the regional genetic alteration of three GBM driver genes (EGFR, PDGFRA, and PTEN) based on features extracted from the corresponding region of five MRI contrast images. For comparison, a variety of existing ML algorithms were also applied. The classification accuracy of each gene was compared between the different algorithms. The SHapley Additive exPlanations (SHAP) method was further applied to compute contribution scores of different contrast images. Finally, the trained WSO-SVM was used to generate prediction maps within the tumoral area of each patient to help visualize the intra-tumoral genetic heterogeneity. This study demonstrated the feasibility of using MRI and WSO-SVM to enable non-invasive prediction of intra-tumoral regional genetic alteration for each GBM patient, which can inform future adaptive therapies for individualized oncology.
<div id='section'>Paperid: <span id='pid'>1093, <a href='https://arxiv.org/pdf/2311.16829.pdf' target='_blank'>https://arxiv.org/pdf/2311.16829.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Boris Meinardus, Mariusz Trzeciakiewicz, Tim Herzig, Monika Kwiatkowski, Simon Matern, Olaf Hellwich
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.16829">Decomposer: Semi-supervised Learning of Image Restoration and Image Decomposition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Decomposer, a semi-supervised reconstruction model that decomposes distorted image sequences into their fundamental building blocks - the original image and the applied augmentations, i.e., shadow, light, and occlusions. To solve this problem, we use the SIDAR dataset that provides a large number of distorted image sequences: each sequence contains images with shadows, lighting, and occlusions applied to an undistorted version. Each distortion changes the original signal in different ways, e.g., additive or multiplicative noise. We propose a transformer-based model to explicitly learn this decomposition. The sequential model uses 3D Swin-Transformers for spatio-temporal encoding and 3D U-Nets as prediction heads for individual parts of the decomposition. We demonstrate that by separately pre-training our model on weakly supervised pseudo labels, we can steer our model to optimize for our ambiguous problem definition and learn to differentiate between the different image distortions.
<div id='section'>Paperid: <span id='pid'>1094, <a href='https://arxiv.org/pdf/2311.09642.pdf' target='_blank'>https://arxiv.org/pdf/2311.09642.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoqi Ni, Ximiao Zhang, Min Xu, Ning Lang, Xiuzhuang Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.09642">Weakly Supervised Anomaly Detection for Chest X-Ray Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Chest X-Ray (CXR) examination is a common method for assessing thoracic diseases in clinical applications. While recent advances in deep learning have enhanced the significance of visual analysis for CXR anomaly detection, current methods often miss key cues in anomaly images crucial for identifying disease regions, as they predominantly rely on unsupervised training with normal images. This letter focuses on a more practical setup in which few-shot anomaly images with only image-level labels are available during training. For this purpose, we propose WSCXR, a weakly supervised anomaly detection framework for CXR. WSCXR firstly constructs sets of normal and anomaly image features respectively. It then refines the anomaly image features by eliminating normal region features through anomaly feature mining, thus fully leveraging the scarce yet crucial features of diseased areas. Additionally, WSCXR employs a linear mixing strategy to augment the anomaly features, facilitating the training of anomaly detector with few-shot anomaly images. Experiments on two CXR datasets demonstrate the effectiveness of our approach.
<div id='section'>Paperid: <span id='pid'>1095, <a href='https://arxiv.org/pdf/2309.03509.pdf' target='_blank'>https://arxiv.org/pdf/2309.03509.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiatai Lin, Guoqiang Han, Xuemiao Xu, Changhong Liang, Tien-Tsin Wong, C. L. Philip Chen, Zaiyi Liu, Chu Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.03509">BroadCAM: Outcome-agnostic Class Activation Mapping for Small-scale Weakly Supervised Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Class activation mapping~(CAM), a visualization technique for interpreting deep learning models, is now commonly used for weakly supervised semantic segmentation~(WSSS) and object localization~(WSOL). It is the weighted aggregation of the feature maps by activating the high class-relevance ones. Current CAM methods achieve it relying on the training outcomes, such as predicted scores~(forward information), gradients~(backward information), etc. However, when with small-scale data, unstable training may lead to less effective model outcomes and generate unreliable weights, finally resulting in incorrect activation and noisy CAM seeds. In this paper, we propose an outcome-agnostic CAM approach, called BroadCAM, for small-scale weakly supervised applications. Since broad learning system (BLS) is independent to the model learning, BroadCAM can avoid the weights being affected by the unreliable model outcomes when with small-scale data. By evaluating BroadCAM on VOC2012 (natural images) and BCSS-WSSS (medical images) for WSSS and OpenImages30k for WSOL, BroadCAM demonstrates superior performance than existing CAM methods with small-scale data (less than 5\%) in different CNN architectures. It also achieves SOTA performance with large-scale training data. Extensive qualitative comparisons are conducted to demonstrate how BroadCAM activates the high class-relevance feature maps and generates reliable CAMs when with small-scale training data.
<div id='section'>Paperid: <span id='pid'>1096, <a href='https://arxiv.org/pdf/2309.01331.pdf' target='_blank'>https://arxiv.org/pdf/2309.01331.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiwen Cao, Yukun Su, Wenjun Wang, Yanxia Liu, Qingyao Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.01331">Semantic-Constraint Matching Transformer for Weakly Supervised Object Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised object localization (WSOL) strives to learn to localize objects with only image-level supervision. Due to the local receptive fields generated by convolution operations, previous CNN-based methods suffer from partial activation issues, concentrating on the object's discriminative part instead of the entire entity scope. Benefiting from the capability of the self-attention mechanism to acquire long-range feature dependencies, Vision Transformer has been recently applied to alleviate the local activation drawbacks. However, since the transformer lacks the inductive localization bias that are inherent in CNNs, it may cause a divergent activation problem resulting in an uncertain distinction between foreground and background. In this work, we proposed a novel Semantic-Constraint Matching Network (SCMN) via a transformer to converge on the divergent activation. Specifically, we first propose a local patch shuffle strategy to construct the image pairs, disrupting local patches while guaranteeing global consistency. The paired images that contain the common object in spatial are then fed into the Siamese network encoder. We further design a semantic-constraint matching module, which aims to mine the co-object part by matching the coarse class activation maps (CAMs) extracted from the pair images, thus implicitly guiding and calibrating the transformer network to alleviate the divergent activation. Extensive experimental results conducted on two challenging benchmarks, including CUB-200-2011 and ILSVRC datasets show that our method can achieve the new state-of-the-art performance and outperform the previous method by a large margin.
<div id='section'>Paperid: <span id='pid'>1097, <a href='https://arxiv.org/pdf/2309.00543.pdf' target='_blank'>https://arxiv.org/pdf/2309.00543.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sydney Pugh, Ivan Ruchkin, Insup Lee, James Weimer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.00543">Curating Naturally Adversarial Datasets for Learning-Enabled Medical Cyber-Physical Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning models have shown promising predictive accuracy for time-series healthcare applications. However, ensuring the robustness of these models is vital for building trustworthy AI systems. Existing research predominantly focuses on robustness to synthetic adversarial examples, crafted by adding imperceptible perturbations to clean input data. However, these synthetic adversarial examples do not accurately reflect the most challenging real-world scenarios, especially in the context of healthcare data. Consequently, robustness to synthetic adversarial examples may not necessarily translate to robustness against naturally occurring adversarial examples, which is highly desirable for trustworthy AI. We propose a method to curate datasets comprised of natural adversarial examples to evaluate model robustness. The method relies on probabilistic labels obtained from automated weakly-supervised labeling that combines noisy and cheap-to-obtain labeling heuristics. Based on these labels, our method adversarially orders the input data and uses this ordering to construct a sequence of increasingly adversarial datasets. Our evaluation on six medical case studies and three non-medical case studies demonstrates the efficacy and statistical validity of our approach to generating naturally adversarial datasets
<div id='section'>Paperid: <span id='pid'>1098, <a href='https://arxiv.org/pdf/2307.03407.pdf' target='_blank'>https://arxiv.org/pdf/2307.03407.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dahyun Kang, Piotr Koniusz, Minsu Cho, Naila Murray
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.03407">Distilling Self-Supervised Vision Transformers for Weakly-Supervised Few-Shot Classification & Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address the task of weakly-supervised few-shot image classification and segmentation, by leveraging a Vision Transformer (ViT) pretrained with self-supervision. Our proposed method takes token representations from the self-supervised ViT and leverages their correlations, via self-attention, to produce classification and segmentation predictions through separate task heads. Our model is able to effectively learn to perform classification and segmentation in the absence of pixel-level labels during training, using only image-level labels. To do this it uses attention maps, created from tokens generated by the self-supervised ViT backbone, as pixel-level pseudo-labels. We also explore a practical setup with ``mixed" supervision, where a small number of training images contains ground-truth pixel-level labels and the remaining images have only image-level labels. For this mixed setup, we propose to improve the pseudo-labels using a pseudo-label enhancer that was trained using the available ground-truth pixel-level labels. Experiments on Pascal-5i and COCO-20i demonstrate significant performance gains in a variety of supervision settings, and in particular when little-to-no pixel-level labels are available.
<div id='section'>Paperid: <span id='pid'>1099, <a href='https://arxiv.org/pdf/2305.15776.pdf' target='_blank'>https://arxiv.org/pdf/2305.15776.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Xie, Yu Liu, Ming Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.15776">AUC Optimization from Multiple Unlabeled Datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised learning aims to empower machine learning when the perfect supervision is unavailable, which has drawn great attention from researchers. Among various types of weak supervision, one of the most challenging cases is to learn from multiple unlabeled (U) datasets with only a little knowledge of the class priors, or U$^m$ learning for short. In this paper, we study the problem of building an AUC (area under ROC curve) optimization model from multiple unlabeled datasets, which maximizes the pairwise ranking ability of the classifier. We propose U$^m$-AUC, an AUC optimization approach that converts the U$^m$ data into a multi-label AUC optimization problem, and can be trained efficiently. We show that the proposed U$^m$-AUC is effective theoretically and empirically.
<div id='section'>Paperid: <span id='pid'>1100, <a href='https://arxiv.org/pdf/2305.14258.pdf' target='_blank'>https://arxiv.org/pdf/2305.14258.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Xie, Yu Liu, Hao-Yuan He, Ming Li, Zhi-Hua Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.14258">Weakly Supervised AUC Optimization: A Unified Partial AUC Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Since acquiring perfect supervision is usually difficult, real-world machine learning tasks often confront inaccurate, incomplete, or inexact supervision, collectively referred to as weak supervision. In this work, we present WSAUC, a unified framework for weakly supervised AUC optimization problems, which covers noisy label learning, positive-unlabeled learning, multi-instance learning, and semi-supervised learning scenarios. Within the WSAUC framework, we first frame the AUC optimization problems in various weakly supervised scenarios as a common formulation of minimizing the AUC risk on contaminated sets, and demonstrate that the empirical risk minimization problems are consistent with the true AUC. Then, we introduce a new type of partial AUC, specifically, the reversed partial AUC (rpAUC), which serves as a robust training objective for AUC maximization in the presence of contaminated labels. WSAUC offers a universal solution for AUC optimization in various weakly supervised scenarios by maximizing the empirical rpAUC. Theoretical and experimental results under multiple settings support the effectiveness of WSAUC on a range of weakly supervised AUC optimization tasks.
<div id='section'>Paperid: <span id='pid'>1101, <a href='https://arxiv.org/pdf/2305.13904.pdf' target='_blank'>https://arxiv.org/pdf/2305.13904.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxiao Li, Santiago Mazuelas, Yuan Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.13904">Deep GEM-Based Network for Weakly Supervised UWB Ranging Error Mitigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ultra-wideband (UWB)-based techniques, while becoming mainstream approaches for high-accurate positioning, tend to be challenged by ranging bias in harsh environments. The emerging learning-based methods for error mitigation have shown great performance improvement via exploiting high semantic features from raw data. However, these methods rely heavily on fully labeled data, leading to a high cost for data acquisition. We present a learning framework based on weak supervision for UWB ranging error mitigation. Specifically, we propose a deep learning method based on the generalized expectation-maximization (GEM) algorithm for robust UWB ranging error mitigation under weak supervision. Such method integrate probabilistic modeling into the deep learning scheme, and adopt weakly supervised labels as prior information. Extensive experiments in various supervision scenarios illustrate the superiority of the proposed method.
<div id='section'>Paperid: <span id='pid'>1102, <a href='https://arxiv.org/pdf/2303.16533.pdf' target='_blank'>https://arxiv.org/pdf/2303.16533.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mehdi Naouar, Gabriel Kalweit, Ignacio Mastroleo, Philipp Poxleitner, Marc Metzger, Joschka Boedecker, Maria Kalweit
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.16533">Robust Tumor Detection from Coarse Annotations via Multi-Magnification Ensembles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cancer detection and classification from gigapixel whole slide images of stained tissue specimens has recently experienced enormous progress in computational histopathology. The limitation of available pixel-wise annotated scans shifted the focus from tumor localization to global slide-level classification on the basis of (weakly-supervised) multiple-instance learning despite the clinical importance of local cancer detection. However, the worse performance of these techniques in comparison to fully supervised methods has limited their usage until now for diagnostic interventions in domains of life-threatening diseases such as cancer. In this work, we put the focus back on tumor localization in form of a patch-level classification task and take up the setting of so-called coarse annotations, which provide greater training supervision while remaining feasible from a clinical standpoint. To this end, we present a novel ensemble method that not only significantly improves the detection accuracy of metastasis on the open CAMELYON16 data set of sentinel lymph nodes of breast cancer patients, but also considerably increases its robustness against noise while training on coarse annotations. Our experiments show that better results can be achieved with our technique making it clinically feasible to use for cancer diagnosis and opening a new avenue for translational and clinical research.
<div id='section'>Paperid: <span id='pid'>1103, <a href='https://arxiv.org/pdf/2303.07896.pdf' target='_blank'>https://arxiv.org/pdf/2303.07896.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Erik Ostrowski, Bharath Srinivas Prabakaran, Muhammad Shafique
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.07896">Exploring Weakly Supervised Semantic Segmentation Ensembles for Medical Imaging Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reliable classification and detection of certain medical conditions, in images, with state-of-the-art semantic segmentation networks, require vast amounts of pixel-wise annotation. However, the public availability of such datasets is minimal. Therefore, semantic segmentation with image-level labels presents a promising alternative to this problem. Nevertheless, very few works have focused on evaluating this technique and its applicability to the medical sector. Due to their complexity and the small number of training examples in medical datasets, classifier-based weakly supervised networks like class activation maps (CAMs) struggle to extract useful information from them. However, most state-of-the-art approaches rely on them to achieve their improvements. Therefore, we propose a framework that can still utilize the low-quality CAM predictions of complicated datasets to improve the accuracy of our results. Our framework achieves that by first utilizing lower threshold CAMs to cover the target object with high certainty; second, by combining multiple low-threshold CAMs that even out their errors while highlighting the target object. We performed exhaustive experiments on the popular multi-modal BRATS and prostate DECATHLON segmentation challenge datasets. Using the proposed framework, we have demonstrated an improved dice score of up to 8% on BRATS and 6% on DECATHLON datasets compared to the previous state-of-the-art.
<div id='section'>Paperid: <span id='pid'>1104, <a href='https://arxiv.org/pdf/2303.01804.pdf' target='_blank'>https://arxiv.org/pdf/2303.01804.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jieqi Shi, Peiliang Li, Xiaozhi Chen, Shaojie Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.01804">Are All Point Clouds Suitable for Completion? Weakly Supervised Quality Evaluation Network for Point Cloud Completion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the practical application of point cloud completion tasks, real data quality is usually much worse than the CAD datasets used for training. A small amount of noisy data will usually significantly impact the overall system's accuracy. In this paper, we propose a quality evaluation network to score the point clouds and help judge the quality of the point cloud before applying the completion model. We believe our scoring method can help researchers select more appropriate point clouds for subsequent completion and reconstruction and avoid manual parameter adjustment. Moreover, our evaluation model is fast and straightforward and can be directly inserted into any model's training or use process to facilitate the automatic selection and post-processing of point clouds. We propose a complete dataset construction and model evaluation method based on ShapeNet. We verify our network using detection and flow estimation tasks on KITTI, a real-world dataset for autonomous driving. The experimental results show that our model can effectively distinguish the quality of point clouds and help in practical tasks.
<div id='section'>Paperid: <span id='pid'>1105, <a href='https://arxiv.org/pdf/2301.09350.pdf' target='_blank'>https://arxiv.org/pdf/2301.09350.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anastasios Nentidis, Thomas Chatzopoulos, Anastasia Krithara, Grigorios Tsoumakas, Georgios Paliouras
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.09350">Large-scale investigation of weakly-supervised deep learning for the fine-grained semantic indexing of biomedical literature</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Objective: Semantic indexing of biomedical literature is usually done at the level of MeSH descriptors with several related but distinct biomedical concepts often grouped together and treated as a single topic. This study proposes a new method for the automated refinement of subject annotations at the level of MeSH concepts. Methods: Lacking labelled data, we rely on weak supervision based on concept occurrence in the abstract of an article, which is also enhanced by dictionary-based heuristics. In addition, we investigate deep learning approaches, making design choices to tackle the particular challenges of this task. The new method is evaluated on a large-scale retrospective scenario, based on concepts that have been promoted to descriptors. Results: In our experiments concept occurrence was the strongest heuristic achieving a macro-F1 score of about 0.63 across several labels. The proposed method improved it further by more than 4pp. Conclusion: The results suggest that concept occurrence is a strong heuristic for refining the coarse-grained labels at the level of MeSH concepts and the proposed method improves it further.
<div id='section'>Paperid: <span id='pid'>1106, <a href='https://arxiv.org/pdf/2212.10340.pdf' target='_blank'>https://arxiv.org/pdf/2212.10340.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Petra BevandiÄ, Marin OrÅ¡iÄ, Ivan GrubiÅ¡iÄ, Josip Å ariÄ, SiniÅ¡a Å egviÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.10340">Weakly supervised training of universal visual concepts for multi-domain semantic segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep supervised models have an unprecedented capacity to absorb large quantities of training data. Hence, training on multiple datasets becomes a method of choice towards strong generalization in usual scenes and graceful performance degradation in edge cases. Unfortunately, different datasets often have incompatible labels. For instance, the Cityscapes road class subsumes all driving surfaces, while Vistas defines separate classes for road markings, manholes etc. Furthermore, many datasets have overlapping labels. For instance, pickups are labeled as trucks in VIPER, cars in Vistas, and vans in ADE20k. We address this challenge by considering labels as unions of universal visual concepts. This allows seamless and principled learning on multi-domain dataset collections without requiring any relabeling effort. Our method achieves competitive within-dataset and cross-dataset generalization, as well as ability to learn visual concepts which are not separately labeled in any of the training datasets. Experiments reveal competitive or state-of-the-art performance on two multi-domain dataset collections and on the WildDash 2 benchmark.
<div id='section'>Paperid: <span id='pid'>1107, <a href='https://arxiv.org/pdf/2211.16740.pdf' target='_blank'>https://arxiv.org/pdf/2211.16740.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhangir Azerbayev, Ansong Ni, Hailey Schoelkopf, Dragomir Radev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.16740">Explicit Knowledge Transfer for Weakly-Supervised Code Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) can acquire strong code-generation capabilities through few-shot learning. In contrast, supervised fine-tuning is still needed for smaller models to achieve good performance. Such fine-tuning demands a large number of task-specific NL-code pairs, which are expensive to obtain. In this paper, we attempt to transfer the code generation ability of an LLM to a smaller model with the aid of weakly-supervised data. More specifically, we propose explicit knowledge transfer (EKT), which uses the few-shot capabilities of a teacher LLM to create NL-code pairs that we then filter for correctness and fine-tune the student on. We evaluate EKT on the task of generating code solutions to math word problems from the GSM8k dataset. We find that EKT not only yields better performance than training with expert iteration, but also outperforms knowledge distillation, another form of knowledge transfer. A GPT-Neo 1.3B model trained using EKT with a GPT-J teacher achieves a 12.4% pass@100 on GSM8k, while the same student and teacher trained with knowledge distillation yield only a 3.7% pass@100. We also show that it is possible for a student model to outperform the teacher using EKT.
<div id='section'>Paperid: <span id='pid'>1108, <a href='https://arxiv.org/pdf/2211.14491.pdf' target='_blank'>https://arxiv.org/pdf/2211.14491.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wentao Pan, Jiangpeng Yan, Hanbo Chen, Jiawei Yang, Zhe Xu, Xiu Li, Jianhua Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.14491">Human-machine Interactive Tissue Prototype Learning for Label-efficient Histopathology Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, deep neural networks have greatly advanced histopathology image segmentation but usually require abundant annotated data. However, due to the gigapixel scale of whole slide images and pathologists' heavy daily workload, obtaining pixel-level labels for supervised learning in clinical practice is often infeasible. Alternatively, weakly-supervised segmentation methods have been explored with less laborious image-level labels, but their performance is unsatisfactory due to the lack of dense supervision. Inspired by the recent success of self-supervised learning methods, we present a label-efficient tissue prototype dictionary building pipeline and propose to use the obtained prototypes to guide histopathology image segmentation. Particularly, taking advantage of self-supervised contrastive learning, an encoder is trained to project the unlabeled histopathology image patches into a discriminative embedding space where these patches are clustered to identify the tissue prototypes by efficient pathologists' visual examination. Then, the encoder is used to map the images into the embedding space and generate pixel-level pseudo tissue masks by querying the tissue prototype dictionary. Finally, the pseudo masks are used to train a segmentation network with dense supervision for better performance. Experiments on two public datasets demonstrate that our human-machine interactive tissue prototype learning method can achieve comparable segmentation performance as the fully-supervised baselines with less annotation burden and outperform other weakly-supervised methods. Codes will be available upon publication.
<div id='section'>Paperid: <span id='pid'>1109, <a href='https://arxiv.org/pdf/2209.08172.pdf' target='_blank'>https://arxiv.org/pdf/2209.08172.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Banafshe Felfeliyan, Abhilash Hareendranathan, Gregor Kuntze, Stephanie Wichuk, Nils D. Forkert, Jacob L. Jaremko, Janet L. Ronsky
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.08172">Weakly Supervised Medical Image Segmentation With Soft Labels and Noise Robust Loss</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in deep learning algorithms have led to significant benefits for solving many medical image analysis problems. Training deep learning models commonly requires large datasets with expert-labeled annotations. However, acquiring expert-labeled annotation is not only expensive but also is subjective, error-prone, and inter-/intra- observer variability introduces noise to labels. This is particularly a problem when using deep learning models for segmenting medical images due to the ambiguous anatomical boundaries. Image-based medical diagnosis tools using deep learning models trained with incorrect segmentation labels can lead to false diagnoses and treatment suggestions. Multi-rater annotations might be better suited to train deep learning models with small training sets compared to single-rater annotations. The aim of this paper was to develop and evaluate a method to generate probabilistic labels based on multi-rater annotations and anatomical knowledge of the lesion features in MRI and a method to train segmentation models using probabilistic labels using normalized active-passive loss as a "noise-tolerant loss" function. The model was evaluated by comparing it to binary ground truth for 17 knees MRI scans for clinical segmentation and detection of bone marrow lesions (BML). The proposed method successfully improved precision 14, recall 22, and Dice score 8 percent compared to a binary cross-entropy loss function. Overall, the results of this work suggest that the proposed normalized active-passive loss using soft labels successfully mitigated the effects of noisy labels.
<div id='section'>Paperid: <span id='pid'>1110, <a href='https://arxiv.org/pdf/2208.11468.pdf' target='_blank'>https://arxiv.org/pdf/2208.11468.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ron Keuth, Mattias Heinrich, Martin Eichenlaub, Marian Himstedt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.11468">Weakly Supervised Airway Orifice Segmentation in Video Bronchoscopy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video bronchoscopy is routinely conducted for biopsies of lung tissue suspected for cancer, monitoring of COPD patients and clarification of acute respiratory problems at intensive care units. The navigation within complex bronchial trees is particularly challenging and physically demanding, requiring long-term experiences of physicians. This paper addresses the automatic segmentation of bronchial orifices in bronchoscopy videos. Deep learning-based approaches to this task are currently hampered due to the lack of readily-available ground truth segmentation data. Thus, we present a data-driven pipeline consisting of a k-means followed by a compact marker-based watershed algorithm which enables to generate airway instance segmentation maps from given depth images. In this way, these traditional algorithms serve as weak supervision for training a shallow CNN directly on RGB images solely based on a phantom dataset. We evaluate generalization capabilities of this model on two in-vivo datasets covering 250 frames on 21 different bronchoscopies. We demonstrate that its performance is comparable to those models being directly trained on in-vivo data, reaching an average error of 11 vs 5 pixels for the detected centers of the airway segmentation by an image resolution of 128x128. Our quantitative and qualitative results indicate that in the context of video bronchoscopy, phantom data and weak supervision using non-learning-based approaches enable to gain a semantic understanding of airway structures.
<div id='section'>Paperid: <span id='pid'>1111, <a href='https://arxiv.org/pdf/2108.01809.pdf' target='_blank'>https://arxiv.org/pdf/2108.01809.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengpei Xu, Wenjing Jia, Tingcheng Cui, Ruomei Wang, Yuan-fang Zhang, Xiangjian He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2108.01809">What's Wrong with the Bottom-up Methods in Arbitrary-shape Scene Text Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The latest trend in the bottom-up perspective for arbitrary-shape scene text detection is to reason the links between text segments using Graph Convolutional Network (GCN). Notwithstanding, the performance of the best performing bottom-up method is still inferior to that of the best performing top-down method even with the help of GCN. We argue that this is not mainly caused by the limited feature capturing ability of the text proposal backbone or GCN, but by their failure to make a full use of visual-relational features for suppressing false detection, as well as the sub-optimal route-finding mechanism used for grouping text segments. In this paper, we revitalize the classic text detection frameworks by aggregating the visual-relational features of text with two effective false positive/negative suppression mechanisms. First, dense overlapping text segments depicting the `characterness' and `streamline' of text are generated for further relational reasoning and weakly supervised segment classification. Here, relational graph features are used for suppressing false positives/negatives. Then, to fuse the relational features with visual features, a Location-Aware Transfer (LAT) module is designed to transfer text's relational features into visual compatible features with a Fuse Decoding (FD) module to enhance the representation of text regions for the second step suppression. Finally, a novel multiple-text-map-aware contour-approximation strategy is developed, instead of the widely-used route-finding process. Experiments conducted on five benchmark datasets, i.e., CTW1500, Total-Text, ICDAR2015, MSRA-TD500, and MLT2017 demonstrate that our method outperforms the state-of-the-art performance when being embedded in a classic text detection framework, which revitalises the superb strength of the bottom-up methods.
<div id='section'>Paperid: <span id='pid'>1112, <a href='https://arxiv.org/pdf/1712.00796.pdf' target='_blank'>https://arxiv.org/pdf/1712.00796.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giorgos Bouritsas, Petros Koutras, Athanasia Zlatintsi, Petros Maragos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1712.00796">Multimodal Visual Concept Learning with Weakly Supervised Techniques</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the availability of a huge amount of video data accompanied by descriptive texts, it is not always easy to exploit the information contained in natural language in order to automatically recognize video concepts. Towards this goal, in this paper we use textual cues as means of supervision, introducing two weakly supervised techniques that extend the Multiple Instance Learning (MIL) framework: the Fuzzy Sets Multiple Instance Learning (FSMIL) and the Probabilistic Labels Multiple Instance Learning (PLMIL). The former encodes the spatio-temporal imprecision of the linguistic descriptions with Fuzzy Sets, while the latter models different interpretations of each description's semantics with Probabilistic Labels, both formulated through a convex optimization algorithm. In addition, we provide a novel technique to extract weak labels in the presence of complex semantics, that consists of semantic similarity computations. We evaluate our methods on two distinct problems, namely face and action recognition, in the challenging and realistic setting of movies accompanied by their screenplays, contained in the COGNIMUSE database. We show that, on both tasks, our method considerably outperforms a state-of-the-art weakly supervised approach, as well as other baselines.
<div id='section'>Paperid: <span id='pid'>1113, <a href='https://arxiv.org/pdf/2512.01294.pdf' target='_blank'>https://arxiv.org/pdf/2512.01294.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Song Zhang, Ruohan Guo, Xiaohua Ge, Perter Mahon, Weixiang Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.01294">Experimental Methods, Health Indicators, and Diagnostic Strategies for Retired Lithium-ion Batteries: A Comprehensive Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reliable health assessment of retired lithium-ion batteries is essential for safe and economically viable second-life deployment, yet remains difficult due to sparse measurements, incomplete historical records, heterogeneous chemistries, and limited or noisy battery health labels. Conventional laboratory diagnostics, such as full charge-discharge cycling, pulse tests, Electrochemical Impedance Spectroscopy (EIS) measurements, and thermal characterization, provide accurate degradation information but are too time-consuming, equipment-intensive, or condition-sensitive to be applied at scale during retirement-stage sorting, leaving real-world datasets fragmented and inconsistent. This review synthesizes recent advances that address these constraints through physical health indicators, experiment testing methods, data-generation and augmentation techniques, and a spectrum of learning-based modeling routes spanning supervised, semi-supervised, weakly supervised, and unsupervised paradigms. We highlight how minimal-test features, synthetic data, domain-invariant representations, and uncertainty-aware prediction enable robust inference under limited or approximate labels and across mixed chemistries and operating histories. A comparative evaluation further reveals trade-offs in accuracy, interpretability, scalability, and computational burden. Looking forward, progress toward physically constrained generative models, cross-chemistry generalization, calibrated uncertainty estimation, and standardized benchmarks will be crucial for building reliable, scalable, and deployment-ready health prediction tools tailored to the realities of retired-battery applications.
<div id='section'>Paperid: <span id='pid'>1114, <a href='https://arxiv.org/pdf/2511.17346.pdf' target='_blank'>https://arxiv.org/pdf/2511.17346.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marius Rodrigues, Louis Bahrman, Roland Badeau, Gaël Richard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17346">Is Phase Really Needed for Weakly-Supervised Dereverberation ?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In unsupervised or weakly-supervised approaches for speech dereverberation, the target clean (dry) signals are considered to be unknown during training. In that context, evaluating to what extent information can be retrieved from the sole knowledge of reverberant (wet) speech becomes critical. This work investigates the role of the reverberant (wet) phase in the time-frequency domain. Based on Statistical Wave Field Theory, we show that late reverberation perturbs phase components with white, uniformly distributed noise, except at low frequencies. Consequently, the wet phase carries limited useful information and is not essential for weakly supervised dereverberation. To validate this finding, we train dereverberation models under a recent weak supervision framework and demonstrate that performance can be significantly improved by excluding the reverberant phase from the loss function.
<div id='section'>Paperid: <span id='pid'>1115, <a href='https://arxiv.org/pdf/2511.16268.pdf' target='_blank'>https://arxiv.org/pdf/2511.16268.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Erwan Dereure, Robin Louiset, Laura Parkkinen, David A Menassa, David Holcman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.16268">Weakly Supervised Segmentation and Classification of Alpha-Synuclein Aggregates in Brightfield Midbrain Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Parkinson's disease (PD) is a neurodegenerative disorder associated with the accumulation of misfolded alpha-synuclein aggregates, forming Lewy bodies and neuritic shape used for pathology diagnostics. Automatic analysis of immunohistochemistry histopathological images with Deep Learning provides a promising tool for better understanding the spatial organization of these aggregates. In this study, we develop an automated image processing pipeline to segment and classify these aggregates in whole-slide images (WSIs) of midbrain tissue from PD and incidental Lewy Body Disease (iLBD) cases based on weakly supervised segmentation, robust to immunohistochemical labelling variability, with a ResNet50 classifier. Our approach allows to differentiate between major aggregate morphologies, including Lewy bodies and neurites with a balanced accuracy of $80\%$. This framework paves the way for large-scale characterization of the spatial distribution and heterogeneity of alpha-synuclein aggregates in brightfield immunohistochemical tissue, and for investigating their poorly understood relationships with surrounding cells such as microglia and astrocytes.
<div id='section'>Paperid: <span id='pid'>1116, <a href='https://arxiv.org/pdf/2510.09306.pdf' target='_blank'>https://arxiv.org/pdf/2510.09306.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alemu Sisay Nigru, Michele Svanera, Austin Dibble, Connor Dalby, Mattia Savardi, Sergio Benini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09306">Rewiring Development in Brain Segmentation: Leveraging Adult Brain Priors for Enhancing Infant MRI Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate segmentation of infant brain MRI is critical for studying early neurodevelopment and diagnosing neurological disorders. Yet, it remains a fundamental challenge due to continuously evolving anatomy of the subjects, motion artifacts, and the scarcity of high-quality labeled data. In this work, we present LODi, a novel framework that utilizes prior knowledge from an adult brain MRI segmentation model to enhance the segmentation performance of infant scans. Given the abundance of publicly available adult brain MRI data, we pre-train a segmentation model on a large adult dataset as a starting point. Through transfer learning and domain adaptation strategies, we progressively adapt the model to the 0-2 year-old population, enabling it to account for the anatomical and imaging variability typical of infant scans. The adaptation of the adult model is carried out using weakly supervised learning on infant brain scans, leveraging silver-standard ground truth labels obtained with FreeSurfer. By introducing a novel training strategy that integrates hierarchical feature refinement and multi-level consistency constraints, our method enables fast, accurate, age-adaptive segmentation, while mitigating scanner and site-specific biases. Extensive experiments on both internal and external datasets demonstrate the superiority of our approach over traditional supervised learning and domain-specific models. Our findings highlight the advantage of leveraging adult brain priors as a foundation for age-flexible neuroimaging analysis, paving the way for more reliable and generalizable brain MRI segmentation across the lifespan.
<div id='section'>Paperid: <span id='pid'>1117, <a href='https://arxiv.org/pdf/2510.00654.pdf' target='_blank'>https://arxiv.org/pdf/2510.00654.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaocong Zhu, Zhiwei Li, Xinghua Li, Huanfeng Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00654">Weakly Supervised Cloud Detection Combining Spectral Features and Multi-Scale Deep Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Clouds significantly affect the quality of optical satellite images, which seriously limits their precise application. Recently, deep learning has been widely applied to cloud detection and has achieved satisfactory results. However, the lack of distinctive features in thin clouds and the low quality of training samples limit the cloud detection accuracy of deep learning methods, leaving space for further improvements. In this paper, we propose a weakly supervised cloud detection method that combines spectral features and multi-scale scene-level deep network (SpecMCD) to obtain highly accurate pixel-level cloud masks. The method first utilizes a progressive training framework with a multi-scale scene-level dataset to train the multi-scale scene-level cloud detection network. Pixel-level cloud probability maps are then obtained by combining the multi-scale probability maps and cloud thickness map based on the characteristics of clouds in dense cloud coverage and large cloud-area coverage images. Finally, adaptive thresholds are generated based on the differentiated regions of the scene-level cloud masks at different scales and combined with distance-weighted optimization to obtain binary cloud masks. Two datasets, WDCD and GF1MS-WHU, comprising a total of 60 Gaofen-1 multispectral (GF1-MS) images, were used to verify the effectiveness of the proposed method. Compared to the other weakly supervised cloud detection methods such as WDCD and WSFNet, the F1-score of the proposed SpecMCD method shows an improvement of over 7.82%, highlighting the superiority and potential of the SpecMCD method for cloud detection under different cloud coverage conditions.
<div id='section'>Paperid: <span id='pid'>1118, <a href='https://arxiv.org/pdf/2509.17702.pdf' target='_blank'>https://arxiv.org/pdf/2509.17702.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Patrick Schmidt, Vasileios Belagiannis, Lazaros Nalpantidis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17702">Depth Edge Alignment Loss: DEALing with Depth in Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous robotic systems applied to new domains require an abundance of expensive, pixel-level dense labels to train robust semantic segmentation models under full supervision. This study proposes a model-agnostic Depth Edge Alignment Loss to improve Weakly Supervised Semantic Segmentation models across different datasets. The methodology generates pixel-level semantic labels from image-level supervision, avoiding expensive annotation processes. While weak supervision is widely explored in traditional computer vision, our approach adds supervision with pixel-level depth information, a modality commonly available in robotic systems. We demonstrate how our approach improves segmentation performance across datasets and models, but can also be combined with other losses for even better performance, with improvements up to +5.439, +1.274 and +16.416 points in mean Intersection over Union on the PASCAL VOC / MS COCO validation, and the HOPE static onboarding split, respectively. Our code will be made publicly available.
<div id='section'>Paperid: <span id='pid'>1119, <a href='https://arxiv.org/pdf/2509.06485.pdf' target='_blank'>https://arxiv.org/pdf/2509.06485.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrea Marelli, Alberto Foresti, Leonardo Pesce, Giacomo Boracchi, Mario Grosso
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06485">WS$^2$: Weakly Supervised Segmentation using Before-After Supervision in Waste Sorting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In industrial quality control, to visually recognize unwanted items within a moving heterogeneous stream, human operators are often still indispensable. Waste-sorting stands as a significant example, where operators on multiple conveyor belts manually remove unwanted objects to select specific materials. To automate this recognition problem, computer vision systems offer great potential in accurately identifying and segmenting unwanted items in such settings. Unfortunately, considering the multitude and the variety of sorting tasks, fully supervised approaches are not a viable option to address this challange, as they require extensive labeling efforts. Surprisingly, weakly supervised alternatives that leverage the implicit supervision naturally provided by the operator in his removal action are relatively unexplored. In this paper, we define the concept of Before-After Supervision, illustrating how to train a segmentation network by leveraging only the visual differences between images acquired \textit{before} and \textit{after} the operator. To promote research in this direction, we introduce WS$^2$ (Weakly Supervised segmentation for Waste-Sorting), the first multiview dataset consisting of more than 11 000 high-resolution video frames captured on top of a conveyor belt, including "before" and "after" images. We also present a robust end-to-end pipeline, used to benchmark several state-of-the-art weakly supervised segmentation methods on WS$^2$.
<div id='section'>Paperid: <span id='pid'>1120, <a href='https://arxiv.org/pdf/2508.01338.pdf' target='_blank'>https://arxiv.org/pdf/2508.01338.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqi Sheng, Junyan Wu, Wei Lu, Jiantao Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01338">Weakly-Supervised Image Forgery Localization via Vision-Language Collaborative Reasoning Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image forgery localization aims to precisely identify tampered regions within images, but it commonly depends on costly pixel-level annotations. To alleviate this annotation burden, weakly supervised image forgery localization (WSIFL) has emerged, yet existing methods still achieve limited localization performance as they mainly exploit intra-image consistency clues and lack external semantic guidance to compensate for weak supervision. In this paper, we propose ViLaCo, a vision-language collaborative reasoning framework that introduces auxiliary semantic supervision distilled from pre-trained vision-language models (VLMs), enabling accurate pixel-level localization using only image-level labels. Specifically, ViLaCo first incorporates semantic knowledge through a vision-language feature modeling network, which jointly extracts textual and visual priors using pre-trained VLMs. Next, an adaptive vision-language reasoning network aligns textual semantics and visual features through mutual interactions, producing semantically aligned representations. Subsequently, these representations are passed into dual prediction heads, where the coarse head performs image-level classification and the fine head generates pixel-level localization masks, thereby bridging the gap between weak supervision and fine-grained localization. Moreover, a contrastive patch consistency module is introduced to cluster tampered features while separating authentic ones, facilitating more reliable forgery discrimination. Extensive experiments on multiple public datasets demonstrate that ViLaCo substantially outperforms existing WSIFL methods, achieving state-of-the-art performance in both detection and localization accuracy.
<div id='section'>Paperid: <span id='pid'>1121, <a href='https://arxiv.org/pdf/2507.10594.pdf' target='_blank'>https://arxiv.org/pdf/2507.10594.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengda Zhuo, Di Wu, Yi He, Shuqiang Huang, Xindong Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10594">Extension OL-MDISF: Online Learning from Mix-Typed, Drifted, and Incomplete Streaming Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Online learning, where feature spaces can change over time, offers a flexible learning paradigm that has attracted considerable attention. However, it still faces three significant challenges. First, the heterogeneity of real-world data streams with mixed feature types presents challenges for traditional parametric modeling. Second, data stream distributions can shift over time, causing an abrupt and substantial decline in model performance. Additionally, the time and cost constraints make it infeasible to label every data instance in a supervised setting. To overcome these challenges, we propose a new algorithm Online Learning from Mix-typed, Drifted, and Incomplete Streaming Features (OL-MDISF), which aims to relax restrictions on both feature types, data distribution, and supervision information. Our approach involves utilizing copula models to create a comprehensive latent space, employing an adaptive sliding window for detecting drift points to ensure model stability, and establishing label proximity information based on geometric structural relationships. To demonstrate the model's efficiency and effectiveness, we provide theoretical analysis and comprehensive experimental results.
  This extension serves as a standalone technical reference to the original OL-MDISF method. It provides (i) a contextual analysis of OL-MDISF within the broader landscape of online learning, covering recent advances in mixed-type feature modeling, concept drift adaptation, and weak supervision, and (ii) a comprehensive set of experiments across 14 real-world datasets under two types of drift scenarios. These include full CER trends, ablation studies, sensitivity analyses, and temporal ensemble dynamics. We hope this document can serve as a reproducible benchmark and technical resource for researchers working on nonstationary, heterogeneous, and weakly supervised data streams.
<div id='section'>Paperid: <span id='pid'>1122, <a href='https://arxiv.org/pdf/2507.07297.pdf' target='_blank'>https://arxiv.org/pdf/2507.07297.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengfei Wu, Ronald Seoh, Bingxuan Li, Liqiang Zhang, Fengrong Han, Dan Goldwasser
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07297">MagiC: Evaluating Multimodal Cognition Toward Grounded Visual Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in large vision-language models have led to impressive performance in visual question answering and multimodal reasoning. However, it remains unclear whether these models genuinely perform grounded visual reasoning or rely on superficial patterns and dataset biases. In this work, we introduce MagiC, a comprehensive benchmark designed to evaluate grounded multimodal cognition, assessing not only answer accuracy but also the quality of step-by-step reasoning and its alignment with relevant visual evidence. Our benchmark includes approximately 5,500 weakly supervised QA examples generated from strong model outputs and 900 human-curated examples with fine-grained annotations, including answers, rationales, and bounding box groundings. We evaluate 15 vision-language models ranging from 7B to 70B parameters across four dimensions: final answer correctness, reasoning validity, grounding fidelity, and self-correction ability. MagiC further includes diagnostic settings to probe model robustness under adversarial visual cues and assess their capacity for introspective error correction. We introduce new metrics such as MagiScore and StepSense, and provide comprehensive analyses that reveal key limitations and opportunities in current approaches to grounded visual reasoning.
<div id='section'>Paperid: <span id='pid'>1123, <a href='https://arxiv.org/pdf/2506.22866.pdf' target='_blank'>https://arxiv.org/pdf/2506.22866.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hang-Cheng Dong, Lu Zou, Bingguo Liu, Dong Ye, Guodong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22866">Region-Aware CAM: High-Resolution Weakly-Supervised Defect Segmentation via Salient Region Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Surface defect detection plays a critical role in industrial quality inspection. Recent advances in artificial intelligence have significantly enhanced the automation level of detection processes. However, conventional semantic segmentation and object detection models heavily rely on large-scale annotated datasets, which conflicts with the practical requirements of defect detection tasks. This paper proposes a novel weakly supervised semantic segmentation framework comprising two key components: a region-aware class activation map (CAM) and pseudo-label training. To address the limitations of existing CAM methods, especially low-resolution thermal maps, and insufficient detail preservation, we introduce filtering-guided backpropagation (FGBP), which refines target regions by filtering gradient magnitudes to identify areas with higher relevance to defects. Building upon this, we further develop a region-aware weighted module to enhance spatial precision. Finally, pseudo-label segmentation is implemented to refine the model's performance iteratively. Comprehensive experiments on industrial defect datasets demonstrate the superiority of our method. The proposed framework effectively bridges the gap between weakly supervised learning and high-precision defect segmentation, offering a practical solution for resource-constrained industrial scenarios.
<div id='section'>Paperid: <span id='pid'>1124, <a href='https://arxiv.org/pdf/2505.23524.pdf' target='_blank'>https://arxiv.org/pdf/2505.23524.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Xia, Dan Jiang, Quan Zhang, Ke Zhang, Chun Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23524">CLIP-AE: CLIP-assisted Cross-view Audio-Visual Enhancement for Unsupervised Temporal Action Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Temporal Action Localization (TAL) has garnered significant attention in information retrieval. Existing supervised or weakly supervised methods heavily rely on labeled temporal boundaries and action categories, which are labor-intensive and time-consuming. Consequently, unsupervised temporal action localization (UTAL) has gained popularity. However, current methods face two main challenges: 1) Classification pre-trained features overly focus on highly discriminative regions; 2) Solely relying on visual modality information makes it difficult to determine contextual boundaries. To address these issues, we propose a CLIP-assisted cross-view audiovisual enhanced UTAL method. Specifically, we introduce visual language pre-training (VLP) and classification pre-training-based collaborative enhancement to avoid excessive focus on highly discriminative regions; we also incorporate audio perception to provide richer contextual boundary information. Finally, we introduce a self-supervised cross-view learning paradigm to achieve multi-view perceptual enhancement without additional annotations. Extensive experiments on two public datasets demonstrate our model's superiority over several state-of-the-art competitors.
<div id='section'>Paperid: <span id='pid'>1125, <a href='https://arxiv.org/pdf/2503.20722.pdf' target='_blank'>https://arxiv.org/pdf/2503.20722.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>A. Candito, A. Dragan, R. Holbrey, A. Ribeiro, R. Donners, C. Messiou, N. Tunariu, D. -M. Koh, M. D. Blackledge, The Institute of Cancer Research, London, United Kingdom, The Royal Marsden NHS Foundation Trust, London, United Kingdom, University Hospital Basel, Basel, Switzerland
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20722">A weakly-supervised deep learning model for fast localisation and delineation of the skeleton, internal organs, and spinal canal on Whole-Body Diffusion-Weighted MRI (WB-DWI)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Background: Apparent Diffusion Coefficient (ADC) values and Total Diffusion Volume (TDV) from Whole-body diffusion-weighted MRI (WB-DWI) are recognized cancer imaging biomarkers. However, manual disease delineation for ADC and TDV measurements is unfeasible in clinical practice, demanding automation. As a first step, we propose an algorithm to generate fast and reproducible probability maps of the skeleton, adjacent internal organs (liver, spleen, urinary bladder, and kidneys), and spinal canal. Methods: We developed an automated deep-learning pipeline based on a 3D patch-based Residual U-Net architecture that localizes and delineates these anatomical structures on WB-DWI. The algorithm was trained using "soft-labels" (non-binary segmentations) derived from a computationally intensive atlas-based approach. For training and validation, we employed a multi-center WB-DWI dataset comprising 532 scans from patients with Advanced Prostate Cancer (APC) or Multiple Myeloma (MM), with testing on 45 patients. Results: Our weakly-supervised deep learning model achieved an average dice score/precision/recall of 0.66/0.6/0.73 for skeletal delineations, 0.8/0.79/0.81 for internal organs, and 0.85/0.79/0.94 for spinal canal, with surface distances consistently below 3 mm. Relative median ADC and log-transformed volume differences between automated and manual expert-defined full-body delineations were below 10% and 4%, respectively. The computational time for generating probability maps was 12x faster than the atlas-based registration algorithm (25 s vs. 5 min). An experienced radiologist rated the model's accuracy "good" or "excellent" on test datasets. Conclusion: Our model offers fast and reproducible probability maps for localizing and delineating body regions on WB-DWI, enabling ADC and TDV quantification, potentially supporting clinicians in disease staging and treatment response assessment.
<div id='section'>Paperid: <span id='pid'>1126, <a href='https://arxiv.org/pdf/2503.04165.pdf' target='_blank'>https://arxiv.org/pdf/2503.04165.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bodong Zhang, Hamid Manoochehri, Xiwen Li, Beatrice S. Knudsen, Tolga Tasdizen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04165">WeakSupCon: Weakly Supervised Contrastive Learning for Encoder Pre-training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised multiple instance learning (MIL) is a challenging task given that only bag-level labels are provided, while each bag typically contains multiple instances. This topic has been extensively studied in histopathological image analysis, where labels are usually available only at the whole slide image (WSI) level, while each WSI could be divided into thousands of small image patches for training. The dominant MIL approaches focus on feature aggregation and take fixed patch features as inputs. However, weakly supervised feature representation learning in MIL settings is always neglected. Those features used to be generated by self-supervised learning methods that do not utilize weak labels, or by foundation encoders pre-trained on other large datasets. In this paper, we propose a novel weakly supervised feature representation learning method called Weakly Supervised Contrastive Learning (WeakSupCon) that utilizes bag-level labels. In our method, we employ multi-task learning and define distinct contrastive losses for samples with different bag labels. Our experiments demonstrate that the features generated using WeakSupCon with limited computing resources significantly enhance MIL classification performance compared to self-supervised approaches across three datasets. Our WeakSupCon code is available at github.com/BzhangURU/Paper_WeakSupCon
<div id='section'>Paperid: <span id='pid'>1127, <a href='https://arxiv.org/pdf/2502.20678.pdf' target='_blank'>https://arxiv.org/pdf/2502.20678.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aaryan Garg, Akash Kumar, Yogesh S Rawat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20678">STPro: Spatial and Temporal Progressive Learning for Weakly Supervised Spatio-Temporal Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work we study Weakly Supervised Spatio-Temporal Video Grounding (WSTVG), a challenging task of localizing subjects spatio-temporally in videos using only textual queries and no bounding box supervision. Inspired by recent advances in vision-language foundation models, we investigate their utility for WSTVG, leveraging their zero-shot grounding capabilities. However, we find that a simple adaptation lacks essential spatio-temporal grounding abilities. To bridge this gap, we introduce Tubelet Referral Grounding (TRG), which connects textual queries to tubelets to enable spatio-temporal predictions. Despite its promise, TRG struggles with compositional action understanding and dense scene scenarios. To address these limitations, we propose STPro, a novel progressive learning framework with two key modules: (1) Sub-Action Temporal Curriculum Learning (SA-TCL), which incrementally builds compositional action understanding, and (2) Congestion-Guided Spatial Curriculum Learning (CG-SCL), which adapts the model to complex scenes by spatially increasing task difficulty. STPro achieves state-of-the-art results on three benchmark datasets, with improvements of 1.0% on VidSTG-Declarative and 3.0% on HCSTVG-v1.
<div id='section'>Paperid: <span id='pid'>1128, <a href='https://arxiv.org/pdf/2501.19048.pdf' target='_blank'>https://arxiv.org/pdf/2501.19048.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rita Pereira, M. Rita Verdelho, Catarina Barata, Carlos Santiago
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.19048">The Role of Graph-based MIL and Interventional Training in the Generalization of WSI Classifiers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Whole Slide Imaging (WSI), which involves high-resolution digital scans of pathology slides, has become the gold standard for cancer diagnosis, but its gigapixel resolution and the scarcity of annotated datasets present challenges for deep learning models. Multiple Instance Learning (MIL), a widely-used weakly supervised approach, bypasses the need for patch-level annotations. However, conventional MIL methods overlook the spatial relationships between patches, which are crucial for tasks such as cancer grading and diagnosis. To address this, graph-based approaches have gained prominence by incorporating spatial information through node connections. Despite their potential, both MIL and graph-based models are vulnerable to learning spurious associations, like color variations in WSIs, affecting their robustness. In this dissertation, we conduct an extensive comparison of multiple graph construction techniques, MIL models, graph-MIL approaches, and interventional training, introducing a new framework, Graph-based Multiple Instance Learning with Interventional Training (GMIL-IT), for WSI classification. We evaluate their impact on model generalization through domain shift analysis and demonstrate that graph-based models alone achieve the generalization initially anticipated from interventional training. Our code is available here: github.com/ritamartinspereira/GMIL-IT
<div id='section'>Paperid: <span id='pid'>1129, <a href='https://arxiv.org/pdf/2501.11124.pdf' target='_blank'>https://arxiv.org/pdf/2501.11124.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Quan Zhang, Yuxin Qi, Xi Tang, Rui Yuan, Xi Lin, Ke Zhang, Chun Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.11124">Rethinking Pseudo-Label Guided Learning for Weakly Supervised Temporal Action Localization from the Perspective of Noise Correction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pseudo-label learning methods have been widely applied in weakly-supervised temporal action localization. Existing works directly utilize weakly-supervised base model to generate instance-level pseudo-labels for training the fully-supervised detection head. We argue that the noise in pseudo-labels would interfere with the learning of fully-supervised detection head, leading to significant performance leakage. Issues with noisy labels include:(1) inaccurate boundary localization; (2) undetected short action clips; (3) multiple adjacent segments incorrectly detected as one segment. To target these issues, we introduce a two-stage noisy label learning strategy to harness every potential useful signal in noisy labels. First, we propose a frame-level pseudo-label generation model with a context-aware denoising algorithm to refine the boundaries. Second, we introduce an online-revised teacher-student framework with a missing instance compensation module and an ambiguous instance correction module to solve the short-action-missing and many-to-one problems. Besides, we apply a high-quality pseudo-label mining loss in our online-revised teacher-student framework to add different weights to the noisy labels to train more effectively. Our model outperforms the previous state-of-the-art method in detection accuracy and inference speed greatly upon the THUMOS14 and ActivityNet v1.2 benchmarks.
<div id='section'>Paperid: <span id='pid'>1130, <a href='https://arxiv.org/pdf/2412.14295.pdf' target='_blank'>https://arxiv.org/pdf/2412.14295.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anna Manasyan, Maximilian Seitzer, Filip Radovic, Georg Martius, Andrii Zadaianchuk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14295">Temporally Consistent Object-Centric Learning by Contrasting Slots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsupervised object-centric learning from videos is a promising approach to extract structured representations from large, unlabeled collections of videos. To support downstream tasks like autonomous control, these representations must be both compositional and temporally consistent. Existing approaches based on recurrent processing often lack long-term stability across frames because their training objective does not enforce temporal consistency. In this work, we introduce a novel object-level temporal contrastive loss for video object-centric models that explicitly promotes temporal consistency. Our method significantly improves the temporal consistency of the learned object-centric representations, yielding more reliable video decompositions that facilitate challenging downstream tasks such as unsupervised object dynamics prediction. Furthermore, the inductive bias added by our loss strongly improves object discovery, leading to state-of-the-art results on both synthetic and real-world datasets, outperforming even weakly-supervised methods that leverage motion masks as additional cues.
<div id='section'>Paperid: <span id='pid'>1131, <a href='https://arxiv.org/pdf/2411.08466.pdf' target='_blank'>https://arxiv.org/pdf/2411.08466.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Quan Zhang, Jinwei Fang, Rui Yuan, Xi Tang, Yuxin Qi, Ke Zhang, Chun Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08466">Weakly Supervised Temporal Action Localization via Dual-Prior Collaborative Learning Guided by Multimodal Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent breakthroughs in Multimodal Large Language Models (MLLMs) have gained significant recognition within the deep learning community, where the fusion of the Video Foundation Models (VFMs) and Large Language Models(LLMs) has proven instrumental in constructing robust video understanding systems, effectively surmounting constraints associated with predefined visual tasks. These sophisticated MLLMs exhibit remarkable proficiency in comprehending videos, swiftly attaining unprecedented performance levels across diverse benchmarks. However, their operation demands substantial memory and computational resources, underscoring the continued importance of traditional models in video comprehension tasks. In this paper, we introduce a novel learning paradigm termed MLLM4WTAL. This paradigm harnesses the potential of MLLM to offer temporal action key semantics and complete semantic priors for conventional Weakly-supervised Temporal Action Localization (WTAL) methods. MLLM4WTAL facilitates the enhancement of WTAL by leveraging MLLM guidance. It achieves this by integrating two distinct modules: Key Semantic Matching (KSM) and Complete Semantic Reconstruction (CSR). These modules work in tandem to effectively address prevalent issues like incomplete and over-complete outcomes common in WTAL methods. Rigorous experiments are conducted to validate the efficacy of our proposed approach in augmenting the performance of various heterogeneous WTAL models.
<div id='section'>Paperid: <span id='pid'>1132, <a href='https://arxiv.org/pdf/2409.05199.pdf' target='_blank'>https://arxiv.org/pdf/2409.05199.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giannis Karamanolakis, Daniel Hsu, Luis Gravano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.05199">Interactive Machine Teaching by Labeling Rules and Instances</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised learning aims to reduce the cost of labeling data by using expert-designed labeling rules. However, existing methods require experts to design effective rules in a single shot, which is difficult in the absence of proper guidance and tooling. Therefore, it is still an open question whether experts should spend their limited time writing rules or instead providing instance labels via active learning. In this paper, we investigate how to exploit an expert's limited time to create effective supervision. First, to develop practical guidelines for rule creation, we conduct an exploratory analysis of diverse collections of existing expert-designed rules and find that rule precision is more important than coverage across datasets. Second, we compare rule creation to individual instance labeling via active learning and demonstrate the importance of both across 6 datasets. Third, we propose an interactive learning framework, INTERVAL, that achieves efficiency by automatically extracting candidate rules based on rich patterns (e.g., by prompting a language model), and effectiveness by soliciting expert feedback on both candidate rules and individual instances. Across 6 datasets, INTERVAL outperforms state-of-the-art weakly supervised approaches by 7% in F1. Furthermore, it requires as few as 10 queries for expert feedback to reach F1 values that existing active learning methods cannot match even with 100 queries.
<div id='section'>Paperid: <span id='pid'>1133, <a href='https://arxiv.org/pdf/2408.10777.pdf' target='_blank'>https://arxiv.org/pdf/2408.10777.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huafeng Chen, Dian Shao, Guangqian Guo, Shan Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.10777">Just a Hint: Point-Supervised Camouflaged Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Camouflaged Object Detection (COD) demands models to expeditiously and accurately distinguish objects which conceal themselves seamlessly in the environment. Owing to the subtle differences and ambiguous boundaries, COD is not only a remarkably challenging task for models but also for human annotators, requiring huge efforts to provide pixel-wise annotations. To alleviate the heavy annotation burden, we propose to fulfill this task with the help of only one point supervision. Specifically, by swiftly clicking on each object, we first adaptively expand the original point-based annotation to a reasonable hint area. Then, to avoid partial localization around discriminative parts, we propose an attention regulator to scatter model attention to the whole object through partially masking labeled regions. Moreover, to solve the unstable feature representation of camouflaged objects under only point-based annotation, we perform unsupervised contrastive learning based on differently augmented image pairs (e.g. changing color or doing translation). On three mainstream COD benchmarks, experimental results show that our model outperforms several weakly-supervised methods by a large margin across various metrics.
<div id='section'>Paperid: <span id='pid'>1134, <a href='https://arxiv.org/pdf/2408.10760.pdf' target='_blank'>https://arxiv.org/pdf/2408.10760.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huafeng Chen, Pengxu Wei, Guangqian Guo, Shan Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.10760">SAM-COD: SAM-guided Unified Framework for Weakly-Supervised Camouflaged Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most Camouflaged Object Detection (COD) methods heavily rely on mask annotations, which are time-consuming and labor-intensive to acquire. Existing weakly-supervised COD approaches exhibit significantly inferior performance compared to fully-supervised methods and struggle to simultaneously support all the existing types of camouflaged object labels, including scribbles, bounding boxes, and points. Even for Segment Anything Model (SAM), it is still problematic to handle the weakly-supervised COD and it typically encounters challenges of prompt compatibility of the scribble labels, extreme response, semantically erroneous response, and unstable feature representations, producing unsatisfactory results in camouflaged scenes. To mitigate these issues, we propose a unified COD framework in this paper, termed SAM-COD, which is capable of supporting arbitrary weakly-supervised labels. Our SAM-COD employs a prompt adapter to handle scribbles as prompts based on SAM. Meanwhile, we introduce response filter and semantic matcher modules to improve the quality of the masks obtained by SAM under COD prompts. To alleviate the negative impacts of inaccurate mask predictions, a new strategy of prompt-adaptive knowledge distillation is utilized to ensure a reliable feature representation. To validate the effectiveness of our approach, we have conducted extensive empirical experiments on three mainstream COD benchmarks. The results demonstrate the superiority of our method against state-of-the-art weakly-supervised and even fully-supervised methods.
<div id='section'>Paperid: <span id='pid'>1135, <a href='https://arxiv.org/pdf/2408.09615.pdf' target='_blank'>https://arxiv.org/pdf/2408.09615.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyang Li, Xinyi Ying, Ruojing Li, Yongxian Liu, Yangsi Shi, Miao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09615">The First Competition on Resource-Limited Infrared Small Target Detection Challenge: Methods and Results</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we briefly summarize the first competition on resource-limited infrared small target detection (namely, LimitIRSTD). This competition has two tracks, including weakly-supervised infrared small target detection (Track 1) and lightweight infrared small target detection (Track 2). 46 and 60 teams successfully registered and took part in Tracks 1 and Track 2, respectively. The top-performing methods and their results in each track are described with details. This competition inspires the community to explore the tough problems in the application of infrared small target detection, and ultimately promote the deployment of this technology under limited resource.
<div id='section'>Paperid: <span id='pid'>1136, <a href='https://arxiv.org/pdf/2407.09159.pdf' target='_blank'>https://arxiv.org/pdf/2407.09159.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abid Ali, Mahmoud Ali, Jean-Marc Odobez, Camilla Barbini, SÃ©verine Dubuisson, Francois Bremond, Susanne ThÃ¼mmler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.09159">Weakly-supervised Autism Severity Assessment in Long Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autism Spectrum Disorder (ASD) is a diverse collection of neurobiological conditions marked by challenges in social communication and reciprocal interactions, as well as repetitive and stereotypical behaviors. Atypical behavior patterns in a long, untrimmed video can serve as biomarkers for children with ASD. In this paper, we propose a video-based weakly-supervised method that takes spatio-temporal features of long videos to learn typical and atypical behaviors for autism detection. On top of that, we propose a shallow TCN-MLP network, which is designed to further categorize the severity score. We evaluate our method on actual evaluation videos of children with autism collected and annotated (for severity score) by clinical professionals. Experimental results demonstrate the effectiveness of behavioral biomarkers that could help clinicians in autism spectrum analysis.
<div id='section'>Paperid: <span id='pid'>1137, <a href='https://arxiv.org/pdf/2406.14510.pdf' target='_blank'>https://arxiv.org/pdf/2406.14510.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rotem Shalev-Arkushin, Aharon Azulay, Tavi Halperin, Eitan Richardson, Amit H. Bermano, Ohad Fried
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.14510">V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion-based generative models have recently shown remarkable image and video editing capabilities. However, local video editing, particularly removal of small attributes like glasses, remains a challenge. Existing methods either alter the videos excessively, generate unrealistic artifacts, or fail to perform the requested edit consistently throughout the video. In this work, we focus on consistent and identity-preserving removal of glasses in videos, using it as a case study for consistent local attribute removal in videos. Due to the lack of paired data, we adopt a weakly supervised approach and generate synthetic imperfect data, using an adjusted pretrained diffusion model. We show that despite data imperfection, by learning from our generated data and leveraging the prior of pretrained diffusion models, our model is able to perform the desired edit consistently while preserving the original video content. Furthermore, we exemplify the generalization ability of our method to other local video editing tasks by applying it successfully to facial sticker-removal. Our approach demonstrates significant improvement over existing methods, showcasing the potential of leveraging synthetic data and strong video priors for local video editing tasks.
<div id='section'>Paperid: <span id='pid'>1138, <a href='https://arxiv.org/pdf/2406.00474.pdf' target='_blank'>https://arxiv.org/pdf/2406.00474.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zimin Xia, Yujiao Shi, Hongdong Li, Julian F. P. Kooij
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00474">Adapting Fine-Grained Cross-View Localization to Areas without Fine Ground Truth</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Given a ground-level query image and a geo-referenced aerial image that covers the query's local surroundings, fine-grained cross-view localization aims to estimate the location of the ground camera inside the aerial image. Recent works have focused on developing advanced networks trained with accurate ground truth (GT) locations of ground images. However, the trained models always suffer a performance drop when applied to images in a new target area that differs from training. In most deployment scenarios, acquiring fine GT, i.e. accurate GT locations, for target-area images to re-train the network can be expensive and sometimes infeasible. In contrast, collecting images with noisy GT with errors of tens of meters is often easy. Motivated by this, our paper focuses on improving the performance of a trained model in a new target area by leveraging only the target-area images without fine GT. We propose a weakly supervised learning approach based on knowledge self-distillation. This approach uses predictions from a pre-trained model as pseudo GT to supervise a copy of itself. Our approach includes a mode-based pseudo GT generation for reducing uncertainty in pseudo GT and an outlier filtering method to remove unreliable pseudo GT. Our approach is validated using two recent state-of-the-art models on two benchmarks. The results demonstrate that it consistently and considerably boosts the localization accuracy in the target area.
<div id='section'>Paperid: <span id='pid'>1139, <a href='https://arxiv.org/pdf/2405.18148.pdf' target='_blank'>https://arxiv.org/pdf/2405.18148.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>JuneHyoung Kwon, Eunju Lee, Yunsung Cho, YoungBin Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.18148">Learning to Detour: Shortcut Mitigating Augmentation for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation (WSSS) employing weak forms of labels has been actively studied to alleviate the annotation cost of acquiring pixel-level labels. However, classifiers trained on biased datasets tend to exploit shortcut features and make predictions based on spurious correlations between certain backgrounds and objects, leading to a poor generalization performance. In this paper, we propose shortcut mitigating augmentation (SMA) for WSSS, which generates synthetic representations of object-background combinations not seen in the training data to reduce the use of shortcut features. Our approach disentangles the object-relevant and background features. We then shuffle and combine the disentangled representations to create synthetic features of diverse object-background combinations. SMA-trained classifier depends less on contexts and focuses more on the target object when making predictions. In addition, we analyzed the behavior of the classifier on shortcut usage after applying our augmentation using an attribution method-based metric. The proposed method achieved the improved performance of semantic segmentation result on PASCAL VOC 2012 and MS COCO 2014 datasets.
<div id='section'>Paperid: <span id='pid'>1140, <a href='https://arxiv.org/pdf/2404.12832.pdf' target='_blank'>https://arxiv.org/pdf/2404.12832.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dmytro Shvetsov, Joonas Ariva, Marharyta Domnich, Raul Vicente, Dmytro Fishman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.12832">COIN: Counterfactual inpainting for weakly supervised semantic segmentation for medical images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning is dramatically transforming the field of medical imaging and radiology, enabling the identification of pathologies in medical images, including computed tomography (CT) and X-ray scans. However, the performance of deep learning models, particularly in segmentation tasks, is often limited by the need for extensive annotated datasets. To address this challenge, the capabilities of weakly supervised semantic segmentation are explored through the lens of Explainable AI and the generation of counterfactual explanations. The scope of this research is development of a novel counterfactual inpainting approach (COIN) that flips the predicted classification label from abnormal to normal by using a generative model. For instance, if the classifier deems an input medical image X as abnormal, indicating the presence of a pathology, the generative model aims to inpaint the abnormal region, thus reversing the classifier's original prediction label. The approach enables us to produce precise segmentations for pathologies without depending on pre-existing segmentation masks. Crucially, image-level labels are utilized, which are substantially easier to acquire than creating detailed segmentation masks. The effectiveness of the method is demonstrated by segmenting synthetic targets and actual kidney tumors from CT images acquired from Tartu University Hospital in Estonia. The findings indicate that COIN greatly surpasses established attribution methods, such as RISE, ScoreCAM, and LayerCAM, as well as an alternative counterfactual explanation method introduced by Singla et al. This evidence suggests that COIN is a promising approach for semantic segmentation of tumors in CT images, and presents a step forward in making deep learning applications more accessible and effective in healthcare, where annotated data is scarce.
<div id='section'>Paperid: <span id='pid'>1141, <a href='https://arxiv.org/pdf/2404.10242.pdf' target='_blank'>https://arxiv.org/pdf/2404.10242.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Oren Kraus, Kian Kenyon-Dean, Saber Saberian, Maryam Fallah, Peter McLean, Jess Leung, Vasudev Sharma, Ayla Khan, Jia Balakrishnan, Safiye Celik, Dominique Beaini, Maciej Sypetkowski, Chi Vicky Cheng, Kristen Morse, Maureen Makes, Ben Mabey, Berton Earnshaw
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10242">Masked Autoencoders for Microscopy are Scalable Learners of Cellular Biology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Featurizing microscopy images for use in biological research remains a significant challenge, especially for large-scale experiments spanning millions of images. This work explores the scaling properties of weakly supervised classifiers and self-supervised masked autoencoders (MAEs) when training with increasingly larger model backbones and microscopy datasets. Our results show that ViT-based MAEs outperform weakly supervised classifiers on a variety of tasks, achieving as much as a 11.5% relative improvement when recalling known biological relationships curated from public databases. Additionally, we develop a new channel-agnostic MAE architecture (CA-MAE) that allows for inputting images of different numbers and orders of channels at inference time. We demonstrate that CA-MAEs effectively generalize by inferring and evaluating on a microscopy image dataset (JUMP-CP) generated under different experimental conditions with a different channel structure than our pretraining data (RPI-93M). Our findings motivate continued research into scaling self-supervised learning on microscopy data in order to create powerful foundation models of cellular biology that have the potential to catalyze advancements in drug discovery and beyond.
<div id='section'>Paperid: <span id='pid'>1142, <a href='https://arxiv.org/pdf/2404.08531.pdf' target='_blank'>https://arxiv.org/pdf/2404.08531.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiwei Yang, Jing Liu, Peng Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.08531">Text Prompt with Normality Guidance for Weakly Supervised Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised video anomaly detection (WSVAD) is a challenging task. Generating fine-grained pseudo-labels based on weak-label and then self-training a classifier is currently a promising solution. However, since the existing methods use only RGB visual modality and the utilization of category text information is neglected, thus limiting the generation of more accurate pseudo-labels and affecting the performance of self-training. Inspired by the manual labeling process based on the event description, in this paper, we propose a novel pseudo-label generation and self-training framework based on Text Prompt with Normality Guidance (TPWNG) for WSVAD. Our idea is to transfer the rich language-visual knowledge of the contrastive language-image pre-training (CLIP) model for aligning the video event description text and corresponding video frames to generate pseudo-labels. Specifically, We first fine-tune the CLIP for domain adaptation by designing two ranking losses and a distributional inconsistency loss. Further, we propose a learnable text prompt mechanism with the assist of a normality visual prompt to further improve the matching accuracy of video event description text and video frames. Then, we design a pseudo-label generation module based on the normality guidance to infer reliable frame-level pseudo-labels. Finally, we introduce a temporal context self-adaptive learning module to learn the temporal dependencies of different video events more flexibly and accurately. Extensive experiments show that our method achieves state-of-the-art performance on two benchmark datasets, UCF-Crime and XD-Viole
<div id='section'>Paperid: <span id='pid'>1143, <a href='https://arxiv.org/pdf/2403.04865.pdf' target='_blank'>https://arxiv.org/pdf/2403.04865.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gabriele Campanella, Eugene Fluder, Jennifer Zeng, Chad Vanderbilt, Thomas J. Fuchs
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.04865">Beyond Multiple Instance Learning: Full Resolution All-In-Memory End-To-End Pathology Slide Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Artificial Intelligence (AI) has great potential to improve health outcomes by training systems on vast digitized clinical datasets. Computational Pathology, with its massive amounts of microscopy image data and impact on diagnostics and biomarkers, is at the forefront of this development. Gigapixel pathology slides pose a unique challenge due to their enormous size and are usually divided into tens of thousands of smaller tiles for analysis. This results in a discontinuity in the machine learning process by separating the training of tile-level encoders from slide-level aggregators and the need to adopt weakly supervised learning strategies. Training models from entire pathology slides end-to-end has been largely unexplored due to its computational challenges. To overcome this problem, we propose a novel approach to jointly train both a tile encoder and a slide-aggregator fully in memory and end-to-end at high-resolution, bridging the gap between input and slide-level supervision. While more computationally expensive, detailed quantitative validation shows promise for large-scale pre-training and fine-tuning of pathology foundation models.
<div id='section'>Paperid: <span id='pid'>1144, <a href='https://arxiv.org/pdf/2403.02746.pdf' target='_blank'>https://arxiv.org/pdf/2403.02746.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuohong Li, Wei He, Jiepan Li, Fangxiao Lu, Hongyan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.02746">Learning without Exact Guidance: Updating Large-scale High-resolution Land Cover Maps from Low-resolution Historical Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale high-resolution (HR) land-cover mapping is a vital task to survey the Earth's surface and resolve many challenges facing humanity. However, it is still a non-trivial task hindered by complex ground details, various landforms, and the scarcity of accurate training labels over a wide-span geographic area. In this paper, we propose an efficient, weakly supervised framework (Paraformer) to guide large-scale HR land-cover mapping with easy-access historical land-cover data of low resolution (LR). Specifically, existing land-cover mapping approaches reveal the dominance of CNNs in preserving local ground details but still suffer from insufficient global modeling in various landforms. Therefore, we design a parallel CNN-Transformer feature extractor in Paraformer, consisting of a downsampling-free CNN branch and a Transformer branch, to jointly capture local and global contextual information. Besides, facing the spatial mismatch of training data, a pseudo-label-assisted training (PLAT) module is adopted to reasonably refine LR labels for weakly supervised semantic segmentation of HR images. Experiments on two large-scale datasets demonstrate the superiority of Paraformer over other state-of-the-art methods for automatically updating HR land-cover maps from LR historical labels.
<div id='section'>Paperid: <span id='pid'>1145, <a href='https://arxiv.org/pdf/2403.01381.pdf' target='_blank'>https://arxiv.org/pdf/2403.01381.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Feng, Hao Huang, Junpeng Zhang, Weisheng Dong, Dingwen Zhang, Licheng Jiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.01381">SA-MixNet: Structure-aware Mixup and Invariance Learning for Scribble-supervised Road Extraction in Remote Sensing Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mainstreamed weakly supervised road extractors rely on highly confident pseudo-labels propagated from scribbles, and their performance often degrades gradually as the image scenes tend various. We argue that such degradation is due to the poor model's invariance to scenes with different complexities, whereas existing solutions to this problem are commonly based on crafted priors that cannot be derived from scribbles. To eliminate the reliance on such priors, we propose a novel Structure-aware Mixup and Invariance Learning framework (SA-MixNet) for weakly supervised road extraction that improves the model invariance in a data-driven manner. Specifically, we design a structure-aware Mixup scheme to paste road regions from one image onto another for creating an image scene with increased complexity while preserving the road's structural integrity. Then an invariance regularization is imposed on the predictions of constructed and origin images to minimize their conflicts, which thus forces the model to behave consistently on various scenes. Moreover, a discriminator-based regularization is designed for enhancing the connectivity meanwhile preserving the structure of roads. Combining these designs, our framework demonstrates superior performance on the DeepGlobe, Wuhan, and Massachusetts datasets outperforming the state-of-the-art techniques by 1.47%, 2.12%, 4.09% respectively in IoU metrics, and showing its potential of plug-and-play. The code will be made publicly available.
<div id='section'>Paperid: <span id='pid'>1146, <a href='https://arxiv.org/pdf/2402.19116.pdf' target='_blank'>https://arxiv.org/pdf/2402.19116.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiamin Luo, Jianing Zhao, Jingjing Wang, Guodong Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.19116">How to Understand "Support"? An Implicit-enhanced Causal Inference Approach for Weakly-supervised Phrase Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised Phrase Grounding (WPG) is an emerging task of inferring the fine-grained phrase-region matching, while merely leveraging the coarse-grained sentence-image pairs for training. However, existing studies on WPG largely ignore the implicit phrase-region matching relations, which are crucial for evaluating the capability of models in understanding the deep multimodal semantics. To this end, this paper proposes an Implicit-Enhanced Causal Inference (IECI) approach to address the challenges of modeling the implicit relations and highlighting them beyond the explicit. Specifically, this approach leverages both the intervention and counterfactual techniques to tackle the above two challenges respectively. Furthermore, a high-quality implicit-enhanced dataset is annotated to evaluate IECI and detailed evaluations show the great advantages of IECI over the state-of-the-art baselines. Particularly, we observe an interesting finding that IECI outperforms the advanced multimodal LLMs by a large margin on this implicit-enhanced dataset, which may facilitate more research to evaluate the multimodal LLMs in this direction.
<div id='section'>Paperid: <span id='pid'>1147, <a href='https://arxiv.org/pdf/2402.07685.pdf' target='_blank'>https://arxiv.org/pdf/2402.07685.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jacob Tyo, Zachary C. Lipton
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.07685">Contrastive Multiple Instance Learning for Weakly Supervised Person ReID</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The acquisition of large-scale, precisely labeled datasets for person re-identification (ReID) poses a significant challenge. Weakly supervised ReID has begun to address this issue, although its performance lags behind fully supervised methods. In response, we introduce Contrastive Multiple Instance Learning (CMIL), a novel framework tailored for more effective weakly supervised ReID. CMIL distinguishes itself by requiring only a single model and no pseudo labels while leveraging contrastive losses -- a technique that has significantly enhanced traditional ReID performance yet is absent in all prior MIL-based approaches. Through extensive experiments and analysis across three datasets, CMIL not only matches state-of-the-art performance on the large-scale SYSU-30k dataset with fewer assumptions but also consistently outperforms all baselines on the WL-market1501 and Weakly Labeled MUddy racer re-iDentification dataset (WL-MUDD) datasets. We introduce and release the WL-MUDD dataset, an extension of the MUDD dataset featuring naturally occurring weak labels from the real-world application at PerformancePhoto.co. All our code and data are accessible at https://drive.google.com/file/d/1rjMbWB6m-apHF3Wg_cfqc8QqKgQ21AsT/view?usp=drive_link.
<div id='section'>Paperid: <span id='pid'>1148, <a href='https://arxiv.org/pdf/2312.06978.pdf' target='_blank'>https://arxiv.org/pdf/2312.06978.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bodong Zhang, Hamid Manoochehri, Man Minh Ho, Fahimeh Fooladgar, Yosep Chong, Beatrice S. Knudsen, Deepika Sirohi, Tolga Tasdizen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.06978">CLASS-M: Adaptive stain separation-based contrastive learning with pseudo-labeling for histopathological image classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Histopathological image classification is an important task in medical image analysis. Recent approaches generally rely on weakly supervised learning due to the ease of acquiring case-level labels from pathology reports. However, patch-level classification is preferable in applications where only a limited number of cases are available or when local prediction accuracy is critical. On the other hand, acquiring extensive datasets with localized labels for training is not feasible. In this paper, we propose a semi-supervised patch-level histopathological image classification model, named CLASS-M, that does not require extensively labeled datasets. CLASS-M is formed by two main parts: a contrastive learning module that uses separated Hematoxylin and Eosin images generated through an adaptive stain separation process, and a module with pseudo-labels using MixUp. We compare our model with other state-of-the-art models on two clear cell renal cell carcinoma datasets. We demonstrate that our CLASS-M model has the best performance on both datasets. Our code is available at github.com/BzhangURU/Paper_CLASS-M/tree/main
<div id='section'>Paperid: <span id='pid'>1149, <a href='https://arxiv.org/pdf/2312.06699.pdf' target='_blank'>https://arxiv.org/pdf/2312.06699.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zaber Ibn Abdul Hakim, Najibul Haque Sarker, Rahul Pratap Singh, Bishmoy Paul, Ali Dabouei, Min Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.06699">Leveraging Generative Language Models for Weakly Supervised Sentence Component Analysis in Video-Language Joint Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A thorough comprehension of textual data is a fundamental element in multi-modal video analysis tasks. However, recent works have shown that the current models do not achieve a comprehensive understanding of the textual data during the training for the target downstream tasks. Orthogonal to the previous approaches to this limitation, we postulate that understanding the significance of the sentence components according to the target task can potentially enhance the performance of the models. Hence, we utilize the knowledge of a pre-trained large language model (LLM) to generate text samples from the original ones, targeting specific sentence components. We propose a weakly supervised importance estimation module to compute the relative importance of the components and utilize them to improve different video-language tasks. Through rigorous quantitative analysis, our proposed method exhibits significant improvement across several video-language tasks. In particular, our approach notably enhances video-text retrieval by a relative improvement of 8.3\% in video-to-text and 1.4\% in text-to-video retrieval over the baselines, in terms of R@1. Additionally, in video moment retrieval, average mAP shows a relative improvement ranging from 2.0\% to 13.7 \% across different baselines.
<div id='section'>Paperid: <span id='pid'>1150, <a href='https://arxiv.org/pdf/2311.12601.pdf' target='_blank'>https://arxiv.org/pdf/2311.12601.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Petru Manescu, Joseph Geradts, Delmiro Fernandez-Reyes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.12601">Deep learning-based detection of morphological features associated with hypoxia in H&E breast cancer whole slide images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hypoxia occurs when tumour cells outgrow their blood supply, leading to regions of low oxygen levels within the tumour. Calculating hypoxia levels can be an important step in understanding the biology of tumours, their clinical progression and response to treatment. This study demonstrates a novel application of deep learning to evaluate hypoxia in the context of breast cancer histomorphology. More precisely, we show that Weakly Supervised Deep Learning (WSDL) models can accurately detect hypoxia associated features in routine Hematoxylin and Eosin (H&E) whole slide images (WSI). We trained and evaluated a deep Multiple Instance Learning model on tiles from WSI H&E tissue from breast cancer primary sites (n=240) obtaining on average an AUC of 0.87 on a left-out test set. We also showed significant differences between features of hypoxic and normoxic tissue regions as distinguished by the WSDL models. Such DL hypoxia H&E WSI detection models could potentially be extended to other tumour types and easily integrated into the pathology workflow without requiring additional costly assays.
<div id='section'>Paperid: <span id='pid'>1151, <a href='https://arxiv.org/pdf/2311.08151.pdf' target='_blank'>https://arxiv.org/pdf/2311.08151.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yating Xu, Conghui Hu, Gim Hee Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.08151">Rethink Cross-Modal Fusion in Weakly-Supervised Audio-Visual Video Parsing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing works on weakly-supervised audio-visual video parsing adopt hybrid attention network (HAN) as the multi-modal embedding to capture the cross-modal context. It embeds the audio and visual modalities with a shared network, where the cross-attention is performed at the input. However, such an early fusion method highly entangles the two non-fully correlated modalities and leads to sub-optimal performance in detecting single-modality events. To deal with this problem, we propose the messenger-guided mid-fusion transformer to reduce the uncorrelated cross-modal context in the fusion. The messengers condense the full cross-modal context into a compact representation to only preserve useful cross-modal information. Furthermore, due to the fact that microphones capture audio events from all directions, while cameras only record visual events within a restricted field of view, there is a more frequent occurrence of unaligned cross-modal context from audio for visual event predictions. We thus propose cross-audio prediction consistency to suppress the impact of irrelevant audio information on visual event prediction. Experiments consistently illustrate the superior performance of our framework compared to existing state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1152, <a href='https://arxiv.org/pdf/2311.04584.pdf' target='_blank'>https://arxiv.org/pdf/2311.04584.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dragos Tantaru, Elisabeta Oneata, Dan Oneata
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.04584">Weakly-supervised deepfake localization in diffusion-generated images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The remarkable generative capabilities of denoising diffusion models have raised new concerns regarding the authenticity of the images we see every day on the Internet. However, the vast majority of existing deepfake detection models are tested against previous generative approaches (e.g. GAN) and usually provide only a "fake" or "real" label per image. We believe a more informative output would be to augment the per-image label with a localization map indicating which regions of the input have been manipulated. To this end, we frame this task as a weakly-supervised localization problem and identify three main categories of methods (based on either explanations, local scores or attention), which we compare on an equal footing by using the Xception network as the common backbone architecture. We provide a careful analysis of all the main factors that parameterize the design space: choice of method, type of supervision, dataset and generator used in the creation of manipulated images; our study is enabled by constructing datasets in which only one of the components is varied. Our results show that weakly-supervised localization is attainable, with the best performing detection method (based on local scores) being less sensitive to the looser supervision than to the mismatch in terms of dataset or generator.
<div id='section'>Paperid: <span id='pid'>1153, <a href='https://arxiv.org/pdf/2309.03978.pdf' target='_blank'>https://arxiv.org/pdf/2309.03978.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taesik Gong, Josh Belanich, Krishna Somandepalli, Arsha Nagrani, Brian Eoff, Brendan Jou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.03978">LanSER: Language-Model Supported Speech Emotion Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech emotion recognition (SER) models typically rely on costly human-labeled data for training, making scaling methods to large speech datasets and nuanced emotion taxonomies difficult. We present LanSER, a method that enables the use of unlabeled data by inferring weak emotion labels via pre-trained large language models through weakly-supervised learning. For inferring weak labels constrained to a taxonomy, we use a textual entailment approach that selects an emotion label with the highest entailment score for a speech transcript extracted via automatic speech recognition. Our experimental results show that models pre-trained on large datasets with this weak supervision outperform other baseline models on standard SER datasets when fine-tuned, and show improved label efficiency. Despite being pre-trained on labels derived only from text, we show that the resulting representations appear to model the prosodic content of speech.
<div id='section'>Paperid: <span id='pid'>1154, <a href='https://arxiv.org/pdf/2308.11757.pdf' target='_blank'>https://arxiv.org/pdf/2308.11757.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kshitij Nikhal, Benjamin S. Riggan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.11757">Weakly Supervised Face and Whole Body Recognition in Turbulent Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face and person recognition have recently achieved remarkable success under challenging scenarios, such as off-pose and cross-spectrum matching. However, long-range recognition systems are often hindered by atmospheric turbulence, leading to spatially and temporally varying distortions in the image. Current solutions rely on generative models to reconstruct a turbulent-free image, but often preserve photo-realism instead of discriminative features that are essential for recognition. This can be attributed to the lack of large-scale datasets of turbulent and pristine paired images, necessary for optimal reconstruction. To address this issue, we propose a new weakly supervised framework that employs a parameter-efficient self-attention module to generate domain agnostic representations, aligning turbulent and pristine images into a common subspace. Additionally, we introduce a new tilt map estimator that predicts geometric distortions observed in turbulent images. This estimate is used to re-rank gallery matches, resulting in up to 13.86\% improvement in rank-1 accuracy. Our method does not require synthesizing turbulent-free images or ground-truth paired images, and requires significantly fewer annotated samples, enabling more practical and rapid utility of increasingly large datasets. We analyze our framework using two datasets -- Long-Range Face Identification Dataset (LRFID) and BRIAR Government Collection 1 (BGC1) -- achieving enhanced discriminability under varying turbulence and standoff distance.
<div id='section'>Paperid: <span id='pid'>1155, <a href='https://arxiv.org/pdf/2308.10959.pdf' target='_blank'>https://arxiv.org/pdf/2308.10959.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sijin Wu, Dan Zhang, Teng Hu, Shikun Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.10959">DocPrompt: Large-scale continue pretrain for zero-shot and few-shot document question answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose Docprompt for document question answering tasks with powerful zero-shot and few-shot performance. We proposed a novel weakly supervised data generation method, a novel multl-stage training method and a novel understanding model \& generation model ensemble method. We achieved state-of-the-art performance on 4 document question answering tasks. This method greatly improves the delivery efficiency and model performance of document question answering customer projects, reducing annotation costs and labor costs. Our demo can be found at https://huggingface.co/spaces/PaddlePaddle/ERNIE-Layout.
<div id='section'>Paperid: <span id='pid'>1156, <a href='https://arxiv.org/pdf/2308.02118.pdf' target='_blank'>https://arxiv.org/pdf/2308.02118.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hang-Cheng Dong, Yuhao Jiang, Yingyan Huang, Jingxiao Liao, Bingguo Liu, Dong Ye, Guodong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.02118">Rethinking Class Activation Maps for Segmentation: Revealing Semantic Information in Shallow Layers by Reducing Noise</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Class activation maps are widely used for explaining deep neural networks. Due to its ability to highlight regions of interest, it has evolved in recent years as a key step in weakly supervised learning. A major limitation to the performance of the class activation maps is the small spatial resolution of the feature maps in the last layer of the convolutional neural network. Therefore, we expect to generate high-resolution feature maps that result in high-quality semantic information. In this paper, we rethink the properties of semantic information in shallow feature maps. We find that the shallow feature maps still have fine-grained non-discriminative features while mixing considerable non-target noise. Furthermore, we propose a simple gradient-based denoising method to filter the noise by truncating the positive gradient. Our proposed scheme can be easily deployed in other CAM-related methods, facilitating these methods to obtain higher-quality class activation maps. We evaluate the proposed approach through a weakly-supervised semantic segmentation task, and a large number of experiments demonstrate the effectiveness of our approach.
<div id='section'>Paperid: <span id='pid'>1157, <a href='https://arxiv.org/pdf/2307.01226.pdf' target='_blank'>https://arxiv.org/pdf/2307.01226.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weijie Xu, Xiaoyu Jiang, Srinivasan H. Sengamedu, Francis Iannacci, Jinjin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.01226">vONTSS: vMF based semi-supervised neural topic modeling with optimal transport</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, Neural Topic Models (NTM), inspired by variational autoencoders, have attracted a lot of research interest; however, these methods have limited applications in the real world due to the challenge of incorporating human knowledge. This work presents a semi-supervised neural topic modeling method, vONTSS, which uses von Mises-Fisher (vMF) based variational autoencoders and optimal transport. When a few keywords per topic are provided, vONTSS in the semi-supervised setting generates potential topics and optimizes topic-keyword quality and topic classification. Experiments show that vONTSS outperforms existing semi-supervised topic modeling methods in classification accuracy and diversity. vONTSS also supports unsupervised topic modeling. Quantitative and qualitative experiments show that vONTSS in the unsupervised setting outperforms recent NTMs on multiple aspects: vONTSS discovers highly clustered and coherent topics on benchmark datasets. It is also much faster than the state-of-the-art weakly supervised text classification method while achieving similar classification performance. We further prove the equivalence of optimal transport loss and cross-entropy loss at the global minimum.
<div id='section'>Paperid: <span id='pid'>1158, <a href='https://arxiv.org/pdf/2305.19879.pdf' target='_blank'>https://arxiv.org/pdf/2305.19879.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Subhankar Roy, Riccardo Volpi, Gabriela Csurka, Diane Larlus
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.19879">RaSP: Relation-aware Semantic Prior for Weakly Supervised Incremental Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Class-incremental semantic image segmentation assumes multiple model updates, each enriching the model to segment new categories. This is typically carried out by providing expensive pixel-level annotations to the training algorithm for all new objects, limiting the adoption of such methods in practical applications. Approaches that solely require image-level labels offer an attractive alternative, yet, such coarse annotations lack precise information about the location and boundary of the new objects. In this paper we argue that, since classes represent not just indices but semantic entities, the conceptual relationships between them can provide valuable information that should be leveraged. We propose a weakly supervised approach that exploits such semantic relations to transfer objectness prior from the previously learned classes into the new ones, complementing the supervisory signal from image-level labels. We validate our approach on a number of continual learning tasks, and show how even a simple pairwise interaction between classes can significantly improve the segmentation mask quality of both old and new classes. We show these conclusions still hold for longer and, hence, more realistic sequences of tasks and for a challenging few-shot scenario.
<div id='section'>Paperid: <span id='pid'>1159, <a href='https://arxiv.org/pdf/2305.18797.pdf' target='_blank'>https://arxiv.org/pdf/2305.18797.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaogang Peng, Hao Wen, Yikai Luo, Xiao Zhou, Keyang Yu, Ping Yang, Zizhao Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.18797">Learning Weakly Supervised Audio-Visual Violence Detection in Hyperbolic Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, the task of weakly supervised audio-visual violence detection has gained considerable attention. The goal of this task is to identify violent segments within multimodal data based on video-level labels. Despite advances in this field, traditional Euclidean neural networks, which have been used in prior research, encounter difficulties in capturing highly discriminative representations due to limitations of the feature space. To overcome this, we propose HyperVD, a novel framework that learns snippet embeddings in hyperbolic space to improve model discrimination. Our framework comprises a detour fusion module for multimodal fusion, effectively alleviating modality inconsistency between audio and visual signals. Additionally, we contribute two branches of fully hyperbolic graph convolutional networks that excavate feature similarities and temporal relationships among snippets in hyperbolic space. By learning snippet representations in this space, the framework effectively learns semantic discrepancies between violent and normal events. Extensive experiments on the XD-Violence benchmark demonstrate that our method outperforms state-of-the-art methods by a sizable margin.
<div id='section'>Paperid: <span id='pid'>1160, <a href='https://arxiv.org/pdf/2304.10054.pdf' target='_blank'>https://arxiv.org/pdf/2304.10054.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuoran Zheng, Xiuyi Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.10054">Complex Mixer for MedMNIST Classification Decathlon</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the development of the medical image field, researchers seek to develop a class of datasets to block the need for medical knowledge, such as \text{MedMNIST} (v2). MedMNIST (v2) includes a large number of small-sized (28 $\times$ 28 or 28 $\times$ 28 $\times$ 28) medical samples and the corresponding expert annotations (class label). The existing baseline model (Google AutoML Vision, ResNet-50+3D) can reach an average accuracy of over 70\% on MedMNIST (v2) datasets, which is comparable to the performance of expert decision-making. Nevertheless, we note that there are two insurmountable obstacles to modeling on MedMNIST (v2): 1) the raw images are cropped to low scales may cause effective recognition information to be dropped and the classifier to have difficulty in tracing accurate decision boundaries; 2) the labelers' subjective insight may cause many uncertainties in the label space. To address these issues, we develop a Complex Mixer (C-Mixer) with a pre-training framework to alleviate the problem of insufficient information and uncertainty in the label space by introducing an incentive imaginary matrix and a self-supervised scheme with random masking. Our method (incentive learning and self-supervised learning with masking) shows surprising potential on both the standard MedMNIST (v2) dataset, the customized weakly supervised datasets, and other image enhancement tasks.
<div id='section'>Paperid: <span id='pid'>1161, <a href='https://arxiv.org/pdf/2304.07295.pdf' target='_blank'>https://arxiv.org/pdf/2304.07295.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongquan Yang, Jie Chen, Yani Wei, Mohammad Alobaidi, Hong Bu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.07295">Experts' cognition-driven safe noisy labels learning for precise segmentation of residual tumor in breast cancer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Precise segmentation of residual tumor in breast cancer (PSRTBC) after neoadjuvant chemotherapy is a fundamental key technique in the treatment process of breast cancer. However, achieving PSRTBC is still a challenge, since the breast cancer tissue and tumor cells commonly have complex and varied morphological changes after neoadjuvant chemotherapy, which inevitably increases the difficulty to produce a predictive model that has good generalization with usual supervised learning (SL). To alleviate this situation, in this paper, we propose an experts' cognition-driven safe noisy labels learning (ECDSNLL) approach. In the concept of safe noisy labels learning, which is a typical type of safe weakly supervised learning, ECDSNLL is constructed by integrating the pathology experts' cognition about identifying residual tumor in breast cancer and the artificial intelligence experts' cognition about data modeling with provided data basis. Experimental results show that, compared with usual SL, ECDSNLL can significantly improve the lower bound of a number of UNet variants with 2.42% and 4.1% respectively in recall and fIoU for PSRTBC, while being able to achieve improvements in mean value and upper bound as well.
<div id='section'>Paperid: <span id='pid'>1162, <a href='https://arxiv.org/pdf/2303.17294.pdf' target='_blank'>https://arxiv.org/pdf/2303.17294.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifu Liu, Xiaoxia Li, Zhiling Luo, Wei Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.17294">JCDNet: Joint of Common and Definite phases Network for Weakly Supervised Temporal Action Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised temporal action localization aims to localize action instances in untrimmed videos with only video-level supervision. We witness that different actions record common phases, e.g., the run-up in the HighJump and LongJump. These different actions are defined as conjoint actions, whose rest parts are definite phases, e.g., leaping over the bar in a HighJump. Compared with the common phases, the definite phases are more easily localized in existing researches. Most of them formulate this task as a Multiple Instance Learning paradigm, in which the common phases are tended to be confused with the background, and affect the localization completeness of the conjoint actions. To tackle this challenge, we propose a Joint of Common and Definite phases Network (JCDNet) by improving feature discriminability of the conjoint actions. Specifically, we design a Class-Aware Discriminative module to enhance the contribution of the common phases in classification by the guidance of the coarse definite-phase features. Besides, we introduce a temporal attention module to learn robust action-ness scores via modeling temporal dependencies, distinguishing the common phases from the background. Extensive experiments on three datasets (THUMOS14, ActivityNetv1.2, and a conjoint-action subset) demonstrate that JCDNet achieves competitive performance against the state-of-the-art methods. Keywords: weakly-supervised learning, temporal action localization, conjoint action
<div id='section'>Paperid: <span id='pid'>1163, <a href='https://arxiv.org/pdf/2303.08689.pdf' target='_blank'>https://arxiv.org/pdf/2303.08689.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Patrick Zimmer, Michael Halstead, Chris McCool
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.08689">Panoptic One-Click Segmentation: Applied to Agricultural Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In weed control, precision agriculture can help to greatly reduce the use of herbicides, resulting in both economical and ecological benefits. A key element is the ability to locate and segment all the plants from image data. Modern instance segmentation techniques can achieve this, however, training such systems requires large amounts of hand-labelled data which is expensive and laborious to obtain. Weakly supervised training can help to greatly reduce labelling efforts and costs. We propose panoptic one-click segmentation, an efficient and accurate offline tool to produce pseudo-labels from click inputs which reduces labelling effort. Our approach jointly estimates the pixel-wise location of all N objects in the scene, compared to traditional approaches which iterate independently through all N objects; this greatly reduces training time. Using just 10% of the data to train our panoptic one-click segmentation approach yields 68.1% and 68.8% mean object intersection over union (IoU) on challenging sugar beet and corn image data respectively, providing comparable performance to traditional one-click approaches while being approximately 12 times faster to train. We demonstrate the applicability of our system by generating pseudo-labels from clicks on the remaining 90% of the data. These pseudo-labels are then used to train Mask R-CNN, in a semi-supervised manner, improving the absolute performance (of mean foreground IoU) by 9.4 and 7.9 points for sugar beet and corn data respectively. Finally, we show that our technique can recover missed clicks during annotation outlining a further benefit over traditional approaches.
<div id='section'>Paperid: <span id='pid'>1164, <a href='https://arxiv.org/pdf/2303.02967.pdf' target='_blank'>https://arxiv.org/pdf/2303.02967.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liwen Zou, Zhenghua Cai, Liang Mao, Ziwei Nie, Yudong Qiu, Xiaoping Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.02967">Automated Peripancreatic Vessel Segmentation and Labeling Based on Iterative Trunk Growth and Weakly Supervised Mechanism</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Peripancreatic vessel segmentation and anatomical labeling play extremely important roles to assist the early diagnosis, surgery planning and prognosis for patients with pancreatic tumors. However, most current techniques cannot achieve satisfactory segmentation performance for peripancreatic veins and usually make predictions with poor integrity and connectivity. Besides, unsupervised labeling algorithms cannot deal with complex anatomical variation while fully supervised methods require a large number of voxel-wise annotations for training, which is very labor-intensive and time-consuming. To address these problems, we propose our Automated Peripancreatic vEssel Segmentation and lAbeling (APESA) framework, to not only highly improve the segmentation performance for peripancreatic veins, but also efficiently identify the peripancreatic artery branches. There are two core modules in our proposed APESA framework: iterative trunk growth module (ITGM) for vein segmentation and weakly supervised labeling mechanism (WSLM) for artery branch identification. Our proposed ITGM is composed of a series of trunk growth modules, each of which chooses the most reliable trunk of a basic vessel prediction by the largest connected constraint, and seeks for the possible growth branches by branch proposal network. Our designed iterative process guides the raw trunk to be more complete and fully connected. Our proposed WSLM consists of an unsupervised rule-based preprocessing for generating pseudo branch annotations, and an anatomical labeling network to learn the branch distribution voxel by voxel. We achieve Dice of 94.01% for vein segmentation on our collected dataset, which boosts the accuracy by nearly 10% compared with the state-of-the-art methods. Additionally, we also achieve Dice of 97.01% on segmentation and competitive performance on anatomical labeling for peripancreatic arteries.
<div id='section'>Paperid: <span id='pid'>1165, <a href='https://arxiv.org/pdf/2302.05132.pdf' target='_blank'>https://arxiv.org/pdf/2302.05132.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingjie Wang, Yande Li, Jun Zhou, Graham W. Taylor, Minglun Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.05132">GCNet: Probing Self-Similarity Learning for Generalized Counting Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The class-agnostic counting (CAC) problem has caught increasing attention recently due to its wide societal applications and arduous challenges. To count objects of different categories, existing approaches rely on user-provided exemplars, which is hard-to-obtain and limits their generality. In this paper, we aim to empower the framework to recognize adaptive exemplars within the whole images. A zero-shot Generalized Counting Network (GCNet) is developed, which uses a pseudo-Siamese structure to automatically and effectively learn pseudo exemplar clues from inherent repetition patterns. In addition, a weakly-supervised scheme is presented to reduce the burden of laborious density maps required by all contemporary CAC models, allowing GCNet to be trained using count-level supervisory signals in an end-to-end manner. Without providing any spatial location hints, GCNet is capable of adaptively capturing them through a carefully-designed self-similarity learning strategy. Extensive experiments and ablation studies on the prevailing benchmark FSC147 for zero-shot CAC demonstrate the superiority of our GCNet. It performs on par with existing exemplar-dependent methods and shows stunning cross-dataset generality on crowd-specific datasets, e.g., ShanghaiTech Part A, Part B and UCF_QNRF.
<div id='section'>Paperid: <span id='pid'>1166, <a href='https://arxiv.org/pdf/2302.04064.pdf' target='_blank'>https://arxiv.org/pdf/2302.04064.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guy Bar-Shalom, George Leifman, Michael Elad, Ehud Rivlin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.04064">Weakly-supervised Representation Learning for Video Alignment and Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many tasks in video analysis and understanding boil down to the need for frame-based feature learning, aiming to encapsulate the relevant visual content so as to enable simpler and easier subsequent processing. While supervised strategies for this learning task can be envisioned, self and weakly-supervised alternatives are preferred due to the difficulties in getting labeled data. This paper introduces LRProp -- a novel weakly-supervised representation learning approach, with an emphasis on the application of temporal alignment between pairs of videos of the same action category. The proposed approach uses a transformer encoder for extracting frame-level features, and employs the DTW algorithm within the training iterations in order to identify the alignment path between video pairs. Through a process referred to as ``pair-wise position propagation'', the probability distributions of these correspondences per location are matched with the similarity of the frame-level features via KL-divergence minimization. The proposed algorithm uses also a regularized SoftDTW loss for better tuning the learned features. Our novel representation learning paradigm consistently outperforms the state of the art on temporal alignment tasks, establishing a new performance bar over several downstream video analysis applications.
<div id='section'>Paperid: <span id='pid'>1167, <a href='https://arxiv.org/pdf/2210.03594.pdf' target='_blank'>https://arxiv.org/pdf/2210.03594.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rattana Pukdee, Dylan Sam, Maria-Florina Balcan, Pradeep Ravikumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.03594">Label Propagation with Weak Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semi-supervised learning and weakly supervised learning are important paradigms that aim to reduce the growing demand for labeled data in current machine learning applications. In this paper, we introduce a novel analysis of the classical label propagation algorithm (LPA) (Zhu & Ghahramani, 2002) that moreover takes advantage of useful prior information, specifically probabilistic hypothesized labels on the unlabeled data. We provide an error bound that exploits both the local geometric properties of the underlying graph and the quality of the prior information. We also propose a framework to incorporate multiple sources of noisy information. In particular, we consider the setting of weak supervision, where our sources of information are weak labelers. We demonstrate the ability of our approach on multiple benchmark weakly supervised classification tasks, showing improvements upon existing semi-supervised and weakly supervised methods.
<div id='section'>Paperid: <span id='pid'>1168, <a href='https://arxiv.org/pdf/2208.13001.pdf' target='_blank'>https://arxiv.org/pdf/2208.13001.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomas A. Ciarfuglia, Ionut M. Motoi, Leonardo Saraceni, Mulham Fawakherji, Alberto Sanfeliu, Daniele Nardi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.13001">Weakly and Semi-Supervised Detection, Segmentation and Tracking of Table Grapes with Limited and Noisy Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detection, segmentation and tracking of fruits and vegetables are three fundamental tasks for precision agriculture, enabling robotic harvesting and yield estimation applications. However, modern algorithms are data hungry and it is not always possible to gather enough data to apply the best performing supervised approaches. Since data collection is an expensive and cumbersome task, the enabling technologies for using computer vision in agriculture are often out of reach for small businesses. Following previous work in this context, where we proposed an initial weakly supervised solution to reduce the data needed to get state-of-the-art detection and segmentation in precision agriculture applications, here we improve that system and explore the problem of tracking fruits in orchards. We present the case of vineyards of table grapes in southern Lazio (Italy) since grapes are a difficult fruit to segment due to occlusion, color and general illumination conditions. We consider the case in which there is some initial labelled data that could work as source data (\eg wine grape data), but it is considerably different from the target data (e.g. table grape data). To improve detection and segmentation on the target data, we propose to train the segmentation algorithm with a weak bounding box label, while for tracking we leverage 3D Structure from Motion algorithms to generate new labels from already labelled samples. Finally, the two systems are combined in a full semi-supervised approach. Comparisons with state-of-the-art supervised solutions show how our methods are able to train new models that achieve high performances with few labelled images and with very simple labelling.
<div id='section'>Paperid: <span id='pid'>1169, <a href='https://arxiv.org/pdf/2203.12459.pdf' target='_blank'>https://arxiv.org/pdf/2203.12459.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arvi Jonnarth, Michael Felsberg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.12459">Importance Sampling CAMs for Weakly-Supervised Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Classification networks can be used to localize and segment objects in images by means of class activation maps (CAMs). However, without pixel-level annotations, classification networks are known to (1) mainly focus on discriminative regions, and (2) to produce diffuse CAMs without well-defined prediction contours. In this work, we approach both problems with two contributions for improving CAM learning. First, we incorporate importance sampling based on the class-wise probability mass function induced by the CAMs to produce stochastic image-level class predictions. This results in CAMs which activate over a larger extent of objects. Second, we formulate a feature similarity loss term which aims to match the prediction contours with edges in the image. As a third contribution, we conduct experiments on the PASCAL VOC 2012 benchmark dataset to demonstrate that these modifications significantly increase the performance in terms of contour accuracy, while being comparable to current state-of-the-art methods in terms of region similarity.
<div id='section'>Paperid: <span id='pid'>1170, <a href='https://arxiv.org/pdf/2105.10996.pdf' target='_blank'>https://arxiv.org/pdf/2105.10996.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuangjun Liu, Michael Wan, Sarah Ostadabbas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2105.10996">Heuristic Weakly Supervised 3D Human Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monocular 3D human pose estimation from RGB images has attracted significant attention in recent years. However, recent models depend on supervised training with 3D pose ground truth data or known pose priors for their target domains. 3D pose data is typically collected with motion capture devices, severely limiting their applicability. In this paper, we present a heuristic weakly supervised 3D human pose (HW-HuP) solution to estimate 3D poses in when no ground truth 3D pose data is available. HW-HuP learns partial pose priors from 3D human pose datasets and uses easy-to-access observations from the target domain to estimate 3D human pose and shape in an optimization and regression cycle. We employ depth data for weak supervision during training, but not inference. We show that HW-HuP meaningfully improves upon state-of-the-art models in two practical settings where 3D pose data can hardly be obtained: human poses in bed, and infant poses in the wild. Furthermore, we show that HW-HuP retains comparable performance to cutting-edge models on public benchmarks, even when such models train on 3D pose data.
<div id='section'>Paperid: <span id='pid'>1171, <a href='https://arxiv.org/pdf/2511.15396.pdf' target='_blank'>https://arxiv.org/pdf/2511.15396.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simon Boeder, Fabian Gigengack, Simon Roesler, Holger Caesar, Benjamin Risse
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.15396">ShelfOcc: Native 3D Supervision beyond LiDAR for Vision-Based Occupancy Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent progress in self- and weakly supervised occupancy estimation has largely relied on 2D projection or rendering-based supervision, which suffers from geometric inconsistencies and severe depth bleeding. We thus introduce ShelfOcc, a vision-only method that overcomes these limitations without relying on LiDAR. ShelfOcc brings supervision into native 3D space by generating metrically consistent semantic voxel labels from video, enabling true 3D supervision without any additional sensors or manual 3D annotations. While recent vision-based 3D geometry foundation models provide a promising source of prior knowledge, they do not work out of the box as a prediction due to sparse or noisy and inconsistent geometry, especially in dynamic driving scenes. Our method introduces a dedicated framework that mitigates these issues by filtering and accumulating static geometry consistently across frames, handling dynamic content and propagating semantic information into a stable voxel representation. This data-centric shift in supervision for weakly/shelf-supervised occupancy estimation allows the use of essentially any SOTA occupancy model architecture without relying on LiDAR data. We argue that such high-quality supervision is essential for robust occupancy learning and constitutes an important complementary avenue to architectural innovation. On the Occ3D-nuScenes benchmark, ShelfOcc substantially outperforms all previous weakly/shelf-supervised methods (up to a 34% relative improvement), establishing a new data-driven direction for LiDAR-free 3D scene understanding.
<div id='section'>Paperid: <span id='pid'>1172, <a href='https://arxiv.org/pdf/2511.13891.pdf' target='_blank'>https://arxiv.org/pdf/2511.13891.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seyed Mohamad Ali Tousi, John A. Lory, G. N. DeSouza
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13891">Weakly Supervised Ephemeral Gully Detection In Remote Sensing Images Using Vision Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Among soil erosion problems, Ephemeral Gullies are one of the most concerning phenomena occurring in agricultural fields. Their short temporal cycles increase the difficulty in automatically detecting them using classical computer vision approaches and remote sensing. Also, due to scarcity of and the difficulty in producing accurate labeled data, automatic detection of ephemeral gullies using Machine Learning is limited to zero-shot approaches which are hard to implement. To overcome these challenges, we present the first weakly supervised pipeline for detection of ephemeral gullies. Our method relies on remote sensing and uses Vision Language Models (VLMs) to drastically reduce the labor-intensive task of manual labeling. In order to achieve that, the method exploits: 1) the knowledge embedded in the VLM's pretraining; 2) a teacher-student model where the teacher learns from noisy labels coming from the VLMs, and the student learns by weak supervision using teacher-generate labels and a noise-aware loss function. We also make available the first-of-its-kind dataset for semi-supervised detection of ephemeral gully from remote-sensed images. The dataset consists of a number of locations labeled by a group of soil and plant scientists, as well as a large number of unlabeled locations. The dataset represent more than 18,000 high-resolution remote-sensing images obtained over the course of 13 years. Our experimental results demonstrate the validity of our approach by showing superior performances compared to VLMs and the label model itself when using weak supervision to train an student model. The code and dataset for this work are made publicly available.
<div id='section'>Paperid: <span id='pid'>1173, <a href='https://arxiv.org/pdf/2511.12269.pdf' target='_blank'>https://arxiv.org/pdf/2511.12269.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rupam Mukherjee, Rajkumar Daniel, Soujanya Hazra, Shirin Dasgupta, Subhamoy Mandal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.12269">RAA-MIL: A Novel Framework for Classification of Oral Cytology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cytology is a valuable tool for early detection of oral squamous cell carcinoma (OSCC). However, manual examination of cytology whole slide images (WSIs) is slow, subjective, and depends heavily on expert pathologists. To address this, we introduce the first weakly supervised deep learning framework for patient-level diagnosis of oral cytology whole slide images, leveraging the newly released Oral Cytology Dataset [1], which provides annotated cytology WSIs from ten medical centres across India. Each patient case is represented as a bag of cytology patches and assigned a diagnosis label (Healthy, Benign, Oral Potentially Malignant Disorders (OPMD), OSCC) by an in-house expert pathologist. These patient-level weak labels form a new extension to the dataset. We evaluate a baseline multiple-instance learning (MIL) model and a proposed Region-Affinity Attention MIL (RAA-MIL) that models spatial relationships between regions within each slide. The RAA-MIL achieves an average accuracy of 72.7%, weighted F1-score of 0.69 on an unseen test set, outperforming the baseline. This study establishes the first patient-level weakly supervised benchmark for oral cytology and moves toward reliable AI-assisted digital pathology.
<div id='section'>Paperid: <span id='pid'>1174, <a href='https://arxiv.org/pdf/2510.12827.pdf' target='_blank'>https://arxiv.org/pdf/2510.12827.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md. Nayeem, Md Shamse Tabrej, Kabbojit Jit Deb, Shaonti Goswami, Md. Azizul Hakim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.12827">Automatic Speech Recognition in the Modern Era: Architectures, Training, and Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic Speech Recognition (ASR) has undergone a profound transformation over the past decade, driven by advances in deep learning. This survey provides a comprehensive overview of the modern era of ASR, charting its evolution from traditional hybrid systems, such as Gaussian Mixture Model-Hidden Markov Models (GMM-HMMs) and Deep Neural Network-HMMs (DNN-HMMs), to the now-dominant end-to-end neural architectures. We systematically review the foundational end-to-end paradigms: Connectionist Temporal Classification (CTC), attention-based encoder-decoder models, and the Recurrent Neural Network Transducer (RNN-T), which established the groundwork for fully integrated speech-to-text systems. We then detail the subsequent architectural shift towards Transformer and Conformer models, which leverage self-attention to capture long-range dependencies with high computational efficiency. A central theme of this survey is the parallel revolution in training paradigms. We examine the progression from fully supervised learning, augmented by techniques like SpecAugment, to the rise of self-supervised learning (SSL) with foundation models such as wav2vec 2.0, which drastically reduce the reliance on transcribed data. Furthermore, we analyze the impact of largescale, weakly supervised models like Whisper, which achieve unprecedented robustness through massive data diversity. The paper also covers essential ecosystem components, including key datasets and benchmarks (e.g., LibriSpeech, Switchboard, CHiME), standard evaluation metrics (e.g., Word Error Rate), and critical considerations for real-world deployment, such as streaming inference, on-device efficiency, and the ethical imperatives of fairness and robustness. We conclude by outlining open challenges and future research directions.
<div id='section'>Paperid: <span id='pid'>1175, <a href='https://arxiv.org/pdf/2510.04477.pdf' target='_blank'>https://arxiv.org/pdf/2510.04477.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soo Yong Kim, Suin Cho, Vincent-Daniel Yun, Gyeongyeon Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04477">MedCLM: Learning to Localize and Reason via a CoT-Curriculum in Medical Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Bridging clinical diagnostic reasoning with AI remains a central challenge in medical imaging. We introduce MedCLM, an automated pipeline that converts detection datasets into large-scale medical visual question answering (VQA) data with Chain-of-Thought (CoT) reasoning by linking lesion boxes to organ segmentation and structured rationales. These contextual signals enable medical vision-language models to generate question-answer pairs with step-by-step reasoning. To utilize this data effectively, we propose an Integrated CoT-Curriculum Strategy composed of an Easy stage with explicit lesion boxes for visual grounding, a Medium stage that encourages implicit localization, and a Hard stage for weakly supervised reasoning. Experimental results demonstrate that MedCLM attains state-of-the-art performance on several medical VQA benchmarks, providing a scalable framework for developing clinically aligned medical vision-language models.
<div id='section'>Paperid: <span id='pid'>1176, <a href='https://arxiv.org/pdf/2509.10184.pdf' target='_blank'>https://arxiv.org/pdf/2509.10184.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leen Almajed, Abeer ALdayel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10184">Incongruent Positivity: When Miscalibrated Positivity Undermines Online Supportive Conversations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In emotionally supportive conversations, well-intended positivity can sometimes misfire, leading to responses that feel dismissive, minimizing, or unrealistically optimistic. We examine this phenomenon of incongruent positivity as miscalibrated expressions of positive support in both human and LLM generated responses. To this end, we collected real user-assistant dialogues from Reddit across a range of emotional intensities and generated additional responses using large language models for the same context. We categorize these conversations by intensity into two levels: Mild, which covers relationship tension and general advice, and Severe, which covers grief and anxiety conversations. This level of categorization enables a comparative analysis of how supportive responses vary across lower and higher stakes contexts. Our analysis reveals that LLMs are more prone to unrealistic positivity through dismissive and minimizing tone, particularly in high-stakes contexts. To further study the underlying dimensions of this phenomenon, we finetune LLMs on datasets with strong and weak emotional reactions. Moreover, we developed a weakly supervised multilabel classifier ensemble (DeBERTa and MentalBERT) that shows improved detection of incongruent positivity types across two sorts of concerns (Mild and Severe). Our findings shed light on the need to move beyond merely generating generic positive responses and instead study the congruent support measures to balance positive affect with emotional acknowledgment. This approach offers insights into aligning large language models with affective expectations in the online supportive dialogue, paving the way toward context-aware and trust preserving online conversation systems.
<div id='section'>Paperid: <span id='pid'>1177, <a href='https://arxiv.org/pdf/2509.04491.pdf' target='_blank'>https://arxiv.org/pdf/2509.04491.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinnian Zhao, Hugo Van Hamme
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04491">Refining Transcripts With TV Subtitles by Prompt-Based Weakly Supervised Training of ASR</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study proposes a novel approach to using TV subtitles within a weakly supervised (WS) Automatic Speech Recognition (ASR) framework. Although TV subtitles are readily available, their imprecise alignment with corresponding audio limits their applicability as supervised targets for verbatim transcription. Rather than using subtitles as direct supervision signals, our method reimagines them as context-rich prompts. This design enables the model to handle discrepancies between spoken audio and subtitle text. Instead, generated pseudo transcripts become the primary targets, with subtitles acting as guiding cues for iterative refinement. To further enhance the process, we introduce a weighted attention mechanism that emphasizes relevant subtitle tokens during inference. Our experiments demonstrate significant improvements in transcription accuracy, highlighting the effectiveness of the proposed method in refining transcripts. These enhanced pseudo-labeled datasets provide high-quality foundational resources for training robust ASR systems.
<div id='section'>Paperid: <span id='pid'>1178, <a href='https://arxiv.org/pdf/2509.01214.pdf' target='_blank'>https://arxiv.org/pdf/2509.01214.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yizhe Yuan, Bingsen Xue, Bangzheng Pu, Chengxiang Wang, Cheng Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01214">PRINTER:Deformation-Aware Adversarial Learning for Virtual IHC Staining with In Situ Fidelity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tumor spatial heterogeneity analysis requires precise correlation between Hematoxylin and Eosin H&E morphology and immunohistochemical (IHC) biomarker expression, yet current methods suffer from spatial misalignment in consecutive sections, severely compromising in situ pathological interpretation. In order to obtain a more accurate virtual staining pattern, We propose PRINTER, a weakly-supervised framework that integrates PRototype-drIven content and staiNing patTERn decoupling and deformation-aware adversarial learning strategies designed to accurately learn IHC staining patterns while preserving H&E staining details. Our approach introduces three key innovations: (1) A prototype-driven staining pattern transfer with explicit content-style decoupling; and (2) A cyclic registration-synthesis framework GapBridge that bridges H&E and IHC domains through deformable structural alignment, where registered features guide cross-modal style transfer while synthesized outputs iteratively refine the registration;(3) Deformation-Aware Adversarial Learning: We propose a training framework where a generator and deformation-aware registration network jointly adversarially optimize a style-focused discriminator. Extensive experiments demonstrate that PRINTER effectively achieves superior performance in preserving H&E staining details and virtual staining fidelity, outperforming state-of-the-art methods. Our work provides a robust and scalable solution for virtual staining, advancing the field of computational pathology.
<div id='section'>Paperid: <span id='pid'>1179, <a href='https://arxiv.org/pdf/2508.12290.pdf' target='_blank'>https://arxiv.org/pdf/2508.12290.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chor Boon Tan, Conghui Hu, Gim Hee Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12290">CLAIR: CLIP-Aided Weakly Supervised Zero-Shot Cross-Domain Image Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent growth of large foundation models that can easily generate pseudo-labels for huge quantity of unlabeled data makes unsupervised Zero-Shot Cross-Domain Image Retrieval (UZS-CDIR) less relevant. In this paper, we therefore turn our attention to weakly supervised ZS-CDIR (WSZS-CDIR) with noisy pseudo labels generated by large foundation models such as CLIP. To this end, we propose CLAIR to refine the noisy pseudo-labels with a confidence score from the similarity between the CLIP text and image features. Furthermore, we design inter-instance and inter-cluster contrastive losses to encode images into a class-aware latent space, and an inter-domain contrastive loss to alleviate domain discrepancies. We also learn a novel cross-domain mapping function in closed-form, using only CLIP text embeddings to project image features from one domain to another, thereby further aligning the image features for retrieval. Finally, we enhance the zero-shot generalization ability of our CLAIR to handle novel categories by introducing an extra set of learnable prompts. Extensive experiments are carried out using TUBerlin, Sketchy, Quickdraw, and DomainNet zero-shot datasets, where our CLAIR consistently shows superior performance compared to existing state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1180, <a href='https://arxiv.org/pdf/2505.18368.pdf' target='_blank'>https://arxiv.org/pdf/2505.18368.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yike Zhang, Jack H. Noble
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18368">Weakly-supervised Mamba-Based Mastoidectomy Shape Prediction for Cochlear Implant Surgery Using 3D T-Distribution Loss</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cochlear implant surgery is a treatment for individuals with severe hearing loss. It involves inserting an array of electrodes inside the cochlea to electrically stimulate the auditory nerve and restore hearing sensation. A crucial step in this procedure is mastoidectomy, a surgical intervention that removes part of the mastoid region of the temporal bone, providing a critical pathway to the cochlea for electrode placement. Accurate prediction of the mastoidectomy region from preoperative imaging assists presurgical planning, reduces surgical risks, and improves surgical outcomes. In previous work, a self-supervised network was introduced to predict the mastoidectomy region using only preoperative CT scans. While promising, the method suffered from suboptimal robustness, limiting its practical application. To address this limitation, we propose a novel weakly-supervised Mamba-based framework to predict accurate mastoidectomy regions directly from preoperative CT scans. Our approach utilizes a 3D T-Distribution loss function inspired by the Student-t distribution, which effectively handles the complex geometric variability inherent in mastoidectomy shapes. Weak supervision is achieved using the segmentation results from the prior self-supervised network to eliminate the need for manual data cleaning or labeling throughout the training process. The proposed method is extensively evaluated against state-of-the-art approaches, demonstrating superior performance in predicting accurate and clinically relevant mastoidectomy regions. Our findings highlight the robustness and efficiency of the weakly-supervised learning framework with the proposed novel 3D T-Distribution loss.
<div id='section'>Paperid: <span id='pid'>1181, <a href='https://arxiv.org/pdf/2505.13911.pdf' target='_blank'>https://arxiv.org/pdf/2505.13911.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruijie Zhao, Zuopeng Tan, Xiao Xue, Longfei Zhao, Bing Li, Zicheng Liao, Ying Ming, Jiaru Wang, Ran Xiao, Sirong Piao, Rui Zhao, Qiqi Xu, Wei Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13911">Bronchovascular Tree-Guided Weakly Supervised Learning Method for Pulmonary Segment Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pulmonary segment segmentation is crucial for cancer localization and surgical planning. However, the pixel-wise annotation of pulmonary segments is laborious, as the boundaries between segments are indistinguishable in medical images. To this end, we propose a weakly supervised learning (WSL) method, termed Anatomy-Hierarchy Supervised Learning (AHSL), which consults the precise clinical anatomical definition of pulmonary segments to perform pulmonary segment segmentation. Since pulmonary segments reside within the lobes and are determined by the bronchovascular tree, i.e., artery, airway and vein, the design of the loss function is founded on two principles. First, segment-level labels are utilized to directly supervise the output of the pulmonary segments, ensuring that they accurately encompass the appropriate bronchovascular tree. Second, lobe-level supervision indirectly oversees the pulmonary segment, ensuring their inclusion within the corresponding lobe. Besides, we introduce a two-stage segmentation strategy that incorporates bronchovascular priori information. Furthermore, a consistency loss is proposed to enhance the smoothness of segment boundaries, along with an evaluation metric designed to measure the smoothness of pulmonary segment boundaries. Visual inspection and evaluation metrics from experiments conducted on a private dataset demonstrate the effectiveness of our method.
<div id='section'>Paperid: <span id='pid'>1182, <a href='https://arxiv.org/pdf/2505.09615.pdf' target='_blank'>https://arxiv.org/pdf/2505.09615.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yung-Hsuan Lai, Janek Ebbers, Yu-Chiang Frank Wang, FranÃ§ois Germain, Michael Jeffrey Jones, Moitreya Chatterjee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09615">UWAV: Uncertainty-weighted Weakly-supervised Audio-Visual Video Parsing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-Visual Video Parsing (AVVP) entails the challenging task of localizing both uni-modal events (i.e., those occurring exclusively in either the visual or acoustic modality of a video) and multi-modal events (i.e., those occurring in both modalities concurrently). Moreover, the prohibitive cost of annotating training data with the class labels of all these events, along with their start and end times, imposes constraints on the scalability of AVVP techniques unless they can be trained in a weakly-supervised setting, where only modality-agnostic, video-level labels are available in the training data. To this end, recently proposed approaches seek to generate segment-level pseudo-labels to better guide model training. However, the absence of inter-segment dependencies when generating these pseudo-labels and the general bias towards predicting labels that are absent in a segment limit their performance. This work proposes a novel approach towards overcoming these weaknesses called Uncertainty-weighted Weakly-supervised Audio-visual Video Parsing (UWAV). Additionally, our innovative approach factors in the uncertainty associated with these estimated pseudo-labels and incorporates a feature mixup based training regularization for improved training. Empirical results show that UWAV outperforms state-of-the-art methods for the AVVP task on multiple metrics, across two different datasets, attesting to its effectiveness and generalizability.
<div id='section'>Paperid: <span id='pid'>1183, <a href='https://arxiv.org/pdf/2504.14300.pdf' target='_blank'>https://arxiv.org/pdf/2504.14300.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Liang, Hao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14300">Learning and Generating Diverse Residential Load Patterns Using GAN with Weakly-Supervised Training and Weight Selection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The scarcity of high-quality residential load data can pose obstacles for decarbonizing the residential sector as well as effective grid planning and operation. The above challenges have motivated research into generating synthetic load data, but existing methods faced limitations in terms of scalability, diversity, and similarity. This paper proposes a Generative Adversarial Network-based Synthetic Residential Load Pattern (RLP-GAN) generation model, a novel weakly-supervised GAN framework, leveraging an over-complete autoencoder to capture dependencies within complex and diverse load patterns and learn household-level data distribution at scale. We incorporate a model weight selection method to address the mode collapse problem and generate load patterns with high diversity. We develop a holistic evaluation method to validate the effectiveness of RLP-GAN using real-world data of 417 households. The results demonstrate that RLP-GAN outperforms state-of-the-art models in capturing temporal dependencies and generating load patterns with higher similarity to real data. Furthermore, we have publicly released the RLP-GAN generated synthetic dataset, which comprises one million synthetic residential load pattern profiles.
<div id='section'>Paperid: <span id='pid'>1184, <a href='https://arxiv.org/pdf/2503.18384.pdf' target='_blank'>https://arxiv.org/pdf/2503.18384.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Gao, Shaobo Xia, Pu Wang, Xiaohuan Xi, Sheng Nie, Cheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18384">LiDAR Remote Sensing Meets Weak Supervision: Concepts, Methods, and Perspectives</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LiDAR (Light Detection and Ranging) enables rapid and accurate acquisition of three-dimensional spatial data, widely applied in remote sensing areas such as surface mapping, environmental monitoring, urban modeling, and forestry inventory. LiDAR remote sensing primarily includes data interpretation and LiDAR-based inversion. However, LiDAR interpretation typically relies on dense and precise annotations, which are costly and time-consuming. Similarly, LiDAR inversion depends on scarce supervisory signals and expensive field surveys for annotations. To address this challenge, weakly supervised learning has gained significant attention in recent years, with many methods emerging to tackle LiDAR remote sensing tasks using incomplete, inaccurate, and inexact annotations, as well as annotations from other domains. Existing review articles treat LiDAR interpretation and inversion as separate tasks. This review, for the first time, adopts a unified weakly supervised learning perspective to systematically examine research on both LiDAR interpretation and inversion. We summarize the latest advancements, provide a comprehensive review of the development and application of weakly supervised techniques in LiDAR remote sensing, and discuss potential future research directions in this field.
<div id='section'>Paperid: <span id='pid'>1185, <a href='https://arxiv.org/pdf/2503.16546.pdf' target='_blank'>https://arxiv.org/pdf/2503.16546.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Saddam Hussain Khan, Rashid Iqbal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16546">A Comprehensive Survey on Architectural Advances in Deep CNNs: Challenges, Applications, and Emerging Research Directions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep Convolutional Neural Networks (CNNs) have significantly advanced deep learning, driving breakthroughs in computer vision, natural language processing, medical diagnosis, object detection, and speech recognition. Architectural innovations including 1D, 2D, and 3D convolutional models, dilated and grouped convolutions, depthwise separable convolutions, and attention mechanisms address domain-specific challenges and enhance feature representation and computational efficiency. Structural refinements such as spatial-channel exploitation, multi-path design, and feature-map enhancement contribute to robust hierarchical feature extraction and improved generalization, particularly through transfer learning. Efficient preprocessing strategies, including Fourier transforms, structured transforms, low-precision computation, and weight compression, optimize inference speed and facilitate deployment in resource-constrained environments. This survey presents a unified taxonomy that classifies CNN architectures based on spatial exploitation, multi-path structures, depth, width, dimensionality expansion, channel boosting, and attention mechanisms. It systematically reviews CNN applications in face recognition, pose estimation, action recognition, text classification, statistical language modeling, disease diagnosis, radiological analysis, cryptocurrency sentiment prediction, 1D data processing, video analysis, and speech recognition. In addition to consolidating architectural advancements, the review highlights emerging learning paradigms such as few-shot, zero-shot, weakly supervised, federated learning frameworks and future research directions include hybrid CNN-transformer models, vision-language integration, generative learning, etc. This review provides a comprehensive perspective on CNN's evolution from 2015 to 2025, outlining key innovations, challenges, and opportunities.
<div id='section'>Paperid: <span id='pid'>1186, <a href='https://arxiv.org/pdf/2503.13693.pdf' target='_blank'>https://arxiv.org/pdf/2503.13693.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eitan Shaar, Ariel Shaulov, Gal Chechik, Lior Wolf
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13693">Adapting to the Unknown: Training-Free Audio-Visual Event Perception with Dynamic Thresholds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the domain of audio-visual event perception, which focuses on the temporal localization and classification of events across distinct modalities (audio and visual), existing approaches are constrained by the vocabulary available in their training data. This limitation significantly impedes their capacity to generalize to novel, unseen event categories. Furthermore, the annotation process for this task is labor-intensive, requiring extensive manual labeling across modalities and temporal segments, limiting the scalability of current methods. Current state-of-the-art models ignore the shifts in event distributions over time, reducing their ability to adjust to changing video dynamics. Additionally, previous methods rely on late fusion to combine audio and visual information. While straightforward, this approach results in a significant loss of multimodal interactions. To address these challenges, we propose Audio-Visual Adaptive Video Analysis ($\text{AV}^2\text{A}$), a model-agnostic approach that requires no further training and integrates a score-level fusion technique to retain richer multimodal interactions. $\text{AV}^2\text{A}$ also includes a within-video label shift algorithm, leveraging input video data and predictions from prior frames to dynamically adjust event distributions for subsequent frames. Moreover, we present the first training-free, open-vocabulary baseline for audio-visual event perception, demonstrating that $\text{AV}^2\text{A}$ achieves substantial improvements over naive training-free baselines. We demonstrate the effectiveness of $\text{AV}^2\text{A}$ on both zero-shot and weakly-supervised state-of-the-art methods, achieving notable improvements in performance metrics over existing approaches.
<div id='section'>Paperid: <span id='pid'>1187, <a href='https://arxiv.org/pdf/2502.18883.pdf' target='_blank'>https://arxiv.org/pdf/2502.18883.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanfu Yan, Viet Duong, Huajie Shao, Denys Poshyvanyk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18883">Towards More Trustworthy Deep Code Models by Enabling Out-of-Distribution Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Numerous machine learning (ML) models have been developed, including those for software engineering (SE) tasks, under the assumption that training and testing data come from the same distribution. However, training and testing distributions often differ, as training datasets rarely encompass the entire distribution, while testing distribution tends to shift over time. Hence, when confronted with out-of-distribution (OOD) instances that differ from the training data, a reliable and trustworthy SE ML model must be capable of detecting them to either abstain from making predictions, or potentially forward these OODs to appropriate models handling other categories or tasks.
  In this paper, we develop two types of SE-specific OOD detection models, unsupervised and weakly-supervised OOD detection for code. The unsupervised OOD detection approach is trained solely on in-distribution samples while the weakly-supervised approach utilizes a tiny number of OOD samples to further enhance the detection performance in various OOD scenarios. Extensive experimental results demonstrate that our proposed methods significantly outperform the baselines in detecting OOD samples from four different scenarios simultaneously and also positively impact a main code understanding task.
<div id='section'>Paperid: <span id='pid'>1188, <a href='https://arxiv.org/pdf/2502.17288.pdf' target='_blank'>https://arxiv.org/pdf/2502.17288.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simon Boeder, Fabian Gigengack, Benjamin Risse
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17288">GaussianFlowOcc: Sparse and Weakly Supervised Occupancy Estimation using Gaussian Splatting and Temporal Flow</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Occupancy estimation has become a prominent task in 3D computer vision, particularly within the autonomous driving community. In this paper, we present a novel approach to occupancy estimation, termed GaussianFlowOcc, which is inspired by Gaussian Splatting and replaces traditional dense voxel grids with a sparse 3D Gaussian representation. Our efficient model architecture based on a Gaussian Transformer significantly reduces computational and memory requirements by eliminating the need for expensive 3D convolutions used with inefficient voxel-based representations that predominantly represent empty 3D spaces. GaussianFlowOcc effectively captures scene dynamics by estimating temporal flow for each Gaussian during the overall network training process, offering a straightforward solution to a complex problem that is often neglected by existing methods. Moreover, GaussianFlowOcc is designed for scalability, as it employs weak supervision and does not require costly dense 3D voxel annotations based on additional data (e.g., LiDAR). Through extensive experimentation, we demonstrate that GaussianFlowOcc significantly outperforms all previous methods for weakly supervised occupancy estimation on the nuScenes dataset while featuring an inference speed that is 50 times faster than current SOTA.
<div id='section'>Paperid: <span id='pid'>1189, <a href='https://arxiv.org/pdf/2502.12484.pdf' target='_blank'>https://arxiv.org/pdf/2502.12484.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junrui Wen, Yifei Li, Bart Selman, Kun He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.12484">LocalEscaper: A Weakly-supervised Framework with Regional Reconstruction for Scalable Neural TSP Solvers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural solvers have shown significant potential in solving the Traveling Salesman Problem (TSP), yet current approaches face significant challenges. Supervised learning (SL)-based solvers require large amounts of high-quality labeled data, while reinforcement learning (RL)-based solvers, though less dependent on such data, often suffer from inefficiencies. To address these limitations, we propose LocalEscaper, a novel weakly-supervised learning framework for large-scale TSP. LocalEscaper effectively combines the advantages of both SL and RL, enabling effective training on datasets with low-quality labels. To further enhance solution quality, we introduce a regional reconstruction strategy, which is the key technique of this paper and mitigates the local-optima problem common in existing local reconstruction methods. Experimental results on both synthetic and real-world datasets demonstrate that LocalEscaper outperforms existing neural solvers, achieving remarkable results.
<div id='section'>Paperid: <span id='pid'>1190, <a href='https://arxiv.org/pdf/2502.09080.pdf' target='_blank'>https://arxiv.org/pdf/2502.09080.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiwei Wang, Shaoxun Wu, Yujiao Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09080">BevSplat: Resolving Height Ambiguity via Feature-Based Gaussian Primitives for Weakly-Supervised Cross-View Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the problem of weakly supervised cross-view localization, where the goal is to estimate the pose of a ground camera relative to a satellite image with noisy ground truth annotations. A common approach to bridge the cross-view domain gap for pose estimation is Bird's-Eye View (BEV) synthesis. However, existing methods struggle with height ambiguity due to the lack of depth information in ground images and satellite height maps. Previous solutions either assume a flat ground plane or rely on complex models, such as cross-view transformers. We propose BevSplat, a novel method that resolves height ambiguity by using feature-based Gaussian primitives. Each pixel in the ground image is represented by a 3D Gaussian with semantic and spatial features, which are synthesized into a BEV feature map for relative pose estimation. Additionally, to address challenges with panoramic query images, we introduce an icosphere-based supervision strategy for the Gaussian primitives. We validate our method on the widely used KITTI and VIGOR datasets, which include both pinhole and panoramic query images. Experimental results show that BevSplat significantly improves localization accuracy over prior approaches.
<div id='section'>Paperid: <span id='pid'>1191, <a href='https://arxiv.org/pdf/2502.05129.pdf' target='_blank'>https://arxiv.org/pdf/2502.05129.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Van Brunt, Justin Kay, Timm Haucke, Pietro Perona, Grant Van Horn, Sara Beery
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05129">Counting Fish with Temporal Representations of Sonar Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate estimates of salmon escapement - the number of fish migrating upstream to spawn - are key data for conservation and fishery management. Existing methods for salmon counting using high-resolution imaging sonar hardware are non-invasive and compatible with computer vision processing. Prior work in this area has utilized object detection and tracking based methods for automated salmon counting. However, these techniques remain inaccessible to many sonar deployment sites due to limited compute and connectivity in the field. We propose an alternative lightweight computer vision method for fish counting based on analyzing echograms - temporal representations that compress several hundred frames of imaging sonar video into a single image. We predict upstream and downstream counts within 200-frame time windows directly from echograms using a ResNet-18 model, and propose a set of domain-specific image augmentations and a weakly-supervised training protocol to further improve results. We achieve a count error of 23% on representative data from the Kenai River in Alaska, demonstrating the feasibility of our approach.
<div id='section'>Paperid: <span id='pid'>1192, <a href='https://arxiv.org/pdf/2502.03212.pdf' target='_blank'>https://arxiv.org/pdf/2502.03212.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jakob Poncelet, Hugo Van hamme
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03212">Leveraging Broadcast Media Subtitle Transcripts for Automatic Speech Recognition and Subtitling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent advancement of speech recognition technology has been driven by large-scale datasets and attention-based architectures, but many challenges still remain, especially for low-resource languages and dialects. This paper explores the integration of weakly supervised transcripts from TV subtitles into automatic speech recognition (ASR) systems, aiming to improve both verbatim transcriptions and automatically generated subtitles. To this end, verbatim data and subtitles are regarded as different domains or languages, due to their distinct characteristics. We propose and compare several end-to-end architectures that are designed to jointly model both modalities with separate or shared encoders and decoders. The proposed methods are able to jointly generate a verbatim transcription and a subtitle. Evaluation on Flemish (Belgian Dutch) demonstrates that a model with cascaded encoders and separate decoders allows to represent the differences between the two data types most efficiently while improving on both domains. Despite differences in domain and linguistic variations, combining verbatim transcripts with subtitle data leads to notable ASR improvements without the need for extensive preprocessing. Additionally, experiments with a large-scale subtitle dataset show the scalability of the proposed approach. The methods not only improve ASR accuracy but also generate subtitles that closely match standard written text, offering several potential applications.
<div id='section'>Paperid: <span id='pid'>1193, <a href='https://arxiv.org/pdf/2501.04666.pdf' target='_blank'>https://arxiv.org/pdf/2501.04666.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nannan Li, Kevin J. Shih, Bryan A. Plummer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04666">Enhancing Virtual Try-On with Synthetic Pairs and Error-Aware Noise Scheduling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Given an isolated garment image in a canonical product view and a separate image of a person, the virtual try-on task aims to generate a new image of the person wearing the target garment. Prior virtual try-on works face two major challenges in achieving this goal: a) the paired (human, garment) training data has limited availability; b) generating textures on the human that perfectly match that of the prompted garment is difficult, often resulting in distorted text and faded textures. Our work explores ways to tackle these issues through both synthetic data as well as model refinement. We introduce a garment extraction model that generates (human, synthetic garment) pairs from a single image of a clothed individual. The synthetic pairs can then be used to augment the training of virtual try-on. We also propose an Error-Aware Refinement-based SchrÃ¶dinger Bridge (EARSB) that surgically targets localized generation errors for correcting the output of a base virtual try-on model. To identify likely errors, we propose a weakly-supervised error classifier that localizes regions for refinement, subsequently augmenting the SchrÃ¶dinger Bridge's noise schedule with its confidence heatmap. Experiments on VITON-HD and DressCode-Upper demonstrate that our synthetic data augmentation enhances the performance of prior work, while EARSB improves the overall image quality. In user studies, our model is preferred by the users in an average of 59% of cases.
<div id='section'>Paperid: <span id='pid'>1194, <a href='https://arxiv.org/pdf/2411.18475.pdf' target='_blank'>https://arxiv.org/pdf/2411.18475.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuze Wang, Aoran Hu, Ji Qi, Yang Liu, Chao Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18475">Weakly Supervised Framework Considering Multi-temporal Information for Large-scale Cropland Mapping with Satellite Imagery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately mapping large-scale cropland is crucial for agricultural production management and planning. Currently, the combination of remote sensing data and deep learning techniques has shown outstanding performance in cropland mapping. However, those approaches require massive precise labels, which are labor-intensive. To reduce the label cost, this study presented a weakly supervised framework considering multi-temporal information for large-scale cropland mapping. Specifically, we extract high-quality labels according to their consistency among global land cover (GLC) products to construct the supervised learning signal. On the one hand, to alleviate the overfitting problem caused by the model's over-trust of remaining errors in high-quality labels, we encode the similarity/aggregation of cropland in the visual/spatial domain to construct the unsupervised learning signal, and take it as the regularization term to constrain the supervised part. On the other hand, to sufficiently leverage the plentiful information in the samples without high-quality labels, we also incorporate the unsupervised learning signal in these samples, enriching the diversity of the feature space. After that, to capture the phenological features of croplands, we introduce dense satellite image time series (SITS) to extend the proposed framework in the temporal dimension. We also visualized the high dimensional phenological features to uncover how multi-temporal information benefits cropland extraction, and assessed the method's robustness under conditions of data scarcity. The proposed framework has been experimentally validated for strong adaptability across three study areas (Hunan Province, Southeast France, and Kansas) in large-scale cropland mapping, and the internal mechanism and temporal generalizability are also investigated.
<div id='section'>Paperid: <span id='pid'>1195, <a href='https://arxiv.org/pdf/2411.11636.pdf' target='_blank'>https://arxiv.org/pdf/2411.11636.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiman Li, Jiayue Zhao, Shaolei Liu, Xiaokun Dai, Chenxi Zhang, Zhijian Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11636">SP${ }^3$ : Superpixel-propagated pseudo-label learning for weakly semi-supervised medical image segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning-based medical image segmentation helps assist diagnosis and accelerate the treatment process while the model training usually requires large-scale dense annotation datasets. Weakly semi-supervised medical image segmentation is an essential application because it only requires a small amount of scribbles and a large number of unlabeled data to train the model, which greatly reduces the clinician's effort to fully annotate images. To handle the inadequate supervisory information challenge in weakly semi-supervised segmentation (WSSS), a SuperPixel-Propagated Pseudo-label (SP${}^3$) learning method is proposed, using the structural information contained in superpixel for supplemental information. Specifically, the annotation of scribbles is propagated to superpixels and thus obtains a dense annotation for supervised training. Since the quality of pseudo-labels is limited by the low-quality annotation, the beneficial superpixels selected by dynamic thresholding are used to refine pseudo-labels. Furthermore, aiming to alleviate the negative impact of noise in pseudo-label, superpixel-level uncertainty is incorporated to guide the pseudo-label supervision for stable learning. Our method achieves state-of-the-art performance on both tumor and organ segmentation datasets under the WSSS setting, using only 3\% of the annotation workload compared to fully supervised methods and attaining approximately 80\% Dice score. Additionally, our method outperforms eight weakly and semi-supervised methods under both weakly supervised and semi-supervised settings. Results of extensive experiments validate the effectiveness and annotation efficiency of our weakly semi-supervised segmentation, which can assist clinicians in achieving automated segmentation for organs or tumors quickly and ultimately benefit patients.
<div id='section'>Paperid: <span id='pid'>1196, <a href='https://arxiv.org/pdf/2409.19600.pdf' target='_blank'>https://arxiv.org/pdf/2409.19600.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayu Hu, Senlin Shu, Beibei Li, Tao Xiang, Zhongshi He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19600">An Unbiased Risk Estimator for Partial Label Learning with Augmented Classes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partial Label Learning (PLL) is a typical weakly supervised learning task, which assumes each training instance is annotated with a set of candidate labels containing the ground-truth label. Recent PLL methods adopt identification-based disambiguation to alleviate the influence of false positive labels and achieve promising performance. However, they require all classes in the test set to have appeared in the training set, ignoring the fact that new classes will keep emerging in real applications. To address this issue, in this paper, we focus on the problem of Partial Label Learning with Augmented Class (PLLAC), where one or more augmented classes are not visible in the training stage but appear in the inference stage. Specifically, we propose an unbiased risk estimator with theoretical guarantees for PLLAC, which estimates the distribution of augmented classes by differentiating the distribution of known classes from unlabeled data and can be equipped with arbitrary PLL loss functions. Besides, we provide a theoretical analysis of the estimation error bound of the estimator, which guarantees the convergence of the empirical risk minimizer to the true risk minimizer as the number of training data tends to infinity. Furthermore, we add a risk-penalty regularization term in the optimization objective to alleviate the influence of the over-fitting issue caused by negative empirical risk. Extensive experiments on benchmark, UCI and real-world datasets demonstrate the effectiveness of the proposed approach.
<div id='section'>Paperid: <span id='pid'>1197, <a href='https://arxiv.org/pdf/2409.18434.pdf' target='_blank'>https://arxiv.org/pdf/2409.18434.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siru Li, Ziyang Hong, Yushuai Chen, Liang Hu, Jiahu Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18434">Get It For Free: Radar Segmentation without Expert Labels and Its Application in Odometry and Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel weakly supervised semantic segmentation method for radar segmentation, where the existing LiDAR semantic segmentation models are employed to generate semantic labels, which then serve as supervision signals for training a radar semantic segmentation model. The obtained radar semantic segmentation model outperforms LiDAR-based models, providing more consistent and robust segmentation under all-weather conditions, particularly in the snow, rain and fog. To mitigate potential errors in LiDAR semantic labels, we design a dedicated refinement scheme that corrects erroneous labels based on structural features and distribution patterns. The semantic information generated by our radar segmentation model is used in two downstream tasks, achieving significant performance improvements. In large-scale radar-based localization using OpenStreetMap, it leads to localization error reduction by 20.55\% over prior methods. For the odometry task, it improves translation accuracy by 16.4\% compared to the second-best method, securing the first place in the radar odometry competition at the Radar in Robotics workshop of ICRA 2024, Japan
<div id='section'>Paperid: <span id='pid'>1198, <a href='https://arxiv.org/pdf/2409.09616.pdf' target='_blank'>https://arxiv.org/pdf/2409.09616.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cagri Gungor, Adriana Kovashka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09616">Enhancing Weakly-Supervised Object Detection on Static Images through (Hallucinated) Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While motion has garnered attention in various tasks, its potential as a modality for weakly-supervised object detection (WSOD) in static images remains unexplored. Our study introduces an approach to enhance WSOD methods by integrating motion information. This method involves leveraging hallucinated motion from static images to improve WSOD on image datasets, utilizing a Siamese network for enhanced representation learning with motion, addressing camera motion through motion normalization, and selectively training images based on object motion. Experimental validation on the COCO and YouTube-BB datasets demonstrates improvements over a state-of-the-art method.
<div id='section'>Paperid: <span id='pid'>1199, <a href='https://arxiv.org/pdf/2408.05562.pdf' target='_blank'>https://arxiv.org/pdf/2408.05562.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Utkarsh Tiwari, Snehashis Majhi, Michal Balazia, FranÃ§ois BrÃ©mond
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.05562">What Matters in Autonomous Driving Anomaly Detection: A Weakly Supervised Horizon</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video anomaly detection (VAD) in autonomous driving scenario is an important task, however it involves several challenges due to the ego-centric views and moving camera. Due to this, it remains largely under-explored. While recent developments in weakly-supervised VAD methods have shown remarkable progress in detecting critical real-world anomalies in static camera scenario, the development and validation of such methods are yet to be explored for moving camera VAD. This is mainly due to existing datasets like DoTA not following training pre-conditions of weakly-supervised learning. In this paper, we aim to promote weakly-supervised method development for autonomous driving VAD. We reorganize the DoTA dataset and aim to validate recent powerful weakly-supervised VAD methods on moving camera scenarios. Further, we provide a detailed analysis of what modifications on state-of-the-art methods can significantly improve the detection performance. Towards this, we propose a "feature transformation block" and through experimentation we show that our propositions can empower existing weakly-supervised VAD methods significantly in improving the VAD in autonomous driving. Our codes/dataset/demo will be released at github.com/ut21/WSAD-Driving
<div id='section'>Paperid: <span id='pid'>1200, <a href='https://arxiv.org/pdf/2408.02039.pdf' target='_blank'>https://arxiv.org/pdf/2408.02039.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ye Du, Zehua Fu, Qingjie Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.02039">Pixel-Level Domain Adaptation: A New Perspective for Enhancing Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent attention has been devoted to the pursuit of learning semantic segmentation models exclusively from image tags, a paradigm known as image-level Weakly Supervised Semantic Segmentation (WSSS). Existing attempts adopt the Class Activation Maps (CAMs) as priors to mine object regions yet observe the imbalanced activation issue, where only the most discriminative object parts are located. In this paper, we argue that the distribution discrepancy between the discriminative and the non-discriminative parts of objects prevents the model from producing complete and precise pseudo masks as ground truths. For this purpose, we propose a Pixel-Level Domain Adaptation (PLDA) method to encourage the model in learning pixel-wise domain-invariant features. Specifically, a multi-head domain classifier trained adversarially with the feature extraction is introduced to promote the emergence of pixel features that are invariant with respect to the shift between the source (i.e., the discriminative object parts) and the target (\textit{i.e.}, the non-discriminative object parts) domains. In addition, we come up with a Confident Pseudo-Supervision strategy to guarantee the discriminative ability of each pixel for the segmentation task, which serves as a complement to the intra-image domain adversarial training. Our method is conceptually simple, intuitive and can be easily integrated into existing WSSS methods. Taking several strong baseline models as instances, we experimentally demonstrate the effectiveness of our approach under a wide range of settings.
<div id='section'>Paperid: <span id='pid'>1201, <a href='https://arxiv.org/pdf/2407.19821.pdf' target='_blank'>https://arxiv.org/pdf/2407.19821.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianhang Nan, Hao Quan, Yong Ding, Xingyu Li, Kai Yang, Xiaoyu Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.19821">Distilling High Diagnostic Value Patches for Whole Slide Image Classification Using Attention Mechanism</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple Instance Learning (MIL) has garnered widespread attention in the field of Whole Slide Image (WSI) classification as it replaces pixel-level manual annotation with diagnostic reports as labels, significantly reducing labor costs. Recent research has shown that bag-level MIL methods often yield better results because they can consider all patches of the WSI as a whole. However, a drawback of such methods is the incorporation of more redundant patches, leading to interference. To extract patches with high diagnostic value while excluding interfering patches to address this issue, we developed an attention-based feature distillation multi-instance learning (AFD-MIL) approach. This approach proposed the exclusion of redundant patches as a preprocessing operation in weakly supervised learning, directly mitigating interference from extensive noise. It also pioneers the use of attention mechanisms to distill features with high diagnostic value, as opposed to the traditional practice of indiscriminately and forcibly integrating all patches. Additionally, we introduced global loss optimization to finely control the feature distillation module. AFD-MIL is orthogonal to many existing MIL methods, leading to consistent performance improvements. This approach has surpassed the current state-of-the-art method, achieving 91.47% ACC (accuracy) and 94.29% AUC (area under the curve) on the Camelyon16 (Camelyon Challenge 2016, breast cancer), while 93.33% ACC and 98.17% AUC on the TCGA-NSCLC (The Cancer Genome Atlas Program: non-small cell lung cancer). Different feature distillation methods were used for the two datasets, tailored to the specific diseases, thereby improving performance and interpretability.
<div id='section'>Paperid: <span id='pid'>1202, <a href='https://arxiv.org/pdf/2407.12307.pdf' target='_blank'>https://arxiv.org/pdf/2407.12307.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufei Zhang, Jeffrey O. Kephart, Qiang Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.12307">Weakly-Supervised 3D Hand Reconstruction with Knowledge Prior and Uncertainty Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fully-supervised monocular 3D hand reconstruction is often difficult because capturing the requisite 3D data entails deploying specialized equipment in a controlled environment. We introduce a weakly-supervised method that avoids such requirements by leveraging fundamental principles well-established in the understanding of the human hand's unique structure and functionality. Specifically, we systematically study hand knowledge from different sources, including biomechanics, functional anatomy, and physics. We effectively incorporate these valuable foundational insights into 3D hand reconstruction models through an appropriate set of differentiable training losses. This enables training solely with readily-obtainable 2D hand landmark annotations and eliminates the need for expensive 3D supervision. Moreover, we explicitly model the uncertainty that is inherent in image observations. We enhance the training process by exploiting a simple yet effective Negative Log Likelihood (NLL) loss that incorporates uncertainty into the loss function. Through extensive experiments, we demonstrate that our method significantly outperforms state-of-the-art weakly-supervised methods. For example, our method achieves nearly a 21\% performance improvement on the widely adopted FreiHAND dataset.
<div id='section'>Paperid: <span id='pid'>1203, <a href='https://arxiv.org/pdf/2407.12206.pdf' target='_blank'>https://arxiv.org/pdf/2407.12206.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amit Roth, Arnon Turetzky, Yossi Adi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.12206">A Language Modeling Approach to Diacritic-Free Hebrew TTS</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We tackle the task of text-to-speech (TTS) in Hebrew. Traditional Hebrew contains Diacritics, which dictate the way individuals should pronounce given words, however, modern Hebrew rarely uses them. The lack of diacritics in modern Hebrew results in readers expected to conclude the correct pronunciation and understand which phonemes to use based on the context. This imposes a fundamental challenge on TTS systems to accurately map between text-to-speech. In this work, we propose to adopt a language modeling Diacritics-Free approach, for the task of Hebrew TTS. The model operates on discrete speech representations and is conditioned on a word-piece tokenizer. We optimize the proposed method using in-the-wild weakly supervised data and compare it to several diacritic-based TTS systems. Results suggest the proposed method is superior to the evaluated baselines considering both content preservation and naturalness of the generated speech. Samples can be found under the following link: pages.cs.huji.ac.il/adiyoss-lab/HebTTS/
<div id='section'>Paperid: <span id='pid'>1204, <a href='https://arxiv.org/pdf/2407.02389.pdf' target='_blank'>https://arxiv.org/pdf/2407.02389.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sayan Nag, Koustava Goswami, Srikrishna Karanam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.02389">SafaRi:Adaptive Sequence Transformer for Weakly Supervised Referring Expression Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring Expression Segmentation (RES) aims to provide a segmentation mask of the target object in an image referred to by the text (i.e., referring expression). Existing methods require large-scale mask annotations. Moreover, such approaches do not generalize well to unseen/zero-shot scenarios. To address the aforementioned issues, we propose a weakly-supervised bootstrapping architecture for RES with several new algorithmic innovations. To the best of our knowledge, ours is the first approach that considers only a fraction of both mask and box annotations (shown in Figure 1 and Table 1) for training. To enable principled training of models in such low-annotation settings, improve image-text region-level alignment, and further enhance spatial localization of the target object in the image, we propose Cross-modal Fusion with Attention Consistency module. For automatic pseudo-labeling of unlabeled samples, we introduce a novel Mask Validity Filtering routine based on a spatially aware zero-shot proposal scoring approach. Extensive experiments show that with just 30% annotations, our model SafaRi achieves 59.31 and 48.26 mIoUs as compared to 58.93 and 48.19 mIoUs obtained by the fully-supervised SOTA method SeqTR respectively on RefCOCO+@testA and RefCOCO+testB datasets. SafaRi also outperforms SeqTR by 11.7% (on RefCOCO+testA) and 19.6% (on RefCOCO+testB) in a fully-supervised setting and demonstrates strong generalization capabilities in unseen/zero-shot tasks.
<div id='section'>Paperid: <span id='pid'>1205, <a href='https://arxiv.org/pdf/2406.18576.pdf' target='_blank'>https://arxiv.org/pdf/2406.18576.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Zhang, Chuang Zhu, Guoqing Yang, Siqi Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.18576">Negative Prototypes Guided Contrastive Learning for WSOD</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Object Detection (WSOD) with only image-level annotation has recently attracted wide attention. Many existing methods ignore the inter-image relationship of instances which share similar characteristics while can certainly be determined not to belong to the same category. Therefore, in order to make full use of the weak label, we propose the Negative Prototypes Guided Contrastive learning (NPGC) architecture. Firstly, we define Negative Prototype as the proposal with the highest confidence score misclassified for the category that does not appear in the label. Unlike other methods that only utilize category positive feature, we construct an online updated global feature bank to store both positive prototypes and negative prototypes. Meanwhile, we propose a pseudo label sampling module to mine reliable instances and discard the easily misclassified instances based on the feature similarity with corresponding prototypes in global feature bank. Finally, we follow the contrastive learning paradigm to optimize the proposal's feature representation by attracting same class samples closer and pushing different class samples away in the embedding space. Extensive experiments have been conducted on VOC07, VOC12 datasets, which shows that our proposed method achieves the state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>1206, <a href='https://arxiv.org/pdf/2406.09147.pdf' target='_blank'>https://arxiv.org/pdf/2406.09147.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xu Tan, Junqi Chen, Sylwan Rahardja, Jiawei Yang, Susanto Rahardja
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.09147">Weakly-supervised anomaly detection for multimodal data distributions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised anomaly detection can outperform existing unsupervised methods with the assistance of a very small number of labeled anomalies, which attracts increasing attention from researchers. However, existing weakly-supervised anomaly detection methods are limited as these methods do not factor in the multimodel nature of the real-world data distribution. To mitigate this, we propose the Weakly-supervised Variational-mixture-model-based Anomaly Detector (WVAD). WVAD excels in multimodal datasets. It consists of two components: a deep variational mixture model, and an anomaly score estimator. The deep variational mixture model captures various features of the data from different clusters, then these features are delivered to the anomaly score estimator to assess the anomaly levels. Experimental results on three real-world datasets demonstrate WVAD's superiority.
<div id='section'>Paperid: <span id='pid'>1207, <a href='https://arxiv.org/pdf/2406.04933.pdf' target='_blank'>https://arxiv.org/pdf/2406.04933.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>AhcÃ¨ne Boubekki, Samuel G. Fadel, Sebastian Mair
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04933">Leveraging Activations for Superpixel Explanations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Saliency methods have become standard in the explanation toolkit of deep neural networks. Recent developments specific to image classifiers have investigated region-based explanations with either new methods or by adapting well-established ones using ad-hoc superpixel algorithms. In this paper, we aim to avoid relying on these segmenters by extracting a segmentation from the activations of a deep neural network image classifier without fine-tuning the network. Our so-called Neuro-Activated Superpixels (NAS) can isolate the regions of interest in the input relevant to the model's prediction, which boosts high-threshold weakly supervised object localization performance. This property enables the semi-supervised semantic evaluation of saliency methods. The aggregation of NAS with existing saliency methods eases their interpretation and reveals the inconsistencies of the widely used area under the relevance curve metric.
<div id='section'>Paperid: <span id='pid'>1208, <a href='https://arxiv.org/pdf/2405.14334.pdf' target='_blank'>https://arxiv.org/pdf/2405.14334.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yitao Peng, Lianghua He, Die Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14334">Hierarchical Salient Patch Identification for Interpretable Fundus Disease Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the widespread application of deep learning technology in medical image analysis, the effective explanation of model predictions and improvement of diagnostic accuracy have become urgent problems that need to be solved. Attribution methods have become key tools to help doctors better understand the diagnostic basis of models, and are used to explain and localize diseases in medical images. However, previous methods suffer from inaccurate and incomplete localization problems for fundus diseases with complex and diverse structures. To solve these problems, we propose a weakly supervised interpretable fundus disease localization method called hierarchical salient patch identification (HSPI) that can achieve interpretable disease localization using only image-level labels and a neural network classifier (NNC). First, we propose salient patch identification (SPI), which divides the image into several patches and optimizes consistency loss to identify which patch in the input image is most important for the network's prediction, in order to locate the disease. Second, we propose a hierarchical identification strategy to force SPI to analyze the importance of different areas to neural network classifier's prediction to comprehensively locate disease areas. Conditional peak focusing is then introduced to ensure that the mask vector can accurately locate the disease area. Finally, we propose patch selection based on multi-sized intersections to filter out incorrectly or additionally identified non-disease regions. We conduct disease localization experiments on fundus image datasets and achieve the best performance on multiple evaluation metrics compared to previous interpretable attribution methods. Additional ablation studies are conducted to verify the effectiveness of each method.
<div id='section'>Paperid: <span id='pid'>1209, <a href='https://arxiv.org/pdf/2405.03726.pdf' target='_blank'>https://arxiv.org/pdf/2405.03726.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andac Demir, Elizaveta Solovyeva, James Boylan, Mei Xiao, Fabrizio Serluca, Sebastian Hoersch, Jeremy Jenkins, Murthy Devarakonda, Bulent Kiziltan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.03726">sc-OTGM: Single-Cell Perturbation Modeling by Solving Optimal Mass Transport on the Manifold of Gaussian Mixtures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Influenced by breakthroughs in LLMs, single-cell foundation models are emerging. While these models show successful performance in cell type clustering, phenotype classification, and gene perturbation response prediction, it remains to be seen if a simpler model could achieve comparable or better results, especially with limited data. This is important, as the quantity and quality of single-cell data typically fall short of the standards in textual data used for training LLMs. Single-cell sequencing often suffers from technical artifacts, dropout events, and batch effects. These challenges are compounded in a weakly supervised setting, where the labels of cell states can be noisy, further complicating the analysis. To tackle these challenges, we present sc-OTGM, streamlined with less than 500K parameters, making it approximately 100x more compact than the foundation models, offering an efficient alternative. sc-OTGM is an unsupervised model grounded in the inductive bias that the scRNAseq data can be generated from a combination of the finite multivariate Gaussian distributions. The core function of sc-OTGM is to create a probabilistic latent space utilizing a GMM as its prior distribution and distinguish between distinct cell populations by learning their respective marginal PDFs. It uses a Hit-and-Run Markov chain sampler to determine the OT plan across these PDFs within the GMM framework. We evaluated our model against a CRISPR-mediated perturbation dataset, called CROP-seq, consisting of 57 one-gene perturbations. Our results demonstrate that sc-OTGM is effective in cell state classification, aids in the analysis of differential gene expression, and ranks genes for target identification through a recommender system. It also predicts the effects of single-gene perturbations on downstream gene regulation and generates synthetic scRNA-seq data conditioned on specific cell states.
<div id='section'>Paperid: <span id='pid'>1210, <a href='https://arxiv.org/pdf/2404.11998.pdf' target='_blank'>https://arxiv.org/pdf/2404.11998.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiyuan Dai, Sibei Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.11998">Curriculum Point Prompting for Weakly-Supervised Referring Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring image segmentation (RIS) aims to precisely segment referents in images through corresponding natural language expressions, yet relying on cost-intensive mask annotations. Weakly supervised RIS thus learns from image-text pairs to pixel-level semantics, which is challenging for segmenting fine-grained masks. A natural approach to enhancing segmentation precision is to empower weakly supervised RIS with the image segmentation foundation model SAM. Nevertheless, we observe that simply integrating SAM yields limited benefits and can even lead to performance regression due to the inevitable noise issues and challenges in excessive focus on object parts. In this paper, we present an innovative framework, Point PrompTing (PPT), incorporated with the proposed multi-source curriculum learning strategy to address these challenges. Specifically, the core of PPT is a point generator that not only harnesses CLIP's text-image alignment capability and SAM's powerful mask generation ability but also generates negative point prompts to address the noisy and excessive focus issues inherently and effectively. In addition, we introduce a curriculum learning strategy with object-centric images to help PPT gradually learn from simpler yet precise semantic alignment to more complex RIS. Experiments demonstrate that our PPT significantly and consistently outperforms prior weakly supervised techniques on mIoU by 11.34%, 14.14%, and 6.97% across RefCOCO, RefCOCO+, and G-Ref, respectively.
<div id='section'>Paperid: <span id='pid'>1211, <a href='https://arxiv.org/pdf/2404.00918.pdf' target='_blank'>https://arxiv.org/pdf/2404.00918.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Beomyoung Kim, Donghyun Kim, Sung Ju Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00918">Rethinking Saliency-Guided Weakly-Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a fresh perspective on the role of saliency maps in weakly-supervised semantic segmentation (WSSS) and offers new insights and research directions based on our empirical findings. We conduct comprehensive experiments and observe that the quality of the saliency map is a critical factor in saliency-guided WSSS approaches. Nonetheless, we find that the saliency maps used in previous works are often arbitrarily chosen, despite their significant impact on WSSS. Additionally, we observe that the choice of the threshold, which has received less attention before, is non-trivial in WSSS. To facilitate more meaningful and rigorous research for saliency-guided WSSS, we introduce \texttt{WSSS-BED}, a standardized framework for conducting research under unified conditions. \texttt{WSSS-BED} provides various saliency maps and activation maps for seven WSSS methods, as well as saliency maps from unsupervised salient object detection models.
<div id='section'>Paperid: <span id='pid'>1212, <a href='https://arxiv.org/pdf/2401.10578.pdf' target='_blank'>https://arxiv.org/pdf/2401.10578.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lintai Wu, Junhui Hou, Linqi Song, Yong Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.10578">3D Shape Completion on Unseen Categories:A Weakly-supervised Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D shapes captured by scanning devices are often incomplete due to occlusion. 3D shape completion methods have been explored to tackle this limitation. However, most of these methods are only trained and tested on a subset of categories, resulting in poor generalization to unseen categories. In this paper, we introduce a novel weakly-supervised framework to reconstruct the complete shapes from unseen categories. We first propose an end-to-end prior-assisted shape learning network that leverages data from the seen categories to infer a coarse shape. Specifically, we construct a prior bank consisting of representative shapes from the seen categories. Then, we design a multi-scale pattern correlation module for learning the complete shape of the input by analyzing the correlation between local patterns within the input and the priors at various scales. In addition, we propose a self-supervised shape refinement model to further refine the coarse shape. Considering the shape variability of 3D objects across categories, we construct a category-specific prior bank to facilitate shape refinement. Then, we devise a voxel-based partial matching loss and leverage the partial scans to drive the refinement process. Extensive experimental results show that our approach is superior to state-of-the-art methods by a large margin.
<div id='section'>Paperid: <span id='pid'>1213, <a href='https://arxiv.org/pdf/2401.10011.pdf' target='_blank'>https://arxiv.org/pdf/2401.10011.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinpeng Zhao, Yanwei Zheng, Chuanlin Lan, Xiaowei Zhang, Bowen Huang, Jibin Yang, Dongxiao Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.10011">CPCL: Cross-Modal Prototypical Contrastive Learning for Weakly Supervised Text-based Person Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised text-based person retrieval seeks to retrieve images of a target person using textual descriptions, without relying on identity annotations and is more challenging and practical. The primary challenge is the intra-class differences, encompassing intra-modal feature variations and cross-modal semantic gaps. Prior works have focused on instance-level samples and ignored prototypical features of each person which are intrinsic and invariant. Toward this, we propose a Cross-Modal Prototypical Contrastive Learning (CPCL) method. In practice, the CPCL introduces the CLIP model to weakly supervised text-based person retrieval to map visual and textual instances into a shared latent space. Subsequently, the proposed Prototypical Multi-modal Memory (PMM) module captures associations between heterogeneous modalities of image-text pairs belonging to the same person through the Hybrid Cross-modal Matching (HCM) module in a many-to-many mapping fashion. Moreover, the Outlier Pseudo Label Mining (OPLM) module further distinguishes valuable outlier samples from each modality, enhancing the creation of more reliable clusters by mining implicit relationships between image-text pairs. We conduct extensive experiments on popular benchmarks of weakly supervised text-based person retrieval, which validate the effectiveness, generalizability of CPCL.
<div id='section'>Paperid: <span id='pid'>1214, <a href='https://arxiv.org/pdf/2311.17960.pdf' target='_blank'>https://arxiv.org/pdf/2311.17960.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aayush Kumar Tyagi, Vaibhav Mishra, Prathosh A. P., Mausam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17960">Guided Prompting in SAM for Weakly Supervised Cell Segmentation in Histopathological Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cell segmentation in histopathological images plays a crucial role in understanding, diagnosing, and treating many diseases. However, data annotation for this is expensive since there can be a large number of cells per image, and expert pathologists are needed for labelling images. Instead, our paper focuses on using weak supervision -- annotation from related tasks -- to induce a segmenter. Recent foundation models, such as Segment Anything (SAM), can use prompts to leverage additional supervision during inference. SAM has performed remarkably well in natural image segmentation tasks; however, its applicability to cell segmentation has not been explored.
  In response, we investigate guiding the prompting procedure in SAM for weakly supervised cell segmentation when only bounding box supervision is available. We develop two workflows: (1) an object detector's output as a test-time prompt to SAM (D-SAM), and (2) SAM as pseudo mask generator over training data to train a standalone segmentation model (SAM-S). On finding that both workflows have some complementary strengths, we develop an integer programming-based approach to reconcile the two sets of segmentation masks, achieving yet higher performance. We experiment on three publicly available cell segmentation datasets namely, ConSep, MoNuSeg, and TNBC, and find that all SAM-based solutions hugely outperform existing weakly supervised image segmentation models, obtaining 9-15 pt Dice gains.
<div id='section'>Paperid: <span id='pid'>1215, <a href='https://arxiv.org/pdf/2311.13946.pdf' target='_blank'>https://arxiv.org/pdf/2311.13946.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyuan Li, Zhou Zhao, Zhu Zhang, Zhijie Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.13946">Weakly-Supervised Video Moment Retrieval via Regularized Two-Branch Proposal Networks with Erasing Mechanism</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video moment retrieval is to identify the target moment according to the given sentence in an untrimmed video. Due to temporal boundary annotations of the video are extremely time-consuming to acquire, modeling in the weakly-supervised setting is increasingly focused, where we only have access to the video-sentence pairs during training. Most existing weakly-supervised methods adopt a MIL-based framework to develop inter-sample confrontment, but neglect the intra-sample confrontment between moments with similar semantics. Therefore, these methods fail to distinguish the correct moment from plausible negative moments. Further, the previous attention models in cross-modal interaction tend to focus on a few dominant words exorbitantly, ignoring the comprehensive video-sentence correspondence. In this paper, we propose a novel Regularized Two-Branch Proposal Network with Erasing Mechanism to consider the inter-sample and intra-sample confrontments simultaneously. Concretely, we first devise a language-aware visual filter to generate both enhanced and suppressed video streams. Then, we design the sharable two-branch proposal module to generate positive and plausible negative proposals from the enhanced and suppressed branch respectively, contributing to sufficient confrontment. Besides, we introduce an attention-guided dynamic erasing mechanism in enhanced branch to discover the complementary video-sentence relation. Moreover, we apply two types of proposal regularization to stabilize the training process and improve model performance. The extensive experiments on ActivityCaption, Charades-STA and DiDeMo datasets show the effectiveness of our method.
<div id='section'>Paperid: <span id='pid'>1216, <a href='https://arxiv.org/pdf/2311.09809.pdf' target='_blank'>https://arxiv.org/pdf/2311.09809.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomas Flinkow, Barak A. Pearlmutter, Rosemary Monahan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.09809">Comparing Differentiable Logics for Learning Systems: A Research Preview</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Extensive research on formal verification of machine learning (ML) systems indicates that learning from data alone often fails to capture underlying background knowledge. A variety of verifiers have been developed to ensure that a machine-learnt model satisfies correctness and safety properties, however, these verifiers typically assume a trained network with fixed weights. ML-enabled autonomous systems are required to not only detect incorrect predictions, but should also possess the ability to self-correct, continuously improving and adapting. A promising approach for creating ML models that inherently satisfy constraints is to encode background knowledge as logical constraints that guide the learning process via so-called differentiable logics. In this research preview, we compare and evaluate various logics from the literature in weakly-supervised contexts, presenting our findings and highlighting open problems for future work. Our experimental results are broadly consistent with results reported previously in literature; however, learning with differentiable logics introduces a new hyperparameter that is difficult to tune and has significant influence on the effectiveness of the logics.
<div id='section'>Paperid: <span id='pid'>1217, <a href='https://arxiv.org/pdf/2311.00189.pdf' target='_blank'>https://arxiv.org/pdf/2311.00189.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Hajialigol, Hanwen Liu, Xuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.00189">XAI-CLASS: Explanation-Enhanced Text Classification with Extremely Weak Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text classification aims to effectively categorize documents into pre-defined categories. Traditional methods for text classification often rely on large amounts of manually annotated training data, making the process time-consuming and labor-intensive. To address this issue, recent studies have focused on weakly-supervised and extremely weakly-supervised settings, which require minimal or no human annotation, respectively. In previous methods of weakly supervised text classification, pseudo-training data is generated by assigning pseudo-labels to documents based on their alignment (e.g., keyword matching) with specific classes. However, these methods ignore the importance of incorporating the explanations of the generated pseudo-labels, or saliency of individual words, as additional guidance during the text classification training process. To address this limitation, we propose XAI-CLASS, a novel explanation-enhanced extremely weakly-supervised text classification method that incorporates word saliency prediction as an auxiliary task. XAI-CLASS begins by employing a multi-round question-answering process to generate pseudo-training data that promotes the mutual enhancement of class labels and corresponding explanation word generation. This pseudo-training data is then used to train a multi-task framework that simultaneously learns both text classification and word saliency prediction. Extensive experiments on several weakly-supervised text classification datasets show that XAI-CLASS outperforms other weakly-supervised text classification methods significantly. Moreover, experiments demonstrate that XAI-CLASS enhances both model performance and explainability.
<div id='section'>Paperid: <span id='pid'>1218, <a href='https://arxiv.org/pdf/2310.13026.pdf' target='_blank'>https://arxiv.org/pdf/2310.13026.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaozheng Chen, Qianru Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.13026">Weakly-Supervised Semantic Segmentation with Image-Level Labels: from Traditional Models to Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid development of deep learning has driven significant progress in image semantic segmentation - a fundamental task in computer vision. Semantic segmentation algorithms often depend on the availability of pixel-level labels (i.e., masks of objects), which are expensive, time-consuming, and labor-intensive. Weakly-supervised semantic segmentation (WSSS) is an effective solution to avoid such labeling. It utilizes only partial or incomplete annotations and provides a cost-effective alternative to fully-supervised semantic segmentation. In this journal, our focus is on the WSSS with image-level labels, which is the most challenging form of WSSS. Our work has two parts. First, we conduct a comprehensive survey on traditional methods, primarily focusing on those presented at premier research conferences. We categorize them into four groups based on where their methods operate: pixel-wise, image-wise, cross-image, and external data. Second, we investigate the applicability of visual foundation models, such as the Segment Anything Model (SAM), in the context of WSSS. We scrutinize SAM in two intriguing scenarios: text prompting and zero-shot learning. We provide insights into the potential and challenges of deploying visual foundational models for WSSS, facilitating future developments in this exciting research area.
<div id='section'>Paperid: <span id='pid'>1219, <a href='https://arxiv.org/pdf/2309.14057.pdf' target='_blank'>https://arxiv.org/pdf/2309.14057.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jia Zhang, Bo Peng, Xi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.14057">Weakly Supervised Semantic Segmentation by Knowledge Graph Inference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Currently, existing efforts in Weakly Supervised Semantic Segmentation (WSSS) based on Convolutional Neural Networks (CNNs) have predominantly focused on enhancing the multi-label classification network stage, with limited attention given to the equally important downstream segmentation network. Furthermore, CNN-based local convolutions lack the ability to model the extensive inter-category dependencies. Therefore, this paper introduces a graph reasoning-based approach to enhance WSSS. The aim is to improve WSSS holistically by simultaneously enhancing both the multi-label classification and segmentation network stages. In the multi-label classification network segment, external knowledge is integrated, coupled with GCNs, to globally reason about inter-class dependencies. This encourages the network to uncover features in non-salient regions of images, thereby refining the completeness of generated pseudo-labels. In the segmentation network segment, the proposed Graph Reasoning Mapping (GRM) module is employed to leverage knowledge obtained from textual databases, facilitating contextual reasoning for class representation within image regions. This GRM module enhances feature representation in high-level semantics of the segmentation network's local convolutions, while dynamically learning semantic coherence for individual samples. Using solely image-level supervision, we have achieved state-of-the-art performance in WSSS on the PASCAL VOC 2012 and MS-COCO datasets. Extensive experimentation on both the multi-label classification and segmentation network stages underscores the effectiveness of the proposed graph reasoning approach for advancing WSSS.
<div id='section'>Paperid: <span id='pid'>1220, <a href='https://arxiv.org/pdf/2309.12242.pdf' target='_blank'>https://arxiv.org/pdf/2309.12242.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Theodoros Kouzelis, Vassilis Katsouros
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.12242">Weakly-supervised Automated Audio Captioning via text only training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, datasets of paired audio and captions have enabled remarkable success in automatically generating descriptions for audio clips, namely Automated Audio Captioning (AAC). However, it is labor-intensive and time-consuming to collect a sufficient number of paired audio and captions. Motivated by the recent advances in Contrastive Language-Audio Pretraining (CLAP), we propose a weakly-supervised approach to train an AAC model assuming only text data and a pre-trained CLAP model, alleviating the need for paired target data. Our approach leverages the similarity between audio and text embeddings in CLAP. During training, we learn to reconstruct the text from the CLAP text embedding, and during inference, we decode using the audio embeddings. To mitigate the modality gap between the audio and text embeddings we employ strategies to bridge the gap during training and inference stages. We evaluate our proposed method on Clotho and AudioCaps datasets demonstrating its ability to achieve a relative performance of up to ~$83\%$ compared to fully supervised approaches trained with paired target data.
<div id='section'>Paperid: <span id='pid'>1221, <a href='https://arxiv.org/pdf/2309.04172.pdf' target='_blank'>https://arxiv.org/pdf/2309.04172.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yeonghwan Song, Seokwoo Jang, Dina Katabi, Jeany Son
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.04172">Unsupervised Object Localization with Representer Point Selection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel unsupervised object localization method that allows us to explain the predictions of the model by utilizing self-supervised pre-trained models without additional finetuning. Existing unsupervised and self-supervised object localization methods often utilize class-agnostic activation maps or self-similarity maps of a pre-trained model. Although these maps can offer valuable information for localization, their limited ability to explain how the model makes predictions remains challenging. In this paper, we propose a simple yet effective unsupervised object localization method based on representer point selection, where the predictions of the model can be represented as a linear combination of representer values of training points. By selecting representer points, which are the most important examples for the model predictions, our model can provide insights into how the model predicts the foreground object by providing relevant examples as well as their importance. Our method outperforms the state-of-the-art unsupervised and self-supervised object localization methods on various datasets with significant margins and even outperforms recent weakly supervised and few-shot methods.
<div id='section'>Paperid: <span id='pid'>1222, <a href='https://arxiv.org/pdf/2308.15618.pdf' target='_blank'>https://arxiv.org/pdf/2308.15618.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anirudh Choudhary, Mosbah Aouad, Krishnakant Saboo, Angelina Hwang, Jacob Kechter, Blake Bordeaux, Puneet Bhullar, David DiCaudo, Steven Nelson, Nneka Comfere, Emma Johnson, Olayemi Sokumbi, Jason Sluzevich, Leah Swanson, Dennis Murphree, Aaron Mangold, Ravishankar Iyer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.15618">RACR-MIL: Rank-aware contextual reasoning for weakly supervised grading of squamous cell carcinoma using whole slide images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Squamous cell carcinoma (SCC) is the most common cancer subtype, with an increasing incidence and a significant impact on cancer-related mortality. SCC grading using whole slide images is inherently challenging due to the lack of a reliable protocol and substantial tissue heterogeneity. We propose RACR-MIL, the first weakly-supervised SCC grading approach achieving robust generalization across multiple anatomies (skin, head and neck, lung). RACR-MIL is an attention-based multiple-instance learning framework that enhances grade-relevant contextual representation learning and addresses tumor heterogeneity through two key innovations: (1) a hybrid WSI graph that captures both local tissue context and non-local phenotypical dependencies between tumor regions, and (2) a rank-ordering constraint in the attention mechanism that consistently prioritizes higher-grade tumor regions, aligning with pathologists diagnostic process. Our model achieves state-of-the-art performance across multiple SCC datasets, achieving 3-9% higher grading accuracy, resilience to class imbalance, and up to 16% improved tumor localization. In a pilot study, pathologists reported that RACR-MIL improved grading efficiency in 60% of cases, underscoring its potential as a clinically viable cancer diagnosis and grading assistant.
<div id='section'>Paperid: <span id='pid'>1223, <a href='https://arxiv.org/pdf/2308.09515.pdf' target='_blank'>https://arxiv.org/pdf/2308.09515.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryan Wong, Necati Cihan Camgoz, Richard Bowden
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.09515">Learnt Contrastive Concept Embeddings for Sign Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In natural language processing (NLP) of spoken languages, word embeddings have been shown to be a useful method to encode the meaning of words. Sign languages are visual languages, which require sign embeddings to capture the visual and linguistic semantics of sign. Unlike many common approaches to Sign Recognition, we focus on explicitly creating sign embeddings that bridge the gap between sign language and spoken language. We propose a learning framework to derive LCC (Learnt Contrastive Concept) embeddings for sign language, a weakly supervised contrastive approach to learning sign embeddings. We train a vocabulary of embeddings that are based on the linguistic labels for sign video. Additionally, we develop a conceptual similarity loss which is able to utilise word embeddings from NLP methods to create sign embeddings that have better sign language to spoken language correspondence. These learnt representations allow the model to automatically localise the sign in time. Our approach achieves state-of-the-art keypoint-based sign recognition performance on the WLASL and BOBSL datasets.
<div id='section'>Paperid: <span id='pid'>1224, <a href='https://arxiv.org/pdf/2308.05137.pdf' target='_blank'>https://arxiv.org/pdf/2308.05137.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Bai, Xiaohan Xing, Yutian Shen, Han Ma, Max Q. -H. Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.05137">Discrepancy-based Active Learning for Weakly Supervised Bleeding Segmentation in Wireless Capsule Endoscopy Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised methods, such as class activation maps (CAM) based, have been applied to achieve bleeding segmentation with low annotation efforts in Wireless Capsule Endoscopy (WCE) images. However, the CAM labels tend to be extremely noisy, and there is an irreparable gap between CAM labels and ground truths for medical images. This paper proposes a new Discrepancy-basEd Active Learning (DEAL) approach to bridge the gap between CAMs and ground truths with a few annotations. Specifically, to liberate labor, we design a novel discrepancy decoder model and a CAMPUS (CAM, Pseudo-label and groUnd-truth Selection) criterion to replace the noisy CAMs with accurate model predictions and a few human labels. The discrepancy decoder model is trained with a unique scheme to generate standard, coarse and fine predictions. And the CAMPUS criterion is proposed to predict the gaps between CAMs and ground truths based on model divergence and CAM divergence. We evaluate our method on the WCE dataset and results show that our method outperforms the state-of-the-art active learning methods and reaches comparable performance to those trained with full annotated datasets with only 10% of the training data labeled.
<div id='section'>Paperid: <span id='pid'>1225, <a href='https://arxiv.org/pdf/2308.03805.pdf' target='_blank'>https://arxiv.org/pdf/2308.03805.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taoran Sheng, Manfred Huber
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.03805">Weakly Supervised Multi-Task Representation Learning for Human Activity Analysis Using Wearables</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sensor data streams from wearable devices and smart environments are widely studied in areas like human activity recognition (HAR), person identification, or health monitoring. However, most of the previous works in activity and sensor stream analysis have been focusing on one aspect of the data, e.g. only recognizing the type of the activity or only identifying the person who performed the activity. We instead propose an approach that uses a weakly supervised multi-output siamese network that learns to map the data into multiple representation spaces, where each representation space focuses on one aspect of the data. The representation vectors of the data samples are positioned in the space such that the data with the same semantic meaning in that aspect are closely located to each other. Therefore, as demonstrated with a set of experiments, the trained model can provide metrics for clustering data based on multiple aspects, allowing it to address multiple tasks simultaneously and even to outperform single task supervised methods in many situations. In addition, further experiments are presented that in more detail analyze the effect of the architecture and of using multiple tasks within this framework, that investigate the scalability of the model to include additional tasks, and that demonstrate the ability of the framework to combine data for which only partial relationship information with respect to the target tasks is available.
<div id='section'>Paperid: <span id='pid'>1226, <a href='https://arxiv.org/pdf/2308.01721.pdf' target='_blank'>https://arxiv.org/pdf/2308.01721.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shichao Dong, Guosheng Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.01721">Weakly Supervised 3D Instance Segmentation without Instance-level Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D semantic scene understanding tasks have achieved great success with the emergence of deep learning, but often require a huge amount of manually annotated training data. To alleviate the annotation cost, we propose the first weakly-supervised 3D instance segmentation method that only requires categorical semantic labels as supervision, and we do not need instance-level labels. The required semantic annotations can be either dense or extreme sparse (e.g. 0.02% of total points). Even without having any instance-related ground-truth, we design an approach to break point clouds into raw fragments and find the most confident samples for learning instance centroids. Furthermore, we construct a recomposed dataset using pseudo instances, which is used to learn our defined multilevel shape-aware objectness signal. An asymmetrical object inference algorithm is followed to process core points and boundary points with different strategies, and generate high-quality pseudo instance labels to guide iterative training. Experiments demonstrate that our method can achieve comparable results with recent fully supervised methods. By generating pseudo instance labels from categorical semantic labels, our designed approach can also assist existing methods for learning 3D instance segmentation at reduced annotation cost.
<div id='section'>Paperid: <span id='pid'>1227, <a href='https://arxiv.org/pdf/2308.00799.pdf' target='_blank'>https://arxiv.org/pdf/2308.00799.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufei Zhang, Hanjing Wang, Jeffrey O. Kephart, Qiang Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.00799">Body Knowledge and Uncertainty Modeling for Monocular 3D Human Body Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While 3D body reconstruction methods have made remarkable progress recently, it remains difficult to acquire the sufficiently accurate and numerous 3D supervisions required for training. In this paper, we propose \textbf{KNOWN}, a framework that effectively utilizes body \textbf{KNOW}ledge and u\textbf{N}certainty modeling to compensate for insufficient 3D supervisions. KNOWN exploits a comprehensive set of generic body constraints derived from well-established body knowledge. These generic constraints precisely and explicitly characterize the reconstruction plausibility and enable 3D reconstruction models to be trained without any 3D data. Moreover, existing methods typically use images from multiple datasets during training, which can result in data noise (\textit{e.g.}, inconsistent joint annotation) and data imbalance (\textit{e.g.}, minority images representing unusual poses or captured from challenging camera views). KNOWN solves these problems through a novel probabilistic framework that models both aleatoric and epistemic uncertainty. Aleatoric uncertainty is encoded in a robust Negative Log-Likelihood (NLL) training loss, while epistemic uncertainty is used to guide model refinement. Experiments demonstrate that KNOWN's body reconstruction outperforms prior weakly-supervised approaches, particularly on the challenging minority images.
<div id='section'>Paperid: <span id='pid'>1228, <a href='https://arxiv.org/pdf/2307.10316.pdf' target='_blank'>https://arxiv.org/pdf/2307.10316.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lizhao Liu, Zhuangwei Zhuang, Shangxin Huang, Xunlong Xiao, Tianhang Xiang, Cen Chen, Jingdong Wang, Mingkui Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.10316">CPCM: Contextual Point Cloud Modeling for Weakly-supervised Point Cloud Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study the task of weakly-supervised point cloud semantic segmentation with sparse annotations (e.g., less than 0.1% points are labeled), aiming to reduce the expensive cost of dense annotations. Unfortunately, with extremely sparse annotated points, it is very difficult to extract both contextual and object information for scene understanding such as semantic segmentation. Motivated by masked modeling (e.g., MAE) in image and video representation learning, we seek to endow the power of masked modeling to learn contextual information from sparsely-annotated points. However, directly applying MAE to 3D point clouds with sparse annotations may fail to work. First, it is nontrivial to effectively mask out the informative visual context from 3D point clouds. Second, how to fully exploit the sparse annotations for context modeling remains an open question. In this paper, we propose a simple yet effective Contextual Point Cloud Modeling (CPCM) method that consists of two parts: a region-wise masking (RegionMask) strategy and a contextual masked training (CMT) method. Specifically, RegionMask masks the point cloud continuously in geometric space to construct a meaningful masked prediction task for subsequent context learning. CMT disentangles the learning of supervised segmentation and unsupervised masked context prediction for effectively learning the very limited labeled points and mass unlabeled points, respectively. Extensive experiments on the widely-tested ScanNet V2 and S3DIS benchmarks demonstrate the superiority of CPCM over the state-of-the-art.
<div id='section'>Paperid: <span id='pid'>1229, <a href='https://arxiv.org/pdf/2307.08944.pdf' target='_blank'>https://arxiv.org/pdf/2307.08944.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taoran Sheng, Manfred Huber
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.08944">Siamese Networks for Weakly Supervised Human Activity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning has been successfully applied to human activity recognition. However, training deep neural networks requires explicitly labeled data which is difficult to acquire. In this paper, we present a model with multiple siamese networks that are trained by using only the information about the similarity between pairs of data samples without knowing the explicit labels. The trained model maps the activity data samples into fixed size representation vectors such that the distance between the vectors in the representation space approximates the similarity of the data samples in the input space. Thus, the trained model can work as a metric for a wide range of different clustering algorithms. The training process minimizes a similarity loss function that forces the distance metric to be small for pairs of samples from the same kind of activity, and large for pairs of samples from different kinds of activities. We evaluate the model on three datasets to verify its effectiveness in segmentation and recognition of continuous human activity sequences.
<div id='section'>Paperid: <span id='pid'>1230, <a href='https://arxiv.org/pdf/2307.01878.pdf' target='_blank'>https://arxiv.org/pdf/2307.01878.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weijie Xu, Xiaoyu Jiang, Jay Desai, Bin Han, Fuqin Yan, Francis Iannacci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.01878">KDSTM: Neural Semi-supervised Topic Modeling with Knowledge Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In text classification tasks, fine tuning pretrained language models like BERT and GPT-3 yields competitive accuracy; however, both methods require pretraining on large text datasets. In contrast, general topic modeling methods possess the advantage of analyzing documents to extract meaningful patterns of words without the need of pretraining. To leverage topic modeling's unsupervised insights extraction on text classification tasks, we develop the Knowledge Distillation Semi-supervised Topic Modeling (KDSTM). KDSTM requires no pretrained embeddings, few labeled documents and is efficient to train, making it ideal under resource constrained settings. Across a variety of datasets, our method outperforms existing supervised topic modeling methods in classification accuracy, robustness and efficiency and achieves similar performance compare to state of the art weakly supervised text classification methods.
<div id='section'>Paperid: <span id='pid'>1231, <a href='https://arxiv.org/pdf/2306.16714.pdf' target='_blank'>https://arxiv.org/pdf/2306.16714.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuming Zhong, Yi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.16714">SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Breast dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) plays an important role in the screening and prognosis assessment of high-risk breast cancer. The segmentation of cancerous regions is essential useful for the subsequent analysis of breast MRI. To alleviate the annotation effort to train the segmentation networks, we propose a weakly-supervised strategy using extreme points as annotations for breast cancer segmentation. Without using any bells and whistles, our strategy focuses on fully exploiting the learning capability of the routine training procedure, i.e., the train - fine-tune - retrain process. The network first utilizes the pseudo-masks generated using the extreme points to train itself, by minimizing a contrastive loss, which encourages the network to learn more representative features for cancerous voxels. Then the trained network fine-tunes itself by using a similarity-aware propagation learning (SimPLe) strategy, which leverages feature similarity between unlabeled and positive voxels to propagate labels. Finally the network retrains itself by employing the pseudo-masks generated using previous fine-tuned network. The proposed method is evaluated on our collected DCE-MRI dataset containing 206 patients with biopsy-proven breast cancers. Experimental results demonstrate our method effectively fine-tunes the network by using the SimPLe strategy, and achieves a mean Dice value of 81%.
<div id='section'>Paperid: <span id='pid'>1232, <a href='https://arxiv.org/pdf/2306.06823.pdf' target='_blank'>https://arxiv.org/pdf/2306.06823.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sujoy Paul, Gagan Madan, Akankshya Mishra, Narayan Hegde, Pradeep Kumar, Gaurav Aggarwal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.06823">Weakly supervised information extraction from inscrutable handwritten document images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>State-of-the-art information extraction methods are limited by OCR errors. They work well for printed text in form-like documents, but unstructured, handwritten documents still remain a challenge. Adapting existing models to domain-specific training data is quite expensive, because of two factors, 1) limited availability of the domain-specific documents (such as handwritten prescriptions, lab notes, etc.), and 2) annotations become even more challenging as one needs domain-specific knowledge to decode inscrutable handwritten document images. In this work, we focus on the complex problem of extracting medicine names from handwritten prescriptions using only weakly labeled data. The data consists of images along with the list of medicine names in it, but not their location in the image. We solve the problem by first identifying the regions of interest, i.e., medicine lines from just weak labels and then injecting a domain-specific medicine language model learned using only synthetically generated data. Compared to off-the-shelf state-of-the-art methods, our approach performs >2.5x better in medicine names extraction from prescriptions.
<div id='section'>Paperid: <span id='pid'>1233, <a href='https://arxiv.org/pdf/2306.05644.pdf' target='_blank'>https://arxiv.org/pdf/2306.05644.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiyu Wu, Masaaki Nagata, Yoshimasa Tsuruoka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.05644">WSPAlign: Word Alignment Pre-training via Large-Scale Weakly Supervised Span Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most existing word alignment methods rely on manual alignment datasets or parallel corpora, which limits their usefulness. Here, to mitigate the dependence on manual data, we broaden the source of supervision by relaxing the requirement for correct, fully-aligned, and parallel sentences. Specifically, we make noisy, partially aligned, and non-parallel paragraphs. We then use such a large-scale weakly-supervised dataset for word alignment pre-training via span prediction. Extensive experiments with various settings empirically demonstrate that our approach, which is named WSPAlign, is an effective and scalable way to pre-train word aligners without manual data. When fine-tuned on standard benchmarks, WSPAlign has set a new state-of-the-art by improving upon the best-supervised baseline by 3.3~6.1 points in F1 and 1.5~6.1 points in AER. Furthermore, WSPAlign also achieves competitive performance compared with the corresponding baselines in few-shot, zero-shot and cross-lingual tests, which demonstrates that WSPAlign is potentially more practical for low-resource languages than existing methods.
<div id='section'>Paperid: <span id='pid'>1234, <a href='https://arxiv.org/pdf/2305.04186.pdf' target='_blank'>https://arxiv.org/pdf/2305.04186.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xijun Wang, Aggelos K. Katsaggelos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.04186">Video-Specific Query-Key Attention Modeling for Weakly-Supervised Temporal Action Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised temporal action localization aims to identify and localize the action instances in the untrimmed videos with only video-level action labels. When humans watch videos, we can adapt our abstract-level knowledge about actions in different video scenarios and detect whether some actions are occurring. In this paper, we mimic how humans do and bring a new perspective for locating and identifying multiple actions in a video. We propose a network named VQK-Net with a video-specific query-key attention modeling that learns a unique query for each action category of each input video. The learned queries not only contain the actions' knowledge features at the abstract level but also have the ability to fit this knowledge into the target video scenario, and they will be used to detect the presence of the corresponding action along the temporal dimension. To better learn these action category queries, we exploit not only the features of the current input video but also the correlation between different videos through a novel video-specific action category query learner worked with a query similarity loss. Finally, we conduct extensive experiments on three commonly used datasets (THUMOS14, ActivityNet1.2, and ActivityNet1.3) and achieve state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>1235, <a href='https://arxiv.org/pdf/2305.03761.pdf' target='_blank'>https://arxiv.org/pdf/2305.03761.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mariel Pettee, Sowmya Thanvantri, Benjamin Nachman, David Shih, Matthew R. Buckley, Jack H. Collins
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.03761">Weakly-Supervised Anomaly Detection in the Milky Way</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale astrophysics datasets present an opportunity for new machine learning techniques to identify regions of interest that might otherwise be overlooked by traditional searches. To this end, we use Classification Without Labels (CWoLa), a weakly-supervised anomaly detection method, to identify cold stellar streams within the more than one billion Milky Way stars observed by the Gaia satellite. CWoLa operates without the use of labeled streams or knowledge of astrophysical principles. Instead, we train a classifier to distinguish between mixed samples for which the proportions of signal and background samples are unknown. This computationally lightweight strategy is able to detect both simulated streams and the known stream GD-1 in data. Originally designed for high-energy collider physics, this technique may have broad applicability within astrophysics as well as other domains interested in identifying localized anomalies.
<div id='section'>Paperid: <span id='pid'>1236, <a href='https://arxiv.org/pdf/2305.01275.pdf' target='_blank'>https://arxiv.org/pdf/2305.01275.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng-Tao Jiang, Yuqi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.01275">Segment Anything is A Good Pseudo-label Generator for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation with weak labels is a long-lived ill-posed problem. Mainstream methods mainly focus on improving the quality of pseudo labels. In this report, we attempt to explore the potential of 'prompt to masks' from the powerful class-agnostic large segmentation model, segment-anything. Specifically, different weak labels are used as prompts to the segment-anything model, generating precise class masks. The class masks are utilized to generate pseudo labels to train the segmentation networks. We have conducted extensive experiments on PASCAL VOC 2012 dataset. Experiments demonstrate that segment-anything can serve as a good pseudo-label generator. The code will be made publicly available.
<div id='section'>Paperid: <span id='pid'>1237, <a href='https://arxiv.org/pdf/2304.13265.pdf' target='_blank'>https://arxiv.org/pdf/2304.13265.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikita Dvornik, Isma Hadji, Ran Zhang, Konstantinos G. Derpanis, Animesh Garg, Richard P. Wildes, Allan D. Jepson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.13265">StepFormer: Self-supervised Step Discovery and Localization in Instructional Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Instructional videos are an important resource to learn procedural tasks from human demonstrations. However, the instruction steps in such videos are typically short and sparse, with most of the video being irrelevant to the procedure. This motivates the need to temporally localize the instruction steps in such videos, i.e. the task called key-step localization. Traditional methods for key-step localization require video-level human annotations and thus do not scale to large datasets. In this work, we tackle the problem with no human supervision and introduce StepFormer, a self-supervised model that discovers and localizes instruction steps in a video. StepFormer is a transformer decoder that attends to the video with learnable queries, and produces a sequence of slots capturing the key-steps in the video. We train our system on a large dataset of instructional videos, using their automatically-generated subtitles as the only source of supervision. In particular, we supervise our system with a sequence of text narrations using an order-aware loss function that filters out irrelevant phrases. We show that our model outperforms all previous unsupervised and weakly-supervised approaches on step detection and localization by a large margin on three challenging benchmarks. Moreover, our model demonstrates an emergent property to solve zero-shot multi-step localization and outperforms all relevant baselines at this task.
<div id='section'>Paperid: <span id='pid'>1238, <a href='https://arxiv.org/pdf/2303.15092.pdf' target='_blank'>https://arxiv.org/pdf/2303.15092.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vasileios Sevetlidis, George Pavlidis, Vasiliki Balaska, Athanasios Psomoulis, Spyridon Mouroutsos, Antonios Gasteratos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.15092">Defect detection using weakly supervised learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In many real-world scenarios, obtaining large amounts of labeled data can be a daunting task. Weakly supervised learning techniques have gained significant attention in recent years as an alternative to traditional supervised learning, as they enable training models using only a limited amount of labeled data. In this paper, the performance of a weakly supervised classifier to its fully supervised counterpart is compared on the task of defect detection. Experiments are conducted on a dataset of images containing defects, and evaluate the two classifiers based on their accuracy, precision, and recall. Our results show that the weakly supervised classifier achieves comparable performance to the supervised classifier, while requiring significantly less labeled data.
<div id='section'>Paperid: <span id='pid'>1239, <a href='https://arxiv.org/pdf/2303.12095.pdf' target='_blank'>https://arxiv.org/pdf/2303.12095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ricardo Mokhtari, Azam Hamidinekoo, Daniel Sutton, Arthur Lewis, Bastian Angermann, Ulf Gehrmann, Pal Lundin, Hibret Adissu, Junmei Cairns, Jessica Neisen, Emon Khan, Daniel Marks, Nia Khachapuridze, Talha Qaiser, Nikolay Burlutskiy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.12095">Interpretable histopathology-based prediction of disease relevant features in Inflammatory Bowel Disease biopsies using weakly-supervised deep learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Crohn's Disease (CD) and Ulcerative Colitis (UC) are the two main Inflammatory Bowel Disease (IBD) types. We developed deep learning models to identify histological disease features for both CD and UC using only endoscopic labels. We explored fine-tuning and end-to-end training of two state-of-the-art self-supervised models for predicting three different endoscopic categories (i) CD vs UC (AUC=0.87), (ii) normal vs lesional (AUC=0.81), (iii) low vs high disease severity score (AUC=0.80). We produced visual attention maps to interpret what the models learned and validated them with the support of a pathologist, where we observed a strong association between the models' predictions and histopathological inflammatory features of the disease. Additionally, we identified several cases where the model incorrectly predicted normal samples as lesional but were correct on the microscopic level when reviewed by the pathologist. This tendency of histological presentation to be more severe than endoscopic presentation was previously published in the literature. In parallel, we utilised a model trained on the Colon Nuclei Identification and Counting (CoNIC) dataset to predict and explore 6 cell populations. We observed correlation between areas enriched with the predicted immune cells in biopsies and the pathologist's feedback on the attention maps. Finally, we identified several cell level features indicative of disease severity in CD and UC. These models can enhance our understanding about the pathology behind IBD and can shape our strategies for patient stratification in clinical trials.
<div id='section'>Paperid: <span id='pid'>1240, <a href='https://arxiv.org/pdf/2303.10937.pdf' target='_blank'>https://arxiv.org/pdf/2303.10937.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cagri Gungor, Adriana Kovashka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.10937">Boosting Weakly Supervised Object Detection using Fusion and Priors from Hallucinated Depth</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite recent attention and exploration of depth for various tasks, it is still an unexplored modality for weakly-supervised object detection (WSOD). We propose an amplifier method for enhancing the performance of WSOD by integrating depth information. Our approach can be applied to any WSOD method based on multiple-instance learning, without necessitating additional annotations or inducing large computational expenses. Our proposed method employs a monocular depth estimation technique to obtain hallucinated depth information, which is then incorporated into a Siamese WSOD network using contrastive loss and fusion. By analyzing the relationship between language context and depth, we calculate depth priors to identify the bounding box proposals that may contain an object of interest. These depth priors are then utilized to update the list of pseudo ground-truth boxes, or adjust the confidence of per-box predictions. Our proposed method is evaluated on six datasets (COCO, PASCAL VOC, Conceptual Captions, Clipart1k, Watercolor2k, and Comic2k) by implementing it on top of two state-of-the-art WSOD methods, and we demonstrate a substantial enhancement in performance.
<div id='section'>Paperid: <span id='pid'>1241, <a href='https://arxiv.org/pdf/2303.10334.pdf' target='_blank'>https://arxiv.org/pdf/2303.10334.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaozheng Chen, Qianru Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.10334">Extracting Class Activation Maps from Non-Discriminative Features as well</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Extracting class activation maps (CAM) from a classification model often results in poor coverage on foreground objects, i.e., only the discriminative region (e.g., the "head" of "sheep") is recognized and the rest (e.g., the "leg" of "sheep") mistakenly as background. The crux behind is that the weight of the classifier (used to compute CAM) captures only the discriminative features of objects. We tackle this by introducing a new computation method for CAM that explicitly captures non-discriminative features as well, thereby expanding CAM to cover whole objects. Specifically, we omit the last pooling layer of the classification model, and perform clustering on all local features of an object class, where "local" means "at a spatial pixel position". We call the resultant K cluster centers local prototypes - represent local semantics like the "head", "leg", and "body" of "sheep". Given a new image of the class, we compare its unpooled features to every prototype, derive K similarity matrices, and then aggregate them into a heatmap (i.e., our CAM). Our CAM thus captures all local features of the class without discrimination. We evaluate it in the challenging tasks of weakly-supervised semantic segmentation (WSSS), and plug it in multiple state-of-the-art WSSS methods, such as MCTformer and AMN, by simply replacing their original CAM with ours. Our extensive experiments on standard WSSS benchmarks (PASCAL VOC and MS COCO) show the superiority of our method: consistent improvements with little computational overhead.
<div id='section'>Paperid: <span id='pid'>1242, <a href='https://arxiv.org/pdf/2302.10697.pdf' target='_blank'>https://arxiv.org/pdf/2302.10697.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Binwei Xu, Haoran Liang, Weihua Gong, Ronghua Liang, Peng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.10697">A Visual Representation-guided Framework with Global Affinity for Weakly Supervised Salient Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fully supervised salient object detection (SOD) methods have made considerable progress in performance, yet these models rely heavily on expensive pixel-wise labels. Recently, to achieve a trade-off between labeling burden and performance, scribble-based SOD methods have attracted increasing attention. Previous scribble-based models directly implement the SOD task only based on SOD training data with limited information, it is extremely difficult for them to understand the image and further achieve a superior SOD task. In this paper, we propose a simple yet effective framework guided by general visual representations with rich contextual semantic knowledge for scribble-based SOD. These general visual representations are generated by self-supervised learning based on large-scale unlabeled datasets. Our framework consists of a task-related encoder, a general visual module, and an information integration module to efficiently combine the general visual representations with task-related features to perform the SOD task based on understanding the contextual connections of images. Meanwhile, we propose a novel global semantic affinity loss to guide the model to perceive the global structure of the salient objects. Experimental results on five public benchmark datasets demonstrate that our method, which only utilizes scribble annotations without introducing any extra label, outperforms the state-of-the-art weakly supervised SOD methods. Specifically, it outperforms the previous best scribble-based method on all datasets with an average gain of 5.5% for max f-measure, 5.8% for mean f-measure, 24% for MAE, and 3.1% for E-measure. Moreover, our method achieves comparable or even superior performance to the state-of-the-art fully supervised models.
<div id='section'>Paperid: <span id='pid'>1243, <a href='https://arxiv.org/pdf/2302.01887.pdf' target='_blank'>https://arxiv.org/pdf/2302.01887.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tanwi Mallick, Joshua David Bergerson, Duane R. Verner, John K Hutchison, Leslie-Anne Levy, Prasanna Balaprakash
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.01887">Analyzing the impact of climate change on critical infrastructure from the scientific literature: A weakly supervised NLP approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Natural language processing (NLP) is a promising approach for analyzing large volumes of climate-change and infrastructure-related scientific literature. However, best-in-practice NLP techniques require large collections of relevant documents (corpus). Furthermore, NLP techniques using machine learning and deep learning techniques require labels grouping the articles based on user-defined criteria for a significant subset of a corpus in order to train the supervised model. Even labeling a few hundred documents with human subject-matter experts is a time-consuming process. To expedite this process, we developed a weak supervision-based NLP approach that leverages semantic similarity between categories and documents to (i) establish a topic-specific corpus by subsetting a large-scale open-access corpus and (ii) generate category labels for the topic-specific corpus. In comparison with a months-long process of subject-matter expert labeling, we assign category labels to the whole corpus using weak supervision and supervised learning in about 13 hours. The labeled climate and NCF corpus enable targeted, efficient identification of documents discussing a topic (or combination of topics) of interest and identification of various effects of climate change on critical infrastructure, improving the usability of scientific literature and ultimately supporting enhanced policy and decision making. To demonstrate this capability, we conduct topic modeling on pairs of climate hazards and NCFs to discover trending topics at the intersection of these categories. This method is useful for analysts and decision-makers to quickly grasp the relevant topics and most important documents linked to the topic.
<div id='section'>Paperid: <span id='pid'>1244, <a href='https://arxiv.org/pdf/2212.01764.pdf' target='_blank'>https://arxiv.org/pdf/2212.01764.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Binwei Xu, Haoran Liang, Ronghua Liang, Peng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.01764">Synthesize Boundaries: A Boundary-aware Self-consistent Framework for Weakly Supervised Salient Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fully supervised salient object detection (SOD) has made considerable progress based on expensive and time-consuming data with pixel-wise annotations. Recently, to relieve the labeling burden while maintaining performance, some scribble-based SOD methods have been proposed. However, learning precise boundary details from scribble annotations that lack edge information is still difficult. In this paper, we propose to learn precise boundaries from our designed synthetic images and labels without introducing any extra auxiliary data. The synthetic image creates boundary information by inserting synthetic concave regions that simulate the real concave regions of salient objects. Furthermore, we propose a novel self-consistent framework that consists of a global integral branch (GIB) and a boundary-aware branch (BAB) to train a saliency detector. GIB aims to identify integral salient objects, whose input is the original image. BAB aims to help predict accurate boundaries, whose input is the synthetic image. These two branches are connected through a self-consistent loss to guide the saliency detector to predict precise boundaries while identifying salient objects. Experimental results on five benchmarks demonstrate that our method outperforms the state-of-the-art weakly supervised SOD methods and further narrows the gap with the fully supervised methods.
<div id='section'>Paperid: <span id='pid'>1245, <a href='https://arxiv.org/pdf/2209.09500.pdf' target='_blank'>https://arxiv.org/pdf/2209.09500.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei-I Lin, Hsuan-Tien Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.09500">Reduction from Complementary-Label Learning to Probability Estimates</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Complementary-Label Learning (CLL) is a weakly-supervised learning problem that aims to learn a multi-class classifier from only complementary labels, which indicate a class to which an instance does not belong. Existing approaches mainly adopt the paradigm of reduction to ordinary classification, which applies specific transformations and surrogate losses to connect CLL back to ordinary classification. Those approaches, however, face several limitations, such as the tendency to overfit or be hooked on deep models. In this paper, we sidestep those limitations with a novel perspective--reduction to probability estimates of complementary classes. We prove that accurate probability estimates of complementary labels lead to good classifiers through a simple decoding step. The proof establishes a reduction framework from CLL to probability estimates. The framework offers explanations of several key CLL approaches as its special cases and allows us to design an improved algorithm that is more robust in noisy environments. The framework also suggests a validation procedure based on the quality of probability estimates, leading to an alternative way to validate models with only complementary labels. The flexible framework opens a wide range of unexplored opportunities in using deep and non-deep models for probability estimates to solve the CLL problem. Empirical experiments further verified the framework's efficacy and robustness in various settings.
<div id='section'>Paperid: <span id='pid'>1246, <a href='https://arxiv.org/pdf/2208.03326.pdf' target='_blank'>https://arxiv.org/pdf/2208.03326.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michele Cozzatti, Federico Simonetta, Stavros Ntalampiras
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.03326">Variational Autoencoders for Anomaly Detection in Respiratory Sounds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a weakly-supervised machine learning-based approach aiming at a tool to alert patients about possible respiratory diseases. Various types of pathologies may affect the respiratory system, potentially leading to severe diseases and, in certain cases, death. In general, effective prevention practices are considered as major actors towards the improvement of the patient's health condition. The proposed method strives to realize an easily accessible tool for the automatic diagnosis of respiratory diseases. Specifically, the method leverages Variational Autoencoder architectures permitting the usage of training pipelines of limited complexity and relatively small-sized datasets. Importantly, it offers an accuracy of 57 %, which is in line with the existing strongly-supervised approaches.
<div id='section'>Paperid: <span id='pid'>1247, <a href='https://arxiv.org/pdf/2206.01203.pdf' target='_blank'>https://arxiv.org/pdf/2206.01203.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julian Chibane, Francis Engelmann, Tuan Anh Tran, Gerard Pons-Moll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.01203">Box2Mask: Weakly Supervised 3D Semantic Instance Segmentation Using Bounding Boxes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current 3D segmentation methods heavily rely on large-scale point-cloud datasets, which are notoriously laborious to annotate. Few attempts have been made to circumvent the need for dense per-point annotations. In this work, we look at weakly-supervised 3D semantic instance segmentation. The key idea is to leverage 3D bounding box labels which are easier and faster to annotate. Indeed, we show that it is possible to train dense segmentation models using only bounding box labels. At the core of our method, \name{}, lies a deep model, inspired by classical Hough voting, that directly votes for bounding box parameters, and a clustering method specifically tailored to bounding box votes. This goes beyond commonly used center votes, which would not fully exploit the bounding box annotations. On ScanNet test, our weakly supervised model attains leading performance among other weakly supervised approaches (+18 mAP@50). Remarkably, it also achieves 97% of the mAP@50 score of current fully supervised models. To further illustrate the practicality of our work, we train Box2Mask on the recently released ARKitScenes dataset which is annotated with 3D bounding boxes only, and show, for the first time, compelling 3D instance segmentation masks.
<div id='section'>Paperid: <span id='pid'>1248, <a href='https://arxiv.org/pdf/2203.15441.pdf' target='_blank'>https://arxiv.org/pdf/2203.15441.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Subhrajyoti Dasgupta, Arindam Das, Senthil Yogamani, Sudip Das, Ciaran Eising, Andrei Bursuc, Ujjwal Bhattacharya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.15441">UnShadowNet: Illumination Critic Guided Contrastive Learning For Shadow Removal</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Shadows are frequently encountered natural phenomena that significantly hinder the performance of computer vision perception systems in practical settings, e.g., autonomous driving. A solution to this would be to eliminate shadow regions from the images before the processing of the perception system. Yet, training such a solution requires pairs of aligned shadowed and non-shadowed images which are difficult to obtain. We introduce a novel weakly supervised shadow removal framework UnShadowNet trained using contrastive learning. It is composed of a DeShadower network responsible for the removal of the extracted shadow under the guidance of an Illumination network which is trained adversarially by the illumination critic and a Refinement network to further remove artefacts. We show that UnShadowNet can be easily extended to a fully-supervised set-up to exploit the ground-truth when available. UnShadowNet outperforms existing state-of-the-art approaches on three publicly available shadow datasets (ISTD, adjusted ISTD, SRD) in both the weakly and fully supervised setups.
<div id='section'>Paperid: <span id='pid'>1249, <a href='https://arxiv.org/pdf/2112.06311.pdf' target='_blank'>https://arxiv.org/pdf/2112.06311.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tomer Wolfson, Daniel Deutch, Jonathan Berant
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2112.06311">Weakly Supervised Text-to-SQL Parsing through Question Decomposition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-SQL parsers are crucial in enabling non-experts to effortlessly query relational data. Training such parsers, by contrast, generally requires expertise in annotating natural language (NL) utterances with corresponding SQL queries. In this work, we propose a weak supervision approach for training text-to-SQL parsers. We take advantage of the recently proposed question meaning representation called QDMR, an intermediate between NL and formal query languages. Given questions, their QDMR structures (annotated by non-experts or automatically predicted), and the answers, we are able to automatically synthesize SQL queries that are used to train text-to-SQL models. We test our approach by experimenting on five benchmark datasets. Our results show that the weakly supervised models perform competitively with those trained on annotated NL-SQL data. Overall, we effectively train text-to-SQL parsers, while using zero SQL annotations.
<div id='section'>Paperid: <span id='pid'>1250, <a href='https://arxiv.org/pdf/2112.01261.pdf' target='_blank'>https://arxiv.org/pdf/2112.01261.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyi Feng, Yong Luo, Shuang Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2112.01261">ViF-SD2E: A Robust Weakly-Supervised Method for Neural Decoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural decoding plays a vital role in the interaction between the brain and the outside world. In this paper, we directly decode the movement track of a finger based on the neural signals of a macaque. Supervised regression methods may overfit to actual labels containing noise, and require a high labeling cost, while unsupervised approaches often have unsatisfactory accuracy. Besides, the spatial and temporal information is often ignored or not well exploited by those methods. This motivates us to propose a robust weakly-supervised method, called ViF-SD2E, for neural decoding. In particular, it consists of a space-division (SD) module and a exploration--exploitation (2E) strategy, to effectively exploit both the spatial information of the outside world and the temporal information of neural activity, where the SD2E output is analogized with the weak 0/1 vision-feedback (ViF) label for training. It is worth noting that the designed ViF-SD2E is based on a symmetric phenomenon between the unsupervised decoding trajectory and the real trajectory in previous observations, then a cognitive pattern of fuzzy (robust) interaction in the nervous system may be discovered by us. Extensive experiments demonstrate the effectiveness of our method, which can be sometimes comparable to supervised counterparts.
<div id='section'>Paperid: <span id='pid'>1251, <a href='https://arxiv.org/pdf/2104.06722.pdf' target='_blank'>https://arxiv.org/pdf/2104.06722.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Oishik Chatterjee, Isha Pandey, Aashish Waikar, Vishwajeet Kumar, Ganesh Ramakrishnan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2104.06722">WARM: A Weakly (+Semi) Supervised Model for Solving Math word Problems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Solving math word problems (MWPs) is an important and challenging problem in natural language processing. Existing approaches to solve MWPs require full supervision in the form of intermediate equations. However, labeling every MWP with its corresponding equations is a time-consuming and expensive task. In order to address this challenge of equation annotation, we propose a weakly supervised model for solving MWPs by requiring only the final answer as supervision. We approach this problem by first learning to generate the equation using the problem description and the final answer, which we subsequently use to train a supervised MWP solver. We propose and compare various weakly supervised techniques to learn to generate equations directly from the problem description and answer. Through extensive experiments, we demonstrate that without using equations for supervision, our approach achieves accuracy gains of 4.5% and 32% over the state-of-the-art weakly supervised approach, on the standard Math23K and AllArith datasets respectively. Additionally, we curate and release new datasets of roughly 10k MWPs each in English and in Hindi (a low resource language).These datasets are suitable for training weakly supervised models. We also present an extension of WARMM to semi-supervised learning and present further improvements on results, along with insights.
<div id='section'>Paperid: <span id='pid'>1252, <a href='https://arxiv.org/pdf/2512.06171.pdf' target='_blank'>https://arxiv.org/pdf/2512.06171.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jessica Plassmann, Nicolas Schuler, Michael Schuth, Georg von Freymann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.06171">Automated Annotation of Shearographic Measurements Enabling Weakly Supervised Defect Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Shearography is an interferometric technique sensitive to surface displacement gradients, providing high sensitivity for detecting subsurface defects in safety-critical components. A key limitation to industrial adoption is the lack of high-quality annotated datasets, since manual labeling remains labor-intensive, subjective, and difficult to standardize. We introduce an automated workflow that generates defect annotations from shearography measurements using deep learning, producing high-resolution segmentation and bounding-box labels. Evaluation against expert-labeled data demonstrates sufficient accuracy to enable weakly supervised training, reducing manual effort and supporting scalable dataset creation for robust defect detection.
<div id='section'>Paperid: <span id='pid'>1253, <a href='https://arxiv.org/pdf/2512.05922.pdf' target='_blank'>https://arxiv.org/pdf/2512.05922.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Khang Le, Anh Mai Vu, Thi Kim Trang Vo, Ha Thach, Ngoc Bui Lam Quang, Thanh-Huy Nguyen, Minh H. N. Le, Zhu Han, Chandra Mohan, Hien Van Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.05922">LPD: Learnable Prototypes with Diversity Regularization for Weakly Supervised Histopathology Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation (WSSS) in histopathology reduces pixel-level labeling by learning from image-level labels, but it is hindered by inter-class homogeneity, intra-class heterogeneity, and CAM-induced region shrinkage (global pooling-based class activation maps whose activations highlight only the most distinctive areas and miss nearby class regions). Recent works address these challenges by constructing a clustering prototype bank and then refining masks in a separate stage; however, such two-stage pipelines are costly, sensitive to hyperparameters, and decouple prototype discovery from segmentation learning, limiting their effectiveness and efficiency. We propose a cluster-free, one-stage learnable-prototype framework with diversity regularization to enhance morphological intra-class heterogeneity coverage. Our approach achieves state-of-the-art (SOTA) performance on BCSS-WSSS, outperforming prior methods in mIoU and mDice. Qualitative segmentation maps show sharper boundaries and fewer mislabels, and activation heatmaps further reveal that, compared with clustering-based prototypes, our learnable prototypes cover more diverse and complementary regions within each class, providing consistent qualitative evidence for their effectiveness.
<div id='section'>Paperid: <span id='pid'>1254, <a href='https://arxiv.org/pdf/2511.17392.pdf' target='_blank'>https://arxiv.org/pdf/2511.17392.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runxun Zhang, Yizhou Liu, Li Dongrui, Bo XU, Jingwei Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17392">MorphSeek: Fine-grained Latent Representation-Level Policy Optimization for Deformable Image Registration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deformable image registration (DIR) remains a fundamental yet challenging problem in medical image analysis, largely due to the prohibitively high-dimensional deformation space of dense displacement fields and the scarcity of voxel-level supervision. Existing reinforcement learning frameworks often project this space into coarse, low-dimensional representations, limiting their ability to capture spatially variant deformations. We propose MorphSeek, a fine-grained representation-level policy optimization paradigm that reformulates DIR as a spatially continuous optimization process in the latent feature space. MorphSeek introduces a stochastic Gaussian policy head atop the encoder to model a distribution over latent features, facilitating efficient exploration and coarse-to-fine refinement. The framework integrates unsupervised warm-up with weakly supervised fine-tuning through Group Relative Policy Optimization, where multi-trajectory sampling stabilizes training and improves label efficiency. Across three 3D registration benchmarks (OASIS brain MRI, LiTS liver CT, and Abdomen MR-CT), MorphSeek achieves consistent Dice improvements over competitive baselines while maintaining high label efficiency with minimal parameter cost and low step-level latency overhead. Beyond optimizer specifics, MorphSeek advances a representation-level policy learning paradigm that achieves spatially coherent and data-efficient deformation optimization, offering a principled, backbone-agnostic, and optimizer-agnostic solution for scalable visual alignment in high-dimensional settings.
<div id='section'>Paperid: <span id='pid'>1255, <a href='https://arxiv.org/pdf/2511.13276.pdf' target='_blank'>https://arxiv.org/pdf/2511.13276.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Noam Tsfaty, Avishai Weizman, Liav Cohen, Moshe Tshuva, Yehudit Aperstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13276">Recognition of Abnormal Events in Surveillance Videos using Weakly Supervised Dual-Encoder Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address the challenge of detecting rare and diverse anomalies in surveillance videos using only video-level supervision. Our dual-backbone framework combines convolutional and transformer representations through top-k pooling, achieving 90.7% area under the curve (AUC) on the UCF-Crime dataset.
<div id='section'>Paperid: <span id='pid'>1256, <a href='https://arxiv.org/pdf/2510.12182.pdf' target='_blank'>https://arxiv.org/pdf/2510.12182.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Youngju Yoo, Seho Kim, Changick Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.12182">BEEP3D: Box-Supervised End-to-End Pseudo-Mask Generation for 3D Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D instance segmentation is crucial for understanding complex 3D environments, yet fully supervised methods require dense point-level annotations, resulting in substantial annotation costs and labor overhead. To mitigate this, box-level annotations have been explored as a weaker but more scalable form of supervision. However, box annotations inherently introduce ambiguity in overlapping regions, making accurate point-to-instance assignment challenging. Recent methods address this ambiguity by generating pseudo-masks through training a dedicated pseudo-labeler in an additional training stage. However, such two-stage pipelines often increase overall training time and complexity, hinder end-to-end optimization. To overcome these challenges, we propose BEEP3D-Box-supervised End-to-End Pseudo-mask generation for 3D instance segmentation. BEEP3D adopts a student-teacher framework, where the teacher model serves as a pseudo-labeler and is updated by the student model via an Exponential Moving Average. To better guide the teacher model to generate precise pseudo-masks, we introduce an instance center-based query refinement that enhances position query localization and leverages features near instance centers. Additionally, we design two novel losses-query consistency loss and masked feature consistency loss-to align semantic and geometric signals between predictions and pseudo-masks. Extensive experiments on ScanNetV2 and S3DIS datasets demonstrate that BEEP3D achieves competitive or superior performance compared to state-of-the-art weakly supervised methods while remaining computationally efficient.
<div id='section'>Paperid: <span id='pid'>1257, <a href='https://arxiv.org/pdf/2508.18958.pdf' target='_blank'>https://arxiv.org/pdf/2508.18958.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matteo Contini, Victor Illien, Sylvain Poulain, Serge Bernard, Julien Barde, Sylvain Bonhommeau, Alexis Joly
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18958">The point is the mask: scaling coral reef segmentation with weak supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monitoring coral reefs at large spatial scales remains an open challenge, essential for assessing ecosystem health and informing conservation efforts. While drone-based aerial imagery offers broad spatial coverage, its limited resolution makes it difficult to reliably distinguish fine-scale classes, such as coral morphotypes. At the same time, obtaining pixel-level annotations over large spatial extents is costly and labor-intensive, limiting the scalability of deep learning-based segmentation methods for aerial imagery. We present a multi-scale weakly supervised semantic segmentation framework that addresses this challenge by transferring fine-scale ecological information from underwater imagery to aerial data. Our method enables large-scale coral reef mapping from drone imagery with minimal manual annotation, combining classification-based supervision, spatial interpolation and self-distillation techniques. We demonstrate the efficacy of the approach, enabling large-area segmentation of coral morphotypes and demonstrating flexibility for integrating new classes. This study presents a scalable, cost-effective methodology for high-resolution reef monitoring, combining low-cost data collection, weakly supervised deep learning and multi-scale remote sensing.
<div id='section'>Paperid: <span id='pid'>1258, <a href='https://arxiv.org/pdf/2508.11826.pdf' target='_blank'>https://arxiv.org/pdf/2508.11826.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dehn Xu, Tim Katzke, Emmanuel MÃ¼ller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11826">From Pixels to Graphs: Deep Graph-Level Anomaly Detection on Dermoscopic Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph Neural Networks (GNNs) have emerged as a powerful approach for graph-based machine learning tasks. Previous work applied GNNs to image-derived graph representations for various downstream tasks such as classification or anomaly detection. These transformations include segmenting images, extracting features from segments, mapping them to nodes, and connecting them. However, to the best of our knowledge, no study has rigorously compared the effectiveness of the numerous potential image-to-graph transformation approaches for GNN-based graph-level anomaly detection (GLAD). In this study, we systematically evaluate the efficacy of multiple segmentation schemes, edge construction strategies, and node feature sets based on color, texture, and shape descriptors to produce suitable image-derived graph representations to perform graph-level anomaly detection. We conduct extensive experiments on dermoscopic images using state-of-the-art GLAD models, examining performance and efficiency in purely unsupervised, weakly supervised, and fully supervised regimes. Our findings reveal, for example, that color descriptors contribute the best standalone performance, while incorporating shape and texture features consistently enhances detection efficacy. In particular, our best unsupervised configuration using OCGTL achieves a competitive AUC-ROC score of up to 0.805 without relying on pretrained backbones like comparable image-based approaches. With the inclusion of sparse labels, the performance increases substantially to 0.872 and with full supervision to 0.914 AUC-ROC.
<div id='section'>Paperid: <span id='pid'>1259, <a href='https://arxiv.org/pdf/2508.08095.pdf' target='_blank'>https://arxiv.org/pdf/2508.08095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chun Wang, Chenyang Liu, Wenze Xu, Weihong Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08095">Dual Information Speech Language Models for Emotional Conversations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conversational systems relying on text-based large language models (LLMs) often overlook paralinguistic cues, essential for understanding emotions and intentions. Speech-language models (SLMs), which use speech as input, are emerging as a promising solution. However, SLMs built by extending frozen LLMs struggle to capture paralinguistic information and exhibit reduced context understanding. We identify entangled information and improper training strategies as key issues. To address these issues, we propose two heterogeneous adapters and suggest a weakly supervised training strategy. Our approach disentangles paralinguistic and linguistic information, enabling SLMs to interpret speech through structured representations. It also preserves contextual understanding by avoiding the generation of task-specific vectors through controlled randomness. This approach trains only the adapters on common datasets, ensuring parameter and data efficiency. Experiments demonstrate competitive performance in emotional conversation tasks, showcasing the model's ability to effectively integrate both paralinguistic and linguistic information within contextual settings.
<div id='section'>Paperid: <span id='pid'>1260, <a href='https://arxiv.org/pdf/2508.05019.pdf' target='_blank'>https://arxiv.org/pdf/2508.05019.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sadia Kamal, Tim Oates, Joy Wan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05019">Skin-SOAP: A Weakly Supervised Framework for Generating Structured SOAP Notes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skin carcinoma is the most prevalent form of cancer globally, accounting for over $8 billion in annual healthcare expenditures. Early diagnosis, accurate and timely treatment are critical to improving patient survival rates. In clinical settings, physicians document patient visits using detailed SOAP (Subjective, Objective, Assessment, and Plan) notes. However, manually generating these notes is labor-intensive and contributes to clinician burnout. In this work, we propose skin-SOAP, a weakly supervised multimodal framework to generate clinically structured SOAP notes from limited inputs, including lesion images and sparse clinical text. Our approach reduces reliance on manual annotations, enabling scalable, clinically grounded documentation while alleviating clinician burden and reducing the need for large annotated data. Our method achieves performance comparable to GPT-4o, Claude, and DeepSeek Janus Pro across key clinical relevance metrics. To evaluate this clinical relevance, we introduce two novel metrics MedConceptEval and Clinical Coherence Score (CCS) which assess semantic alignment with expert medical concepts and input features, respectively.
<div id='section'>Paperid: <span id='pid'>1261, <a href='https://arxiv.org/pdf/2508.03724.pdf' target='_blank'>https://arxiv.org/pdf/2508.03724.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jia Li, Yapeng Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03724">From Waveforms to Pixels: A Survey on Audio-Visual Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-Visual Segmentation (AVS) aims to identify and segment sound-producing objects in videos by leveraging both visual and audio modalities. It has emerged as a significant research area in multimodal perception, enabling fine-grained object-level understanding. In this survey, we present a comprehensive overview of the AVS field, covering its problem formulation, benchmark datasets, evaluation metrics, and the progression of methodologies. We analyze a wide range of approaches, including architectures for unimodal and multimodal encoding, key strategies for audio-visual fusion, and various decoder designs. Furthermore, we examine major training paradigms, from fully supervised learning to weakly supervised and training-free methods. Notably, we provide an extensive comparison of AVS methods across standard benchmarks, highlighting the impact of different architectural choices, fusion strategies, and training paradigms on performance. Finally, we outline the current challenges, such as limited temporal modeling, modality bias toward vision, lack of robustness in complex environments, and high computational demands, and propose promising future directions, including improving temporal reasoning and multimodal fusion, leveraging foundation models for better generalization and few-shot learning, reducing reliance on labeled data through selfand weakly supervised learning, and incorporating higher-level reasoning for more intelligent AVS systems.
<div id='section'>Paperid: <span id='pid'>1262, <a href='https://arxiv.org/pdf/2508.02844.pdf' target='_blank'>https://arxiv.org/pdf/2508.02844.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anghong Du, Nay Aung, Theodoros N. Arvanitis, Stefan K. Piechnik, Joao A C Lima, Steffen E. Petersen, Le Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02844">RefineSeg: Dual Coarse-to-Fine Learning for Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-quality pixel-level annotations of medical images are essential for supervised segmentation tasks, but obtaining such annotations is costly and requires medical expertise. To address this challenge, we propose a novel coarse-to-fine segmentation framework that relies entirely on coarse-level annotations, encompassing both target and complementary drawings, despite their inherent noise. The framework works by introducing transition matrices in order to model the inaccurate and incomplete regions in the coarse annotations. By jointly training on multiple sets of coarse annotations, it progressively refines the network's outputs and infers the true segmentation distribution, achieving a robust approximation of precise labels through matrix-based modeling. To validate the flexibility and effectiveness of the proposed method, we demonstrate the results on two public cardiac imaging datasets, ACDC and MSCMRseg, and further evaluate its performance on the UK Biobank dataset. Experimental results indicate that our approach surpasses the state-of-the-art weakly supervised methods and closely matches the fully supervised approach.
<div id='section'>Paperid: <span id='pid'>1263, <a href='https://arxiv.org/pdf/2507.06848.pdf' target='_blank'>https://arxiv.org/pdf/2507.06848.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joelle Hanna, Damian Borth
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06848">Know Your Attention Maps: Class-specific Token Masking for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Semantic Segmentation (WSSS) is a challenging problem that has been extensively studied in recent years. Traditional approaches often rely on external modules like Class Activation Maps to highlight regions of interest and generate pseudo segmentation masks. In this work, we propose an end-to-end method that directly utilizes the attention maps learned by a Vision Transformer (ViT) for WSSS. We propose training a sparse ViT with multiple [CLS] tokens (one for each class), using a random masking strategy to promote [CLS] token - class assignment. At inference time, we aggregate the different self-attention maps of each [CLS] token corresponding to the predicted labels to generate pseudo segmentation masks. Our proposed approach enhances the interpretability of self-attention maps and ensures accurate class assignments. Extensive experiments on two standard benchmarks and three specialized datasets demonstrate that our method generates accurate pseudo-masks, outperforming related works. Those pseudo-masks can be used to train a segmentation model which achieves results comparable to fully-supervised models, significantly reducing the need for fine-grained labeled data.
<div id='section'>Paperid: <span id='pid'>1264, <a href='https://arxiv.org/pdf/2507.01721.pdf' target='_blank'>https://arxiv.org/pdf/2507.01721.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongwen Zhang, Yuri Boykov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01721">Soft Self-labeling and Potts Relaxations for Weakly-Supervised Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider weakly supervised segmentation where only a fraction of pixels have ground truth labels (scribbles) and focus on a self-labeling approach optimizing relaxations of the standard unsupervised CRF/Potts loss on unlabeled pixels. While WSSS methods can directly optimize such losses via gradient descent, prior work suggests that higher-order optimization can improve network training by introducing hidden pseudo-labels and powerful CRF sub-problem solvers, e.g. graph cut. However, previously used hard pseudo-labels can not represent class uncertainty or errors, which motivates soft self-labeling. We derive a principled auxiliary loss and systematically evaluate standard and new CRF relaxations (convex and non-convex), neighborhood systems, and terms connecting network predictions with soft pseudo-labels. We also propose a general continuous sub-problem solver. Using only standard architectures, soft self-labeling consistently improves scribble-based training and outperforms significantly more complex specialized WSSS systems. It can outperform full pixel-precise supervision. Our general ideas apply to other weakly-supervised problems/systems.
<div id='section'>Paperid: <span id='pid'>1265, <a href='https://arxiv.org/pdf/2506.10328.pdf' target='_blank'>https://arxiv.org/pdf/2506.10328.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sadia Kamal, Tim Oates, Joy Wan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10328">Towards Scalable SOAP Note Generation: A Weakly Supervised Multimodal Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skin carcinoma is the most prevalent form of cancer globally, accounting for over $8 billion in annual healthcare expenditures. In clinical settings, physicians document patient visits using detailed SOAP (Subjective, Objective, Assessment, and Plan) notes. However, manually generating these notes is labor-intensive and contributes to clinician burnout. In this work, we propose a weakly supervised multimodal framework to generate clinically structured SOAP notes from limited inputs, including lesion images and sparse clinical text. Our approach reduces reliance on manual annotations, enabling scalable, clinically grounded documentation while alleviating clinician burden and reducing the need for large annotated data. Our method achieves performance comparable to GPT-4o, Claude, and DeepSeek Janus Pro across key clinical relevance metrics. To evaluate clinical quality, we introduce two novel metrics MedConceptEval and Clinical Coherence Score (CCS) which assess semantic alignment with expert medical concepts and input features, respectively.
<div id='section'>Paperid: <span id='pid'>1266, <a href='https://arxiv.org/pdf/2506.07076.pdf' target='_blank'>https://arxiv.org/pdf/2506.07076.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyi Wu, Haohong Wang, Aggelos K. Katsaggelos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07076">Harmony-Aware Music-driven Motion Synthesis with Perceptual Constraint on UGC Datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the popularity of video-based user-generated content (UGC) on social media, harmony, as dictated by human perceptual principles, is critical in assessing the rhythmic consistency of audio-visual UGCs for better user engagement. In this work, we propose a novel harmony-aware GAN framework, following a specifically designed harmony evaluation strategy to enhance rhythmic synchronization in the automatic music-to-motion synthesis using a UGC dance dataset. This harmony strategy utilizes refined cross-modal beat detection to capture closely correlated audio and visual rhythms in an audio-visual pair. To mimic human attention mechanism, we introduce saliency-based beat weighting and interval-driven beat alignment, which ensures accurate harmony score estimation consistent with human perception. Building on this strategy, our model, employing efficient encoder-decoder and depth-lifting designs, is adversarially trained based on categorized musical meter segments to generate realistic and rhythmic 3D human motions. We further incorporate our harmony evaluation strategy as a weakly supervised perceptual constraint to flexibly guide the synchronized audio-visual rhythms during the generation process. Experimental results show that our proposed model significantly outperforms other leading music-to-motion methods in rhythmic harmony, both quantitatively and qualitatively, even with limited UGC training data. Live samples 15 can be watched at: https://youtu.be/tWwz7yq4aUs
<div id='section'>Paperid: <span id='pid'>1267, <a href='https://arxiv.org/pdf/2506.03570.pdf' target='_blank'>https://arxiv.org/pdf/2506.03570.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lin Sun, Chuang Liu, Xiaofeng Ma, Tao Yang, Weijia Lu, Ning Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03570">FreePRM: Training Process Reward Models Without Ground Truth Process Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in Large Language Models (LLMs) have demonstrated that Process Reward Models (PRMs) play a crucial role in enhancing model performance. However, training PRMs typically requires step-level labels, either manually annotated or automatically generated, which can be costly and difficult to obtain at scale. To address this challenge, we introduce FreePRM, a weakly supervised framework for training PRMs without access to ground-truth step-level labels. FreePRM first generates pseudo step-level labels based on the correctness of final outcome, and then employs Buffer Probability to eliminate impact of noise inherent in pseudo labeling. Experimental results show that FreePRM achieves an average F1 score of 53.0% on ProcessBench, outperforming fully supervised PRM trained on Math-Shepherd by +24.1%. Compared to other open-source PRMs, FreePRM outperforms upon RLHFlow-PRM-Mistral-8B (28.4%) by +24.6%, EurusPRM (31.3%) by +21.7%, and Skywork-PRM-7B (42.1%) by +10.9%. This work introduces a new paradigm in PRM training, significantly reducing reliance on costly step-level annotations while maintaining strong performance.
<div id='section'>Paperid: <span id='pid'>1268, <a href='https://arxiv.org/pdf/2506.02451.pdf' target='_blank'>https://arxiv.org/pdf/2506.02451.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pratheeksha Nair, Reihaneh Rabbany
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02451">Weak Supervision for Real World Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Node classification in real world graphs often suffers from label scarcity and noise, especially in high stakes domains like human trafficking detection and misinformation monitoring. While direct supervision is limited, such graphs frequently contain weak signals, noisy or indirect cues, that can still inform learning. We propose WSNET, a novel weakly supervised graph contrastive learning framework that leverages these weak signals to guide robust representation learning. WSNET integrates graph structure, node features, and multiple noisy supervision sources through a contrastive objective tailored for weakly labeled data. Across three real world datasets and synthetic benchmarks with controlled noise, WSNET consistently outperforms state of the art contrastive and noisy label learning methods by up to 15% in F1 score. Our results highlight the effectiveness of contrastive learning under weak supervision and the promise of exploiting imperfect labels in graph based settings.
<div id='section'>Paperid: <span id='pid'>1269, <a href='https://arxiv.org/pdf/2505.09955.pdf' target='_blank'>https://arxiv.org/pdf/2505.09955.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaeho Kim, Seulki Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09955">TransPL: VQ-Code Transition Matrices for Pseudo-Labeling of Time Series Unsupervised Domain Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsupervised domain adaptation (UDA) for time series data remains a critical challenge in deep learning, with traditional pseudo-labeling strategies failing to capture temporal patterns and channel-wise shifts between domains, producing sub-optimal pseudo-labels. As such, we introduce TransPL, a novel approach that addresses these limitations by modeling the joint distribution $P(\mathbf{X}, y)$ of the source domain through code transition matrices, where the codes are derived from vector quantization (VQ) of time series patches. Our method constructs class- and channel-wise code transition matrices from the source domain and employs Bayes' rule for target domain adaptation, generating pseudo-labels based on channel-wise weighted class-conditional likelihoods. TransPL offers three key advantages: explicit modeling of temporal transitions and channel-wise shifts between different domains, versatility towards different UDA scenarios (e.g., weakly-supervised UDA), and explainable pseudo-label generation. We validate TransPL's effectiveness through extensive analysis on four time series UDA benchmarks and confirm that it consistently outperforms state-of-the-art pseudo-labeling methods by a strong margin (6.1% accuracy improvement, 4.9% F1 improvement), while providing interpretable insights into the domain adaptation process through its learned code transition matrices.
<div id='section'>Paperid: <span id='pid'>1270, <a href='https://arxiv.org/pdf/2505.09011.pdf' target='_blank'>https://arxiv.org/pdf/2505.09011.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Antonio Candito, Matthew D Blackledge, Richard Holbrey, Nuria Porta, Ana Ribeiro, Fabio Zugni, Luca D'Erme, Francesca Castagnoli, Alina Dragan, Ricardo Donners, Christina Messiou, Nina Tunariu, Dow-Mu Koh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09011">Signal-based AI-driven software solution for automated quantification of metastatic bone disease and treatment response assessment using Whole-Body Diffusion-Weighted MRI (WB-DWI) biomarkers in Advanced Prostate Cancer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We developed an AI-driven software solution to quantify metastatic bone disease from WB-DWI scans. Core technologies include: (i) a weakly-supervised Residual U-Net model generating a skeleton probability map to isolate bone; (ii) a statistical framework for WB-DWI intensity normalisation, obtaining a signal-normalised b=900s/mm^2 (b900) image; and (iii) a shallow convolutional neural network that processes outputs from (i) and (ii) to generate a mask of suspected bone lesions, characterised by higher b900 signal intensity due to restricted water diffusion. This mask is applied to the gADC map to extract TDV and gADC statistics. We tested the tool using expert-defined metastatic bone disease delineations on 66 datasets, assessed repeatability of imaging biomarkers (N=10), and compared software-based response assessment with a construct reference standard based on clinical, laboratory and imaging assessments (N=118). Dice score between manual and automated delineations was 0.6 for lesions within pelvis and spine, with an average surface distance of 2mm. Relative differences for log-transformed TDV (log-TDV) and median gADC were below 9% and 5%, respectively. Repeatability analysis showed coefficients of variation of 4.57% for log-TDV and 3.54% for median gADC, with intraclass correlation coefficients above 0.9. The software achieved 80.5% accuracy, 84.3% sensitivity, and 85.7% specificity in assessing response to treatment compared to the construct reference standard. Computation time generating a mask averaged 90 seconds per scan. Our software enables reproducible TDV and gADC quantification from WB-DWI scans for monitoring metastatic bone disease response, thus providing potentially useful measurements for clinical decision-making in APC patients.
<div id='section'>Paperid: <span id='pid'>1271, <a href='https://arxiv.org/pdf/2504.09430.pdf' target='_blank'>https://arxiv.org/pdf/2504.09430.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruiwen Ding, Lin Li, Rajath Soans, Tosha Shah, Radha Krishnan, Marc Alexander Sze, Sasha Lukyanov, Yash Deshpande, Antong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09430">Predicting ulcer in H&E images of inflammatory bowel disease using domain-knowledge-driven graph neural network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inflammatory bowel disease (IBD) involves chronic inflammation of the digestive tract, with treatment options often burdened by adverse effects. Identifying biomarkers for personalized treatment is crucial. While immune cells play a key role in IBD, accurately identifying ulcer regions in whole slide images (WSIs) is essential for characterizing these cells and exploring potential therapeutics. Multiple instance learning (MIL) approaches have advanced WSI analysis but they lack spatial context awareness. In this work, we propose a weakly-supervised model called DomainGCN that employs a graph convolution neural network (GCN) and incorporates domain-specific knowledge of ulcer features, specifically, the presence of epithelium, lymphocytes, and debris for WSI-level ulcer prediction in IBD. We demonstrate that DomainGCN outperforms various state-of-the-art (SOTA) MIL methods and show the added value of domain knowledge.
<div id='section'>Paperid: <span id='pid'>1272, <a href='https://arxiv.org/pdf/2504.05700.pdf' target='_blank'>https://arxiv.org/pdf/2504.05700.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seth Z. Zhao, Reza Ghoddoosian, Isht Dwivedi, Nakul Agarwal, Behzad Dariush
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.05700">Pose-Aware Weakly-Supervised Action Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding human behavior is an important problem in the pursuit of visual intelligence. A challenge in this endeavor is the extensive and costly effort required to accurately label action segments. To address this issue, we consider learning methods that demand minimal supervision for segmentation of human actions in long instructional videos. Specifically, we introduce a weakly-supervised framework that uniquely incorporates pose knowledge during training while omitting its use during inference, thereby distilling pose knowledge pertinent to each action component. We propose a pose-inspired contrastive loss as a part of the whole weakly-supervised framework which is trained to distinguish action boundaries more effectively. Our approach, validated through extensive experiments on representative datasets, outperforms previous state-of-the-art (SOTA) in segmenting long instructional videos under both online and offline settings. Additionally, we demonstrate the framework's adaptability to various segmentation backbones and pose extractors across different datasets.
<div id='section'>Paperid: <span id='pid'>1273, <a href='https://arxiv.org/pdf/2503.21616.pdf' target='_blank'>https://arxiv.org/pdf/2503.21616.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahui Chen, Yang Huan, Runhua Shi, Chanfan Ding, Xiaoqi Mo, Siyu Xiong, Yinong He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.21616">Audio-driven Gesture Generation via Deviation Feature in the Latent Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gestures are essential for enhancing co-speech communication, offering visual emphasis and complementing verbal interactions. While prior work has concentrated on point-level motion or fully supervised data-driven methods, we focus on co-speech gestures, advocating for weakly supervised learning and pixel-level motion deviations. We introduce a weakly supervised framework that learns latent representation deviations, tailored for co-speech gesture video generation. Our approach employs a diffusion model to integrate latent motion features, enabling more precise and nuanced gesture representation. By leveraging weakly supervised deviations in latent space, we effectively generate hand gestures and mouth movements, crucial for realistic video production. Experiments show our method significantly improves video quality, surpassing current state-of-the-art techniques.
<div id='section'>Paperid: <span id='pid'>1274, <a href='https://arxiv.org/pdf/2503.18509.pdf' target='_blank'>https://arxiv.org/pdf/2503.18509.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nijesh Upreti, Vaishak Belle
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18509">Neuro-symbolic Weak Supervision: Theory and Semantics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weak supervision allows machine learning models to learn from limited or noisy labels, but it introduces challenges in interpretability and reliability - particularly in multi-instance partial label learning (MI-PLL), where models must resolve both ambiguous labels and uncertain instance-label mappings. We propose a semantics for neuro-symbolic framework that integrates Inductive Logic Programming (ILP) to improve MI-PLL by providing structured relational constraints that guide learning. Within our semantic characterization, ILP defines a logical hypothesis space for label transitions, clarifies classifier semantics, and establishes interpretable performance standards. This hybrid approach improves robustness, transparency, and accountability in weakly supervised settings, ensuring neural predictions align with domain knowledge. By embedding weak supervision into a logical framework, we enhance both interpretability and learning, making weak supervision more suitable for real-world, high-stakes applications.
<div id='section'>Paperid: <span id='pid'>1275, <a href='https://arxiv.org/pdf/2503.14395.pdf' target='_blank'>https://arxiv.org/pdf/2503.14395.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Wang, Ruirui Liu, Yu Lei, Michael J. Baine, Tian Liu, Yang Lei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14395">Weakly Supervised Spatial Implicit Neural Representation Learning for 3D MRI-Ultrasound Deformable Image Registration in HDR Prostate Brachytherapy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Purpose: Accurate 3D MRI-ultrasound (US) deformable registration is critical for real-time guidance in high-dose-rate (HDR) prostate brachytherapy. We present a weakly supervised spatial implicit neural representation (SINR) method to address modality differences and pelvic anatomy challenges.
  Methods: The framework uses sparse surface supervision from MRI/US segmentations instead of dense intensity matching. SINR models deformations as continuous spatial functions, with patient-specific surface priors guiding a stationary velocity field for biologically plausible deformations. Validation included 20 public Prostate-MRI-US-Biopsy cases and 10 institutional HDR cases, evaluated via Dice similarity coefficient (DSC), mean surface distance (MSD), and 95% Hausdorff distance (HD95).
  Results: The proposed method achieved robust registration. For the public dataset, prostate DSC was $0.93 \pm 0.05$, MSD $0.87 \pm 0.10$ mm, and HD95 $1.58 \pm 0.37$ mm. For the institutional dataset, prostate CTV achieved DSC $0.88 \pm 0.09$, MSD $1.21 \pm 0.38$ mm, and HD95 $2.09 \pm 1.48$ mm. Bladder and rectum performance was lower due to ultrasound's limited field of view. Visual assessments confirmed accurate alignment with minimal discrepancies.
  Conclusion: This study introduces a novel weakly supervised SINR-based approach for 3D MRI-US deformable registration. By leveraging sparse surface supervision and spatial priors, it achieves accurate, robust, and computationally efficient registration, enhancing real-time image guidance in HDR prostate brachytherapy and improving treatment precision.
<div id='section'>Paperid: <span id='pid'>1276, <a href='https://arxiv.org/pdf/2501.07020.pdf' target='_blank'>https://arxiv.org/pdf/2501.07020.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anh Thi-Hoang Nguyen, Dung Ha Nguyen, Kiet Van Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07020">ViSoLex: An Open-Source Repository for Vietnamese Social Media Lexical Normalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>ViSoLex is an open-source system designed to address the unique challenges of lexical normalization for Vietnamese social media text. The platform provides two core services: Non-Standard Word (NSW) Lookup and Lexical Normalization, enabling users to retrieve standard forms of informal language and standardize text containing NSWs. ViSoLex's architecture integrates pre-trained language models and weakly supervised learning techniques to ensure accurate and efficient normalization, overcoming the scarcity of labeled data in Vietnamese. This paper details the system's design, functionality, and its applications for researchers and non-technical users. Additionally, ViSoLex offers a flexible, customizable framework that can be adapted to various datasets and research requirements. By publishing the source code, ViSoLex aims to contribute to the development of more robust Vietnamese natural language processing tools and encourage further research in lexical normalization. Future directions include expanding the system's capabilities for additional languages and improving the handling of more complex non-standard linguistic patterns.
<div id='section'>Paperid: <span id='pid'>1277, <a href='https://arxiv.org/pdf/2501.05933.pdf' target='_blank'>https://arxiv.org/pdf/2501.05933.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Olivier Morelle, Justus Bisten, Maximilian W. M. Wintergerst, Robert P. Finger, Thomas Schultz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.05933">Weakly Supervised Segmentation of Hyper-Reflective Foci with Compact Convolutional Transformers and SAM2</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised segmentation has the potential to greatly reduce the annotation effort for training segmentation models for small structures such as hyper-reflective foci (HRF) in optical coherence tomography (OCT). However, most weakly supervised methods either involve a strong downsampling of input images, or only achieve localization at a coarse resolution, both of which are unsatisfactory for small structures. We propose a novel framework that increases the spatial resolution of a traditional attention-based Multiple Instance Learning (MIL) approach by using Layer-wise Relevance Propagation (LRP) to prompt the Segment Anything Model (SAM~2), and increases recall with iterative inference. Moreover, we demonstrate that replacing MIL with a Compact Convolutional Transformer (CCT), which adds a positional encoding, and permits an exchange of information between different regions of the OCT image, leads to a further and substantial increase in segmentation accuracy.
<div id='section'>Paperid: <span id='pid'>1278, <a href='https://arxiv.org/pdf/2412.12829.pdf' target='_blank'>https://arxiv.org/pdf/2412.12829.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elena Bueno-Benito, Mariella Dimiccoli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12829">2by2: Weakly-Supervised Learning for Global Action Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a simple yet effective approach for the poorly investigated task of global action segmentation, aiming at grouping frames capturing the same action across videos of different activities. Unlike the case of videos depicting all the same activity, the temporal order of actions is not roughly shared among all videos, making the task even more challenging. We propose to use activity labels to learn, in a weakly-supervised fashion, action representations suitable for global action segmentation. For this purpose, we introduce a triadic learning approach for video pairs, to ensure intra-video action discrimination, as well as inter-video and inter-activity action association. For the backbone architecture, we use a Siamese network based on sparse transformers that takes as input video pairs and determine whether they belong to the same activity. The proposed approach is validated on two challenging benchmark datasets: Breakfast and YouTube Instructions, outperforming state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1279, <a href='https://arxiv.org/pdf/2412.07384.pdf' target='_blank'>https://arxiv.org/pdf/2412.07384.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Florin Condrea, Saikiran Rapaka, Marius Leordeanu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07384">Label up: Learning Pulmonary Embolism Segmentation from Image Level Annotation through Model Explainability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pulmonary Embolisms (PE) are a leading cause of cardiovascular death. Computed tomographic pulmonary angiography (CTPA) stands as the gold standard for diagnosing pulmonary embolisms (PE) and there has been a lot of interest in developing AI-based models for assisting in PE diagnosis. Performance of these algorithms has been hindered by the scarcity of annotated data, especially those with fine-grained delineation of the thromboembolic burden. In this paper we attempt to address this issue by introducing a weakly supervised learning pipeline, that leverages model explainability to generate fine-grained (pixel level) masks for embolisms starting from more coarse-grained (binary, image level) PE annotations. Furthermore, we show that training models using the automatically generated pixel annotations yields good PE localization performance. We demonstrate the effectiveness of our pipeline on the large-scale, multi-center RSPECT augmented dataset for PE detection and localization.
<div id='section'>Paperid: <span id='pid'>1280, <a href='https://arxiv.org/pdf/2412.02250.pdf' target='_blank'>https://arxiv.org/pdf/2412.02250.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Javier UreÃ±a Santiago, Thomas StrÃ¶hle, Antonio RodrÃ­guez-SÃ¡nchez, Ruth Breu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02250">Vision Transformers for Weakly-Supervised Microorganism Enumeration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Microorganism enumeration is an essential task in many applications, such as assessing contamination levels or ensuring health standards when evaluating surface cleanliness. However, it's traditionally performed by human-supervised methods that often require manual counting, making it tedious and time-consuming. Previous research suggests automating this task using computer vision and machine learning methods, primarily through instance segmentation or density estimation techniques. This study conducts a comparative analysis of vision transformers (ViTs) for weakly-supervised counting in microorganism enumeration, contrasting them with traditional architectures such as ResNet and investigating ViT-based models such as TransCrowd. We trained different versions of ViTs as the architectural backbone for feature extraction using four microbiology datasets to determine potential new approaches for total microorganism enumeration in images. Results indicate that while ResNets perform better overall, ViTs performance demonstrates competent results across all datasets, opening up promising lines of research in microorganism enumeration. This comparative study contributes to the field of microbial image analysis by presenting innovative approaches to the recurring challenge of microorganism enumeration and by highlighting the capabilities of ViTs in the task of regression counting.
<div id='section'>Paperid: <span id='pid'>1281, <a href='https://arxiv.org/pdf/2412.00426.pdf' target='_blank'>https://arxiv.org/pdf/2412.00426.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ayoub Hammal, Benno Uthayasooriyar, Caio Corro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.00426">Few-Shot Domain Adaptation for Named-Entity Recognition via Joint Constrained k-Means and Subspace Selection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Named-entity recognition (NER) is a task that typically requires large annotated datasets, which limits its applicability across domains with varying entity definitions. This paper addresses few-shot NER, aiming to transfer knowledge to new domains with minimal supervision. Unlike previous approaches that rely solely on limited annotated data, we propose a weakly supervised algorithm that combines small labeled datasets with large amounts of unlabeled data. Our method extends the k-means algorithm with label supervision, cluster size constraints and domain-specific discriminative subspace selection. This unified framework achieves state-of-the-art results in few-shot NER on several English datasets.
<div id='section'>Paperid: <span id='pid'>1282, <a href='https://arxiv.org/pdf/2410.09999.pdf' target='_blank'>https://arxiv.org/pdf/2410.09999.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sandeep Sricharan Mukku, Abinesh Kanagarajan, Pushpendu Ghosh, Chetan Aggarwal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.09999">Leveraging Customer Feedback for Multi-modal Insight Extraction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Businesses can benefit from customer feedback in different modalities, such as text and images, to enhance their products and services. However, it is difficult to extract actionable and relevant pairs of text segments and images from customer feedback in a single pass. In this paper, we propose a novel multi-modal method that fuses image and text information in a latent space and decodes it to extract the relevant feedback segments using an image-text grounded text decoder. We also introduce a weakly-supervised data generation technique that produces training data for this task. We evaluate our model on unseen data and demonstrate that it can effectively mine actionable insights from multi-modal customer feedback, outperforming the existing baselines by $14$ points in F1 score.
<div id='section'>Paperid: <span id='pid'>1283, <a href='https://arxiv.org/pdf/2410.04789.pdf' target='_blank'>https://arxiv.org/pdf/2410.04789.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>MÃ³nica Apellaniz Portos, Roberto Labadie-Tamayo, Claudius Stemmler, Erwin Feyersinger, Andreas Babic, Franziska Bruckner, VrÃ¤Ã¤th Ãhner, Matthias Zeppelzauer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.04789">Analysis of Hybrid Compositions in Animation Film with Weakly Supervised Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present an approach for the analysis of hybrid visual compositions in animation in the domain of ephemeral film. We combine ideas from semi-supervised and weakly supervised learning to train a model that can segment hybrid compositions without requiring pre-labeled segmentation masks. We evaluate our approach on a set of ephemeral films from 13 film archives. Results demonstrate that the proposed learning strategy yields a performance close to a fully supervised baseline. On a qualitative level the performed analysis provides interesting insights on hybrid compositions in animation film.
<div id='section'>Paperid: <span id='pid'>1284, <a href='https://arxiv.org/pdf/2409.20467.pdf' target='_blank'>https://arxiv.org/pdf/2409.20467.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dung Ha Nguyen, Anh Thi Hoang Nguyen, Kiet Van Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.20467">A Weakly Supervised Data Labeling Framework for Machine Lexical Normalization in Vietnamese Social Media</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study introduces an innovative automatic labeling framework to address the challenges of lexical normalization in social media texts for low-resource languages like Vietnamese. Social media data is rich and diverse, but the evolving and varied language used in these contexts makes manual labeling labor-intensive and expensive. To tackle these issues, we propose a framework that integrates semi-supervised learning with weak supervision techniques. This approach enhances the quality of training dataset and expands its size while minimizing manual labeling efforts. Our framework automatically labels raw data, converting non-standard vocabulary into standardized forms, thereby improving the accuracy and consistency of the training data. Experimental results demonstrate the effectiveness of our weak supervision framework in normalizing Vietnamese text, especially when utilizing Pre-trained Language Models. The proposed framework achieves an impressive F1-score of 82.72% and maintains vocabulary integrity with an accuracy of up to 99.22%. Additionally, it effectively handles undiacritized text under various conditions. This framework significantly enhances natural language normalization quality and improves the accuracy of various NLP tasks, leading to an average accuracy increase of 1-3%.
<div id='section'>Paperid: <span id='pid'>1285, <a href='https://arxiv.org/pdf/2409.15491.pdf' target='_blank'>https://arxiv.org/pdf/2409.15491.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyu Su, Yongxin Guo, Robert Wesolowski, Gary Tozbikian, Nathaniel S. O'Connell, M. Khalid Khan Niazi, Metin N. Gurcan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.15491">Computational Pathology for Accurate Prediction of Breast Cancer Recurrence: Development and Validation of a Deep Learning-based Tool</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate recurrence risk stratification is crucial for optimizing treatment plans for breast cancer patients. Current prognostic tools like Oncotype DX (ODX) offer valuable genomic insights for HR+/HER2- patients but are limited by cost and accessibility, particularly in underserved populations. In this study, we present Deep-BCR-Auto, a deep learning-based computational pathology approach that predicts breast cancer recurrence risk from routine H&E-stained whole slide images (WSIs). Our methodology was validated on two independent cohorts: the TCGA-BRCA dataset and an in-house dataset from The Ohio State University (OSU). Deep-BCR-Auto demonstrated robust performance in stratifying patients into low- and high-recurrence risk categories. On the TCGA-BRCA dataset, the model achieved an area under the receiver operating characteristic curve (AUROC) of 0.827, significantly outperforming existing weakly supervised models (p=0.041). In the independent OSU dataset, Deep-BCR-Auto maintained strong generalizability, achieving an AUROC of 0.832, along with 82.0% accuracy, 85.0% specificity, and 67.7% sensitivity. These findings highlight the potential of computational pathology as a cost-effective alternative for recurrence risk assessment, broadening access to personalized treatment strategies. This study underscores the clinical utility of integrating deep learning-based computational pathology into routine pathological assessment for breast cancer prognosis across diverse clinical settings.
<div id='section'>Paperid: <span id='pid'>1286, <a href='https://arxiv.org/pdf/2409.03236.pdf' target='_blank'>https://arxiv.org/pdf/2409.03236.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenglizhao Chen, Xinyu Liu, Mengke Song, Luming Li, Xu Yu, Shanchen Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.03236">Unveiling Context-Related Anomalies: Knowledge Graph Empowered Decoupling of Scene and Action for Human-Related Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detecting anomalies in human-related videos is crucial for surveillance applications. Current methods primarily include appearance-based and action-based techniques. Appearance-based methods rely on low-level visual features such as color, texture, and shape. They learn a large number of pixel patterns and features related to known scenes during training, making them effective in detecting anomalies within these familiar contexts. However, when encountering new or significantly changed scenes, i.e., unknown scenes, they often fail because existing SOTA methods do not effectively capture the relationship between actions and their surrounding scenes, resulting in low generalization. In contrast, action-based methods focus on detecting anomalies in human actions but are usually less informative because they tend to overlook the relationship between actions and their scenes, leading to incorrect detection. For instance, the normal event of running on the beach and the abnormal event of running on the street might both be considered normal due to the lack of scene information. In short, current methods struggle to integrate low-level visual and high-level action features, leading to poor anomaly detection in varied and complex scenes. To address this challenge, we propose a novel decoupling-based architecture for human-related video anomaly detection (DecoAD). DecoAD significantly improves the integration of visual and action features through the decoupling and interweaving of scenes and actions, thereby enabling a more intuitive and accurate understanding of complex behaviors and scenes. DecoAD supports fully supervised, weakly supervised, and unsupervised settings.
<div id='section'>Paperid: <span id='pid'>1287, <a href='https://arxiv.org/pdf/2409.01676.pdf' target='_blank'>https://arxiv.org/pdf/2409.01676.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenyang Hu, Gaetan Frusque, Tianyang Wang, Fulei Chu, Olga Fink
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01676">Classifier-Free Diffusion-Based Weakly-Supervised Approach for Health Indicator Derivation in Rotating Machines: Advancing Early Fault Detection and Condition Monitoring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deriving health indicators of rotating machines is crucial for their maintenance. However, this process is challenging for the prevalent adopted intelligent methods since they may take the whole data distributions, not only introducing noise interference but also lacking the explainability. To address these issues, we propose a diffusion-based weakly-supervised approach for deriving health indicators of rotating machines, enabling early fault detection and continuous monitoring of condition evolution. This approach relies on a classifier-free diffusion model trained using healthy samples and a few anomalies. This model generates healthy samples. and by comparing the differences between the original samples and the generated ones in the envelope spectrum, we construct an anomaly map that clearly identifies faults. Health indicators are then derived, which can explain the fault types and mitigate noise interference. Comparative studies on two cases demonstrate that the proposed method offers superior health monitoring effectiveness and robustness compared to baseline models.
<div id='section'>Paperid: <span id='pid'>1288, <a href='https://arxiv.org/pdf/2408.14498.pdf' target='_blank'>https://arxiv.org/pdf/2408.14498.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhijin Dong, Hongzhi Liu, Boyuan Ren, Weimin Xiong, Zhonghai Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.14498">Multi-Normal Prototypes Learning for Weakly Supervised Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anomaly detection is a crucial task in various domains. Most of the existing methods assume the normal sample data clusters around a single central prototype while the real data may consist of multiple categories or subgroups. In addition, existing methods always assume all unlabeled samples are normal while some of them are inevitably being anomalies. To address these issues, we propose a novel anomaly detection framework that can efficiently work with limited labeled anomalies. Specifically, we assume the normal sample data may consist of multiple subgroups, and propose to learn multi-normal prototypes to represent them with deep embedding clustering and contrastive learning. Additionally, we propose a method to estimate the likelihood of each unlabeled sample being normal during model training, which can help to learn more efficient data encoder and normal prototypes for anomaly detection. Extensive experiments on various datasets demonstrate the superior performance of our method compared to state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1289, <a href='https://arxiv.org/pdf/2408.09952.pdf' target='_blank'>https://arxiv.org/pdf/2408.09952.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ik Jun Moon, Junho Moon, Ikbeom Jang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09952">Weakly Supervised Pretraining and Multi-Annotator Supervised Finetuning for Facial Wrinkle Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>1. Research question: With the growing interest in skin diseases and skin aesthetics, the ability to predict facial wrinkles is becoming increasingly important. This study aims to evaluate whether a computational model, convolutional neural networks (CNN), can be trained for automated facial wrinkle segmentation. 2. Findings: Our study presents an effective technique for integrating data from multiple annotators and illustrates that transfer learning can enhance performance, resulting in dependable segmentation of facial wrinkles. 3. Meaning: This approach automates intricate and time-consuming tasks of wrinkle analysis with a deep learning framework. It could be used to facilitate skin treatments and diagnostics.
<div id='section'>Paperid: <span id='pid'>1290, <a href='https://arxiv.org/pdf/2408.05191.pdf' target='_blank'>https://arxiv.org/pdf/2408.05191.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yashika Jain, Ali Dabouei, Min Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.05191">Cross-Domain Learning for Video Anomaly Detection with Limited Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Anomaly Detection (VAD) automates the identification of unusual events, such as security threats in surveillance videos. In real-world applications, VAD models must effectively operate in cross-domain settings, identifying rare anomalies and scenarios not well-represented in the training data. However, existing cross-domain VAD methods focus on unsupervised learning, resulting in performance that falls short of real-world expectations. Since acquiring weak supervision, i.e., video-level labels, for the source domain is cost-effective, we conjecture that combining it with external unlabeled data has notable potential to enhance cross-domain performance. To this end, we introduce a novel weakly-supervised framework for Cross-Domain Learning (CDL) in VAD that incorporates external data during training by estimating its prediction bias and adaptively minimizing that using the predicted uncertainty. We demonstrate the effectiveness of the proposed CDL framework through comprehensive experiments conducted in various configurations on two large-scale VAD datasets: UCF-Crime and XD-Violence. Our method significantly surpasses the state-of-the-art works in cross-domain evaluations, achieving an average absolute improvement of 19.6% on UCF-Crime and 12.87% on XD-Violence.
<div id='section'>Paperid: <span id='pid'>1291, <a href='https://arxiv.org/pdf/2408.04482.pdf' target='_blank'>https://arxiv.org/pdf/2408.04482.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sriram Mandalika, Athira Nambiar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.04482">SegXAL: Explainable Active Learning for Semantic Segmentation in Driving Scene Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most of the sophisticated AI models utilize huge amounts of annotated data and heavy training to achieve high-end performance. However, there are certain challenges that hinder the deployment of AI models "in-the-wild" scenarios, i.e., inefficient use of unlabeled data, lack of incorporation of human expertise, and lack of interpretation of the results. To mitigate these challenges, we propose a novel Explainable Active Learning (XAL) model, XAL-based semantic segmentation model "SegXAL", that can (i) effectively utilize the unlabeled data, (ii) facilitate the "Human-in-the-loop" paradigm, and (iii) augment the model decisions in an interpretable way. In particular, we investigate the application of the SegXAL model for semantic segmentation in driving scene scenarios. The SegXAL model proposes the image regions that require labeling assistance from Oracle by dint of explainable AI (XAI) and uncertainty measures in a weakly-supervised manner. Specifically, we propose a novel Proximity-aware Explainable-AI (PAE) module and Entropy-based Uncertainty (EBU) module to get an Explainable Error Mask, which enables the machine teachers/human experts to provide intuitive reasoning behind the results and to solicit feedback to the AI system via an active learning strategy. Such a mechanism bridges the semantic gap between man and machine through collaborative intelligence, where humans and AI actively enhance each other's complementary strengths. A novel high-confidence sample selection technique based on the DICE similarity coefficient is also presented within the SegXAL framework. Extensive quantitative and qualitative analyses are carried out in the benchmarking Cityscape dataset. Results show the outperformance of our proposed SegXAL against other state-of-the-art models.
<div id='section'>Paperid: <span id='pid'>1292, <a href='https://arxiv.org/pdf/2407.10274.pdf' target='_blank'>https://arxiv.org/pdf/2407.10274.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinsheng He, Xingyu Li, Roger J. Zemp
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10274">Enhancing Weakly-Supervised Histopathology Image Segmentation with Knowledge Distillation on MIL-Based Pseudo-Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Segmenting tumors in histological images is vital for cancer diagnosis. While fully supervised models excel with pixel-level annotations, creating such annotations is labor-intensive and costly. Accurate histopathology image segmentation under weakly-supervised conditions with coarse-grained image labels is still a challenging problem. Although multiple instance learning (MIL) has shown promise in segmentation tasks, surprisingly, no previous pseudo-supervision methods have used MIL-based outputs as pseudo-masks for training. We suspect this stems from concerns over noises in MIL results affecting pseudo supervision quality. To explore the potential of leveraging MIL-based segmentation for pseudo supervision, we propose a novel distillation framework for histopathology image segmentation. This framework introduces a iterative fusion-knowledge distillation strategy, enabling the student model to learn directly from the teacher's comprehensive outcomes. Through dynamic role reversal between the fixed teacher and learnable student models and the incorporation of weighted cross-entropy loss for model optimization, our approach prevents performance deterioration and noise amplification during knowledge distillation. Experimental results on public histopathology datasets, Camelyon16 and Digestpath2019, demonstrate that our approach not only complements various MIL-based segmentation methods but also significantly enhances their performance. Additionally, our method achieves new SOTA in the field.
<div id='section'>Paperid: <span id='pid'>1293, <a href='https://arxiv.org/pdf/2407.10000.pdf' target='_blank'>https://arxiv.org/pdf/2407.10000.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaifu Wang, Efthymia Tsamoura, Dan Roth
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10000">On Characterizing and Mitigating Imbalances in Multi-Instance Partial Label Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>*Multi-Instance Partial Label Learning* (MI-PLL) is a weakly-supervised learning setting encompassing *partial label learning*, *latent structural learning*, and *neurosymbolic learning*. Unlike supervised learning, in MI-PLL, the inputs to the classifiers at training-time are tuples of instances $\mathbf{x}$. At the same time, the supervision signal is generated by a function $Ï$ over the (hidden) gold labels of $\mathbf{x}$. In this work, we make multiple contributions towards addressing a problem that hasn't been studied so far in the context of MI-PLL: that of characterizing and mitigating *learning imbalances*, i.e., major differences in the errors occurring when classifying instances of different classes (aka *class-specific risks*). In terms of theory, we derive class-specific risk bounds for MI-PLL, while making minimal assumptions. Our theory reveals a unique phenomenon: that $Ï$ can greatly impact learning imbalances. This result is in sharp contrast with previous research on supervised and weakly-supervised learning, which only studies learning imbalances under the prism of data imbalances. On the practical side, we introduce a technique for estimating the marginal of the hidden labels using only MI-PLL data. Then, we introduce algorithms that mitigate imbalances at training- and testing-time, by treating the marginal of the hidden labels as a constraint. We demonstrate the effectiveness of our techniques using strong baselines from neurosymbolic and long-tail learning, suggesting performance improvements of up to 14\%.
<div id='section'>Paperid: <span id='pid'>1294, <a href='https://arxiv.org/pdf/2406.08396.pdf' target='_blank'>https://arxiv.org/pdf/2406.08396.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yoshiaki Bando, Tomohiko Nakamura, Shinji Watanabe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.08396">Neural Blind Source Separation and Diarization for Distant Speech Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a neural method for distant speech recognition (DSR) that jointly separates and diarizes speech mixtures without supervision by isolated signals. A standard separation method for multi-talker DSR is a statistical multichannel method called guided source separation (GSS). While GSS does not require signal-level supervision, it relies on speaker diarization results to handle unknown numbers of active speakers. To overcome this limitation, we introduce and train a neural inference model in a weakly-supervised manner, employing the objective function of a statistical separation method. This training requires only multichannel mixtures and their temporal annotations of speaker activities. In contrast to GSS, the trained model can jointly separate and diarize speech mixtures without any auxiliary information. The experiments with the AMI corpus show that our method outperforms GSS with oracle diarization results regarding word error rates. The code is available online.
<div id='section'>Paperid: <span id='pid'>1295, <a href='https://arxiv.org/pdf/2406.06723.pdf' target='_blank'>https://arxiv.org/pdf/2406.06723.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Enshuo Hsu, Kirk Roberts
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.06723">Leveraging Large Language Models for Knowledge-free Weak Supervision in Clinical Natural Language Processing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The performance of deep learning-based natural language processing systems is based on large amounts of labeled training data which, in the clinical domain, are not easily available or affordable. Weak supervision and in-context learning offer partial solutions to this issue, particularly using large language models (LLMs), but their performance still trails traditional supervised methods with moderate amounts of gold-standard data. In particular, inferencing with LLMs is computationally heavy. We propose an approach leveraging fine-tuning LLMs and weak supervision with virtually no domain knowledge that still achieves consistently dominant performance. Using a prompt-based approach, the LLM is used to generate weakly-labeled data for training a downstream BERT model. The weakly supervised model is then further fine-tuned on small amounts of gold standard data. We evaluate this approach using Llama2 on three different n2c2 datasets. With no more than 10 gold standard notes, our final BERT models weakly supervised by fine-tuned Llama2-13B consistently outperformed out-of-the-box PubMedBERT by 4.7% to 47.9% in F1 scores. With only 50 gold standard notes, our models achieved close performance to fully fine-tuned systems.
<div id='section'>Paperid: <span id='pid'>1296, <a href='https://arxiv.org/pdf/2406.02831.pdf' target='_blank'>https://arxiv.org/pdf/2406.02831.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jash Dalvi, Ali Dabouei, Gunjan Dhanuka, Min Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.02831">Distilling Aggregated Knowledge for Weakly-Supervised Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video anomaly detection aims to develop automated models capable of identifying abnormal events in surveillance videos. The benchmark setup for this task is extremely challenging due to: i) the limited size of the training sets, ii) weak supervision provided in terms of video-level labels, and iii) intrinsic class imbalance induced by the scarcity of abnormal events. In this work, we show that distilling knowledge from aggregated representations of multiple backbones into a single-backbone Student model achieves state-of-the-art performance. In particular, we develop a bi-level distillation approach along with a novel disentangled cross-attention-based feature aggregation network. Our proposed approach, DAKD (Distilling Aggregated Knowledge with Disentangled Attention), demonstrates superior performance compared to existing methods across multiple benchmark datasets. Notably, we achieve significant improvements of 1.36%, 0.78%, and 7.02% on the UCF-Crime, ShanghaiTech, and XD-Violence datasets, respectively.
<div id='section'>Paperid: <span id='pid'>1297, <a href='https://arxiv.org/pdf/2406.00164.pdf' target='_blank'>https://arxiv.org/pdf/2406.00164.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huixin Zhan, Zijun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00164">DYNA: Disease-Specific Language Model for Variant Pathogenicity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Clinical variant classification of pathogenic versus benign genetic variants remains a challenge in clinical genetics. Recently, the proposition of genomic foundation models has improved the generic variant effect prediction (VEP) accuracy via weakly-supervised or unsupervised training. However, these VEPs are not disease-specific, limiting their adaptation at the point of care. To address this problem, we propose DYNA: Disease-specificity fine-tuning via a Siamese neural network broadly applicable to all genomic foundation models for more effective variant effect predictions in disease-specific contexts. We evaluate DYNA in two distinct disease-relevant tasks. For coding VEPs, we focus on various cardiovascular diseases, where gene-disease relationships of loss-of-function vs. gain-of-function dictate disease-specific VEP. For non-coding VEPs, we apply DYNA to an essential post-transcriptional regulatory axis of RNA splicing, the most common non-coding pathogenic mechanism in established clinical VEP guidelines. In both cases, DYNA fine-tunes various pre-trained genomic foundation models on small, rare variant sets. The DYNA fine-tuned models show superior performance in the held-out rare variant testing set and are further replicated in large, clinically-relevant variant annotations in ClinVAR. Thus, DYNA offers a potent disease-specific variant effect prediction method, excelling in intra-gene generalization and generalization to unseen genetic variants, making it particularly valuable for disease associations and clinical applicability.
<div id='section'>Paperid: <span id='pid'>1298, <a href='https://arxiv.org/pdf/2405.10456.pdf' target='_blank'>https://arxiv.org/pdf/2405.10456.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammed Patel, Xinwei Chen, Linlin Xu, Yuhao Chen, K Andrea Scott, David A. Clausi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.10456">Region-level labels in ice charts can produce pixel-level segmentation for Sea Ice types</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fully supervised deep learning approaches have demonstrated impressive accuracy in sea ice classification, but their dependence on high-resolution labels presents a significant challenge due to the difficulty of obtaining such data. In response, our weakly supervised learning method provides a compelling alternative by utilizing lower-resolution regional labels from expert-annotated ice charts. This approach achieves exceptional pixel-level classification performance by introducing regional loss representations during training to measure the disparity between predicted and ice chart-derived sea ice type distributions. Leveraging the AI4Arctic Sea Ice Challenge Dataset, our method outperforms the fully supervised U-Net benchmark, the top solution of the AutoIce challenge, in both mapping resolution and class-wise accuracy, marking a significant advancement in automated operational sea ice mapping.
<div id='section'>Paperid: <span id='pid'>1299, <a href='https://arxiv.org/pdf/2405.04913.pdf' target='_blank'>https://arxiv.org/pdf/2405.04913.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Lai, Chi-Man Vong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04913">Weakly-supervised Semantic Segmentation via Dual-stream Contrastive Learning of Cross-image Contextual Information</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation (WSSS) aims at learning a semantic segmentation model with only image-level tags. Despite intensive research on deep learning approaches over a decade, there is still a significant performance gap between WSSS and full semantic segmentation. Most current WSSS methods always focus on a limited single image (pixel-wise) information while ignoring the valuable inter-image (semantic-wise) information. From this perspective, a novel end-to-end WSSS framework called DSCNet is developed along with two innovations: i) pixel-wise group contrast and semantic-wise graph contrast are proposed and introduced into the WSSS framework; ii) a novel dual-stream contrastive learning (DSCL) mechanism is designed to jointly handle pixel-wise and semantic-wise context information for better WSSS performance. Specifically, the pixel-wise group contrast learning (PGCL) and semantic-wise graph contrast learning (SGCL) tasks form a more comprehensive solution. Extensive experiments on PASCAL VOC and MS COCO benchmarks verify the superiority of DSCNet over SOTA approaches and baseline models.
<div id='section'>Paperid: <span id='pid'>1300, <a href='https://arxiv.org/pdf/2405.00260.pdf' target='_blank'>https://arxiv.org/pdf/2405.00260.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yamato Okamoto, Youngmin Baek, Geewook Kim, Ryota Nakao, DongHyun Kim, Moon Bin Yim, Seunghyun Park, Bado Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.00260">CREPE: Coordinate-Aware End-to-End Document Parser</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study, we formulate an OCR-free sequence generation model for visual document understanding (VDU). Our model not only parses text from document images but also extracts the spatial coordinates of the text based on the multi-head architecture. Named as Coordinate-aware End-to-end Document Parser (CREPE), our method uniquely integrates these capabilities by introducing a special token for OCR text, and token-triggered coordinate decoding. We also proposed a weakly-supervised framework for cost-efficient training, requiring only parsing annotations without high-cost coordinate annotations. Our experimental evaluations demonstrate CREPE's state-of-the-art performances on document parsing tasks. Beyond that, CREPE's adaptability is further highlighted by its successful usage in other document understanding tasks such as layout analysis, document visual question answering, and so one. CREPE's abilities including OCR and semantic parsing not only mitigate error propagation issues in existing OCR-dependent methods, it also significantly enhance the functionality of sequence generation models, ushering in a new era for document understanding studies.
<div id='section'>Paperid: <span id='pid'>1301, <a href='https://arxiv.org/pdf/2403.02932.pdf' target='_blank'>https://arxiv.org/pdf/2403.02932.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Miaomiao Li, Jiaqi Zhu, Yang Wang, Yi Yang, Yilin Li, Hongan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.02932">RulePrompt: Weakly Supervised Text Classification with Prompting PLMs and Self-Iterative Logical Rules</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised text classification (WSTC), also called zero-shot or dataless text classification, has attracted increasing attention due to its applicability in classifying a mass of texts within the dynamic and open Web environment, since it requires only a limited set of seed words (label names) for each category instead of labeled data. With the help of recently popular prompting Pre-trained Language Models (PLMs), many studies leveraged manually crafted and/or automatically identified verbalizers to estimate the likelihood of categories, but they failed to differentiate the effects of these category-indicative words, let alone capture their correlations and realize adaptive adjustments according to the unlabeled corpus. In this paper, in order to let the PLM effectively understand each category, we at first propose a novel form of rule-based knowledge using logical expressions to characterize the meanings of categories. Then, we develop a prompting PLM-based approach named RulePrompt for the WSTC task, consisting of a rule mining module and a rule-enhanced pseudo label generation module, plus a self-supervised fine-tuning module to make the PLM align with this task. Within this framework, the inaccurate pseudo labels assigned to texts and the imprecise logical rules associated with categories mutually enhance each other in an alternative manner. That establishes a self-iterative closed loop of knowledge (rule) acquisition and utilization, with seed words serving as the starting point. Extensive experiments validate the effectiveness and robustness of our approach, which markedly outperforms state-of-the-art weakly supervised methods. What is more, our approach yields interpretable category rules, proving its advantage in disambiguating easily-confused categories.
<div id='section'>Paperid: <span id='pid'>1302, <a href='https://arxiv.org/pdf/2402.10887.pdf' target='_blank'>https://arxiv.org/pdf/2402.10887.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyang Wang, Chao Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.10887">Weak-Mamba-UNet: Visual Mamba Makes CNN and ViT Work Better for Scribble-based Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical image segmentation is increasingly reliant on deep learning techniques, yet the promising performance often come with high annotation costs. This paper introduces Weak-Mamba-UNet, an innovative weakly-supervised learning (WSL) framework that leverages the capabilities of Convolutional Neural Network (CNN), Vision Transformer (ViT), and the cutting-edge Visual Mamba (VMamba) architecture for medical image segmentation, especially when dealing with scribble-based annotations. The proposed WSL strategy incorporates three distinct architecture but same symmetrical encoder-decoder networks: a CNN-based UNet for detailed local feature extraction, a Swin Transformer-based SwinUNet for comprehensive global context understanding, and a VMamba-based Mamba-UNet for efficient long-range dependency modeling. The key concept of this framework is a collaborative and cross-supervisory mechanism that employs pseudo labels to facilitate iterative learning and refinement across the networks. The effectiveness of Weak-Mamba-UNet is validated on a publicly available MRI cardiac segmentation dataset with processed scribble annotations, where it surpasses the performance of a similar WSL framework utilizing only UNet or SwinUNet. This highlights its potential in scenarios with sparse or imprecise annotations. The source code is made publicly accessible.
<div id='section'>Paperid: <span id='pid'>1303, <a href='https://arxiv.org/pdf/2402.05373.pdf' target='_blank'>https://arxiv.org/pdf/2402.05373.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingxin Liu, Yunzan Liu, Pengbo Xu, Jiquan Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.05373">Unleashing the Infinity Power of Geometry: A Novel Geometry-Aware Transformer (GOAT) for Whole Slide Histopathology Image Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The histopathology analysis is of great significance for the diagnosis and prognosis of cancers, however, it has great challenges due to the enormous heterogeneity of gigapixel whole slide images (WSIs) and the intricate representation of pathological features. However, recent methods have not adequately exploited geometrical representation in WSIs which is significant in disease diagnosis. Therefore, we proposed a novel weakly-supervised framework, Geometry-Aware Transformer (GOAT), in which we urge the model to pay attention to the geometric characteristics within the tumor microenvironment which often serve as potent indicators. In addition, a context-aware attention mechanism is designed to extract and enhance the morphological features within WSIs.
<div id='section'>Paperid: <span id='pid'>1304, <a href='https://arxiv.org/pdf/2402.04835.pdf' target='_blank'>https://arxiv.org/pdf/2402.04835.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Darshana Saravanan, Naresh Manwani, Vineet Gandhi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04835">Pseudo-labelling meets Label Smoothing for Noisy Partial Label Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We motivate weakly supervised learning as an effective learning paradigm for problems where curating perfectly annotated datasets is expensive and may require domain expertise such as fine-grained classification. We focus on Partial Label Learning (PLL), a weakly-supervised learning paradigm where each training instance is paired with a set of candidate labels (partial label), one of which is the true label. Noisy PLL (NPLL) relaxes this constraint by allowing some partial labels to not contain the true label, enhancing the practicality of the problem. Our work centres on NPLL and presents a framework that initially assigns pseudo-labels to images by exploiting the noisy partial labels through a weighted nearest neighbour algorithm. These pseudo-label and image pairs are then used to train a deep neural network classifier with label smoothing. The classifier's features and predictions are subsequently employed to refine and enhance the accuracy of pseudo-labels. We perform thorough experiments on seven datasets and compare against nine NPLL and PLL methods. We achieve state-of-the-art results in all studied settings from the prior literature, obtaining substantial gains in the simulated fine-grained benchmarks. Further, we show the promising generalisation capability of our framework in realistic, fine-grained, crowd-sourced datasets.
<div id='section'>Paperid: <span id='pid'>1305, <a href='https://arxiv.org/pdf/2401.13998.pdf' target='_blank'>https://arxiv.org/pdf/2401.13998.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haitao Gan, Lingchao Fu, Ran Zhou, Weiyan Gan, Furong Wang, Xiaoyan Wu, Zhi Yang, Zhongwei Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.13998">WAL-Net: Weakly supervised auxiliary task learning network for carotid plaques classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The classification of carotid artery ultrasound images is a crucial means for diagnosing carotid plaques, holding significant clinical relevance for predicting the risk of stroke. Recent research suggests that utilizing plaque segmentation as an auxiliary task for classification can enhance performance by leveraging the correlation between segmentation and classification tasks. However, this approach relies on obtaining a substantial amount of challenging-to-acquire segmentation annotations. This paper proposes a novel weakly supervised auxiliary task learning network model (WAL-Net) to explore the interdependence between carotid plaque classification and segmentation tasks. The plaque classification task is primary task, while the plaque segmentation task serves as an auxiliary task, providing valuable information to enhance the performance of the primary task. Weakly supervised learning is adopted in the auxiliary task to completely break away from the dependence on segmentation annotations. Experiments and evaluations are conducted on a dataset comprising 1270 carotid plaque ultrasound images from Wuhan University Zhongnan Hospital. Results indicate that the proposed method achieved an approximately 1.3% improvement in carotid plaque classification accuracy compared to the baseline network. Specifically, the accuracy of mixed-echoic plaques classification increased by approximately 3.3%, demonstrating the effectiveness of our approach.
<div id='section'>Paperid: <span id='pid'>1306, <a href='https://arxiv.org/pdf/2312.03361.pdf' target='_blank'>https://arxiv.org/pdf/2312.03361.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hamed Hematian Hemati, Arash Lagzian, Moein Salimi Sartakhti, Hamid Beigy, Ehsaneddin Asgari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.03361">KhabarChin: Automatic Detection of Important News in the Persian Language</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Being aware of important news is crucial for staying informed and making well-informed decisions efficiently. Natural Language Processing (NLP) approaches can significantly automate this process. This paper introduces the detection of important news, in a previously unexplored area, and presents a new benchmarking dataset (Khabarchin) for detecting important news in the Persian language. We define important news articles as those deemed significant for a considerable portion of society, capable of influencing their mindset or decision-making. The news articles are obtained from seven different prominent Persian news agencies, resulting in the annotation of 7,869 samples and the creation of the dataset. Two challenges of high disagreement and imbalance between classes were faced, and solutions were provided for them. We also propose several learning-based models, ranging from conventional machine learning to state-of-the-art transformer models, to tackle this task. Furthermore, we introduce the second task of important sentence detection in news articles, as they often come with a significant contextual length that makes it challenging for readers to identify important information. We identify these sentences in a weakly supervised manner.
<div id='section'>Paperid: <span id='pid'>1307, <a href='https://arxiv.org/pdf/2311.11659.pdf' target='_blank'>https://arxiv.org/pdf/2311.11659.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingxin Liu, Yunzan Liu, Hui Cui, Chunquan Li, Jiquan Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.11659">MGCT: Mutual-Guided Cross-Modality Transformer for Survival Outcome Prediction using Integrative Histopathology-Genomic Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapidly emerging field of deep learning-based computational pathology has shown promising results in utilizing whole slide images (WSIs) to objectively prognosticate cancer patients. However, most prognostic methods are currently limited to either histopathology or genomics alone, which inevitably reduces their potential to accurately predict patient prognosis. Whereas integrating WSIs and genomic features presents three main challenges: (1) the enormous heterogeneity of gigapixel WSIs which can reach sizes as large as 150,000x150,000 pixels; (2) the absence of a spatially corresponding relationship between histopathology images and genomic molecular data; and (3) the existing early, late, and intermediate multimodal feature fusion strategies struggle to capture the explicit interactions between WSIs and genomics. To ameliorate these issues, we propose the Mutual-Guided Cross-Modality Transformer (MGCT), a weakly-supervised, attention-based multimodal learning framework that can combine histology features and genomic features to model the genotype-phenotype interactions within the tumor microenvironment. To validate the effectiveness of MGCT, we conduct experiments using nearly 3,600 gigapixel WSIs across five different cancer types sourced from The Cancer Genome Atlas (TCGA). Extensive experimental results consistently emphasize that MGCT outperforms the state-of-the-art (SOTA) methods.
<div id='section'>Paperid: <span id='pid'>1308, <a href='https://arxiv.org/pdf/2311.07234.pdf' target='_blank'>https://arxiv.org/pdf/2311.07234.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Paula Ramirez, Alena Uus, Milou P. M. van Poppel, Irina Grigorescu, Johannes K. Steinweg, David F. A. Lloyd, Kuberan Pushparajah, Andrew P. King, Maria Deprez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.07234">Multi-task learning for joint weakly-supervised segmentation and aortic arch anomaly classification in fetal cardiac MRI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Congenital Heart Disease (CHD) is a group of cardiac malformations present already during fetal life, representing the prevailing category of birth defects globally. Our aim in this study is to aid 3D fetal vessel topology visualisation in aortic arch anomalies, a group which encompasses a range of conditions with significant anatomical heterogeneity. We present a multi-task framework for automated multi-class fetal vessel segmentation from 3D black blood T2w MRI and anomaly classification. Our training data consists of binary manual segmentation masks of the cardiac vessels' region in individual subjects and fully-labelled anomaly-specific population atlases. Our framework combines deep learning label propagation using VoxelMorph with 3D Attention U-Net segmentation and DenseNet121 anomaly classification. We target 11 cardiac vessels and three distinct aortic arch anomalies, including double aortic arch, right aortic arch, and suspected coarctation of the aorta. We incorporate an anomaly classifier into our segmentation pipeline, delivering a multi-task framework with the primary motivation of correcting topological inaccuracies of the segmentation. The hypothesis is that the multi-task approach will encourage the segmenter network to learn anomaly-specific features. As a secondary motivation, an automated diagnosis tool may have the potential to enhance diagnostic confidence in a decision support setting. Our results showcase that our proposed training strategy significantly outperforms label propagation and a network trained exclusively on propagated labels. Our classifier outperforms a classifier trained exclusively on T2w volume images, with an average balanced accuracy of 0.99 (0.01) after joint training. Adding a classifier improves the anatomical and topological accuracy of all correctly classified double aortic arch subjects.
<div id='section'>Paperid: <span id='pid'>1309, <a href='https://arxiv.org/pdf/2311.03537.pdf' target='_blank'>https://arxiv.org/pdf/2311.03537.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eva Breznik, Hoel Kervadec, Filip Malmberg, Joel Kullberg, HÃ¥kan AhlstrÃ¶m, Marleen de Bruijne, Robin Strand
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.03537">Leveraging point annotations in segmentation learning with boundary loss</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates the combination of intensity-based distance maps with boundary loss for point-supervised semantic segmentation. By design the boundary loss imposes a stronger penalty on the false positives the farther away from the object they occur. Hence it is intuitively inappropriate for weak supervision, where the ground truth label may be much smaller than the actual object and a certain amount of false positives (w.r.t. the weak ground truth) is actually desirable. Using intensity-aware distances instead may alleviate this drawback, allowing for a certain amount of false positives without a significant increase to the training loss. The motivation for applying the boundary loss directly under weak supervision lies in its great success for fully supervised segmentation tasks, but also in not requiring extra priors or outside information that is usually required -- in some form -- with existing weakly supervised methods in the literature. This formulation also remains potentially more attractive than existing CRF-based regularizers, due to its simplicity and computational efficiency. We perform experiments on two multi-class datasets; ACDC (heart segmentation) and POEM (whole-body abdominal organ segmentation). Preliminary results are encouraging and show that this supervision strategy has great potential. On ACDC it outperforms the CRF-loss based approach, and on POEM data it performs on par with it. The code for all our experiments is openly available.
<div id='section'>Paperid: <span id='pid'>1310, <a href='https://arxiv.org/pdf/2311.03429.pdf' target='_blank'>https://arxiv.org/pdf/2311.03429.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huixin Zhan, Zijun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.03429">ProPath: Disease-Specific Protein Language Model for Variant Pathogenicity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Clinical variant classification of pathogenic versus benign genetic variants remains a pivotal challenge in clinical genetics. Recently, the proposition of protein language models has improved the generic variant effect prediction (VEP) accuracy via weakly-supervised or unsupervised training. However, these VEPs are not disease-specific, limiting their adaptation at point-of-care. To address this problem, we propose a disease-specific \textsc{pro}tein language model for variant \textsc{path}ogenicity, termed ProPath, to capture the pseudo-log-likelihood ratio in rare missense variants through a siamese network. We evaluate the performance of ProPath against pre-trained language models, using clinical variant sets in inherited cardiomyopathies and arrhythmias that were not seen during training. Our results demonstrate that ProPath surpasses the pre-trained ESM1b with an over $5\%$ improvement in AUC across both datasets. Furthermore, our model achieved the highest performances across all baselines for both datasets. Thus, our ProPath offers a potent disease-specific variant effect prediction, particularly valuable for disease associations and clinical applicability.
<div id='section'>Paperid: <span id='pid'>1311, <a href='https://arxiv.org/pdf/2310.07682.pdf' target='_blank'>https://arxiv.org/pdf/2310.07682.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kshitij Ingale, Sun Hae Hong, Josh S. K. Bell, Abbas Rizvi, Amy Welch, Lingdao Sha, Irvin Ho, Kunal Nagpal, Aicha BenTaieb, Rohan P Joshi, Martin C Stumpe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.07682">Prediction of MET Overexpression in Non-Small Cell Lung Adenocarcinomas from Hematoxylin and Eosin Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>MET protein overexpression is a targetable event in non-small cell lung cancer (NSCLC) and is the subject of active drug development. Challenges in identifying patients for these therapies include lack of access to validated testing, such as standardized immunohistochemistry (IHC) assessment, and consumption of valuable tissue for a single gene/protein assay. Development of pre-screening algorithms using routinely available digitized hematoxylin and eosin (H&E)-stained slides to predict MET overexpression could promote testing for those who will benefit most. While assessment of MET expression using IHC is currently not routinely performed in NSCLC, next-generation sequencing is common and in some cases includes RNA expression panel testing. In this work, we leveraged a large database of matched H&E slides and RNA expression data to train a weakly supervised model to predict MET RNA overexpression directly from H&E images. This model was evaluated on an independent holdout test set of 300 over-expressed and 289 normal patients, demonstrating an ROC-AUC of 0.70 (95th percentile interval: 0.66 - 0.74) with stable performance characteristics across different patient clinical variables and robust to synthetic noise on the test set. These results suggest that H&E-based predictive models could be useful to prioritize patients for confirmatory testing of MET protein or MET gene expression status.
<div id='section'>Paperid: <span id='pid'>1312, <a href='https://arxiv.org/pdf/2310.07155.pdf' target='_blank'>https://arxiv.org/pdf/2310.07155.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shamik Roy, Dan Goldwasser
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.07155">"A Tale of Two Movements": Identifying and Comparing Perspectives in #BlackLivesMatter and #BlueLivesMatter Movements-related Tweets using Weakly Supervised Graph-based Structured Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Social media has become a major driver of social change, by facilitating the formation of online social movements. Automatically understanding the perspectives driving the movement and the voices opposing it, is a challenging task as annotated data is difficult to obtain. We propose a weakly supervised graph-based approach that explicitly models perspectives in #BackLivesMatter-related tweets. Our proposed approach utilizes a social-linguistic representation of the data. We convert the text to a graph by breaking it into structured elements and connect it with the social network of authors, then structured prediction is done over the elements for identifying perspectives. Our approach uses a small seed set of labeled examples. We experiment with large language models for generating artificial training examples, compare them to manual annotation, and find that it achieves comparable performance. We perform quantitative and qualitative analyses using a human-annotated test set. Our model outperforms multitask baselines by a large margin, successfully characterizing the perspectives supporting and opposing #BLM.
<div id='section'>Paperid: <span id='pid'>1313, <a href='https://arxiv.org/pdf/2310.05394.pdf' target='_blank'>https://arxiv.org/pdf/2310.05394.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gang Xu, Shuhao Wang, Lingyu Zhao, Xiao Chen, Tongwei Wang, Lang Wang, Zhenwei Luo, Dahan Wang, Zewen Zhang, Aijun Liu, Wei Ba, Zhigang Song, Huaiyin Shi, Dingrong Zhong, Jianpeng Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.05394">CAMEL2: Enhancing weakly supervised learning for histopathology images by incorporating the significance ratio</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Histopathology image analysis plays a crucial role in cancer diagnosis. However, training a clinically applicable segmentation algorithm requires pathologists to engage in labour-intensive labelling. In contrast, weakly supervised learning methods, which only require coarse-grained labels at the image level, can significantly reduce the labeling efforts. Unfortunately, while these methods perform reasonably well in slide-level prediction, their ability to locate cancerous regions, which is essential for many clinical applications, remains unsatisfactory. Previously, we proposed CAMEL, which achieves comparable results to those of fully supervised baselines in pixel-level segmentation. However, CAMEL requires 1,280x1,280 image-level binary annotations for positive WSIs. Here, we present CAMEL2, by introducing a threshold of the cancerous ratio for positive bags, it allows us to better utilize the information, consequently enabling us to scale up the image-level setting from 1,280x1,280 to 5,120x5,120 while maintaining the accuracy. Our results with various datasets, demonstrate that CAMEL2, with the help of 5,120x5,120 image-level binary annotations, which are easy to annotate, achieves comparable performance to that of a fully supervised baseline in both instance- and slide-level classifications.
<div id='section'>Paperid: <span id='pid'>1314, <a href='https://arxiv.org/pdf/2309.14117.pdf' target='_blank'>https://arxiv.org/pdf/2309.14117.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheolhyun Mun, Sanghuk Lee, Youngjung Uh, Junsuk Choe, Hyeran Byun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.14117">Small Objects Matters in Weakly-supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised semantic segmentation (WSSS) performs pixel-wise classification given only image-level labels for training. Despite the difficulty of this task, the research community has achieved promising results over the last five years. Still, current WSSS literature misses the detailed sense of how well the methods perform on different sizes of objects. Thus we propose a novel evaluation metric to provide a comprehensive assessment across different object sizes and collect a size-balanced evaluation set to complement PASCAL VOC. With these two gadgets, we reveal that the existing WSSS methods struggle in capturing small objects. Furthermore, we propose a size-balanced cross-entropy loss coupled with a proper training strategy. It generally improves existing WSSS methods as validated upon ten baselines on three different datasets.
<div id='section'>Paperid: <span id='pid'>1315, <a href='https://arxiv.org/pdf/2309.11783.pdf' target='_blank'>https://arxiv.org/pdf/2309.11783.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Tao, Yuxing Huang, Xiangdong Wang, Long Yan, Lufeng Zhai, Kazushige Ouchi, Taihao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.11783">Frame Pairwise Distance Loss for Weakly-supervised Sound Event Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised learning has emerged as a promising approach to leverage limited labeled data in various domains by bridging the gap between fully supervised methods and unsupervised techniques. Acquisition of strong annotations for detecting sound events is prohibitively expensive, making weakly supervised learning a more cost-effective and broadly applicable alternative. In order to enhance the recognition rate of the learning of detection of weakly-supervised sound events, we introduce a Frame Pairwise Distance (FPD) loss branch, complemented with a minimal amount of synthesized data. The corresponding sampling and label processing strategies are also proposed. Two distinct distance metrics are employed to evaluate the proposed approach. Finally, the method is validated on the DCASE 2023 task4 dataset. The obtained experimental results corroborated the efficacy of this approach.
<div id='section'>Paperid: <span id='pid'>1316, <a href='https://arxiv.org/pdf/2309.08931.pdf' target='_blank'>https://arxiv.org/pdf/2309.08931.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongran Yu, Xueyan Liu, Shirui Pan, Anchen Li, Bo Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.08931">A Novel Neural-symbolic System under Statistical Relational Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A key objective in the field of artificial intelligence is to develop cognitive models that can exhibit human-like intellectual capabilities. One promising approach to achieving this is through neural-symbolic systems, which combine the strengths of deep learning and symbolic reasoning. However, current methodologies in this area face limitations in integration, generalization, and interpretability. To address these challenges, we propose a neural-symbolic framework based on statistical relational learning, referred to as NSF-SRL. This framework effectively integrates deep learning models with symbolic reasoning in a mutually beneficial manner.In NSF-SRL, the results of symbolic reasoning are utilized to refine and correct the predictions made by deep learning models, while deep learning models enhance the efficiency of the symbolic reasoning process. Through extensive experiments, we demonstrate that our approach achieves high performance and exhibits effective generalization in supervised learning, weakly supervised and zero-shot learning tasks. Furthermore, we introduce a quantitative strategy to evaluate the interpretability of the model's predictions, visualizing the corresponding logic rules that contribute to these predictions and providing insights into the reasoning process. We believe that this approach sets a new standard for neural-symbolic systems and will drive future research in the field of general artificial intelligence.
<div id='section'>Paperid: <span id='pid'>1317, <a href='https://arxiv.org/pdf/2308.16718.pdf' target='_blank'>https://arxiv.org/pdf/2308.16718.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Shi, Dong-Dong Wu, Xin Geng, Min-Ling Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.16718">Robust Representation Learning for Unreliable Partial Label Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partial Label Learning (PLL) is a type of weakly supervised learning where each training instance is assigned a set of candidate labels, but only one label is the ground-truth. However, this idealistic assumption may not always hold due to potential annotation inaccuracies, meaning the ground-truth may not be present in the candidate label set. This is known as Unreliable Partial Label Learning (UPLL) that introduces an additional complexity due to the inherent unreliability and ambiguity of partial labels, often resulting in a sub-optimal performance with existing methods. To address this challenge, we propose the Unreliability-Robust Representation Learning framework (URRL) that leverages unreliability-robust contrastive learning to help the model fortify against unreliable partial labels effectively. Concurrently, we propose a dual strategy that combines KNN-based candidate label set correction and consistency-regularization-based label disambiguation to refine label quality and enhance the ability of representation learning within the URRL framework. Extensive experiments demonstrate that the proposed method outperforms state-of-the-art PLL methods on various datasets with diverse degrees of unreliability and ambiguity. Furthermore, we provide a theoretical analysis of our approach from the perspective of the expectation maximization (EM) algorithm. Upon acceptance, we pledge to make the code publicly accessible.
<div id='section'>Paperid: <span id='pid'>1318, <a href='https://arxiv.org/pdf/2308.12609.pdf' target='_blank'>https://arxiv.org/pdf/2308.12609.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Songchun Zhang, Chunhui Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.12609">Cross-Video Contextual Knowledge Exploration and Exploitation for Ambiguity Reduction in Weakly Supervised Temporal Action Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised temporal action localization (WSTAL) aims to localize actions in untrimmed videos using video-level labels. Despite recent advances, existing approaches mainly follow a localization-by-classification pipeline, generally processing each segment individually, thereby exploiting only limited contextual information. As a result, the model will lack a comprehensive understanding (e.g. appearance and temporal structure) of various action patterns, leading to ambiguity in classification learning and temporal localization. Our work addresses this from a novel perspective, by exploring and exploiting the cross-video contextual knowledge within the dataset to recover the dataset-level semantic structure of action instances via weak labels only, thereby indirectly improving the holistic understanding of fine-grained action patterns and alleviating the aforementioned ambiguities. Specifically, an end-to-end framework is proposed, including a Robust Memory-Guided Contrastive Learning (RMGCL) module and a Global Knowledge Summarization and Aggregation (GKSA) module. First, the RMGCL module explores the contrast and consistency of cross-video action features, assisting in learning more structured and compact embedding space, thus reducing ambiguity in classification learning. Further, the GKSA module is used to efficiently summarize and propagate the cross-video representative action knowledge in a learnable manner to promote holistic action patterns understanding, which in turn allows the generation of high-confidence pseudo-labels for self-learning, thus alleviating ambiguity in temporal localization. Extensive experiments on THUMOS14, ActivityNet1.3, and FineAction demonstrate that our method outperforms the state-of-the-art methods, and can be easily plugged into other WSTAL methods.
<div id='section'>Paperid: <span id='pid'>1319, <a href='https://arxiv.org/pdf/2308.10809.pdf' target='_blank'>https://arxiv.org/pdf/2308.10809.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fangyun Wei, Yutong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.10809">Improving Continuous Sign Language Recognition with Cross-Lingual Signs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work dedicates to continuous sign language recognition (CSLR), which is a weakly supervised task dealing with the recognition of continuous signs from videos, without any prior knowledge about the temporal boundaries between consecutive signs. Data scarcity heavily impedes the progress of CSLR. Existing approaches typically train CSLR models on a monolingual corpus, which is orders of magnitude smaller than that of speech recognition. In this work, we explore the feasibility of utilizing multilingual sign language corpora to facilitate monolingual CSLR. Our work is built upon the observation of cross-lingual signs, which originate from different sign languages but have similar visual signals (e.g., hand shape and motion). The underlying idea of our approach is to identify the cross-lingual signs in one sign language and properly leverage them as auxiliary training data to improve the recognition capability of another. To achieve the goal, we first build two sign language dictionaries containing isolated signs that appear in two datasets. Then we identify the sign-to-sign mappings between two sign languages via a well-optimized isolated sign language recognition model. At last, we train a CSLR model on the combination of the target data with original labels and the auxiliary data with mapped labels. Experimentally, our approach achieves state-of-the-art performance on two widely-used CSLR datasets: Phoenix-2014 and Phoenix-2014T.
<div id='section'>Paperid: <span id='pid'>1320, <a href='https://arxiv.org/pdf/2308.02062.pdf' target='_blank'>https://arxiv.org/pdf/2308.02062.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alessandro Fontanella, Grant Mair, Joanna Wardlaw, Emanuele Trucco, Amos Storkey
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.02062">Diffusion Models for Counterfactual Generation and Anomaly Detection in Brain Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Segmentation masks of pathological areas are useful in many medical applications, such as brain tumour and stroke management. Moreover, healthy counterfactuals of diseased images can be used to enhance radiologists' training files and to improve the interpretability of segmentation models. In this work, we present a weakly supervised method to generate a healthy version of a diseased image and then use it to obtain a pixel-wise anomaly map. To do so, we start by considering a saliency map that approximately covers the pathological areas, obtained with ACAT. Then, we propose a technique that allows to perform targeted modifications to these regions, while preserving the rest of the image. In particular, we employ a diffusion model trained on healthy samples and combine Denoising Diffusion Probabilistic Model (DDPM) and Denoising Diffusion Implicit Model (DDIM) at each step of the sampling process. DDPM is used to modify the areas affected by a lesion within the saliency map, while DDIM guarantees reconstruction of the normal anatomy outside of it. The two parts are also fused at each timestep, to guarantee the generation of a sample with a coherent appearance and a seamless transition between edited and unedited parts. We verify that when our method is applied to healthy samples, the input images are reconstructed without significant modifications. We compare our approach with alternative weakly supervised methods on the task of brain lesion segmentation, achieving the highest mean Dice and IoU scores among the models considered.
<div id='section'>Paperid: <span id='pid'>1321, <a href='https://arxiv.org/pdf/2307.14603.pdf' target='_blank'>https://arxiv.org/pdf/2307.14603.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingxue Wang, Liwen Zou, Jun Chen, Yingying Cao, Zhenghua Cai, Yudong Qiu, Liang Mao, Zhongqiu Wang, Jingya Chen, Luying Gui, Xiaoping Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.14603">A Weakly Supervised Segmentation Network Embedding Cross-scale Attention Guidance and Noise-sensitive Constraint for Detecting Tertiary Lymphoid Structures of Pancreatic Tumors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The presence of tertiary lymphoid structures (TLSs) on pancreatic pathological images is an important prognostic indicator of pancreatic tumors. Therefore, TLSs detection on pancreatic pathological images plays a crucial role in diagnosis and treatment for patients with pancreatic tumors. However, fully supervised detection algorithms based on deep learning usually require a large number of manual annotations, which is time-consuming and labor-intensive. In this paper, we aim to detect the TLSs in a manner of few-shot learning by proposing a weakly supervised segmentation network. We firstly obtain the lymphocyte density maps by combining a pretrained model for nuclei segmentation and a domain adversarial network for lymphocyte nuclei recognition. Then, we establish a cross-scale attention guidance mechanism by jointly learning the coarse-scale features from the original histopathology images and fine-scale features from our designed lymphocyte density attention. A noise-sensitive constraint is introduced by an embedding signed distance function loss in the training procedure to reduce tiny prediction errors. Experimental results on two collected datasets demonstrate that our proposed method significantly outperforms the state-of-the-art segmentation-based algorithms in terms of TLSs detection accuracy. Additionally, we apply our method to study the congruent relationship between the density of TLSs and peripancreatic vascular invasion and obtain some clinically statistical results.
<div id='section'>Paperid: <span id='pid'>1322, <a href='https://arxiv.org/pdf/2306.13796.pdf' target='_blank'>https://arxiv.org/pdf/2306.13796.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaifu Wang, Efthymia Tsamoura, Dan Roth
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.13796">On Learning Latent Models with Multi-Instance Weak Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider a weakly supervised learning scenario where the supervision signal is generated by a transition function $Ï$ of labels associated with multiple input instances. We formulate this problem as \emph{multi-instance Partial Label Learning (multi-instance PLL)}, which is an extension to the standard PLL problem. Our problem is met in different fields, including latent structural learning and neuro-symbolic integration. Despite the existence of many learning techniques, limited theoretical analysis has been dedicated to this problem. In this paper, we provide the first theoretical study of multi-instance PLL with possibly an unknown transition $Ï$. Our main contributions are as follows. Firstly, we propose a necessary and sufficient condition for the learnability of the problem. This condition non-trivially generalizes and relaxes the existing small ambiguity degree in the PLL literature, since we allow the transition to be deterministic. Secondly, we derive Rademacher-style error bounds based on a top-$k$ surrogate loss that is widely used in the neuro-symbolic literature. Furthermore, we conclude with empirical experiments for learning under unknown transitions. The empirical results align with our theoretical findings; however, they also expose the issue of scalability in the weak supervision literature.
<div id='section'>Paperid: <span id='pid'>1323, <a href='https://arxiv.org/pdf/2306.12012.pdf' target='_blank'>https://arxiv.org/pdf/2306.12012.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aakriti Agrawal, Milind Rao, Anit Kumar Sahu, Gopinath Chennupati, Andreas Stolcke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.12012">Learning When to Trust Which Teacher for Weakly Supervised ASR</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic speech recognition (ASR) training can utilize multiple experts as teacher models, each trained on a specific domain or accent. Teacher models may be opaque in nature since their architecture may be not be known or their training cadence is different from that of the student ASR model. Still, the student models are updated incrementally using the pseudo-labels generated independently by the expert teachers. In this paper, we exploit supervision from multiple domain experts in training student ASR models. This training strategy is especially useful in scenarios where few or no human transcriptions are available. To that end, we propose a Smart-Weighter mechanism that selects an appropriate expert based on the input audio, and then trains the student model in an unsupervised setting. We show the efficacy of our approach using LibriSpeech and LibriLight benchmarks and find an improvement of 4 to 25\% over baselines that uniformly weight all the experts, use a single expert model, or combine experts using ROVER.
<div id='section'>Paperid: <span id='pid'>1324, <a href='https://arxiv.org/pdf/2306.02928.pdf' target='_blank'>https://arxiv.org/pdf/2306.02928.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simon Lepage, JÃ©rÃ©mie Mary, David Picard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.02928">LRVS-Fashion: Extending Visual Search with Referring Instructions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a new challenge for image similarity search in the context of fashion, addressing the inherent ambiguity in this domain stemming from complex images. We present Referred Visual Search (RVS), a task allowing users to define more precisely the desired similarity, following recent interest in the industry. We release a new large public dataset, LRVS-Fashion, consisting of 272k fashion products with 842k images extracted from fashion catalogs, designed explicitly for this task. However, unlike traditional visual search methods in the industry, we demonstrate that superior performance can be achieved by bypassing explicit object detection and adopting weakly-supervised conditional contrastive learning on image tuples. Our method is lightweight and demonstrates robustness, reaching Recall at one superior to strong detection-based baselines against 2M distractors. The dataset is available at https://huggingface.co/datasets/Slep/LAION-RVS-Fashion .
<div id='section'>Paperid: <span id='pid'>1325, <a href='https://arxiv.org/pdf/2305.14410.pdf' target='_blank'>https://arxiv.org/pdf/2305.14410.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Harman Singh, Poorva Garg, Mohit Gupta, Kevin Shah, Ashish Goswami, Satyam Modi, Arnab Kumar Mondal, Dinesh Khandelwal, Dinesh Garg, Parag Singla
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.14410">Image Manipulation via Multi-Hop Instructions -- A New Dataset and Weakly-Supervised Neuro-Symbolic Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We are interested in image manipulation via natural language text -- a task that is useful for multiple AI applications but requires complex reasoning over multi-modal spaces. We extend recently proposed Neuro Symbolic Concept Learning (NSCL), which has been quite effective for the task of Visual Question Answering (VQA), for the task of image manipulation. Our system referred to as NeuroSIM can perform complex multi-hop reasoning over multi-object scenes and only requires weak supervision in the form of annotated data for VQA. NeuroSIM parses an instruction into a symbolic program, based on a Domain Specific Language (DSL) comprising of object attributes and manipulation operations, that guides its execution. We create a new dataset for the task, and extensive experiments demonstrate that NeuroSIM is highly competitive with or beats SOTA baselines that make use of supervised data for manipulation.
<div id='section'>Paperid: <span id='pid'>1326, <a href='https://arxiv.org/pdf/2305.05947.pdf' target='_blank'>https://arxiv.org/pdf/2305.05947.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rumeysa Bodur, Erhan Gundogdu, Binod Bhattarai, Tae-Kyun Kim, Michael Donoser, Loris Bazzani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.05947">iEdit: Localised Text-guided Image Editing with Weak Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models (DMs) can generate realistic images with text guidance using large-scale datasets. However, they demonstrate limited controllability in the output space of the generated images. We propose a novel learning method for text-guided image editing, namely \texttt{iEdit}, that generates images conditioned on a source image and a textual edit prompt. As a fully-annotated dataset with target images does not exist, previous approaches perform subject-specific fine-tuning at test time or adopt contrastive learning without a target image, leading to issues on preserving the fidelity of the source image. We propose to automatically construct a dataset derived from LAION-5B, containing pseudo-target images with their descriptive edit prompts given input image-caption pairs. This dataset gives us the flexibility of introducing a weakly-supervised loss function to generate the pseudo-target image from the latent noise of the source image conditioned on the edit prompt. To encourage localised editing and preserve or modify spatial structures in the image, we propose a loss function that uses segmentation masks to guide the editing during training and optionally at inference. Our model is trained on the constructed dataset with 200K samples and constrained GPU resources. It shows favourable results against its counterparts in terms of image fidelity, CLIP alignment score and qualitatively for editing both generated and real images.
<div id='section'>Paperid: <span id='pid'>1327, <a href='https://arxiv.org/pdf/2305.05421.pdf' target='_blank'>https://arxiv.org/pdf/2305.05421.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Iris de GÃ©lis, SÃ©bastien LefÃ¨vre, Thomas Corpetti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.05421">DC3DCD: unsupervised learning for multiclass 3D point cloud change detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In a constant evolving world, change detection is of prime importance to keep updated maps. To better sense areas with complex geometry (urban areas in particular), considering 3D data appears to be an interesting alternative to classical 2D images. In this context, 3D point clouds (PCs), whether obtained through LiDAR or photogrammetric techniques, provide valuable information. While recent studies showed the considerable benefit of using deep learning-based methods to detect and characterize changes into raw 3D PCs, these studies rely on large annotated training data to obtain accurate results. The collection of these annotations are tricky and time-consuming. The availability of unsupervised or weakly supervised approaches is then of prime interest. In this paper, we propose an unsupervised method, called DeepCluster 3D Change Detection (DC3DCD), to detect and categorize multiclass changes at point level. We classify our approach in the unsupervised family given the fact that we extract in a completely unsupervised way a number of clusters associated with potential changes. Let us precise that in the end of the process, the user has only to assign a label to each of these clusters to derive the final change map. Our method builds upon the DeepCluster approach, originally designed for image classification, to handle complex raw 3D PCs and perform change segmentation task. An assessment of the method on both simulated and real public dataset is provided. The proposed method allows to outperform fully-supervised traditional machine learning algorithm and to be competitive with fully-supervised deep learning networks applied on rasterization of 3D PCs with a mean of IoU over classes of change of 57.06\% and 66.69\% for the simulated and the real datasets, respectively.
<div id='section'>Paperid: <span id='pid'>1328, <a href='https://arxiv.org/pdf/2305.02637.pdf' target='_blank'>https://arxiv.org/pdf/2305.02637.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiping Jin, Leo Wanner, Vishakha Laxman Kadam, Alexander Shvets
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.02637">Towards Weakly-Supervised Hate Speech Classification Across Datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As pointed out by several scholars, current research on hate speech (HS) recognition is characterized by unsystematic data creation strategies and diverging annotation schemata. Subsequently, supervised-learning models tend to generalize poorly to datasets they were not trained on, and the performance of the models trained on datasets labeled using different HS taxonomies cannot be compared. To ease this problem, we propose applying extremely weak supervision that only relies on the class name rather than on class samples from the annotated data. We demonstrate the effectiveness of a state-of-the-art weakly-supervised text classification model in various in-dataset and cross-dataset settings. Furthermore, we conduct an in-depth quantitative and qualitative analysis of the source of poor generalizability of HS classification models.
<div id='section'>Paperid: <span id='pid'>1329, <a href='https://arxiv.org/pdf/2304.14299.pdf' target='_blank'>https://arxiv.org/pdf/2304.14299.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheheng Jiang, Hossein Rahmani, Sue Black, Bryan M. Williams
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.14299">A Probabilistic Attention Model with Occlusion-aware Texture Regression for 3D Hand Reconstruction from a Single RGB Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, deep learning based approaches have shown promising results in 3D hand reconstruction from a single RGB image. These approaches can be roughly divided into model-based approaches, which are heavily dependent on the model's parameter space, and model-free approaches, which require large numbers of 3D ground truths to reduce depth ambiguity and struggle in weakly-supervised scenarios. To overcome these issues, we propose a novel probabilistic model to achieve the robustness of model-based approaches and reduced dependence on the model's parameter space of model-free approaches. The proposed probabilistic model incorporates a model-based network as a prior-net to estimate the prior probability distribution of joints and vertices. An Attention-based Mesh Vertices Uncertainty Regression (AMVUR) model is proposed to capture dependencies among vertices and the correlation between joints and mesh vertices to improve their feature representation. We further propose a learning based occlusion-aware Hand Texture Regression model to achieve high-fidelity texture reconstruction. We demonstrate the flexibility of the proposed probabilistic model to be trained in both supervised and weakly-supervised scenarios. The experimental results demonstrate our probabilistic model's state-of-the-art accuracy in 3D hand and texture reconstruction from a single image in both training schemes, including in the presence of severe occlusions.
<div id='section'>Paperid: <span id='pid'>1330, <a href='https://arxiv.org/pdf/2303.18044.pdf' target='_blank'>https://arxiv.org/pdf/2303.18044.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengyang Sun, Xiaojin Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.18044">Long-Short Temporal Co-Teaching for Weakly Supervised Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised video anomaly detection (WS-VAD) is a challenging problem that aims to learn VAD models only with video-level annotations. In this work, we propose a Long-Short Temporal Co-teaching (LSTC) method to address the WS-VAD problem. It constructs two tubelet-based spatio-temporal transformer networks to learn from short- and long-term video clips respectively. Each network is trained with respect to a multiple instance learning (MIL)-based ranking loss, together with a cross-entropy loss when clip-level pseudo labels are available. A co-teaching strategy is adopted to train the two networks. That is, clip-level pseudo labels generated from each network are used to supervise the other one at the next training round, and the two networks are learned alternatively and iteratively. Our proposed method is able to better deal with the anomalies with varying durations as well as subtle anomalies. Extensive experiments on three public datasets demonstrate that our method outperforms state-of-the-art WS-VAD methods.
<div id='section'>Paperid: <span id='pid'>1331, <a href='https://arxiv.org/pdf/2303.07641.pdf' target='_blank'>https://arxiv.org/pdf/2303.07641.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nam Tuan Ly, Atsuhiro Takasu, Phuc Nguyen, Hideaki Takeda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.07641">Rethinking Image-based Table Recognition Using Weakly Supervised Methods</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most of the previous methods for table recognition rely on training datasets containing many richly annotated table images. Detailed table image annotation, e.g., cell or text bounding box annotation, however, is costly and often subjective. In this paper, we propose a weakly supervised model named WSTabNet for table recognition that relies only on HTML (or LaTeX) code-level annotations of table images. The proposed model consists of three main parts: an encoder for feature extraction, a structure decoder for generating table structure, and a cell decoder for predicting the content of each cell in the table. Our system is trained end-to-end by stochastic gradient descent algorithms, requiring only table images and their ground-truth HTML (or LaTeX) representations. To facilitate table recognition with deep learning, we create and release WikiTableSet, the largest publicly available image-based table recognition dataset built from Wikipedia. WikiTableSet contains nearly 4 million English table images, 590K Japanese table images, and 640k French table images with corresponding HTML representation and cell bounding boxes. The extensive experiments on WikiTableSet and two large-scale datasets: FinTabNet and PubTabNet demonstrate that the proposed weakly supervised model achieves better, or similar accuracies compared to the state-of-the-art models on all benchmark datasets.
<div id='section'>Paperid: <span id='pid'>1332, <a href='https://arxiv.org/pdf/2303.05148.pdf' target='_blank'>https://arxiv.org/pdf/2303.05148.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Martijn Oldenhof, Adam Arany, Yves Moreau, Edward De Brouwer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.05148">Weakly Supervised Knowledge Transfer with Probabilistic Logical Reasoning for Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training object detection models usually requires instance-level annotations, such as the positions and labels of all objects present in each image. Such supervision is unfortunately not always available and, more often, only image-level information is provided, also known as weak supervision. Recent works have addressed this limitation by leveraging knowledge from a richly annotated domain. However, the scope of weak supervision supported by these approaches has been very restrictive, preventing them to use all available information. In this work, we propose ProbKT, a framework based on probabilistic logical reasoning that allows to train object detection models with arbitrary types of weak supervision. We empirically show on different datasets that using all available information is beneficial as our ProbKT leads to significant improvement on target domain and better generalization compared to existing baselines. We also showcase the ability of our approach to handle complex logic statements as supervision signal.
<div id='section'>Paperid: <span id='pid'>1333, <a href='https://arxiv.org/pdf/2301.08146.pdf' target='_blank'>https://arxiv.org/pdf/2301.08146.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Deven Santosh Shah, Shiying He, Gosuddin Kamaruddin Siddiqi, Radhika Bansal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.08146">What's happening in your neighborhood? A Weakly Supervised Approach to Detect Local News</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Local news articles are a subset of news that impact users in a geographical area, such as a city, county, or state. Detecting local news (Step 1) and subsequently deciding its geographical location as well as radius of impact (Step 2) are two important steps towards accurate local news recommendation. Naive rule-based methods, such as detecting city names from the news title, tend to give erroneous results due to lack of understanding of the news content. Empowered by the latest development in natural language processing, we develop an integrated pipeline that enables automatic local news detection and content-based local news recommendations. In this paper, we focus on Step 1 of the pipeline, which highlights: (1) a weakly supervised framework incorporated with domain knowledge and auto data processing, and (2) scalability to multi-lingual settings. Compared with Stanford CoreNLP NER model, our pipeline has higher precision and recall evaluated on a real-world and human-labeled dataset. This pipeline has potential to more precise local news to users, helps local businesses get more exposure, and gives people more information about their neighborhood safety.
<div id='section'>Paperid: <span id='pid'>1334, <a href='https://arxiv.org/pdf/2301.04748.pdf' target='_blank'>https://arxiv.org/pdf/2301.04748.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhihua Liu, Bin Yang, Yan Shen, Xuejun Ni, Huiyu Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.04748">LSDM: Long-Short Diffeomorphic Motion for Weakly-Supervised Ultrasound Landmark Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate tracking of an anatomical landmark over time has been of high interests for disease assessment such as minimally invasive surgery and tumor radiation therapy. Ultrasound imaging is a promising modality benefiting from low-cost and real-time acquisition. However, generating a precise landmark tracklet is very challenging, as attempts can be easily distorted by different interference such as landmark deformation, visual ambiguity and partial observation. In this paper, we propose a long-short diffeomorphic motion network, which is a multi-task framework with a learnable deformation prior to search for the plausible deformation of landmark. Specifically, we design a novel diffeomorphism representation in both long and short temporal domains for delineating motion margins and reducing long-term cumulative tracking errors. To further mitigate local anatomical ambiguity, we propose an expectation maximisation motion alignment module to iteratively optimize both long and short deformation, aligning to the same directional and spatial representation. The proposed multi-task system can be trained in a weakly-supervised manner, which only requires few landmark annotations for tracking and zero annotation for long-short deformation learning. We conduct extensive experiments on two ultrasound landmark tracking datasets. Experimental results show that our proposed method can achieve better or competitive landmark tracking performance compared with other state-of-the-art tracking methods, with a strong generalization capability across different scanner types and different ultrasound modalities.
<div id='section'>Paperid: <span id='pid'>1335, <a href='https://arxiv.org/pdf/2301.02933.pdf' target='_blank'>https://arxiv.org/pdf/2301.02933.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pushpak Pati, Guillaume Jaume, Zeineb Ayadi, Kevin Thandiackal, Behzad Bozorgtabar, Maria Gabrani, Orcun Goksel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.02933">Weakly Supervised Joint Whole-Slide Segmentation and Classification in Prostate Cancer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The segmentation and automatic identification of histological regions of diagnostic interest offer a valuable aid to pathologists. However, segmentation methods are hampered by the difficulty of obtaining pixel-level annotations, which are tedious and expensive to obtain for Whole-Slide images (WSI). To remedy this, weakly supervised methods have been developed to exploit the annotations directly available at the image level. However, to our knowledge, none of these techniques is adapted to deal with WSIs. In this paper, we propose WholeSIGHT, a weakly-supervised method, to simultaneously segment and classify WSIs of arbitrary shapes and sizes. Formally, WholeSIGHT first constructs a tissue-graph representation of the WSI, where the nodes and edges depict tissue regions and their interactions, respectively. During training, a graph classification head classifies the WSI and produces node-level pseudo labels via post-hoc feature attribution. These pseudo labels are then used to train a node classification head for WSI segmentation. During testing, both heads simultaneously render class prediction and segmentation for an input WSI. We evaluated WholeSIGHT on three public prostate cancer WSI datasets. Our method achieved state-of-the-art weakly-supervised segmentation performance on all datasets while resulting in better or comparable classification with respect to state-of-the-art weakly-supervised WSI classification methods. Additionally, we quantify the generalization capability of our method in terms of segmentation and classification performance, uncertainty estimation, and model calibration.
<div id='section'>Paperid: <span id='pid'>1336, <a href='https://arxiv.org/pdf/2210.09698.pdf' target='_blank'>https://arxiv.org/pdf/2210.09698.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tommaso Di Noto, Meritxell Bach Cuadra, Chirine Atat, Eduardo Gamito Teiga, Monika Hegi, Andreas Hottinger, Patric Hagmann, Jonas Richiardi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.09698">Weakly Supervised Learning with Automated Labels from Radiology Reports for Glioma Change Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gliomas are the most frequent primary brain tumors in adults. Glioma change detection aims at finding the relevant parts of the image that change over time. Although Deep Learning (DL) shows promising performances in similar change detection tasks, the creation of large annotated datasets represents a major bottleneck for supervised DL applications in radiology. To overcome this, we propose a combined use of weak labels (imprecise, but fast-to-create annotations) and Transfer Learning (TL). Specifically, we explore inductive TL, where source and target domains are identical, but tasks are different due to a label shift: our target labels are created manually by three radiologists, whereas our source weak labels are generated automatically from radiology reports via NLP. We frame knowledge transfer as hyperparameter optimization, thus avoiding heuristic choices that are frequent in related works. We investigate the relationship between model size and TL, comparing a low-capacity VGG with a higher-capacity ResNeXt model. We evaluate our models on 1693 T2-weighted magnetic resonance imaging difference maps created from 183 patients, by classifying them into stable or unstable according to tumor evolution. The weak labels extracted from radiology reports allowed us to increase dataset size more than 3-fold, and improve VGG classification results from 75% to 82% AUC. Mixed training from scratch led to higher performance than fine-tuning or feature extraction. To assess generalizability, we ran inference on an open dataset (BraTS-2015: 15 patients, 51 difference maps), reaching up to 76% AUC. Overall, results suggest that medical imaging problems may benefit from smaller models and different TL strategies with respect to computer vision datasets, and that report-generated weak labels are effective in improving model performances. Code, in-house dataset and BraTS labels are released.
<div id='section'>Paperid: <span id='pid'>1337, <a href='https://arxiv.org/pdf/2210.08677.pdf' target='_blank'>https://arxiv.org/pdf/2210.08677.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jacqueline R. M. A. Maasch, Hao Zhang, Qian Yang, Fei Wang, Volodymyr Kuleshov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.08677">Regularized Data Programming with Automated Bayesian Prior Selection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The cost of manual data labeling can be a significant obstacle in supervised learning. Data programming (DP) offers a weakly supervised solution for training dataset creation, wherein the outputs of user-defined programmatic labeling functions (LFs) are reconciled through unsupervised learning. However, DP can fail to outperform an unweighted majority vote in some scenarios, including low-data contexts. This work introduces a Bayesian extension of classical DP that mitigates failures of unsupervised learning by augmenting the DP objective with regularization terms. Regularized learning is achieved through maximum a posteriori estimation with informative priors. Majority vote is proposed as a proxy signal for automated prior parameter selection. Results suggest that regularized DP improves performance relative to maximum likelihood and majority voting, confers greater interpretability, and bolsters performance in low-data regimes.
<div id='section'>Paperid: <span id='pid'>1338, <a href='https://arxiv.org/pdf/2210.00042.pdf' target='_blank'>https://arxiv.org/pdf/2210.00042.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Divakar Vashisth, Tapan Mukerji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.00042">Direct Estimation of Porosity from Seismic Data using Rock and Wave Physics Informed Neural Networks (RW-PINN)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Petrophysical inversion is an important aspect of reservoir modeling. However due to the lack of a unique and straightforward relationship between seismic traces and rock properties, predicting petrophysical properties directly from seismic data is a complex task. Many studies have attempted to identify the direct end-to-end link using supervised machine learning techniques, but face different challenges such as a lack of large petrophysical training dataset or estimates that may not conform with physics or depositional history of the rocks. We present a rock and wave physics informed neural network (RW-PINN) model that can estimate porosity directly from seismic image traces with no or limited number of wells, with predictions that are consistent with rock physics and geologic knowledge of deposition. As an example, we use the uncemented sand rock physics model and normal-incidence wave physics to guide the learning of RW-PINN to eventually get good estimates of porosities from normal-incidence seismic traces and limited well data. Training RW-PINN with few wells (weakly supervised) helps in tackling the problem of non-uniqueness as different porosity logs can give similar seismic traces. We use weighted normalized root mean square error loss function to train the weakly supervised network and demonstrate the impact of different weights on porosity predictions. The RW-PINN estimated porosities and seismic traces are compared to predictions from a completely supervised model, which gives slightly better porosity estimates but poorly matches the seismic traces, in addition to requiring a large amount of labeled training data. In this paper, we demonstrate the complete workflow for executing petrophysical inversion of seismic data using self-supervised or weakly supervised rock physics informed neural networks.
<div id='section'>Paperid: <span id='pid'>1339, <a href='https://arxiv.org/pdf/2209.11523.pdf' target='_blank'>https://arxiv.org/pdf/2209.11523.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianyong Ai, Wenbo Ding, Jiuhua Zhao, Jiachen Zhong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.11523">WS-3D-Lane: Weakly Supervised 3D Lane Detection With 2D Lane Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Compared to 2D lanes, real 3D lane data is difficult to collect accurately. In this paper, we propose a novel method for training 3D lanes with only 2D lane labels, called weakly supervised 3D lane detection WS-3D-Lane. By assumptions of constant lane width and equal height on adjacent lanes, we indirectly supervise 3D lane heights in the training. To overcome the problem of the dynamic change of the camera pitch during data collection, a camera pitch self-calibration method is proposed. In anchor representation, we propose a double-layer anchor with a improved non-maximum suppression (NMS) method, which enables the anchor-based method to predict two lane lines that are close. Experiments are conducted on the base of 3D-LaneNet under two supervision methods. Under weakly supervised setting, our WS-3D-Lane outperforms previous 3D-LaneNet: F-score rises to 92.3% on Apollo 3D synthetic dataset, and F1 rises to 74.5% on ONCE-3DLanes. Meanwhile, WS-3D-Lane in purely supervised setting makes more increments and outperforms state-of-the-art. To the best of our knowledge, WS-3D-Lane is the first try of 3D lane detection under weakly supervised setting.
<div id='section'>Paperid: <span id='pid'>1340, <a href='https://arxiv.org/pdf/2209.01970.pdf' target='_blank'>https://arxiv.org/pdf/2209.01970.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruyue Xin, Hongyun Liu, Peng Chen, Paola Grosso, Zhiming Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.01970">FIRED: a fine-grained robust performance diagnosis framework for cloud applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To run a cloud application with the required service quality, operators have to continuously monitor the cloud application's run-time status, detect potential performance anomalies, and diagnose the root causes of anomalies. However, existing models of performance anomaly detection often suffer from low re-usability and robustness due to the diversity of system-level metrics being monitored and the lack of high-quality labeled monitoring data for anomalies. Moreover, the current coarse-grained analysis models make it difficult to locate system-level root causes of the application performance anomalies for effective adaptation decisions. We provide a FIne-grained Robust pErformance Diagnosis (FIRED) framework to tackle those challenges. The framework offers an ensemble of several well-selected base models for anomaly detection using a deep neural network, which adopts weakly-supervised learning considering fewer labels exist in reality. The framework also employs a real-time fine-grained analysis model to locate dependent system metrics of the anomaly. Our experiments show that the framework can achieve the best detection accuracy and algorithm robustness, and it can predict anomalies in four minutes with F1 score higher than 0.8. In addition, the framework can accurately localize the first root causes, and with an average accuracy higher than 0.7 of locating first four root causes.
<div id='section'>Paperid: <span id='pid'>1341, <a href='https://arxiv.org/pdf/2112.11679.pdf' target='_blank'>https://arxiv.org/pdf/2112.11679.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingyuan Gong, Yu Liu, Liqiang Zhang, Renhe Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2112.11679">Ghost-dil-NetVLAD: A Lightweight Neural Network for Visual Place Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual place recognition (VPR) is a challenging task with the unbalance between enormous computational cost and high recognition performance. Thanks to the practical feature extraction ability of the lightweight convolution neural networks (CNNs) and the train-ability of the vector of locally aggregated descriptors (VLAD) layer, we propose a lightweight weakly supervised end-to-end neural network consisting of a front-ended perception model called GhostCNN and a learnable VLAD layer as a back-end. GhostCNN is based on Ghost modules that are lightweight CNN-based architectures. They can generate redundant feature maps using linear operations instead of the traditional convolution process, making a good trade-off between computation resources and recognition accuracy. To enhance our proposed lightweight model further, we add dilated convolutions to the Ghost module to get features containing more spatial semantic information, improving accuracy. Finally, rich experiments conducted on a commonly used public benchmark and our private dataset validate that the proposed neural network reduces the FLOPs and parameters of VGG16-NetVLAD by 99.04% and 80.16%, respectively. Besides, both models achieve similar accuracy.
<div id='section'>Paperid: <span id='pid'>1342, <a href='https://arxiv.org/pdf/2108.02980.pdf' target='_blank'>https://arxiv.org/pdf/2108.02980.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongtuo Liu, Dan Xu, Sucheng Ren, Hanjie Wu, Hongmin Cai, Shengfeng He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2108.02980">Fine-grained Domain Adaptive Crowd Counting via Point-derived Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to domain shift, a large performance drop is usually observed when a trained crowd counting model is deployed in the wild. While existing domain-adaptive crowd counting methods achieve promising results, they typically regard each crowd image as a whole and reduce domain discrepancies in a holistic manner, thus limiting further improvement of domain adaptation performance. To this end, we propose to untangle \emph{domain-invariant} crowd and \emph{domain-specific} background from crowd images and design a fine-grained domain adaption method for crowd counting. Specifically, to disentangle crowd from background, we propose to learn crowd segmentation from point-level crowd counting annotations in a weakly-supervised manner. Based on the derived segmentation, we design a crowd-aware domain adaptation mechanism consisting of two crowd-aware adaptation modules, i.e., Crowd Region Transfer (CRT) and Crowd Density Alignment (CDA). The CRT module is designed to guide crowd features transfer across domains beyond background distractions. The CDA module dedicates to regularising target-domain crowd density generation by its own crowd density distribution. Our method outperforms previous approaches consistently in the widely-used adaptation scenarios.
<div id='section'>Paperid: <span id='pid'>1343, <a href='https://arxiv.org/pdf/2012.03707.pdf' target='_blank'>https://arxiv.org/pdf/2012.03707.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Piotr Kicki, Tomasz Gawron, Krzysztof Äwian, Mete Ozay, Piotr SkrzypczyÅski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2012.03707">Learning from Experience for Rapid Generation of Local Car Maneuvers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Being able to rapidly respond to the changing scenes and traffic situations by generating feasible local paths is of pivotal importance for car autonomy. We propose to train a deep neural network (DNN) to plan feasible and nearly-optimal paths for kinematically constrained vehicles in small constant time. Our DNN model is trained using a novel weakly supervised approach and a gradient-based policy search. On real and simulated scenes and a large set of local planning problems, we demonstrate that our approach outperforms the existing planners with respect to the number of successfully completed tasks. While the path generation time is about 40 ms, the generated paths are smooth and comparable to those obtained from conventional path planners.
<div id='section'>Paperid: <span id='pid'>1344, <a href='https://arxiv.org/pdf/1902.09968.pdf' target='_blank'>https://arxiv.org/pdf/1902.09968.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runsheng Zhang, Yaping Huang, Mengyang Pu, Jian Zhang, Qingji Guan, Qi Zou, Haibin Ling
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1902.09968">Object Discovery From a Single Unlabeled Image by Mining Frequent Itemset With Multi-scale Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>TThe goal of our work is to discover dominant objects in a very general setting where only a single unlabeled image is given. This is far more challenge than typical co-localization or weakly-supervised localization tasks. To tackle this problem, we propose a simple but effective pattern mining-based method, called Object Location Mining (OLM), which exploits the advantages of data mining and feature representation of pre-trained convolutional neural networks (CNNs). Specifically, we first convert the feature maps from a pre-trained CNN model into a set of transactions, and then discovers frequent patterns from transaction database through pattern mining techniques. We observe that those discovered patterns, i.e., co-occurrence highlighted regions, typically hold appearance and spatial consistency. Motivated by this observation, we can easily discover and localize possible objects by merging relevant meaningful patterns. Extensive experiments on a variety of benchmarks demonstrate that OLM achieves competitive localization performance compared with the state-of-the-art methods. We also evaluate our approach compared with unsupervised saliency detection methods and achieves competitive results on seven benchmark datasets. Moreover, we conduct experiments on fine-grained classification to show that our proposed method can locate the entire object and parts accurately, which can benefit to improving the classification results significantly.
<div id='section'>Paperid: <span id='pid'>1345, <a href='https://arxiv.org/pdf/2511.16343.pdf' target='_blank'>https://arxiv.org/pdf/2511.16343.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chi-Han Chen, Chieh-Ming Chen, Wen-Huang Cheng, Ching-Chun Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.16343">Aerial View River Landform Video segmentation: A Weakly Supervised Context-aware Temporal Consistency Distillation Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The study of terrain and landform classification through UAV remote sensing diverges significantly from ground vehicle patrol tasks. Besides grappling with the complexity of data annotation and ensuring temporal consistency, it also confronts the scarcity of relevant data and the limitations imposed by the effective range of many technologies. This research substantiates that, in aerial positioning tasks, both the mean Intersection over Union (mIoU) and temporal consistency (TC) metrics are of paramount importance. It is demonstrated that fully labeled data is not the optimal choice, as selecting only key data lacks the enhancement in TC, leading to failures. Hence, a teacher-student architecture, coupled with key frame selection and key frame updating algorithms, is proposed. This framework successfully performs weakly supervised learning and TC knowledge distillation, overcoming the deficiencies of traditional TC training in aerial tasks. The experimental results reveal that our method utilizing merely 30\% of labeled data, concurrently elevates mIoU and temporal consistency ensuring stable localization of terrain objects. Result demo : https://gitlab.com/prophet.ai.inc/drone-based-riverbed-inspection
<div id='section'>Paperid: <span id='pid'>1346, <a href='https://arxiv.org/pdf/2511.15393.pdf' target='_blank'>https://arxiv.org/pdf/2511.15393.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kunyu Zhang, Mingxuan Wang, Xiangjie Shi, Haoxing Xu, Chao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.15393">EVA-Net: Interpretable Anomaly Detection for Brain Health via Learning Continuous Aging Prototypes from One-Class EEG Cohorts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The brain age is a key indicator of brain health. While electroencephalography (EEG) is a practical tool for this task, existing models struggle with the common challenge of imperfect medical data, such as learning a ``normal'' baseline from weakly supervised, healthy-only cohorts. This is a critical anomaly detection task for identifying disease, but standard models are often black boxes lacking an interpretable structure. We propose EVA-Net, a novel framework that recasts brain age as an interpretable anomaly detection problem. EVA-Net uses an efficient, sparsified-attention Transformer to model long EEG sequences. To handle noise and variability in imperfect data, it employs a Variational Information Bottleneck to learn a robust, compressed representation. For interpretability, this representation is aligned to a continuous prototype network that explicitly learns the normative healthy aging manifold. Trained on 1297 healthy subjects, EVA-Net achieves state-of-the-art accuracy. We validated its anomaly detection capabilities on an unseen cohort of 27 MCI and AD patients. This pathological group showed significantly higher brain-age gaps and a novel Prototype Alignment Error, confirming their deviation from the healthy manifold. EVA-Net provides an interpretable framework for healthcare intelligence using imperfect medical data.
<div id='section'>Paperid: <span id='pid'>1347, <a href='https://arxiv.org/pdf/2511.13204.pdf' target='_blank'>https://arxiv.org/pdf/2511.13204.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junhee Lee, ChaeBeen Bang, MyoungChul Kim, MyeongAh Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13204">RefineVAD: Semantic-Guided Feature Recalibration for Weakly Supervised Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-Supervised Video Anomaly Detection aims to identify anomalous events using only video-level labels, balancing annotation efficiency with practical applicability. However, existing methods often oversimplify the anomaly space by treating all abnormal events as a single category, overlooking the diverse semantic and temporal characteristics intrinsic to real-world anomalies. Inspired by how humans perceive anomalies, by jointly interpreting temporal motion patterns and semantic structures underlying different anomaly types, we propose RefineVAD, a novel framework that mimics this dual-process reasoning. Our framework integrates two core modules. The first, Motion-aware Temporal Attention and Recalibration (MoTAR), estimates motion salience and dynamically adjusts temporal focus via shift-based attention and global Transformer-based modeling. The second, Category-Oriented Refinement (CORE), injects soft anomaly category priors into the representation space by aligning segment-level features with learnable category prototypes through cross-attention. By jointly leveraging temporal dynamics and semantic structure, explicitly models both "how" motion evolves and "what" semantic category it resembles. Extensive experiments on WVAD benchmark validate the effectiveness of RefineVAD and highlight the importance of integrating semantic context to guide feature refinement toward anomaly-relevant patterns.
<div id='section'>Paperid: <span id='pid'>1348, <a href='https://arxiv.org/pdf/2511.04892.pdf' target='_blank'>https://arxiv.org/pdf/2511.04892.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vasileios Magoulianitis, Catherine A. Alexander, Jiaxin Yang, C. -C. Jay Kuo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.04892">LG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nuclei segmentation is the cornerstone task in histology image reading, shedding light on the underlying molecular patterns and leading to disease or cancer diagnosis. Yet, it is a laborious task that requires expertise from trained physicians. The large nuclei variability across different organ tissues and acquisition processes challenges the automation of this task. On the other hand, data annotations are expensive to obtain, and thus, Deep Learning (DL) models are challenged to generalize to unseen organs or different domains. This work proposes Local-to-Global NuSegHop (LG-NuSegHop), a self-supervised pipeline developed on prior knowledge of the problem and molecular biology. There are three distinct modules: (1) a set of local processing operations to generate a pseudolabel, (2) NuSegHop a novel data-driven feature extraction model and (3) a set of global operations to post-process the predictions of NuSegHop. Notably, even though the proposed pipeline uses { no manually annotated training data} or domain adaptation, it maintains a good generalization performance on other datasets. Experiments in three publicly available datasets show that our method outperforms other self-supervised and weakly supervised methods while having a competitive standing among fully supervised methods. Remarkably, every module within LG-NuSegHop is transparent and explainable to physicians.
<div id='section'>Paperid: <span id='pid'>1349, <a href='https://arxiv.org/pdf/2510.25134.pdf' target='_blank'>https://arxiv.org/pdf/2510.25134.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingdong Cai, Charith Abhayaratne
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.25134">Region-CAM: Towards Accurate Object Regions in Class Activation Maps for Weakly Supervised Learning Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Class Activation Mapping (CAM) methods are widely applied in weakly supervised learning tasks due to their ability to highlight object regions. However, conventional CAM methods highlight only the most discriminative regions of the target. These highlighted regions often fail to cover the entire object and are frequently misaligned with object boundaries, thereby limiting the performance of downstream weakly supervised learning tasks, particularly Weakly Supervised Semantic Segmentation (WSSS), which demands pixel-wise accurate activation maps to get the best results. To alleviate the above problems, we propose a novel activation method, Region-CAM. Distinct from network feature weighting approaches, Region-CAM generates activation maps by extracting semantic information maps (SIMs) and performing semantic information propagation (SIP) by considering both gradients and features in each of the stages of the baseline classification model. Our approach highlights a greater proportion of object regions while ensuring activation maps to have precise boundaries that align closely with object edges. Region-CAM achieves 60.12% and 58.43% mean intersection over union (mIoU) using the baseline model on the PASCAL VOC training and validation datasets, respectively, which are improvements of 13.61% and 13.13% over the original CAM (46.51% and 45.30%). On the MS COCO validation set, Region-CAM achieves 36.38%, a 16.23% improvement over the original CAM (20.15%). We also demonstrate the superiority of Region-CAM in object localization tasks, using the ILSVRC2012 validation set. Region-CAM achieves 51.7% in Top-1 Localization accuracy Loc1. Compared with LayerCAM, an activation method designed for weakly supervised object localization, Region-CAM achieves 4.5% better performance in Loc1.
<div id='section'>Paperid: <span id='pid'>1350, <a href='https://arxiv.org/pdf/2508.15646.pdf' target='_blank'>https://arxiv.org/pdf/2508.15646.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Swann Emilien CÃ©leste Destouches, Jesse Lahaye, Laurent Valentin Jospin, Jan Skaloud
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15646">Weakly-Supervised Learning for Tree Instances Segmentation in Airborne Lidar Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tree instance segmentation of airborne laser scanning (ALS) data is of utmost importance for forest monitoring, but remains challenging due to variations in the data caused by factors such as sensor resolution, vegetation state at acquisition time, terrain characteristics, etc. Moreover, obtaining a sufficient amount of precisely labeled data to train fully supervised instance segmentation methods is expensive. To address these challenges, we propose a weakly supervised approach where labels of an initial segmentation result obtained either by a non-finetuned model or a closed form algorithm are provided as a quality rating by a human operator. The labels produced during the quality assessment are then used to train a rating model, whose task is to classify a segmentation output into the same classes as specified by the human operator. Finally, the segmentation model is finetuned using feedback from the rating model. This in turn improves the original segmentation model by 34\% in terms of correctly identified tree instances while considerably reducing the number of non-tree instances predicted. Challenges still remain in data over sparsely forested regions characterized by small trees (less than two meters in height) or within complex surroundings containing shrubs, boulders, etc. which can be confused as trees where the performance of the proposed method is reduced.
<div id='section'>Paperid: <span id='pid'>1351, <a href='https://arxiv.org/pdf/2508.09665.pdf' target='_blank'>https://arxiv.org/pdf/2508.09665.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmed Alharbi, Hai Dong, Xun Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09665">Social-Sensor Identity Cloning Detection Using Weakly Supervised Deep Forest and Cryptographic Authentication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent years have witnessed a rising trend in social-sensor cloud identity cloning incidents. However, existing approaches suffer from unsatisfactory performance, a lack of solutions for detecting duplicated accounts, and a lack of large-scale evaluations on real-world datasets. We introduce a novel method for detecting identity cloning in social-sensor cloud service providers. Our proposed technique consists of two primary components: 1) a similar identity detection method and 2) a cryptography-based authentication protocol. Initially, we developed a weakly supervised deep forest model to identify similar identities using non-privacy-sensitive user profile features provided by the service. Subsequently, we designed a cryptography-based authentication protocol to verify whether similar identities were generated by the same provider. Our extensive experiments on a large real-world dataset demonstrate the feasibility and superior performance of our technique compared to current state-of-the-art identity clone detection methods.
<div id='section'>Paperid: <span id='pid'>1352, <a href='https://arxiv.org/pdf/2508.08912.pdf' target='_blank'>https://arxiv.org/pdf/2508.08912.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mahmoud Salhab, Shameed Sait, Mohammad Abusheikh, Hasan Abusheikh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08912">Munsit at NADI 2025 Shared Task 2: Pushing the Boundaries of Multidialectal Arabic ASR with Weakly Supervised Pretraining and Continual Supervised Fine-tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic speech recognition (ASR) plays a vital role in enabling natural human-machine interaction across applications such as virtual assistants, industrial automation, customer support, and real-time transcription. However, developing accurate ASR systems for low-resource languages like Arabic remains a significant challenge due to limited labeled data and the linguistic complexity introduced by diverse dialects. In this work, we present a scalable training pipeline that combines weakly supervised learning with supervised fine-tuning to develop a robust Arabic ASR model. In the first stage, we pretrain the model on 15,000 hours of weakly labeled speech covering both Modern Standard Arabic (MSA) and various Dialectal Arabic (DA) variants. In the subsequent stage, we perform continual supervised fine-tuning using a mixture of filtered weakly labeled data and a small, high-quality annotated dataset. Our approach achieves state-of-the-art results, ranking first in the multi-dialectal Arabic ASR challenge. These findings highlight the effectiveness of weak supervision paired with fine-tuning in overcoming data scarcity and delivering high-quality ASR for low-resource, dialect-rich languages.
<div id='section'>Paperid: <span id='pid'>1353, <a href='https://arxiv.org/pdf/2508.06819.pdf' target='_blank'>https://arxiv.org/pdf/2508.06819.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ayaan Nooruddin Siddiqui, Mahnoor Zaidi, Ayesha Nazneen Shahbaz, Priyadarshini Chatterjee, Krishnan Menon Iyer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06819">VesselRW: Weakly Supervised Subcutaneous Vessel Segmentation via Learned Random Walk Propagation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The task of parsing subcutaneous vessels in clinical images is often hindered by the high cost and limited availability of ground truth data, as well as the challenge of low contrast and noisy vessel appearances across different patients and imaging modalities. In this work, we propose a novel weakly supervised training framework specifically designed for subcutaneous vessel segmentation. This method utilizes low-cost, sparse annotations such as centerline traces, dot markers, or short scribbles to guide the learning process. These sparse annotations are expanded into dense probabilistic supervision through a differentiable random walk label propagation model, which integrates vesselness cues and tubular continuity priors driven by image data. The label propagation process results in per-pixel hitting probabilities and uncertainty estimates, which are incorporated into an uncertainty-weighted loss function to prevent overfitting in ambiguous areas. Notably, the label propagation model is trained jointly with a CNN-based segmentation network, allowing the system to learn vessel boundaries and continuity constraints without the need for explicit edge supervision. Additionally, we introduce a topology-aware regularizer that encourages centerline connectivity and penalizes irrelevant branches, further enhancing clinical applicability. Our experiments on clinical subcutaneous imaging datasets demonstrate that our approach consistently outperforms both naive sparse-label training and traditional dense pseudo-labeling methods, yielding more accurate vascular maps and better-calibrated uncertainty, which is crucial for clinical decision-making. This method significantly reduces the annotation workload while maintaining clinically relevant vessel topology.
<div id='section'>Paperid: <span id='pid'>1354, <a href='https://arxiv.org/pdf/2507.22002.pdf' target='_blank'>https://arxiv.org/pdf/2507.22002.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yida Tao, Yen-Chia Hsu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22002">Bridging Synthetic and Real-World Domains: A Human-in-the-Loop Weakly-Supervised Framework for Industrial Toxic Emission Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industrial smoke segmentation is critical for air-quality monitoring and environmental protection but is often hampered by the high cost and scarcity of pixel-level annotations in real-world settings. We introduce CEDANet, a human-in-the-loop, class-aware domain adaptation framework that uniquely integrates weak, citizen-provided video-level labels with adversarial feature alignment. Specifically, we refine pseudo-labels generated by a source-trained segmentation model using citizen votes, and employ class-specific domain discriminators to transfer rich source-domain representations to the industrial domain. Comprehensive experiments on SMOKE5K and custom IJmond datasets demonstrate that CEDANet achieves an F1-score of 0.414 and a smoke-class IoU of 0.261 with citizen feedback, vastly outperforming the baseline model, which scored 0.083 and 0.043 respectively. This represents a five-fold increase in F1-score and a six-fold increase in smoke-class IoU. Notably, CEDANet with citizen-constrained pseudo-labels achieves performance comparable to the same architecture trained on limited 100 fully annotated images with F1-score of 0.418 and IoU of 0.264, demonstrating its ability to reach small-sampled fully supervised-level accuracy without target-domain annotations. Our research validates the scalability and cost-efficiency of combining citizen science with weakly supervised domain adaptation, offering a practical solution for complex, data-scarce environmental monitoring applications.
<div id='section'>Paperid: <span id='pid'>1355, <a href='https://arxiv.org/pdf/2507.03292.pdf' target='_blank'>https://arxiv.org/pdf/2507.03292.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pattaramanee Arsomngern, Sasikarn Khwanmuang, Matthias NieÃner, Supasorn Suwajanakorn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03292">Zero-shot Inexact CAD Model Alignment from a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One practical approach to infer 3D scene structure from a single image is to retrieve a closely matching 3D model from a database and align it with the object in the image. Existing methods rely on supervised training with images and pose annotations, which limits them to a narrow set of object categories. To address this, we propose a weakly supervised 9-DoF alignment method for inexact 3D models that requires no pose annotations and generalizes to unseen categories. Our approach derives a novel feature space based on foundation features that ensure multi-view consistency and overcome symmetry ambiguities inherent in foundation features using a self-supervised triplet loss. Additionally, we introduce a texture-invariant pose refinement technique that performs dense alignment in normalized object coordinates, estimated through the enhanced feature space. We conduct extensive evaluations on the real-world ScanNet25k dataset, where our method outperforms SOTA weakly supervised baselines by +4.3% mean alignment accuracy and is the only weakly supervised approach to surpass the supervised ROCA by +2.7%. To assess generalization, we introduce SUN2CAD, a real-world test set with 20 novel object categories, where our method achieves SOTA results without prior training on them.
<div id='section'>Paperid: <span id='pid'>1356, <a href='https://arxiv.org/pdf/2506.16745.pdf' target='_blank'>https://arxiv.org/pdf/2506.16745.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi-Ying Sun, Wan-Lei Zhao, Yi-Bo Miao, Chong-Wah Ngo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.16745">Class Agnostic Instance-level Descriptor for Visual Instance Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the great success of the deep features in content-based image retrieval, the visual instance search remains challenging due to the lack of effective instance level feature representation. Supervised or weakly supervised object detection methods are not among the options due to their poor performance on the unknown object categories. In this paper, based on the feature set output from self-supervised ViT, the instance level region discovery is modeled as detecting the compact feature subsets in a hierarchical fashion. The hierarchical decomposition results in a hierarchy of feature subsets. The non-leaf nodes and leaf nodes on the hierarchy correspond to the various instance regions in an image of different semantic scales. The hierarchical decomposition well addresses the problem of object embedding and occlusions, which are widely observed in the real scenarios. The features derived from the nodes on the hierarchy make up a comprehensive representation for the latent instances in the image. Our instance-level descriptor remains effective on both the known and unknown object categories. Empirical studies on three instance search benchmarks show that it outperforms state-of-the-art methods considerably.
<div id='section'>Paperid: <span id='pid'>1357, <a href='https://arxiv.org/pdf/2504.14860.pdf' target='_blank'>https://arxiv.org/pdf/2504.14860.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyi Liu, Yangcen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14860">Bridge the Gap: From Weak to Full Supervision for Temporal Action Localization with PseudoFormer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised Temporal Action Localization (WTAL) has achieved notable success but still suffers from a lack of temporal annotations, leading to a performance and framework gap compared with fully-supervised methods. While recent approaches employ pseudo labels for training, three key challenges: generating high-quality pseudo labels, making full use of different priors, and optimizing training methods with noisy labels remain unresolved. Due to these perspectives, we propose PseudoFormer, a novel two-branch framework that bridges the gap between weakly and fully-supervised Temporal Action Localization (TAL). We first introduce RickerFusion, which maps all predicted action proposals to a global shared space to generate pseudo labels with better quality. Subsequently, we leverage both snippet-level and proposal-level labels with different priors from the weak branch to train the regression-based model in the full branch. Finally, the uncertainty mask and iterative refinement mechanism are applied for training with noisy pseudo labels. PseudoFormer achieves state-of-the-art WTAL results on the two commonly used benchmarks, THUMOS14 and ActivityNet1.3. Besides, extensive ablation studies demonstrate the contribution of each component of our method.
<div id='section'>Paperid: <span id='pid'>1358, <a href='https://arxiv.org/pdf/2504.12254.pdf' target='_blank'>https://arxiv.org/pdf/2504.12254.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mahmoud Salhab, Marwan Elghitany, Shameed Sait, Syed Sibghat Ullah, Mohammad Abusheikh, Hasan Abusheikh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12254">Advancing Arabic Speech Recognition Through Large-Scale Weakly Supervised Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic speech recognition (ASR) is crucial for human-machine interaction in diverse applications like conversational agents, industrial robotics, call center automation, and automated subtitling. However, developing high-performance ASR models remains challenging, particularly for low-resource languages like Arabic, due to the scarcity of large, labeled speech datasets, which are costly and labor-intensive to produce. In this work, we employ weakly supervised learning to train an Arabic ASR model using the Conformer architecture. Our model is trained from scratch on 15,000 hours of weakly annotated speech data covering both Modern Standard Arabic (MSA) and Dialectal Arabic (DA), eliminating the need for costly manual transcriptions. Despite the absence of human-verified labels, our approach achieves state-of-the-art (SOTA) results in Arabic ASR, surpassing both open and closed-source models on standard benchmarks. By demonstrating the effectiveness of weak supervision as a scalable, cost-efficient alternative to traditional supervised approaches, paving the way for improved ASR systems in low resource settings.
<div id='section'>Paperid: <span id='pid'>1359, <a href='https://arxiv.org/pdf/2504.10613.pdf' target='_blank'>https://arxiv.org/pdf/2504.10613.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xing David Wang, Ulf Leser
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10613">Enhancing Document Retrieval for Curating N-ary Relations in Knowledge Bases</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Curation of biomedical knowledge bases (KBs) relies on extracting accurate multi-entity relational facts from the literature - a process that remains largely manual and expert-driven. An essential step in this workflow is retrieving documents that can support or complete partially observed n-ary relations. We present a neural retrieval model designed to assist KB curation by identifying documents that help fill in missing relation arguments and provide relevant contextual evidence.
  To reduce dependence on scarce gold-standard training data, we exploit existing KB records to construct weakly supervised training sets. Our approach introduces two key technical contributions: (i) a layered contrastive loss that enables learning from noisy and incomplete relational structures, and (ii) a balanced sampling strategy that generates high-quality negatives from diverse KB records. On two biomedical retrieval benchmarks, our approach achieves state-of-the-art performance, outperforming strong baselines in NDCG@10 by 5.7 and 3.7 percentage points, respectively.
<div id='section'>Paperid: <span id='pid'>1360, <a href='https://arxiv.org/pdf/2503.22668.pdf' target='_blank'>https://arxiv.org/pdf/2503.22668.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sindhu B Hegde, K R Prajwal, Taein Kwon, Andrew Zisserman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22668">Understanding Co-speech Gestures in-the-wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Co-speech gestures play a vital role in non-verbal communication. In this paper, we introduce a new framework for co-speech gesture understanding in the wild. Specifically, we propose three new tasks and benchmarks to evaluate a model's capability to comprehend gesture-speech-text associations: (i) gesture based retrieval, (ii) gesture word spotting, and (iii) active speaker detection using gestures. We present a new approach that learns a tri-modal video-gesture-speech-text representation to solve these tasks. By leveraging a combination of global phrase contrastive loss and local gesture-word coupling loss, we demonstrate that a strong gesture representation can be learned in a weakly supervised manner from videos in the wild. Our learned representations outperform previous methods, including large vision-language models (VLMs). Further analysis reveals that speech and text modalities capture distinct gesture related signals, underscoring the advantages of learning a shared tri-modal embedding space. The dataset, model, and code are available at: https://www.robots.ox.ac.uk/~vgg/research/jegal.
<div id='section'>Paperid: <span id='pid'>1361, <a href='https://arxiv.org/pdf/2502.17836.pdf' target='_blank'>https://arxiv.org/pdf/2502.17836.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Nawaz, Basma Nasir, Tehseen Zia, Zawar Hussain, Catarina Moreira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17836">TagGAN: A Generative Model for Data Tagging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Precise identification and localization of disease-specific features at the pixel-level are particularly important for early diagnosis, disease progression monitoring, and effective treatment in medical image analysis. However, conventional diagnostic AI systems lack decision transparency and cannot operate well in environments where there is a lack of pixel-level annotations. In this study, we propose a novel Generative Adversarial Networks (GANs)-based framework, TagGAN, which is tailored for weakly-supervised fine-grained disease map generation from purely image-level labeled data. TagGAN generates a pixel-level disease map during domain translation from an abnormal image to a normal representation. Later, this map is subtracted from the input abnormal image to convert it into its normal counterpart while preserving all the critical anatomical details. Our method is first to generate fine-grained disease maps to visualize disease lesions in a weekly supervised setting without requiring pixel-level annotations. This development enhances the interpretability of diagnostic AI by providing precise visualizations of disease-specific regions. It also introduces automated binary mask generation to assist radiologists. Empirical evaluations carried out on the benchmark datasets, CheXpert, TBX11K, and COVID-19, demonstrate the capability of TagGAN to outperform current top models in accurately identifying disease-specific pixels. This outcome highlights the capability of the proposed model to tag medical images, significantly reducing the workload for radiologists by eliminating the need for binary masks during training.
<div id='section'>Paperid: <span id='pid'>1362, <a href='https://arxiv.org/pdf/2502.17824.pdf' target='_blank'>https://arxiv.org/pdf/2502.17824.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Basma Nasir, Tehseen Zia, Muhammad Nawaz, Catarina Moreira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17824">Weakly Supervised Pixel-Level Annotation with Visual Interpretability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical image annotation is essential for diagnosing diseases, yet manual annotation is time-consuming, costly, and prone to variability among experts. To address these challenges, we propose an automated explainable annotation system that integrates ensemble learning, visual explainability, and uncertainty quantification. Our approach combines three pre-trained deep learning models - ResNet50, EfficientNet, and DenseNet - enhanced with XGrad-CAM for visual explanations and Monte Carlo Dropout for uncertainty quantification. This ensemble mimics the consensus of multiple radiologists by intersecting saliency maps from models that agree on the diagnosis while uncertain predictions are flagged for human review. We evaluated our system using the TBX11K medical imaging dataset and a Fire segmentation dataset, demonstrating its robustness across different domains. Experimental results show that our method outperforms baseline models, achieving 93.04% accuracy on TBX11K and 96.4% accuracy on the Fire dataset. Moreover, our model produces precise pixel-level annotations despite being trained with only image-level labels, achieving Intersection over Union IoU scores of 36.07% and 64.7%, respectively. By enhancing the accuracy and interpretability of image annotations, our approach offers a reliable and transparent solution for medical diagnostics and other image analysis tasks.
<div id='section'>Paperid: <span id='pid'>1363, <a href='https://arxiv.org/pdf/2501.04582.pdf' target='_blank'>https://arxiv.org/pdf/2501.04582.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Miaoyang He, Shuyong Gao, Tsui Qin Mok, Weifeng Ge, Wengqiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04582">Boosting Salient Object Detection with Knowledge Distillated from Large Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Salient Object Detection (SOD) aims to identify and segment prominent regions within a scene. Traditional models rely on manually annotated pseudo labels with precise pixel-level accuracy, which is time-consuming. We developed a low-cost, high-precision annotation method by leveraging large foundation models to address the challenges. Specifically, we use a weakly supervised approach to guide large models in generating pseudo-labels through textual prompts. Since large models do not effectively focus on the salient regions of images, we manually annotate a subset of text to fine-tune the model. Based on this approach, which enables precise and rapid generation of pseudo-labels, we introduce a new dataset, BDS-TR. Compared to the previous DUTS-TR dataset, BDS-TR is more prominent in scale and encompasses a wider variety of categories and scenes. This expansion will enhance our model's applicability across a broader range of scenarios and provide a more comprehensive foundational dataset for future SOD research. Additionally, we present an edge decoder based on dynamic upsampling, which focuses on object edges while gradually recovering image feature resolution. Comprehensive experiments on five benchmark datasets demonstrate that our method significantly outperforms state-of-the-art approaches and also surpasses several existing fully-supervised SOD methods. The code and results will be made available.
<div id='section'>Paperid: <span id='pid'>1364, <a href='https://arxiv.org/pdf/2412.14561.pdf' target='_blank'>https://arxiv.org/pdf/2412.14561.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jintao Huang, Yiu-ming Cheung, Chi-man Vong, Wenbin Qian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14561">GBRIP: Granular Ball Representation for Imbalanced Partial Label Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partial label learning (PLL) is a complicated weakly supervised multi-classification task compounded by class imbalance. Currently, existing methods only rely on inter-class pseudo-labeling from inter-class features, often overlooking the significant impact of the intra-class imbalanced features combined with the inter-class. To address these limitations, we introduce Granular Ball Representation for Imbalanced PLL (GBRIP), a novel framework for imbalanced PLL. GBRIP utilizes coarse-grained granular ball representation and multi-center loss to construct a granular ball-based nfeature space through unsupervised learning, effectively capturing the feature distribution within each class. GBRIP mitigates the impact of confusing features by systematically refining label disambiguation and estimating imbalance distributions. The novel multi-center loss function enhances learning by emphasizing the relationships between samples and their respective centers within the granular balls. Extensive experiments on standard benchmarks demonstrate that GBRIP outperforms existing state-of-the-art methods, offering a robust solution to the challenges of imbalanced PLL.
<div id='section'>Paperid: <span id='pid'>1365, <a href='https://arxiv.org/pdf/2412.12331.pdf' target='_blank'>https://arxiv.org/pdf/2412.12331.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>PhÃºc H. Le Khac, Graham Healy, Alan F. Smeaton
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12331">Efficient Object-centric Representation Learning with Pre-trained Geometric Prior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses key challenges in object-centric representation learning of video. While existing approaches struggle with complex scenes, we propose a novel weakly-supervised framework that emphasises geometric understanding and leverages pre-trained vision models to enhance object discovery. Our method introduces an efficient slot decoder specifically designed for object-centric learning, enabling effective representation of multi-object scenes without requiring explicit depth information. Results on synthetic video benchmarks with increasing complexity in terms of objects and their movement, object occlusion and camera motion demonstrate that our approach achieves comparable performance to supervised methods while maintaining computational efficiency. This advances the field towards more practical applications in complex real-world scenarios.
<div id='section'>Paperid: <span id='pid'>1366, <a href='https://arxiv.org/pdf/2412.11467.pdf' target='_blank'>https://arxiv.org/pdf/2412.11467.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuyang Xie, Yan Yang, Yankai Yu, Jie Wang, Yongquan Jiang, Xiao Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11467">Exploring Temporal Event Cues for Dense Video Captioning in Cyclic Co-learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dense video captioning aims to detect and describe all events in untrimmed videos. This paper presents a dense video captioning network called Multi-Concept Cyclic Learning (MCCL), which aims to: (1) detect multiple concepts at the frame level, using these concepts to enhance video features and provide temporal event cues; and (2) design cyclic co-learning between the generator and the localizer within the captioning network to promote semantic perception and event localization. Specifically, we perform weakly supervised concept detection for each frame, and the detected concept embeddings are integrated into the video features to provide event cues. Additionally, video-level concept contrastive learning is introduced to obtain more discriminative concept embeddings. In the captioning network, we establish a cyclic co-learning strategy where the generator guides the localizer for event localization through semantic matching, while the localizer enhances the generator's event semantic perception through location matching, making semantic perception and event localization mutually beneficial. MCCL achieves state-of-the-art performance on the ActivityNet Captions and YouCook2 datasets. Extensive experiments demonstrate its effectiveness and interpretability.
<div id='section'>Paperid: <span id='pid'>1367, <a href='https://arxiv.org/pdf/2411.13528.pdf' target='_blank'>https://arxiv.org/pdf/2411.13528.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>James Willoughby, Irina Voiculescu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13528">Entropy Bootstrapping for Weakly Supervised Nuclei Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Microscopy structure segmentation, such as detecting cells or nuclei, generally requires a human to draw a ground truth contour around each instance. Weakly supervised approaches (e.g. consisting of only single point labels) have the potential to reduce this workload significantly. Our approach uses individual point labels for an entropy estimation to approximate an underlying distribution of cell pixels. We infer full cell masks from this distribution, and use Mask-RCNN to produce an instance segmentation output. We compare this point--annotated approach with training on the full ground truth masks. We show that our method achieves a comparatively good level of performance, despite a 95% reduction in pixel labels.
<div id='section'>Paperid: <span id='pid'>1368, <a href='https://arxiv.org/pdf/2410.19332.pdf' target='_blank'>https://arxiv.org/pdf/2410.19332.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianning Chi, Zelan Li, Huixuan Wu, Wenjun Zhang, Ying Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.19332">Beyond Point Annotation: A Weakly Supervised Network Guided by Multi-Level Labels Generated from Four-Point Annotation for Thyroid Nodule Segmentation in Ultrasound Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised methods typically guided the pixel-wise training by comparing the predictions to single-level labels containing diverse segmentation-related information at once, but struggled to represent delicate feature differences between nodule and background regions and confused incorrect information, resulting in underfitting or overfitting in the segmentation predictions. In this work, we propose a weakly-supervised network that generates multi-level labels from four-point annotation to refine diverse constraints for delicate nodule segmentation. The Distance-Similarity Fusion Prior referring to the points annotations filters out information irrelevant to nodules. The bounding box and pure foreground/background labels, generated from the point annotation, guarantee the rationality of the prediction in the arrangement of target localization and the spatial distribution of target/background regions, respectively. Our proposed network outperforms existing weakly-supervised methods on two public datasets with respect to the accuracy and robustness, improving the applicability of deep-learning based segmentation in the clinical practice of thyroid nodule diagnosis.
<div id='section'>Paperid: <span id='pid'>1369, <a href='https://arxiv.org/pdf/2408.13639.pdf' target='_blank'>https://arxiv.org/pdf/2408.13639.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Yuan, Tania Stathaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.13639">Size Aware Cross-shape Scribble Supervision for Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scribble supervision, a common form of weakly supervised learning, involves annotating pixels using hand-drawn curve lines, which helps reduce the cost of manual labelling. This technique has been widely used in medical image segmentation tasks to fasten network training. However, scribble supervision has limitations in terms of annotation consistency across samples and the availability of comprehensive groundtruth information. Additionally, it often grapples with the challenge of accommodating varying scale targets, particularly in the context of medical images. In this paper, we propose three novel methods to overcome these challenges, namely, 1) the cross-shape scribble annotation method; 2) the pseudo mask method based on cross shapes; and 3) the size-aware multi-branch method. The parameter and structure design are investigated in depth. Experimental results show that the proposed methods have achieved significant improvement in mDice scores across multiple polyp datasets. Notably, the combination of these methods outperforms the performance of state-of-the-art scribble supervision methods designed for medical image segmentation.
<div id='section'>Paperid: <span id='pid'>1370, <a href='https://arxiv.org/pdf/2406.00891.pdf' target='_blank'>https://arxiv.org/pdf/2406.00891.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin-Yi Tong, Runmin Dong, Xiao Xiang Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00891">Global High Categorical Resolution Land Cover Mapping via Weak Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Land cover information is indispensable for advancing the United Nations' sustainable development goals, and land cover mapping under a more detailed category system would significantly contribute to economic livelihood tracking and environmental degradation measurement. However, the substantial difficulty in acquiring fine-grained training data makes the implementation of this task particularly challenging. Here, we propose to combine fully labeled source domain and weakly labeled target domain for weakly supervised domain adaptation (WSDA). This is beneficial as the utilization of sparse and coarse weak labels can considerably alleviate the labor required for precise and detailed land cover annotation. Specifically, we introduce the Prototype-based pseudo-label Rectification and Expansion (PRE) approach, which leverages the prototypes (i.e., the class-wise feature centroids) as the bridge to connect sparse labels and global feature distributions. According to the feature distances to the prototypes, the confidence of pseudo-labels predicted in the unlabeled regions of the target domain is assessed. This confidence is then utilized to guide the dynamic expansion and rectification of pseudo-labels. Based on PRE, we carry out high categorical resolution land cover mapping for 10 cities in different regions around the world, severally using PlanetScope, Gaofen-1, and Sentinel-2 satellite images. In the study areas, we achieve cross-sensor, cross-category, and cross-continent WSDA, with the overall accuracy exceeding 80%. The promising results indicate that PRE is capable of reducing the dependency of land cover classification on high-quality annotations, thereby improving label efficiency. We expect our work to enable global fine-grained land cover mapping, which in turn promote Earth observation to provide more precise and thorough information for environmental monitoring.
<div id='section'>Paperid: <span id='pid'>1371, <a href='https://arxiv.org/pdf/2405.09142.pdf' target='_blank'>https://arxiv.org/pdf/2405.09142.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jenthe Thienpondt, Kris Demuynck
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.09142">Speaker Embeddings With Weakly Supervised Voice Activity Detection For Efficient Speaker Diarization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current speaker diarization systems rely on an external voice activity detection model prior to speaker embedding extraction on the detected speech segments. In this paper, we establish that the attention system of a speaker embedding extractor acts as a weakly supervised internal VAD model and performs equally or better than comparable supervised VAD systems. Subsequently, speaker diarization can be performed efficiently by extracting the VAD logits and corresponding speaker embedding simultaneously, alleviating the need and computational overhead of an external VAD model. We provide an extensive analysis of the behavior of the frame-level attention system in current speaker verification models and propose a novel speaker diarization pipeline using ECAPA2 speaker embeddings for both VAD and embedding extraction. The proposed strategy gains state-of-the-art performance on the AMI, VoxConverse and DIHARD III diarization benchmarks.
<div id='section'>Paperid: <span id='pid'>1372, <a href='https://arxiv.org/pdf/2404.04983.pdf' target='_blank'>https://arxiv.org/pdf/2404.04983.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>AurÃ©lie BeaufrÃ¨re, Nora Ouzir, Paul Emile Zafar, Astrid Laurent-Bellue, Miguel Albuquerque, Gwladys Lubuela, Jules GrÃ©gory, Catherine Guettier, KÃ©vin Mondet, Jean-Christophe Pesquet, ValÃ©rie Paradis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.04983">Primary liver cancer classification from routine tumour biopsy using weakly supervised deep learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The diagnosis of primary liver cancers (PLCs) can be challenging, especially on biopsies and for combined hepatocellular-cholangiocarcinoma (cHCC-CCA). We automatically classified PLCs on routine-stained biopsies using a weakly supervised learning method. Weak tumour/non-tumour annotations served as labels for training a Resnet18 neural network, and the network's last convolutional layer was used to extract new tumour tile features. Without knowledge of the precise labels of the malignancies, we then applied an unsupervised clustering algorithm. Our model identified specific features of hepatocellular carcinoma (HCC) and intrahepatic cholangiocarcinoma (iCCA). Despite no specific features of cHCC-CCA being recognized, the identification of HCC and iCCA tiles within a slide could facilitate the diagnosis of primary liver cancers, particularly cHCC-CCA.
  Method and results: 166 PLC biopsies were divided into training, internal and external validation sets: 90, 29 and 47 samples. Two liver pathologists reviewed each whole-slide hematein eosin saffron (HES)-stained image (WSI). After annotating the tumour/non-tumour areas, 256x256 pixel tiles were extracted from the WSIs and used to train a ResNet18. The network was used to extract new tile features. An unsupervised clustering algorithm was then applied to the new tile features. In a two-cluster model, Clusters 0 and 1 contained mainly HCC and iCCA histological features. The diagnostic agreement between the pathological diagnosis and the model predictions in the internal and external validation sets was 100% (11/11) and 96% (25/26) for HCC and 78% (7/9) and 87% (13/15) for iCCA, respectively. For cHCC-CCA, we observed a highly variable proportion of tiles from each cluster (Cluster 0: 5-97%; Cluster 1: 2-94%).
<div id='section'>Paperid: <span id='pid'>1373, <a href='https://arxiv.org/pdf/2404.01446.pdf' target='_blank'>https://arxiv.org/pdf/2404.01446.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Martim Afonso, Praphulla M. S. Bhawsar, Monjoy Saha, Jonas S. Almeida, Arlindo L. Oliveira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01446">Finding Regions of Interest in Whole Slide Images Using Multiple Instance Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Whole Slide Images (WSI), obtained by high-resolution digital scanning of microscope slides at multiple scales, are the cornerstone of modern Digital Pathology. However, they represent a particular challenge to AI-based/AI-mediated analysis because pathology labeling is typically done at slide-level, instead of tile-level. It is not just that medical diagnostics is recorded at the specimen level, the detection of oncogene mutation is also experimentally obtained, and recorded by initiatives like The Cancer Genome Atlas (TCGA), at the slide level. This configures a dual challenge: a) accurately predicting the overall cancer phenotype and b) finding out what cellular morphologies are associated with it at the tile level. To address these challenges, a weakly supervised Multiple Instance Learning (MIL) approach was explored for two prevalent cancer types, Invasive Breast Carcinoma (TCGA-BRCA) and Lung Squamous Cell Carcinoma (TCGA-LUSC). This approach was explored for tumor detection at low magnification levels and TP53 mutations at various levels. Our results show that a novel additive implementation of MIL matched the performance of reference implementation (AUC 0.96), and was only slightly outperformed by Attention MIL (AUC 0.97). More interestingly from the perspective of the molecular pathologist, these different AI architectures identify distinct sensitivities to morphological features (through the detection of Regions of Interest, RoI) at different amplification levels. Tellingly, TP53 mutation was most sensitive to features at the higher applications where cellular morphology is resolved.
<div id='section'>Paperid: <span id='pid'>1374, <a href='https://arxiv.org/pdf/2404.00626.pdf' target='_blank'>https://arxiv.org/pdf/2404.00626.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minyoung Oh, Duhyun Kim, Jae-Young Sim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00626">Domain Generalizable Person Search Using Unreal Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collecting and labeling real datasets to train the person search networks not only requires a lot of time and effort, but also accompanies privacy issues. The weakly-supervised and unsupervised domain adaptation methods have been proposed to alleviate the labeling burden for target datasets, however, their generalization capability is limited. We introduce a novel person search method based on the domain generalization framework, that uses an automatically labeled unreal dataset only for training but is applicable to arbitrary unseen real datasets. To alleviate the domain gaps when transferring the knowledge from the unreal source dataset to the real target datasets, we estimate the fidelity of person instances which is then used to train the end-to-end network adaptively. Moreover, we devise a domain-invariant feature learning scheme to encourage the network to suppress the domain-related features. Experimental results demonstrate that the proposed method provides the competitive performance to existing person search methods even though it is applicable to arbitrary unseen datasets without any prior knowledge and re-training burdens.
<div id='section'>Paperid: <span id='pid'>1375, <a href='https://arxiv.org/pdf/2403.18600.pdf' target='_blank'>https://arxiv.org/pdf/2403.18600.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Zare, Yulei Niu, Hammad Ayyubi, Shih-fu Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.18600">RAP: Retrieval-Augmented Planner for Adaptive Procedure Planning in Instructional Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Procedure Planning in instructional videos entails generating a sequence of action steps based on visual observations of the initial and target states. Despite the rapid progress in this task, there remain several critical challenges to be solved: (1) Adaptive procedures: Prior works hold an unrealistic assumption that the number of action steps is known and fixed, leading to non-generalizable models in real-world scenarios where the sequence length varies. (2) Temporal relation: Understanding the step temporal relation knowledge is essential in producing reasonable and executable plans. (3) Annotation cost: Annotating instructional videos with step-level labels (i.e., timestamp) or sequence-level labels (i.e., action category) is demanding and labor-intensive, limiting its generalizability to large-scale datasets. In this work, we propose a new and practical setting, called adaptive procedure planning in instructional videos, where the procedure length is not fixed or pre-determined. To address these challenges, we introduce Retrieval-Augmented Planner (RAP) model. Specifically, for adaptive procedures, RAP adaptively determines the conclusion of actions using an auto-regressive model architecture. For temporal relation, RAP establishes an external memory module to explicitly retrieve the most relevant state-action pairs from the training videos and revises the generated procedures. To tackle high annotation cost, RAP utilizes a weakly-supervised learning manner to expand the training dataset to other task-relevant, unannotated videos by generating pseudo labels for action steps. Experiments on CrossTask and COIN benchmarks show the superiority of RAP over traditional fixed-length models, establishing it as a strong baseline solution for adaptive procedure planning.
<div id='section'>Paperid: <span id='pid'>1376, <a href='https://arxiv.org/pdf/2403.15238.pdf' target='_blank'>https://arxiv.org/pdf/2403.15238.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abhinav Sharma, Bojing Liu, Mattias Rantalainen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15238">WEEP: A method for spatial interpretation of weakly supervised CNN models in computational pathology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning enables the modelling of high-resolution histopathology whole-slide images (WSI). Weakly supervised learning of tile-level data is typically applied for tasks where labels only exist on the patient or WSI level (e.g. patient outcomes or histological grading). In this context, there is a need for improved spatial interpretability of predictions from such models. We propose a novel method, Wsi rEgion sElection aPproach (WEEP), for model interpretation. It provides a principled yet straightforward way to establish the spatial area of WSI required for assigning a particular prediction label. We demonstrate WEEP on a binary classification task in the area of breast cancer computational pathology. WEEP is easy to implement, is directly connected to the model-based decision process, and offers information relevant to both research and diagnostic applications.
<div id='section'>Paperid: <span id='pid'>1377, <a href='https://arxiv.org/pdf/2402.08333.pdf' target='_blank'>https://arxiv.org/pdf/2402.08333.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Antoine Habis, Roy Rosman Nathanson, Vannary Meas-Yedid, Elsa D. Angelini, Jean-Christophe Olivo-Marin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.08333">Scribble-based fast weak-supervision and interactive corrections for segmenting whole slide images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a dynamic interactive and weakly supervised segmentation method with minimal user interactions to address two major challenges in the segmentation of whole slide histopathology images. First, the lack of hand-annotated datasets to train algorithms. Second, the lack of interactive paradigms to enable a dialogue between the pathologist and the machine, which can be a major obstacle for use in clinical routine.
  We therefore propose a fast and user oriented method to bridge this gap by giving the pathologist control over the final result while limiting the number of interactions needed to achieve a good result (over 90\% on all our metrics with only 4 correction scribbles).
<div id='section'>Paperid: <span id='pid'>1378, <a href='https://arxiv.org/pdf/2402.03492.pdf' target='_blank'>https://arxiv.org/pdf/2402.03492.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qixiang Ma, Antoine Åucas, Huazhong Shu, Adrien Kaladji, Pascal Haigron
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.03492">Beyond Strong labels: Weakly-supervised Learning Based on Gaussian Pseudo Labels for The Segmentation of Ellipse-like Vascular Structures in Non-contrast CTs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep-learning-based automated segmentation of vascular structures in preoperative CT scans contributes to computer-assisted diagnosis and intervention procedure in vascular diseases. While CT angiography (CTA) is the common standard, non-contrast CT imaging is significant as a contrast-risk-free alternative, avoiding complications associated with contrast agents. However, the challenges of labor-intensive labeling and high labeling variability due to the ambiguity of vascular boundaries hinder conventional strong-label-based, fully-supervised learning in non-contrast CTs. This paper introduces a weakly-supervised framework using ellipses' topology in slices, including 1) an efficient annotation process based on predefined standards, 2) ellipse-fitting processing, 3) the generation of 2D Gaussian heatmaps serving as pseudo labels, 4) a training process through a combination of voxel reconstruction loss and distribution loss with the pseudo labels. We assess the effectiveness of the proposed method on one local and two public datasets comprising non-contrast CT scans, particularly focusing on the abdominal aorta. On the local dataset, our weakly-supervised learning approach based on pseudo labels outperforms strong-label-based fully-supervised learning (1.54\% of Dice score on average), reducing labeling time by around 82.0\%. The efficiency in generating pseudo labels allows the inclusion of label-agnostic external data in the training set, leading to an additional improvement in performance (2.74\% of Dice score on average) with a reduction of 66.3\% labeling time, where the labeling time remains considerably less than that of strong labels. On the public dataset, the pseudo labels achieve an overall improvement of 1.95\% in Dice score for 2D models while a reduction of 11.65 voxel spacing in Hausdorff distance for 3D model.
<div id='section'>Paperid: <span id='pid'>1379, <a href='https://arxiv.org/pdf/2401.11313.pdf' target='_blank'>https://arxiv.org/pdf/2401.11313.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Isaac J. Sledge, Dominic M. Byrne, Jonathan L. King, Steven H. Ostertag, Denton L. Woods, James L. Prater, Jermaine L. Kennedy, Timothy M. Marston, Jose C. Principe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11313">Weakly-Supervised Semantic Segmentation of Circular-Scan, Synthetic-Aperture-Sonar Imagery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a weakly-supervised framework for the semantic segmentation of circular-scan synthetic-aperture-sonar (CSAS) imagery. The first part of our framework is trained in a supervised manner, on image-level labels, to uncover a set of semi-sparse, spatially-discriminative regions in each image. The classification uncertainty of each region is then evaluated. Those areas with the lowest uncertainties are then chosen to be weakly labeled segmentation seeds, at the pixel level, for the second part of the framework. Each of the seed extents are progressively resized according to an unsupervised, information-theoretic loss with structured-prediction regularizers. This reshaping process uses multi-scale, adaptively-weighted features to delineate class-specific transitions in local image content. Content-addressable memories are inserted at various parts of our framework so that it can leverage features from previously seen images to improve segmentation performance for related images.
  We evaluate our weakly-supervised framework using real-world CSAS imagery that contains over ten seafloor classes and ten target classes. We show that our framework performs comparably to nine fully-supervised deep networks. Our framework also outperforms eleven of the best weakly-supervised deep networks. We achieve state-of-the-art performance when pre-training on natural imagery. The average absolute performance gap to the next-best weakly-supervised network is well over ten percent for both natural imagery and sonar imagery. This gap is found to be statistically significant.
<div id='section'>Paperid: <span id='pid'>1380, <a href='https://arxiv.org/pdf/2401.05416.pdf' target='_blank'>https://arxiv.org/pdf/2401.05416.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifeng Wang, Yi Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.05416">Wavelet Dynamic Selection Network for Inertial Sensor Signal Enhancement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As attitude and motion sensing components, inertial sensors are widely used in various portable devices. But the severe errors of inertial sensors restrain their function, especially the trajectory recovery and semantic recognition. As a mainstream signal processing method, wavelet is hailed as the mathematical microscope of signal due to the plentiful and diverse wavelet basis functions. However, complicated noise types and application scenarios of inertial sensors make selecting wavelet basis perplexing. To this end, we propose a wavelet dynamic selection network (WDSNet), which intelligently selects the appropriate wavelet basis for variable inertial signals. In addition, existing deep learning architectures excel at extracting features from input data but neglect to learn the characteristics of target categories, which is essential to enhance the category awareness capability, thereby improving the selection of wavelet basis. Therefore, we propose a category representation mechanism (CRM), which enables the network to extract and represent category features without increasing trainable parameters. Furthermore, CRM transforms the common fully connected network into category representations, which provide closer supervision to the feature extractor than the far and trivial one-hot classification labels. We call this process of imposing interpretability on a network and using it to supervise the feature extractor the feature supervision mechanism, and its effectiveness is demonstrated experimentally and theoretically in this paper. The enhanced inertial signal can perform impracticable tasks with regard to the original signal, such as trajectory reconstruction. Both quantitative and visual results show that WDSNet outperforms the existing methods. Remarkably, WDSNet, as a weakly-supervised method, achieves the state-of-the-art performance of all the compared fully-supervised methods.
<div id='section'>Paperid: <span id='pid'>1381, <a href='https://arxiv.org/pdf/2312.12080.pdf' target='_blank'>https://arxiv.org/pdf/2312.12080.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>James Hong, Lu Yuan, MichaÃ«l Gharbi, Matthew Fisher, Kayvon Fatahalian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.12080">Learning Subject-Aware Cropping by Outpainting Professional Photos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>How to frame (or crop) a photo often depends on the image subject and its context; e.g., a human portrait. Recent works have defined the subject-aware image cropping task as a nuanced and practical version of image cropping. We propose a weakly-supervised approach (GenCrop) to learn what makes a high-quality, subject-aware crop from professional stock images. Unlike supervised prior work, GenCrop requires no new manual annotations beyond the existing stock image collection. The key challenge in learning from this data, however, is that the images are already cropped and we do not know what regions were removed. Our insight is to combine a library of stock images with a modern, pre-trained text-to-image diffusion model. The stock image collection provides diversity and its images serve as pseudo-labels for a good crop, while the text-image diffusion model is used to out-paint (i.e., outward inpainting) realistic uncropped images. Using this procedure, we are able to automatically generate a large dataset of cropped-uncropped training pairs to train a cropping model. Despite being weakly-supervised, GenCrop is competitive with state-of-the-art supervised methods and significantly better than comparable weakly-supervised baselines on quantitative and qualitative evaluation metrics.
<div id='section'>Paperid: <span id='pid'>1382, <a href='https://arxiv.org/pdf/2312.09584.pdf' target='_blank'>https://arxiv.org/pdf/2312.09584.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Kim, Sinhae Cha, Byeongkeun Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.09584">Multiscale Vision Transformer With Deep Clustering-Guided Refinement for Weakly Supervised Object Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work addresses the task of weakly-supervised object localization. The goal is to learn object localization using only image-level class labels, which are much easier to obtain compared to bounding box annotations. This task is important because it reduces the need for labor-intensive ground-truth annotations. However, methods for object localization trained using weak supervision often suffer from limited accuracy in localization. To address this challenge and enhance localization accuracy, we propose a multiscale object localization transformer (MOLT). It comprises multiple object localization transformers that extract patch embeddings across various scales. Moreover, we introduce a deep clustering-guided refinement method that further enhances localization accuracy by utilizing separately extracted image segments. These segments are obtained by clustering pixels using convolutional neural networks. Finally, we demonstrate the effectiveness of our proposed method by conducting experiments on the publicly available ILSVRC-2012 dataset.
<div id='section'>Paperid: <span id='pid'>1383, <a href='https://arxiv.org/pdf/2312.06614.pdf' target='_blank'>https://arxiv.org/pdf/2312.06614.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mu Tian, Qinzhu Yang, Yi Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.06614">AttenScribble: Attentive Similarity Learning for Scribble-Supervised Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The success of deep networks in medical image segmentation relies heavily on massive labeled training data. However, acquiring dense annotations is a time-consuming process. Weakly-supervised methods normally employ less expensive forms of supervision, among which scribbles started to gain popularity lately thanks to its flexibility. However, due to lack of shape and boundary information, it is extremely challenging to train a deep network on scribbles that generalizes on unlabeled pixels. In this paper, we present a straightforward yet effective scribble supervised learning framework. Inspired by recent advances of transformer based segmentation, we create a pluggable spatial self-attention module which could be attached on top of any internal feature layers of arbitrary fully convolutional network (FCN) backbone. The module infuses global interaction while keeping the efficiency of convolutions. Descended from this module, we construct a similarity metric based on normalized and symmetrized attention. This attentive similarity leads to a novel regularization loss that imposes consistency between segmentation prediction and visual affinity. This attentive similarity loss optimizes the alignment of FCN encoders, attention mapping and model prediction. Ultimately, the proposed FCN+Attention architecture can be trained end-to-end guided by a combination of three learning objectives: partial segmentation loss, a customized masked conditional random fields and the proposed attentive similarity loss. Extensive experiments on public datasets (ACDC and CHAOS) showed that our framework not just out-performs existing state-of-the-art, but also delivers close performance to fully-supervised benchmark. Code will be available upon publication.
<div id='section'>Paperid: <span id='pid'>1384, <a href='https://arxiv.org/pdf/2310.16131.pdf' target='_blank'>https://arxiv.org/pdf/2310.16131.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Panfeng Cao, Ye Wang, Qiang Zhang, Zaiqiao Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.16131">GenKIE: Robust Generative Multimodal Document Key Information Extraction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Key information extraction (KIE) from scanned documents has gained increasing attention because of its applications in various domains. Although promising results have been achieved by some recent KIE approaches, they are usually built based on discriminative models, which lack the ability to handle optical character recognition (OCR) errors and require laborious token-level labelling. In this paper, we propose a novel generative end-to-end model, named GenKIE, to address the KIE task. GenKIE is a sequence-to-sequence multimodal generative model that utilizes multimodal encoders to embed visual, layout and textual features and a decoder to generate the desired output. Well-designed prompts are leveraged to incorporate the label semantics as the weakly supervised signals and entice the generation of the key information. One notable advantage of the generative model is that it enables automatic correction of OCR errors. Besides, token-level granular annotation is not required. Extensive experiments on multiple public real-world datasets show that GenKIE effectively generalizes over different types of documents and achieves state-of-the-art results. Our experiments also validate the model's robustness against OCR errors, making GenKIE highly applicable in real-world scenarios.
<div id='section'>Paperid: <span id='pid'>1385, <a href='https://arxiv.org/pdf/2310.13585.pdf' target='_blank'>https://arxiv.org/pdf/2310.13585.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elahe Vahdani, Yingli Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.13585">POTLoc: Pseudo-Label Oriented Transformer for Point-Supervised Temporal Action Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper tackles the challenge of point-supervised temporal action detection, wherein only a single frame is annotated for each action instance in the training set. Most of the current methods, hindered by the sparse nature of annotated points, struggle to effectively represent the continuous structure of actions or the inherent temporal and semantic dependencies within action instances. Consequently, these methods frequently learn merely the most distinctive segments of actions, leading to the creation of incomplete action proposals. This paper proposes POTLoc, a Pseudo-label Oriented Transformer for weakly-supervised Action Localization utilizing only point-level annotation. POTLoc is designed to identify and track continuous action structures via a self-training strategy. The base model begins by generating action proposals solely with point-level supervision. These proposals undergo refinement and regression to enhance the precision of the estimated action boundaries, which subsequently results in the production of `pseudo-labels' to serve as supplementary supervisory signals. The architecture of the model integrates a transformer with a temporal feature pyramid to capture video snippet dependencies and model actions of varying duration. The pseudo-labels, providing information about the coarse locations and boundaries of actions, assist in guiding the transformer for enhanced learning of action dynamics. POTLoc outperforms the state-of-the-art point-supervised methods on THUMOS'14 and ActivityNet-v1.2 datasets.
<div id='section'>Paperid: <span id='pid'>1386, <a href='https://arxiv.org/pdf/2310.00307.pdf' target='_blank'>https://arxiv.org/pdf/2310.00307.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingliang Deng, Zonghan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.00307">Dual-Augmented Transformer Network for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation (WSSS), a fundamental computer vision task, which aims to segment out the object within only class-level labels. The traditional methods adopt the CNN-based network and utilize the class activation map (CAM) strategy to discover the object regions. However, such methods only focus on the most discriminative region of the object, resulting in incomplete segmentation. An alternative is to explore vision transformers (ViT) to encode the image to acquire the global semantic information. Yet, the lack of transductive bias to objects is a flaw of ViT. In this paper, we explore the dual-augmented transformer network with self-regularization constraints for WSSS. Specifically, we propose a dual network with both CNN-based and transformer networks for mutually complementary learning, where both networks augment the final output for enhancement. Massive systemic evaluations on the challenging PASCAL VOC 2012 benchmark demonstrate the effectiveness of our method, outperforming previous state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1387, <a href='https://arxiv.org/pdf/2309.16959.pdf' target='_blank'>https://arxiv.org/pdf/2309.16959.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yukun Su, Jingliang Deng, Zonghan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.16959">COMNet: Co-Occurrent Matching for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image-level weakly supervised semantic segmentation is a challenging task that has been deeply studied in recent years. Most of the common solutions exploit class activation map (CAM) to locate object regions. However, such response maps generated by the classification network usually focus on discriminative object parts. In this paper, we propose a novel Co-Occurrent Matching Network (COMNet), which can promote the quality of the CAMs and enforce the network to pay attention to the entire parts of objects. Specifically, we perform inter-matching on paired images that contain common classes to enhance the corresponded areas, and construct intra-matching on a single image to propagate the semantic features across the object regions. The experiments on the Pascal VOC 2012 and MS-COCO datasets show that our network can effectively boost the performance of the baseline model and achieve new state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>1388, <a href='https://arxiv.org/pdf/2309.13289.pdf' target='_blank'>https://arxiv.org/pdf/2309.13289.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaofan Li, Bo Peng, Jie Hu, Changyou Ma, Daipeng Yang, Zhuyang Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.13289">USL-Net: Uncertainty Self-Learning Network for Unsupervised Skin Lesion Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsupervised skin lesion segmentation offers several benefits, including conserving expert human resources, reducing discrepancies due to subjective human labeling, and adapting to novel environments. However, segmenting dermoscopic images without manual labeling guidance presents significant challenges due to dermoscopic image artifacts such as hair noise, blister noise, and subtle edge differences. To address these challenges, we introduce an innovative Uncertainty Self-Learning Network (USL-Net) designed for skin lesion segmentation. The USL-Net can effectively segment a range of lesions, eliminating the need for manual labeling guidance. Initially, features are extracted using contrastive learning, followed by the generation of Class Activation Maps (CAMs) as saliency maps using these features. The different CAM locations correspond to the importance of the lesion region based on their saliency. High-saliency regions in the map serve as pseudo-labels for lesion regions while low-saliency regions represent the background. However, intermediate regions can be hard to classify, often due to their proximity to lesion edges or interference from hair or blisters. Rather than risk potential pseudo-labeling errors or learning confusion by forcefully classifying these regions, we consider them as uncertainty regions, exempting them from pseudo-labeling and allowing the network to self-learn. Further, we employ connectivity detection and centrality detection to refine foreground pseudo-labels and reduce noise-induced errors. The application of cycle refining enhances performance further. Our method underwent thorough experimental validation on the ISIC-2017, ISIC-2018, and PH2 datasets, demonstrating that its performance is on par with weakly supervised and supervised methods, and exceeds that of other existing unsupervised methods.
<div id='section'>Paperid: <span id='pid'>1389, <a href='https://arxiv.org/pdf/2309.04474.pdf' target='_blank'>https://arxiv.org/pdf/2309.04474.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianan Xie, Ji Liu, Chi Zhang, Xihui Chen, Ping Huai, Jie Zheng, Xiaofeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.04474">Weakly supervised learning for pattern classification in serial femtosecond crystallography</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Serial femtosecond crystallography at X-ray free electron laser facilities opens a new era for the determination of crystal structure. However, the data processing of those experiments is facing unprecedented challenge, because the total number of diffraction patterns needed to determinate a high-resolution structure is huge. Machine learning methods are very likely to play important roles in dealing with such a large volume of data. Convolutional neural networks have made a great success in the field of pattern classification, however, training of the networks need very large datasets with labels. Th is heavy dependence on labeled datasets will seriously restrict the application of networks, because it is very costly to annotate a large number of diffraction patterns. In this article we present our job on the classification of diffraction pattern by weakly supervised algorithms, with the aim of reducing as much as possible the size of the labeled dataset required for training. Our result shows that weakly supervised methods can significantly reduce the need for the number of labeled patterns while achieving comparable accuracy to fully supervised methods.
<div id='section'>Paperid: <span id='pid'>1390, <a href='https://arxiv.org/pdf/2309.04109.pdf' target='_blank'>https://arxiv.org/pdf/2309.04109.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Changming Xiao, Qi Yang, Feng Zhou, Changshui Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.04109">From Text to Mask: Localizing Entities Using the Attention of Text-to-Image Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have revolted the field of text-to-image generation recently. The unique way of fusing text and image information contributes to their remarkable capability of generating highly text-related images. From another perspective, these generative models imply clues about the precise correlation between words and pixels. In this work, a simple but effective method is proposed to utilize the attention mechanism in the denoising network of text-to-image diffusion models. Without re-training nor inference-time optimization, the semantic grounding of phrases can be attained directly. We evaluate our method on Pascal VOC 2012 and Microsoft COCO 2014 under weakly-supervised semantic segmentation setting and our method achieves superior performance to prior methods. In addition, the acquired word-pixel correlation is found to be generalizable for the learned text embedding of customized generation methods, requiring only a few modifications. To validate our discovery, we introduce a new practical task called "personalized referring image segmentation" with a new dataset. Experiments in various situations demonstrate the advantages of our method compared to strong baselines on this task. In summary, our work reveals a novel way to extract the rich multi-modal knowledge hidden in diffusion models for segmentation.
<div id='section'>Paperid: <span id='pid'>1391, <a href='https://arxiv.org/pdf/2309.01369.pdf' target='_blank'>https://arxiv.org/pdf/2309.01369.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryota Yoshihashi, Yuya Otsuka, Kenji Doi, Tomohiro Tanaka, Hirokatsu Kataoka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.01369">Exploring Limits of Diffusion-Synthetic Training with Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advance of generative models for images has inspired various training techniques for image recognition utilizing synthetic images. In semantic segmentation, one promising approach is extracting pseudo-masks from attention maps in text-to-image diffusion models, which enables real-image-and-annotation-free training. However, the pioneering training method using the diffusion-synthetic images and pseudo-masks, i.e., DiffuMask has limitations in terms of mask quality, scalability, and ranges of applicable domains. To overcome these limitations, this work introduces three techniques for diffusion-synthetic semantic segmentation training. First, reliability-aware robust training, originally used in weakly supervised learning, helps segmentation with insufficient synthetic mask quality. %Second, large-scale pretraining of whole segmentation models, not only backbones, on synthetic ImageNet-1k-class images with pixel-labels benefits downstream segmentation tasks. Second, we introduce prompt augmentation, data augmentation to the prompt text set to scale up and diversify training images with a limited text resources. Finally, LoRA-based adaptation of Stable Diffusion enables the transfer to a distant domain, e.g., auto-driving images. Experiments in PASCAL VOC, ImageNet-S, and Cityscapes show that our method effectively closes gap between real and synthetic training in semantic segmentation.
<div id='section'>Paperid: <span id='pid'>1392, <a href='https://arxiv.org/pdf/2308.13035.pdf' target='_blank'>https://arxiv.org/pdf/2308.13035.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shan Guleria, Benjamin Schwartz, Yash Sharma, Philip Fernandes, James Jablonski, Sodiq Adewole, Sanjana Srivastava, Fisher Rhoads, Michael Porter, Michelle Yeghyayan, Dylan Hyatt, Andrew Copland, Lubaina Ehsan, Donald Brown, Sana Syed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.13035">The intersection of video capsule endoscopy and artificial intelligence: addressing unique challenges using machine learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Introduction: Technical burdens and time-intensive review processes limit the practical utility of video capsule endoscopy (VCE). Artificial intelligence (AI) is poised to address these limitations, but the intersection of AI and VCE reveals challenges that must first be overcome. We identified five challenges to address. Challenge #1: VCE data are stochastic and contains significant artifact. Challenge #2: VCE interpretation is cost-intensive. Challenge #3: VCE data are inherently imbalanced. Challenge #4: Existing VCE AIMLT are computationally cumbersome. Challenge #5: Clinicians are hesitant to accept AIMLT that cannot explain their process.
  Methods: An anatomic landmark detection model was used to test the application of convolutional neural networks (CNNs) to the task of classifying VCE data. We also created a tool that assists in expert annotation of VCE data. We then created more elaborate models using different approaches including a multi-frame approach, a CNN based on graph representation, and a few-shot approach based on meta-learning.
  Results: When used on full-length VCE footage, CNNs accurately identified anatomic landmarks (99.1%), with gradient weighted-class activation mapping showing the parts of each frame that the CNN used to make its decision. The graph CNN with weakly supervised learning (accuracy 89.9%, sensitivity of 91.1%), the few-shot model (accuracy 90.8%, precision 91.4%, sensitivity 90.9%), and the multi-frame model (accuracy 97.5%, precision 91.5%, sensitivity 94.8%) performed well. Discussion: Each of these five challenges is addressed, in part, by one of our AI-based models. Our goal of producing high performance using lightweight models that aim to improve clinician confidence was achieved.
<div id='section'>Paperid: <span id='pid'>1393, <a href='https://arxiv.org/pdf/2308.00949.pdf' target='_blank'>https://arxiv.org/pdf/2308.00949.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchen Shen, Dong Zhang, Zhao Zhang, Liyong Fu, Qiaolin Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.00949">Synthetic Instance Segmentation from Semantic Image Segmentation Masks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, instance segmentation has garnered significant attention across various applications. However, training a fully-supervised instance segmentation model requires costly both instance-level and pixel-level annotations. In contrast, weakly-supervised instance segmentation methods, such as those using image-level class labels or point labels, often struggle to satisfy the accuracy and recall requirements of practical scenarios. In this paper, we propose a novel paradigm called Synthetic Instance Segmentation (SISeg). SISeg achieves instance segmentation results by leveraging image masks generated by existing semantic segmentation models, and it is highly efficient as we do not require additional training for semantic segmentation or the use of instance-level image annotations. In other words, the proposed model does not need extra manpower or higher computational expenses. Specifically, we first obtain a semantic segmentation mask of the input image via an existent semantic segmentation model. Then, we calculate a displacement field vector for each pixel based on the segmentation mask, which can indicate representations belonging to the same class but different instances, i.e., obtaining the instance-level object information. Finally, the instance segmentation results are refined by a learnable category-agnostic object boundary branch. Extensive experimental results on two challenging datasets highlight the effectiveness of SISeg in achieving competitive results when compared to state-of-the-art methods, especially fully-supervised methods. The code will be released at: SISeg
<div id='section'>Paperid: <span id='pid'>1394, <a href='https://arxiv.org/pdf/2307.16834.pdf' target='_blank'>https://arxiv.org/pdf/2307.16834.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hoang Viet Pham, Thinh Gia Tran, Chuong Dinh Le, An Dinh Le, Hien Bich Vo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.16834">Benchmarking Jetson Edge Devices with an End-to-end Video-based Anomaly Detection System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Innovative enhancement in embedded system platforms, specifically hardware accelerations, significantly influence the application of deep learning in real-world scenarios. These innovations translate human labor efforts into automated intelligent systems employed in various areas such as autonomous driving, robotics, Internet-of-Things (IoT), and numerous other impactful applications. NVIDIA's Jetson platform is one of the pioneers in offering optimal performance regarding energy efficiency and throughput in the execution of deep learning algorithms. Previously, most benchmarking analysis was based on 2D images with a single deep learning model for each comparison result. In this paper, we implement an end-to-end video-based crime-scene anomaly detection system inputting from surveillance videos and the system is deployed and completely operates on multiple Jetson edge devices (Nano, AGX Xavier, Orin Nano). The comparison analysis includes the integration of Torch-TensorRT as a software developer kit from NVIDIA for the model performance optimisation. The system is built based on the PySlowfast open-source project from Facebook as the coding template. The end-to-end system process comprises the videos from camera, data preprocessing pipeline, feature extractor and the anomaly detection. We provide the experience of an AI-based system deployment on various Jetson Edge devices with Docker technology. Regarding anomaly detectors, a weakly supervised video-based deep learning model called Robust Temporal Feature Magnitude Learning (RTFM) is applied in the system. The approach system reaches 47.56 frames per second (FPS) inference speed on a Jetson edge device with only 3.11 GB RAM usage total. We also discover the promising Jetson device that the AI system achieves 15% better performance than the previous version of Jetson devices while consuming 50% less energy power.
<div id='section'>Paperid: <span id='pid'>1395, <a href='https://arxiv.org/pdf/2307.14236.pdf' target='_blank'>https://arxiv.org/pdf/2307.14236.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Panggih Kusuma Ningrum, Philipp Mayr, Iana Atanassova
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.14236">UnScientify: Detecting Scientific Uncertainty in Scholarly Full Text</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This demo paper presents UnScientify, an interactive system designed to detect scientific uncertainty in scholarly full text. The system utilizes a weakly supervised technique that employs a fine-grained annotation scheme to identify verbally formulated uncertainty at the sentence level in scientific texts. The pipeline for the system includes a combination of pattern matching, complex sentence checking, and authorial reference checking. Our approach automates labeling and annotation tasks for scientific uncertainty identification, taking into account different types of scientific uncertainty, that can serve various applications such as information retrieval, text mining, and scholarly document processing. Additionally, UnScientify provides interpretable results, aiding in the comprehension of identified instances of scientific uncertainty in text.
<div id='section'>Paperid: <span id='pid'>1396, <a href='https://arxiv.org/pdf/2307.09732.pdf' target='_blank'>https://arxiv.org/pdf/2307.09732.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leyao Liu, Tao Kong, Minzhao Zhu, Jiashuo Fan, Lu Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.09732">ClickSeg: 3D Instance Segmentation with Click-Level Weak Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D instance segmentation methods often require fully-annotated dense labels for training, which are costly to obtain. In this paper, we present ClickSeg, a novel click-level weakly supervised 3D instance segmentation method that requires one point per instance annotation merely. Such a problem is very challenging due to the extremely limited labels, which has rarely been solved before. We first develop a baseline weakly-supervised training method, which generates pseudo labels for unlabeled data by the model itself. To utilize the property of click-level annotation setting, we further propose a new training framework. Instead of directly using the model inference way, i.e., mean-shift clustering, to generate the pseudo labels, we propose to use k-means with fixed initial seeds: the annotated points. New similarity metrics are further designed for clustering. Experiments on ScanNetV2 and S3DIS datasets show that the proposed ClickSeg surpasses the previous best weakly supervised instance segmentation result by a large margin (e.g., +9.4% mAP on ScanNetV2). Using 0.02% supervision signals merely, ClickSeg achieves $\sim$90% of the accuracy of the fully-supervised counterpart. Meanwhile, it also achieves state-of-the-art semantic segmentation results among weakly supervised methods that use the same annotation settings.
<div id='section'>Paperid: <span id='pid'>1397, <a href='https://arxiv.org/pdf/2307.09161.pdf' target='_blank'>https://arxiv.org/pdf/2307.09161.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yueyue Han, Yingyan Huang, Hangcheng Dong, Fengdong Chen, Fa Zeng, Zhitao Peng, Qihua Zhu, Guodong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.09161">CG-fusion CAM: Online segmentation of laser-induced damage on large-aperture optics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Online segmentation of laser-induced damage on large-aperture optics in high-power laser facilities is challenged by complicated damage morphology, uneven illumination and stray light interference. Fully supervised semantic segmentation algorithms have achieved state-of-the-art performance, but rely on plenty of pixel-level labels, which are time-consuming and labor-consuming to produce. LayerCAM, an advanced weakly supervised semantic segmentation algorithm, can generate pixel-accurate results using only image-level labels, but its scattered and partially under-activated class activation regions degrade segmentation performance. In this paper, we propose a weakly supervised semantic segmentation method with Continuous Gradient CAM and its nonlinear multi-scale fusion (CG-fusion CAM). The method redesigns the way of back-propagating gradients and non-linearly activates the multi-scale fused heatmaps to generate more fine-grained class activation maps with appropriate activation degree for different sizes of damage sites. Experiments on our dataset show that the proposed method can achieve segmentation performance comparable to that of fully supervised algorithms.
<div id='section'>Paperid: <span id='pid'>1398, <a href='https://arxiv.org/pdf/2306.15732.pdf' target='_blank'>https://arxiv.org/pdf/2306.15732.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Miller Yoder, Ahmad Diab, David West Brown, Kathleen M. Carley
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.15732">A Weakly Supervised Classifier and Dataset of White Supremacist Language</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a dataset and classifier for detecting the language of white supremacist extremism, a growing issue in online hate speech. Our weakly supervised classifier is trained on large datasets of text from explicitly white supremacist domains paired with neutral and anti-racist data from similar domains. We demonstrate that this approach improves generalization performance to new domains. Incorporating anti-racist texts as counterexamples to white supremacist language mitigates bias.
<div id='section'>Paperid: <span id='pid'>1399, <a href='https://arxiv.org/pdf/2306.06622.pdf' target='_blank'>https://arxiv.org/pdf/2306.06622.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Charani Alampalle, Shamanthak Hegde, Soumya Jahagirdar, Shankar Gangisetty
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.06622">Weakly Supervised Visual Question Answer Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Growing interest in conversational agents promote twoway human-computer communications involving asking and answering visual questions have become an active area of research in AI. Thus, generation of visual questionanswer pair(s) becomes an important and challenging task. To address this issue, we propose a weakly-supervised visual question answer generation method that generates a relevant question-answer pairs for a given input image and associated caption. Most of the prior works are supervised and depend on the annotated question-answer datasets. In our work, we present a weakly supervised method that synthetically generates question-answer pairs procedurally from visual information and captions. The proposed method initially extracts list of answer words, then does nearest question generation that uses the caption and answer word to generate synthetic question. Next, the relevant question generator converts the nearest question to relevant language question by dependency parsing and in-order tree traversal, finally, fine-tune a ViLBERT model with the question-answer pair(s) generated at end. We perform an exhaustive experimental analysis on VQA dataset and see that our model significantly outperform SOTA methods on BLEU scores. We also show the results wrt baseline models and ablation study.
<div id='section'>Paperid: <span id='pid'>1400, <a href='https://arxiv.org/pdf/2305.12522.pdf' target='_blank'>https://arxiv.org/pdf/2305.12522.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lucas David, Helio Pedrini, Zanoni Dias
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.12522">P-NOC: adversarial training of CAM generating networks for robust weakly supervised semantic segmentation priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Semantic Segmentation (WSSS) techniques explore individual regularization strategies to refine Class Activation Maps (CAMs). In this work, we first analyze complementary WSSS techniques in the literature, their segmentation properties, and the conditions in which they are most effective. Based on these findings, we devise two new techniques: P-NOC and CCAM-H. In the first, we promote the conjoint training of two adversarial CAM generating networks: the generator, which progressively learns to erase regions containing class-specific features, and a discriminator, which is refined to gradually shift its attention to new class discriminant features. In the latter, we employ the high quality pseudo-segmentation priors produced by P-NOC to guide the learning to saliency information in a weakly supervised fashion. Finally, we employ both pseudo-segmentation priors and pseudo-saliency proposals in the random walk procedure, resulting in higher quality pseudo-semantic segmentation masks, and competitive results with the state of the art.
<div id='section'>Paperid: <span id='pid'>1401, <a href='https://arxiv.org/pdf/2305.05836.pdf' target='_blank'>https://arxiv.org/pdf/2305.05836.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hsiu-Wei Yang, Abhinav Agrawal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.05836">Extracting Complex Named Entities in Legal Documents via Weakly Supervised Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate Named Entity Recognition (NER) is crucial for various information retrieval tasks in industry. However, despite significant progress in traditional NER methods, the extraction of Complex Named Entities remains a relatively unexplored area. In this paper, we propose a novel system that combines object detection for Document Layout Analysis (DLA) with weakly supervised learning to address the challenge of extracting discontinuous complex named entities in legal documents. Notably, to the best of our knowledge, this is the first work to apply weak supervision to DLA. Our experimental results show that the model trained solely on pseudo labels outperforms the supervised baseline when gold-standard data is limited, highlighting the effectiveness of our proposed approach in reducing the dependency on annotated data.
<div id='section'>Paperid: <span id='pid'>1402, <a href='https://arxiv.org/pdf/2304.10001.pdf' target='_blank'>https://arxiv.org/pdf/2304.10001.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weijun Tan, Qi Yao, Jingfeng Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.10001">Weakly Supervised Detection of Baby Cry</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detection of baby cries is an important part of baby monitoring and health care. Almost all existing methods use supervised SVM, CNN, or their varieties. In this work, we propose to use weakly supervised anomaly detection to detect a baby cry. In this weak supervision, we only need weak annotation if there is a cry in an audio file. We design a data mining technique using the pre-trained VGGish feature extractor and an anomaly detection network on long untrimmed audio files. The obtained datasets are used to train a simple CNN feature network for cry/non-cry classification. This CNN is then used as a feature extractor in an anomaly detection framework to achieve better cry detection performance.
<div id='section'>Paperid: <span id='pid'>1403, <a href='https://arxiv.org/pdf/2304.04142.pdf' target='_blank'>https://arxiv.org/pdf/2304.04142.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>James M. Dolezal, Sara Kochanny, Emma Dyer, Andrew Srisuwananukorn, Matteo Sacco, Frederick M. Howard, Anran Li, Prajval Mohan, Alexander T. Pearson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.04142">Slideflow: Deep Learning for Digital Histopathology with Real-Time Whole-Slide Visualization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning methods have emerged as powerful tools for analyzing histopathological images, but current methods are often specialized for specific domains and software environments, and few open-source options exist for deploying models in an interactive interface. Experimenting with different deep learning approaches typically requires switching software libraries and reprocessing data, reducing the feasibility and practicality of experimenting with new architectures. We developed a flexible deep learning library for histopathology called Slideflow, a package which supports a broad array of deep learning methods for digital pathology and includes a fast whole-slide interface for deploying trained models. Slideflow includes unique tools for whole-slide image data processing, efficient stain normalization and augmentation, weakly-supervised whole-slide classification, uncertainty quantification, feature generation, feature space analysis, and explainability. Whole-slide image processing is highly optimized, enabling whole-slide tile extraction at 40X magnification in 2.5 seconds per slide. The framework-agnostic data processing pipeline enables rapid experimentation with new methods built with either Tensorflow or PyTorch, and the graphical user interface supports real-time visualization of slides, predictions, heatmaps, and feature space characteristics on a variety of hardware devices, including ARM-based devices such as the Raspberry Pi.
<div id='section'>Paperid: <span id='pid'>1404, <a href='https://arxiv.org/pdf/2304.00058.pdf' target='_blank'>https://arxiv.org/pdf/2304.00058.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiang Zhang, Taoyue Wang, Xiaotian Li, Huiyuan Yang, Lijun Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.00058">Weakly-Supervised Text-driven Contrastive Learning for Facial Behavior Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contrastive learning has shown promising potential for learning robust representations by utilizing unlabeled data. However, constructing effective positive-negative pairs for contrastive learning on facial behavior datasets remains challenging. This is because such pairs inevitably encode the subject-ID information, and the randomly constructed pairs may push similar facial images away due to the limited number of subjects in facial behavior datasets. To address this issue, we propose to utilize activity descriptions, coarse-grained information provided in some datasets, which can provide high-level semantic information about the image sequences but is often neglected in previous studies. More specifically, we introduce a two-stage Contrastive Learning with Text-Embeded framework for Facial behavior understanding (CLEF). The first stage is a weakly-supervised contrastive learning method that learns representations from positive-negative pairs constructed using coarse-grained activity information. The second stage aims to train the recognition of facial expressions or facial action units by maximizing the similarity between image and the corresponding text label names. The proposed CLEF achieves state-of-the-art performance on three in-the-lab datasets for AU recognition and three in-the-wild datasets for facial expression recognition.
<div id='section'>Paperid: <span id='pid'>1405, <a href='https://arxiv.org/pdf/2303.15270.pdf' target='_blank'>https://arxiv.org/pdf/2303.15270.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryo Hachiuma, Fumiaki Sato, Taiki Sekii
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.15270">Unified Keypoint-based Action Recognition Framework via Structured Keypoint Pooling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper simultaneously addresses three limitations associated with conventional skeleton-based action recognition; skeleton detection and tracking errors, poor variety of the targeted actions, as well as person-wise and frame-wise action recognition. A point cloud deep-learning paradigm is introduced to the action recognition, and a unified framework along with a novel deep neural network architecture called Structured Keypoint Pooling is proposed. The proposed method sparsely aggregates keypoint features in a cascaded manner based on prior knowledge of the data structure (which is inherent in skeletons), such as the instances and frames to which each keypoint belongs, and achieves robustness against input errors. Its less constrained and tracking-free architecture enables time-series keypoints consisting of human skeletons and nonhuman object contours to be efficiently treated as an input 3D point cloud and extends the variety of the targeted action. Furthermore, we propose a Pooling-Switching Trick inspired by Structured Keypoint Pooling. This trick switches the pooling kernels between the training and inference phases to detect person-wise and frame-wise actions in a weakly supervised manner using only video-level action labels. This trick enables our training scheme to naturally introduce novel data augmentation, which mixes multiple point clouds extracted from different videos. In the experiments, we comprehensively verify the effectiveness of the proposed method against the limitations, and the method outperforms state-of-the-art skeleton-based action recognition and spatio-temporal action localization methods.
<div id='section'>Paperid: <span id='pid'>1406, <a href='https://arxiv.org/pdf/2303.05546.pdf' target='_blank'>https://arxiv.org/pdf/2303.05546.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mesut Erhan Unal, Adriana Kovashka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.05546">Weakly-Supervised HOI Detection from Interaction Labels Only and Language/Vision-Language Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-object interaction (HOI) detection aims to extract interacting human-object pairs and their interaction categories from a given natural image. Even though the labeling effort required for building HOI detection datasets is inherently more extensive than for many other computer vision tasks, weakly-supervised directions in this area have not been sufficiently explored due to the difficulty of learning human-object interactions with weak supervision, rooted in the combinatorial nature of interactions over the object and predicate space. In this paper, we tackle HOI detection with the weakest supervision setting in the literature, using only image-level interaction labels, with the help of a pretrained vision-language model (VLM) and a large language model (LLM). We first propose an approach to prune non-interacting human and object proposals to increase the quality of positive pairs within the bag, exploiting the grounding capability of the vision-language model. Second, we use a large language model to query which interactions are possible between a human and a given object category, in order to force the model not to put emphasis on unlikely interactions. Lastly, we use an auxiliary weakly-supervised preposition prediction task to make our model explicitly reason about space. Extensive experiments and ablations show that all of our contributions increase HOI detection performance.
<div id='section'>Paperid: <span id='pid'>1407, <a href='https://arxiv.org/pdf/2303.05073.pdf' target='_blank'>https://arxiv.org/pdf/2303.05073.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaohui Zhu, Linhu Liu, Jiang Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.05073">Learn More for Food Recognition via Progressive Self-Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Food recognition has a wide range of applications, such as health-aware recommendation and self-service restaurants. Most previous methods of food recognition firstly locate informative regions in some weakly-supervised manners and then aggregate their features. However, location errors of informative regions limit the effectiveness of these methods to some extent. Instead of locating multiple regions, we propose a Progressive Self-Distillation (PSD) method, which progressively enhances the ability of network to mine more details for food recognition. The training of PSD simultaneously contains multiple self-distillations, in which a teacher network and a student network share the same embedding network. Since the student network receives a modified image from its teacher network by masking some informative regions, the teacher network outputs stronger semantic representations than the student network. Guided by such teacher network with stronger semantics, the student network is encouraged to mine more useful regions from the modified image by enhancing its own ability. The ability of the teacher network is also enhanced with the shared embedding network. By using progressive training, the teacher network incrementally improves its ability to mine more discriminative regions. In inference phase, only the teacher network is used without the help of the student network. Extensive experiments on three datasets demonstrate the effectiveness of our proposed method and state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>1408, <a href='https://arxiv.org/pdf/2302.14163.pdf' target='_blank'>https://arxiv.org/pdf/2302.14163.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Prashant Pandey, Mustafa Chasmai, Monish Natarajan, Brejesh Lall
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.14163">A Language-Guided Benchmark for Weakly Supervised Open Vocabulary Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Increasing attention is being diverted to data-efficient problem settings like Open Vocabulary Semantic Segmentation (OVSS) which deals with segmenting an arbitrary object that may or may not be seen during training. The closest standard problems related to OVSS are Zero-Shot and Few-Shot Segmentation (ZSS, FSS) and their Cross-dataset variants where zero to few annotations are needed to segment novel classes. The existing FSS and ZSS methods utilize fully supervised pixel-labelled seen classes to segment unseen classes. Pixel-level labels are hard to obtain, and using weak supervision in the form of inexpensive image-level labels is often more practical. To this end, we propose a novel unified weakly supervised OVSS pipeline that can perform ZSS, FSS and Cross-dataset segmentation on novel classes without using pixel-level labels for either the base (seen) or the novel (unseen) classes in an inductive setting. We propose Weakly-Supervised Language-Guided Segmentation Network (WLSegNet), a novel language-guided segmentation pipeline that i) learns generalizable context vectors with batch aggregates (mean) to map class prompts to image features using frozen CLIP (a vision-language model) and ii) decouples weak ZSS/FSS into weak semantic segmentation and Zero-Shot segmentation. The learned context vectors avoid overfitting on seen classes during training and transfer better to novel classes during testing. WLSegNet avoids fine-tuning and the use of external datasets during training. The proposed pipeline beats existing methods for weak generalized Zero-Shot and weak Few-Shot semantic segmentation by 39 and 3 mIOU points respectively on PASCAL VOC and weak Few-Shot semantic segmentation by 5 mIOU points on MS COCO. On a harder setting of 2-way 1-shot weak FSS, WLSegNet beats the baselines by 13 and 22 mIOU points on PASCAL VOC and MS COCO, respectively.
<div id='section'>Paperid: <span id='pid'>1409, <a href='https://arxiv.org/pdf/2302.10527.pdf' target='_blank'>https://arxiv.org/pdf/2302.10527.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunzhong He, Cong Zhang, Ruoyan Kong, Chaitanya Kulkarni, Qing Liu, Ashish Gandhe, Amit Nithianandan, Arul Prakash
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.10527">HierCat: Hierarchical Query Categorization from Weakly Supervised Data at Facebook Marketplace</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Query categorization at customer-to-customer e-commerce platforms like Facebook Marketplace is challenging due to the vagueness of search intent, noise in real-world data, and imbalanced training data across languages. Its deployment also needs to consider challenges in scalability and downstream integration in order to translate modeling advances into better search result relevance. In this paper we present HierCat, the query categorization system at Facebook Marketplace. HierCat addresses these challenges by leveraging multi-task pre-training of dual-encoder architectures with a hierarchical inference step to effectively learn from weakly supervised training data mined from searcher engagement. We show that HierCat not only outperforms popular methods in offline experiments, but also leads to 1.4% improvement in NDCG and 4.3% increase in searcher engagement at Facebook Marketplace Search in online A/B testing.
<div id='section'>Paperid: <span id='pid'>1410, <a href='https://arxiv.org/pdf/2302.07679.pdf' target='_blank'>https://arxiv.org/pdf/2302.07679.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alban Petit, Caio Corro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.07679">On graph-based reentrancy-free semantic parsing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel graph-based approach for semantic parsing that resolves two problems observed in the literature: (1) seq2seq models fail on compositional generalization tasks; (2) previous work using phrase structure parsers cannot cover all the semantic parses observed in treebanks. We prove that both MAP inference and latent tag anchoring (required for weakly-supervised learning) are NP-hard problems. We propose two optimization algorithms based on constraint smoothing and conditional gradient to approximately solve these inference problems. Experimentally, our approach delivers state-of-the-art results on Geoquery, Scan and Clevr, both for i.i.d. splits and for splits that test for compositional generalization.
<div id='section'>Paperid: <span id='pid'>1411, <a href='https://arxiv.org/pdf/2302.04625.pdf' target='_blank'>https://arxiv.org/pdf/2302.04625.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kooshan Hashemifard, Pau Climent-Perez, Francisco Florez-Revuelta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.04625">Weakly Supervised Human Skin Segmentation using Guidance Attention Mechanisms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human skin segmentation is a crucial task in computer vision and biometric systems, yet it poses several challenges such as variability in skin color, pose, and illumination. This paper presents a robust data-driven skin segmentation method for a single image that addresses these challenges through the integration of contextual information and efficient network design. In addition to robustness and accuracy, the integration into real-time systems requires a careful balance between computational power, speed, and performance. The proposed method incorporates two attention modules, Body Attention and Skin Attention, that utilize contextual information to improve segmentation results. These modules draw attention to the desired areas, focusing on the body boundaries and skin pixels, respectively. Additionally, an efficient network architecture is employed in the encoder part to minimize computational power while retaining high performance. To handle the issue of noisy labels in skin datasets, the proposed method uses a weakly supervised training strategy, relying on the Skin Attention module. The results of this study demonstrate that the proposed method is comparable to, or outperforms, state-of-the-art methods on benchmark datasets.
<div id='section'>Paperid: <span id='pid'>1412, <a href='https://arxiv.org/pdf/2302.02125.pdf' target='_blank'>https://arxiv.org/pdf/2302.02125.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Du, Qihua Dong, Yan Xu, Jing Liao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.02125">Weakly-Supervised 3D Medical Image Segmentation using Geometric Prior and Contrastive Similarity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical image segmentation is almost the most important pre-processing procedure in computer-aided diagnosis but is also a very challenging task due to the complex shapes of segments and various artifacts caused by medical imaging, (i.e., low-contrast tissues, and non-homogenous textures). In this paper, we propose a simple yet effective segmentation framework that incorporates the geometric prior and contrastive similarity into the weakly-supervised segmentation framework in a loss-based fashion. The proposed geometric prior built on point cloud provides meticulous geometry to the weakly-supervised segmentation proposal, which serves as better supervision than the inherent property of the bounding-box annotation (i.e., height and width). Furthermore, we propose contrastive similarity to encourage organ pixels to gather around in the contrastive embedding space, which helps better distinguish low-contrast tissues. The proposed contrastive embedding space can make up for the poor representation of the conventionally-used gray space. Extensive experiments are conducted to verify the effectiveness and the robustness of the proposed weakly-supervised segmentation framework. The proposed framework is superior to state-of-the-art weakly-supervised methods on the following publicly accessible datasets: LiTS 2017 Challenge, KiTS 2021 Challenge, and LPBA40. We also dissect our method and evaluate the performance of each component.
<div id='section'>Paperid: <span id='pid'>1413, <a href='https://arxiv.org/pdf/2301.11418.pdf' target='_blank'>https://arxiv.org/pdf/2301.11418.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Edgar Rangel, Fabio Martinez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.11418">Parkinson gait modelling from an anomaly deep representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Parkinson's Disease (PD) is associated with gait movement disorders, such as bradykinesia, stiffness, tremors and postural instability, caused by progressive dopamine deficiency. Today, some approaches have implemented learning representations to quantify kinematic patterns during locomotion, supporting clinical procedures such as diagnosis and treatment planning. These approaches assumes a large amount of stratified and labeled data to optimize discriminative representations. Nonetheless these considerations may restrict the approaches to be operable in real scenarios during clinical practice. This work introduces a self-supervised generative representation to learn gait-motion-related patterns, under the pretext of video reconstruction and an anomaly detection framework. This architecture is trained following a one-class weakly supervised learning to avoid inter-class variance and approach the multiple relationships that represent locomotion. The proposed approach was validated with 14 PD patients and 23 control subjects, and trained with the control population only, achieving an AUC of 95%, homocedasticity level of 70% and shapeness level of 70% in the classification task considering its generalization.
<div id='section'>Paperid: <span id='pid'>1414, <a href='https://arxiv.org/pdf/2301.09624.pdf' target='_blank'>https://arxiv.org/pdf/2301.09624.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Piotr Keller, Muhammad Dawood, Fayyaz ul Amir Afsar Minhas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.09624">Maximum Mean Discrepancy Kernels for Predictive and Prognostic Modeling of Whole Slide Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>How similar are two images? In computational pathology, where Whole Slide Images (WSIs) of digitally scanned tissue samples from patients can be multi-gigapixels in size, determination of degree of similarity between two WSIs is a challenging task with a number of practical applications. In this work, we explore a novel strategy based on kernelized Maximum Mean Discrepancy (MMD) analysis for determination of pairwise similarity between WSIs. The proposed approach works by calculating MMD between two WSIs using kernels over deep features of image patches. This allows representation of an entire dataset of WSIs as a kernel matrix for WSI level clustering, weakly-supervised prediction of TP-53 mutation status in breast cancer patients from their routine WSIs as well as survival analysis with state of the art prediction performance. We believe that this work will open up further avenues for application of WSI-level kernels for predictive and prognostic tasks in computational pathology.
<div id='section'>Paperid: <span id='pid'>1415, <a href='https://arxiv.org/pdf/2210.12921.pdf' target='_blank'>https://arxiv.org/pdf/2210.12921.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahnaf Mozib Samin, M. Humayon Kobir, Md. Mushtaq Shahriyar Rafee, M. Firoz Ahmed, Mehedi Hasan, Partha Ghosh, Shafkat Kibria, M. Shahidur Rahman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.12921">Investigating self-supervised, weakly supervised and fully supervised training approaches for multi-domain automatic speech recognition: a study on Bangladeshi Bangla</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite huge improvements in automatic speech recognition (ASR) employing neural networks, ASR systems still suffer from a lack of robustness and generalizability issues due to domain shifting. This is mainly because principal corpus design criteria are often not identified and examined adequately while compiling ASR datasets. In this study, we investigate the robustness of the state-of-the-art transfer learning approaches such as self-supervised wav2vec 2.0 and weakly supervised Whisper as well as fully supervised convolutional neural networks (CNNs) for multi-domain ASR. We also demonstrate the significance of domain selection while building a corpus by assessing these models on a novel multi-domain Bangladeshi Bangla ASR evaluation benchmark - BanSpeech, which contains approximately 6.52 hours of human-annotated speech and 8085 utterances from 13 distinct domains. SUBAK.KO, a mostly read speech corpus for the morphologically rich language Bangla, has been used to train the ASR systems. Experimental evaluation reveals that self-supervised cross-lingual pre-training is the best strategy compared to weak supervision and full supervision to tackle the multi-domain ASR task. Moreover, the ASR models trained on SUBAK.KO face difficulty recognizing speech from domains with mostly spontaneous speech. The BanSpeech will be publicly available to meet the need for a challenging evaluation benchmark for Bangla ASR.
<div id='section'>Paperid: <span id='pid'>1416, <a href='https://arxiv.org/pdf/2207.12238.pdf' target='_blank'>https://arxiv.org/pdf/2207.12238.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amrest Chinkamol, Vetit Kanjaras, Phattarapong Sawangjai, Yitian Zhao, Thapanun Sudhawiyangkul, Chantana Chantrapornchai, Cuntai Guan, Theerawit Wilaiprasitporn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.12238">OCTAve: 2D en face Optical Coherence Tomography Angiography Vessel Segmentation in Weakly-Supervised Learning with Locality Augmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While there have been increased researches using deep learning techniques for the extraction of vascular structure from the 2D en face OCTA, for such approach, it is known that the data annotation process on the curvilinear structure like the retinal vasculature is very costly and time consuming, albeit few tried to address the annotation problem.
  In this work, we propose the application of the scribble-base weakly-supervised learning method to automate the pixel-level annotation. The proposed method, called OCTAve, combines the weakly-supervised learning using scribble-annotated ground truth augmented with an adversarial and a novel self-supervised deep supervision. Our novel mechanism is designed to utilize the discriminative outputs from the discrimination layer of a UNet-like architecture where the Kullback-Liebler Divergence between the aggregate discriminative outputs and the segmentation map predicate is minimized during the training. This combined method leads to the better localization of the vascular structure as shown in our experiments. We validate our proposed method on the large public datasets i.e., ROSE, OCTA-500. The segmentation performance is compared against both state-of-the-art fully-supervised and scribble-based weakly-supervised approaches. The implementation of our work used in the experiments is located at [LINK].
<div id='section'>Paperid: <span id='pid'>1417, <a href='https://arxiv.org/pdf/2203.07239.pdf' target='_blank'>https://arxiv.org/pdf/2203.07239.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruiwen Li, Zheda Mai, Chiheb Trabelsi, Zhibo Zhang, Jongseong Jang, Scott Sanner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.07239">TransCAM: Transformer Attention-based CAM Refinement for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation (WSSS) with only image-level supervision is a challenging task. Most existing methods exploit Class Activation Maps (CAM) to generate pixel-level pseudo labels for supervised training. However, due to the local receptive field of Convolution Neural Networks (CNN), CAM applied to CNNs often suffers from partial activation -- highlighting the most discriminative part instead of the entire object area. In order to capture both local features and global representations, the Conformer has been proposed to combine a visual transformer branch with a CNN branch. In this paper, we propose TransCAM, a Conformer-based solution to WSSS that explicitly leverages the attention weights from the transformer branch of the Conformer to refine the CAM generated from the CNN branch. TransCAM is motivated by our observation that attention weights from shallow transformer blocks are able to capture low-level spatial feature similarities while attention weights from deep transformer blocks capture high-level semantic context. Despite its simplicity, TransCAM achieves a new state-of-the-art performance of 69.3% and 69.6% on the respective PASCAL VOC 2012 validation and test sets, showing the effectiveness of transformer attention-based refinement of CAM for WSSS.
<div id='section'>Paperid: <span id='pid'>1418, <a href='https://arxiv.org/pdf/2106.09614.pdf' target='_blank'>https://arxiv.org/pdf/2106.09614.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunlu Li, Andreas Morel-Forster, Thomas Vetter, Bernhard Egger, Adam Kortylewski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2106.09614">Robust Model-based Face Reconstruction through Weakly-Supervised Outlier Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we aim to enhance model-based face reconstruction by avoiding fitting the model to outliers, i.e. regions that cannot be well-expressed by the model such as occluders or make-up. The core challenge for localizing outliers is that they are highly variable and difficult to annotate. To overcome this challenging problem, we introduce a joint Face-autoencoder and outlier segmentation approach (FOCUS).In particular, we exploit the fact that the outliers cannot be fitted well by the face model and hence can be localized well given a high-quality model fitting. The main challenge is that the model fitting and the outlier segmentation are mutually dependent on each other, and need to be inferred jointly. We resolve this chicken-and-egg problem with an EM-type training strategy, where a face autoencoder is trained jointly with an outlier segmentation network. This leads to a synergistic effect, in which the segmentation network prevents the face encoder from fitting to the outliers, enhancing the reconstruction quality. The improved 3D face reconstruction, in turn, enables the segmentation network to better predict the outliers. To resolve the ambiguity between outliers and regions that are difficult to fit, such as eyebrows, we build a statistical prior from synthetic data that measures the systematic bias in model fitting. Experiments on the NoW testset demonstrate that FOCUS achieves SOTA 3D face reconstruction performance among all baselines that are trained without 3D annotation. Moreover, our results on CelebA-HQ and the AR database show that the segmentation network can localize occluders accurately despite being trained without any segmentation annotation.
<div id='section'>Paperid: <span id='pid'>1419, <a href='https://arxiv.org/pdf/2002.08412.pdf' target='_blank'>https://arxiv.org/pdf/2002.08412.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seokhyun Chung, Raed Al Kontar, Zhenke Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2002.08412">Weakly-supervised Multi-output Regression via Correlated Gaussian Processes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-output regression seeks to borrow strength and leverage commonalities across different but related outputs in order to enhance learning and prediction accuracy. A fundamental assumption is that the output/group membership labels for all observations are known. This assumption is often violated in real applications. For instance, in healthcare datasets, sensitive attributes such as ethnicity are often missing or unreported. To this end, we introduce a weakly-supervised multi-output model based on dependent Gaussian processes. Our approach is able to leverage data without complete group labels or possibly only prior belief on group memberships to enhance accuracy across all outputs. Through intensive simulations and case studies on an Insulin, Testosterone and Bodyfat dataset, we show that our model excels in multi-output settings with missing labels, while being competitive in traditional fully labeled settings. We end by highlighting the possible use of our approach in fair inference and sequential decision-making.
<div id='section'>Paperid: <span id='pid'>1420, <a href='https://arxiv.org/pdf/2512.05364.pdf' target='_blank'>https://arxiv.org/pdf/2512.05364.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ananth Hariharan, David Mortensen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.05364">Transformer-Enabled Diachronic Analysis of Vedic Sanskrit: Neural Methods for Quantifying Types of Language Change</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study demonstrates how hybrid neural-symbolic methods can yield significant new insights into the evolution of a morphologically rich, low-resource language. We challenge the naive assumption that linguistic change is simplification by quantitatively analyzing over 2,000 years of Sanskrit, demonstrating how weakly-supervised hybrid methods can yield new insights into the evolution of morphologically rich, low-resource languages. Our approach addresses data scarcity through weak supervision, using 100+ high-precision regex patterns to generate pseudo-labels for fine-tuning a multilingual BERT. We then fuse symbolic and neural outputs via a novel confidence-weighted ensemble, creating a system that is both scalable and interpretable. Applying this framework to a 1.47-million-word diachronic corpus, our ensemble achieves a 52.4% overall feature detection rate. Our findings reveal that Sanskrit's overall morphological complexity does not decrease but is instead dynamically redistributed: while earlier verbal features show cyclical patterns of decline, complexity shifts to other domains, evidenced by a dramatic expansion in compounding and the emergence of new philosophical terminology. Critically, our system produces well-calibrated uncertainty estimates, with confidence strongly correlating with accuracy (Pearson r = 0.92) and low overall calibration error (ECE = 0.043), bolstering the reliability of these findings for computational philology.
<div id='section'>Paperid: <span id='pid'>1421, <a href='https://arxiv.org/pdf/2512.02344.pdf' target='_blank'>https://arxiv.org/pdf/2512.02344.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyuan Sun, Yongping Zhang, Hongcheng Zeng, Yamin Wang, Wei Yang, Wanting Yang, Jie Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.02344">A multi-weight self-matching visual explanation for cnns on sar images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, convolutional neural networks (CNNs) have achieved significant success in various synthetic aperture radar (SAR) tasks. However, the complexity and opacity of their internal mechanisms hinder the fulfillment of high-reliability requirements, thereby limiting their application in SAR. Improving the interpretability of CNNs is thus of great importance for their development and deployment in SAR. In this paper, a visual explanation method termed multi-weight self-matching class activation mapping (MS-CAM) is proposed. MS-CAM matches SAR images with the feature maps and corresponding gradients extracted by the CNN, and combines both channel-wise and element-wise weights to visualize the decision basis learned by the model in SAR images. Extensive experiments conducted on a self-constructed SAR target classification dataset demonstrate that MS-CAM more accurately highlights the network's regions of interest and captures detailed target feature information, thereby enhancing network interpretability. Furthermore, the feasibility of applying MS-CAM to weakly-supervised obiect localization is validated. Key factors affecting localization accuracy, such as pixel thresholds, are analyzed in depth to inform future work.
<div id='section'>Paperid: <span id='pid'>1422, <a href='https://arxiv.org/pdf/2511.18012.pdf' target='_blank'>https://arxiv.org/pdf/2511.18012.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaying Zhou, Qingchao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.18012">State and Scene Enhanced Prototypes for Weakly Supervised Open-Vocabulary Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-Vocabulary Object Detection (OVOD) aims to generalize object recognition to novel categories, while Weakly Supervised OVOD (WS-OVOD) extends this by combining box-level annotations with image-level labels. Despite recent progress, two critical challenges persist in this setting. First, existing semantic prototypes, even when enriched by LLMs, are static and limited, failing to capture the rich intra-class visual variations induced by different object states (e.g., a cat's pose). Second, the standard pseudo-box generation introduces a semantic mismatch between visual region proposals (which contain context) and object-centric text embeddings. To tackle these issues, we introduce two complementary prototype enhancement strategies. To capture intra-class variations in appearance and state, we propose the State-Enhanced Semantic Prototypes (SESP), which generates state-aware textual descriptions (e.g., "a sleeping cat") to capture diverse object appearances, yielding more discriminative prototypes. Building on this, we further introduce Scene-Augmented Pseudo Prototypes (SAPP) to address the semantic mismatch. SAPP incorporates contextual semantics (e.g., "cat lying on sofa") and utilizes a soft alignment mechanism to promote contextually consistent visual-textual representations. By integrating SESP and SAPP, our method effectively enhances both the richness of semantic prototypes and the visual-textual alignment, achieving notable improvements.
<div id='section'>Paperid: <span id='pid'>1423, <a href='https://arxiv.org/pdf/2511.03361.pdf' target='_blank'>https://arxiv.org/pdf/2511.03361.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gabriel Pirlogeanu, Alexandru-Lucian Georgescu, Horia Cucu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.03361">Open Source State-Of-the-Art Solution for Romanian Speech Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we present a new state-of-the-art Romanian Automatic Speech Recognition (ASR) system based on NVIDIA's FastConformer architecture--explored here for the first time in the context of Romanian. We train our model on a large corpus of, mostly, weakly supervised transcriptions, totaling over 2,600 hours of speech. Leveraging a hybrid decoder with both Connectionist Temporal Classification (CTC) and Token-Duration Transducer (TDT) branches, we evaluate a range of decoding strategies including greedy, ALSD, and CTC beam search with a 6-gram token-level language model. Our system achieves state-of-the-art performance across all Romanian evaluation benchmarks, including read, spontaneous, and domain-specific speech, with up to 27% relative WER reduction compared to previous best-performing systems. In addition to improved transcription accuracy, our approach demonstrates practical decoding efficiency, making it suitable for both research and deployment in low-latency ASR applications.
<div id='section'>Paperid: <span id='pid'>1424, <a href='https://arxiv.org/pdf/2511.02576.pdf' target='_blank'>https://arxiv.org/pdf/2511.02576.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alix de Langlais, Benjamin Billot, Théo Aguilar Vidal, Marc-Olivier Gauci, Hervé Delingette
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.02576">Resource-efficient Automatic Refinement of Segmentations via Weak Supervision from Light Feedback</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Delineating anatomical regions is a key task in medical image analysis. Manual segmentation achieves high accuracy but is labor-intensive and prone to variability, thus prompting the development of automated approaches. Recently, a breadth of foundation models has enabled automated segmentations across diverse anatomies and imaging modalities, but these may not always meet the clinical accuracy standards. While segmentation refinement strategies can improve performance, current methods depend on heavy user interactions or require fully supervised segmentations for training. Here, we present SCORE (Segmentation COrrection from Regional Evaluations), a weakly supervised framework that learns to refine mask predictions only using light feedback during training. Specifically, instead of relying on dense training image annotations, SCORE introduces a novel loss that leverages region-wise quality scores and over/under-segmentation error labels. We demonstrate SCORE on humerus CT scans, where it considerably improves initial predictions from TotalSegmentator, and achieves performance on par with existing refinement methods, while greatly reducing their supervision requirements and annotation time. Our code is available at: https://gitlab.inria.fr/adelangl/SCORE.
<div id='section'>Paperid: <span id='pid'>1425, <a href='https://arxiv.org/pdf/2507.21959.pdf' target='_blank'>https://arxiv.org/pdf/2507.21959.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheyuan Zhang, Yen-chia Hsu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21959">Mitigating Spurious Correlations in Weakly Supervised Semantic Segmentation via Cross-architecture Consistency Regularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scarcity of pixel-level labels is a significant challenge in practical scenarios. In specific domains like industrial smoke, acquiring such detailed annotations is particularly difficult and often requires expert knowledge. To alleviate this, weakly supervised semantic segmentation (WSSS) has emerged as a promising approach. However, due to the supervision gap and inherent bias in models trained with only image level labels, existing WSSS methods suffer from limitations such as incomplete foreground coverage, inaccurate object boundaries, and spurious correlations, especially in our domain, where emissions are always spatially coupled with chimneys.
  Previous solutions typically rely on additional priors or external knowledge to mitigate these issues, but they often lack scalability and fail to address the model's inherent bias toward co-occurring context. To address this, we propose a novel WSSS framework that directly targets the co-occurrence problem without relying on external supervision. Unlike prior methods that adopt a single network, we employ a teacher-student framework that combines CNNs and ViTs. We introduce a knowledge transfer loss that enforces cross-architecture consistency by aligning internal representations. Additionally, we incorporate post-processing techniques to address partial coverage and further improve pseudo mask quality.
<div id='section'>Paperid: <span id='pid'>1426, <a href='https://arxiv.org/pdf/2507.21587.pdf' target='_blank'>https://arxiv.org/pdf/2507.21587.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheyuan Zhang, Wang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21587">Emerging Trends in Pseudo-Label Refinement for Weakly Supervised Semantic Segmentation with Image-Level Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unlike fully supervised semantic segmentation, weakly supervised semantic segmentation (WSSS) relies on weaker forms of supervision to perform dense prediction tasks. Among the various types of weak supervision, WSSS with image level annotations is considered both the most challenging and the most practical, attracting significant research attention. Therefore, in this review, we focus on WSSS with image level annotations. Additionally, this review concentrates on mainstream research directions, deliberately omitting less influential branches.
  Given the rapid development of new methods and the limitations of existing surveys in capturing recent trends, there is a pressing need for an updated and comprehensive review. Our goal is to fill this gap by synthesizing the latest advancements and state-of-the-art techniques in WSSS with image level labels.
  Basically, we provide a comprehensive review of recent advancements in WSSS with image level labels, categorizing existing methods based on the types and levels of additional supervision involved. We also examine the challenges of applying advanced methods to domain specific datasets in WSSS,a topic that remains underexplored. Finally, we discuss the current challenges, evaluate the limitations of existing approaches, and outline several promising directions for future research. This review is intended for researchers who are already familiar with the fundamental concepts of WSSS and are seeking to deepen their understanding of current advances and methodological innovations.
<div id='section'>Paperid: <span id='pid'>1427, <a href='https://arxiv.org/pdf/2507.16849.pdf' target='_blank'>https://arxiv.org/pdf/2507.16849.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi-Shan Chu, Hsuan-Cheng Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16849">Post-Disaster Affected Area Segmentation with a Vision Transformer (ViT)-based EVAP Model using Sentinel-2 and Formosat-5 Imagery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a vision transformer (ViT)-based deep learning framework to refine disaster-affected area segmentation from remote sensing imagery, aiming to support and enhance the Emergent Value Added Product (EVAP) developed by the Taiwan Space Agency (TASA). The process starts with a small set of manually annotated regions. We then apply principal component analysis (PCA)-based feature space analysis and construct a confidence index (CI) to expand these labels, producing a weakly supervised training set. These expanded labels are then used to train ViT-based encoder-decoder models with multi-band inputs from Sentinel-2 and Formosat-5 imagery. Our architecture supports multiple decoder variants and multi-stage loss strategies to improve performance under limited supervision. During the evaluation, model predictions are compared with higher-resolution EVAP output to assess spatial coherence and segmentation consistency. Case studies on the 2022 Poyang Lake drought and the 2023 Rhodes wildfire demonstrate that our framework improves the smoothness and reliability of segmentation results, offering a scalable approach for disaster mapping when accurate ground truth is unavailable.
<div id='section'>Paperid: <span id='pid'>1428, <a href='https://arxiv.org/pdf/2507.06013.pdf' target='_blank'>https://arxiv.org/pdf/2507.06013.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kushal Gajjar, Harshit Sikchi, Arpit Singh Gautam, Marc Hammons, Saurabh Jha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06013">CogniSQL-R1-Zero: Lightweight Reinforced Reasoning for Efficient SQL Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Translating natural language into SQL (Text-to-SQL) remains a core challenge at the intersection of language understanding and structured data access. Although large language models (LLMs) have improved fluency, generating correct and executable SQL, especially for complex queries, continues to be challenging. We introduce CogniSQL-R1-Zero, a reinforcement learning (RL) framework and model that produces accurate SQL using a lightweight reward signal based on execution correctness and format-tag compliance. By avoiding intermediate supervision, hybrid pipelines and complex reward shaping, our method encourages stable learning and stronger alignment with the ultimate task objective-producing executable programs. CogniSQL-R1-Zero achieves state-of-the-art execution accuracy on Text2SQL benchmark; BIRD bench, outperforming prior supervised and instruction-tuned baselines including SFT CodeS-7B, DeepSeek-Coder 236B, and Mistral 123B-despite being trained on a significantly smaller 7B backbone. This result underscores the scalability and efficiency of our RL-based approach when trained on just four NVIDIA A100 GPUs (40 GB VRAM each). To support further research in efficient and interpretable Text-to-SQL modeling, we release two curated datasets: (i) a collection of 5,024 reasoning traces with varying context lengths, and (ii) a positive-sampled corpus of 36,356 corpus of weakly supervised queries, each annotated with six semantically diverse reasoning paths. Together, these contributions advance scalable, execution-aligned Text-to-SQL generation.
<div id='section'>Paperid: <span id='pid'>1429, <a href='https://arxiv.org/pdf/2507.02308.pdf' target='_blank'>https://arxiv.org/pdf/2507.02308.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pei Guo, Ryan Farrell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02308">LMPNet for Weakly-supervised Keypoint Discovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we explore the task of semantic object keypoint discovery weakly-supervised by only category labels. This is achieved by transforming discriminatively-trained intermediate layer filters into keypoint detectors. We begin by identifying three preferred characteristics of keypoint detectors: (i) spatially sparse activations, (ii) consistency and (iii) diversity. Instead of relying on hand-crafted loss terms, a novel computationally-efficient leaky max pooling (LMP) layer is proposed to explicitly encourage final conv-layer filters to learn "non-repeatable local patterns" that are well aligned with object keypoints. Informed by visualizations, a simple yet effective selection strategy is proposed to ensure consistent filter activations and attention mask-out is then applied to force the network to distribute its attention to the whole object instead of just the most discriminative region. For the final keypoint prediction, a learnable clustering layer is proposed to group keypoint proposals into keypoint predictions. The final model, named LMPNet, is highly interpretable in that it directly manipulates network filters to detect predefined concepts. Our experiments show that LMPNet can (i) automatically discover semantic keypoints that are robust to object pose and (ii) achieves strong prediction accuracy comparable to a supervised pose estimation model.
<div id='section'>Paperid: <span id='pid'>1430, <a href='https://arxiv.org/pdf/2506.13095.pdf' target='_blank'>https://arxiv.org/pdf/2506.13095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Wang, Shiwei Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13095">Learning Event Completeness for Weakly Supervised Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised video anomaly detection (WS-VAD) is tasked with pinpointing temporal intervals containing anomalous events within untrimmed videos, utilizing only video-level annotations. However, a significant challenge arises due to the absence of dense frame-level annotations, often leading to incomplete localization in existing WS-VAD methods. To address this issue, we present a novel LEC-VAD, Learning Event Completeness for Weakly Supervised Video Anomaly Detection, which features a dual structure designed to encode both category-aware and category-agnostic semantics between vision and language. Within LEC-VAD, we devise semantic regularities that leverage an anomaly-aware Gaussian mixture to learn precise event boundaries, thereby yielding more complete event instances. Besides, we develop a novel memory bank-based prototype learning mechanism to enrich concise text descriptions associated with anomaly-event categories. This innovation bolsters the text's expressiveness, which is crucial for advancing WS-VAD. Our LEC-VAD demonstrates remarkable advancements over the current state-of-the-art methods on two benchmark datasets XD-Violence and UCF-Crime.
<div id='section'>Paperid: <span id='pid'>1431, <a href='https://arxiv.org/pdf/2506.02166.pdf' target='_blank'>https://arxiv.org/pdf/2506.02166.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arnav Rustagi, Satvik Bajpai, Nimrat Kaur, Siddharth Siddharth
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02166">Dhvani: A Weakly-supervised Phonemic Error Detection and Personalized Feedback System for Hindi</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computer-Assisted Pronunciation Training (CAPT) has been extensively studied for English. However, there remains a critical gap in its application to Indian languages with a base of 1.5 billion speakers. Pronunciation tools tailored to Indian languages are strikingly lacking despite the fact that millions learn them every year. With over 600 million speakers and being the fourth most-spoken language worldwide, improving Hindi pronunciation is a vital first step toward addressing this gap. This paper proposes 1) Dhvani -- a novel CAPT system for Hindi, 2) synthetic speech generation for Hindi mispronunciations, and 3) a novel methodology for providing personalized feedback to learners. While the system often interacts with learners using Devanagari graphemes, its core analysis targets phonemic distinctions, leveraging Hindi's highly phonetic orthography to analyze mispronounced speech and provide targeted feedback.
<div id='section'>Paperid: <span id='pid'>1432, <a href='https://arxiv.org/pdf/2505.23586.pdf' target='_blank'>https://arxiv.org/pdf/2505.23586.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyong Wang, Charith Abhayaratne
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23586">Weakly-supervised Localization of Manipulated Image Regions Using Multi-resolution Learned Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The explosive growth of digital images and the widespread availability of image editing tools have made image manipulation detection an increasingly critical challenge. Current deep learning-based manipulation detection methods excel in achieving high image-level classification accuracy, they often fall short in terms of interpretability and localization of manipulated regions. Additionally, the absence of pixel-wise annotations in real-world scenarios limits the existing fully-supervised manipulation localization techniques. To address these challenges, we propose a novel weakly-supervised approach that integrates activation maps generated by image-level manipulation detection networks with segmentation maps from pre-trained models. Specifically, we build on our previous image-level work named WCBnet to produce multi-view feature maps which are subsequently fused for coarse localization. These coarse maps are then refined using detailed segmented regional information provided by pre-trained segmentation models (such as DeepLab, SegmentAnything and PSPnet), with Bayesian inference employed to enhance the manipulation localization. Experimental results demonstrate the effectiveness of our approach, highlighting the feasibility to localize image manipulations without relying on pixel-level labels.
<div id='section'>Paperid: <span id='pid'>1433, <a href='https://arxiv.org/pdf/2505.15438.pdf' target='_blank'>https://arxiv.org/pdf/2505.15438.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianyuan Guo, Peike Li, Trevor Cohn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.15438">Bridging Sign and Spoken Languages: Pseudo Gloss Generation for Sign Language Translation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sign Language Translation (SLT) aims to map sign language videos to spoken language text. A common approach relies on gloss annotations as an intermediate representation, decomposing SLT into two sub-tasks: video-to-gloss recognition and gloss-to-text translation. While effective, this paradigm depends on expert-annotated gloss labels, which are costly and rarely available in existing datasets, limiting its scalability. To address this challenge, we propose a gloss-free pseudo gloss generation framework that eliminates the need for human-annotated glosses while preserving the structured intermediate representation. Specifically, we prompt a Large Language Model (LLM) with a few example text-gloss pairs using in-context learning to produce draft sign glosses from spoken language text. To enhance the correspondence between LLM-generated pseudo glosses and the sign sequences in video, we correct the ordering in the pseudo glosses for better alignment via a weakly supervised learning process. This reordering facilitates the incorporation of auxiliary alignment objectives, and allows for the use of efficient supervision via a Connectionist Temporal Classification (CTC) loss. We train our SLT mode, which consists of a vision encoder and a translator, through a three-stage pipeline, which progressively narrows the modality gap between sign language and spoken language. Despite its simplicity, our approach outperforms previous state-of-the-art gloss-free frameworks on two SLT benchmarks and achieves competitive results compared to gloss-based methods.
<div id='section'>Paperid: <span id='pid'>1434, <a href='https://arxiv.org/pdf/2505.10781.pdf' target='_blank'>https://arxiv.org/pdf/2505.10781.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Minkwan Kim, Soeun Lee, Byeongkeun Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10781">Completely Weakly Supervised Class-Incremental Learning for Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work addresses the task of completely weakly supervised class-incremental learning for semantic segmentation to learn segmentation for both base and additional novel classes using only image-level labels. While class-incremental semantic segmentation (CISS) is crucial for handling diverse and newly emerging objects in the real world, traditional CISS methods require expensive pixel-level annotations for training. To overcome this limitation, partially weakly-supervised approaches have recently been proposed. However, to the best of our knowledge, this is the first work to introduce a completely weakly-supervised method for CISS. To achieve this, we propose to generate robust pseudo-labels by combining pseudo-labels from a localizer and a sequence of foundation models based on their uncertainty. Moreover, to mitigate catastrophic forgetting, we introduce an exemplar-guided data augmentation method that generates diverse images containing both previous and novel classes with guidance. Finally, we conduct experiments in three common experimental settings: 15-5 VOC, 10-10 VOC, and COCO-to-VOC, and in two scenarios: disjoint and overlap. The experimental results demonstrate that our completely weakly supervised method outperforms even partially weakly supervised methods in the 15-5 VOC and 10-10 VOC settings while achieving competitive accuracy in the COCO-to-VOC setting.
<div id='section'>Paperid: <span id='pid'>1435, <a href='https://arxiv.org/pdf/2505.07440.pdf' target='_blank'>https://arxiv.org/pdf/2505.07440.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rituraj Singh, Sachin Pawar, Girish Palshikar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07440">Matching Tasks with Industry Groups for Augmenting Commonsense Knowledge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Commonsense knowledge bases (KB) are a source of specialized knowledge that is widely used to improve machine learning applications. However, even for a large KB such as ConceptNet, capturing explicit knowledge from each industry domain is challenging. For example, only a few samples of general {\em tasks} performed by various industries are available in ConceptNet. Here, a task is a well-defined knowledge-based volitional action to achieve a particular goal. In this paper, we aim to fill this gap and present a weakly-supervised framework to augment commonsense KB with tasks carried out by various industry groups (IG). We attempt to {\em match} each task with one or more suitable IGs by training a neural model to learn task-IG affinity and apply clustering to select the top-k tasks per IG. We extract a total of 2339 triples of the form $\langle IG, is~capable~of, task \rangle$ from two publicly available news datasets for 24 IGs with the precision of 0.86. This validates the reliability of the extracted task-IG pairs that can be directly added to existing KBs.
<div id='section'>Paperid: <span id='pid'>1436, <a href='https://arxiv.org/pdf/2505.04150.pdf' target='_blank'>https://arxiv.org/pdf/2505.04150.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Yamaoka, Weng Ian Chan, Shigeto Seno, Soichiro Fukada, Hideo Matsuda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04150">Learning from Similarity Proportion Loss for Classifying Skeletal Muscle Recovery Stages</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Evaluating the regeneration process of damaged muscle tissue is a fundamental analysis in muscle research to measure experimental effect sizes and uncover mechanisms behind muscle weakness due to aging and disease. The conventional approach to assessing muscle tissue regeneration involves whole-slide imaging and expert visual inspection of the recovery stages based on the morphological information of cells and fibers. There is a need to replace these tasks with automated methods incorporating machine learning techniques to ensure a quantitative and objective analysis. Given the limited availability of fully labeled data, a possible approach is Learning from Label Proportions (LLP), a weakly supervised learning method using class label proportions. However, current LLP methods have two limitations: (1) they cannot adapt the feature extractor for muscle tissues, and (2) they treat the classes representing recovery stages and cell morphological changes as nominal, resulting in the loss of ordinal information. To address these issues, we propose Ordinal Scale Learning from Similarity Proportion (OSLSP), which uses a similarity proportion loss derived from two bag combinations. OSLSP can update the feature extractor by using class proportion attention to the ordinal scale of the class. Our model with OSLSP outperforms large-scale pre-trained and fine-tuning models in classification tasks of skeletal muscle recovery stages.
<div id='section'>Paperid: <span id='pid'>1437, <a href='https://arxiv.org/pdf/2504.14302.pdf' target='_blank'>https://arxiv.org/pdf/2504.14302.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yogev Kriger, Shai Fine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14302">Learning to Score</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Common machine learning settings range from supervised tasks, where accurately labeled data is accessible, through semi-supervised and weakly-supervised tasks, where target labels are scant or noisy, to unsupervised tasks where labels are unobtainable. In this paper we study a scenario where the target labels are not available but additional related information is at hand. This information, referred to as Side Information, is either correlated with the unknown labels or imposes constraints on the feature space. We formulate the problem as an ensemble of three semantic components: representation learning, side information and metric learning. The proposed scoring model is advantageous for multiple use-cases. For example, in the healthcare domain it can be used to create a severity score for diseases where the symptoms are known but the criteria for the disease progression are not well defined. We demonstrate the utility of the suggested scoring system on well-known benchmark data-sets and bio-medical patient records.
<div id='section'>Paperid: <span id='pid'>1438, <a href='https://arxiv.org/pdf/2504.13927.pdf' target='_blank'>https://arxiv.org/pdf/2504.13927.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>F. Herrera, U. A. Rozikov, M. V. Velasco
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13927">Ising Models with Hidden Markov Structure: Applications to Probabilistic Inference in Machine Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we investigate tree-indexed Markov chains (Gibbs measures) defined by a Hamiltonian that couples two Ising layers: hidden spins \(s(x) \in \{\pm 1\}\) and observed spins \(Ï(x) \in \{\pm 1\}\) on a Cayley tree. The Hamiltonian incorporates Ising interactions within each layer and site-wise emission couplings between layers, extending hidden Markov models to a bilayer Markov random field.
  Specifically, we explore translation-invariant Gibbs measures (TIGM) of this Hamiltonian on Cayley trees.
  Under certain explicit conditions on the model's parameters, we demonstrate that there can be up to three distinct TIGMs. Each of these measures represents an equilibrium state of the spin system. These measures provide a structured approach to inference on hierarchical data in machine learning. They have practical applications in tasks such as denoising, weakly supervised learning, and anomaly detection. The Cayley tree structure is particularly advantageous for exact inference due to its tractability.
<div id='section'>Paperid: <span id='pid'>1439, <a href='https://arxiv.org/pdf/2504.13297.pdf' target='_blank'>https://arxiv.org/pdf/2504.13297.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andreas Lau Hansen, Lukas Wanzeck, Dim P. Papadopoulos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13297">Weak Cube R-CNN: Weakly Supervised 3D Detection using only 2D Bounding Boxes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monocular 3D object detection is an essential task in computer vision, and it has several applications in robotics and virtual reality. However, 3D object detectors are typically trained in a fully supervised way, relying extensively on 3D labeled data, which is labor-intensive and costly to annotate. This work focuses on weakly-supervised 3D detection to reduce data needs using a monocular method that leverages a singlecamera system over expensive LiDAR sensors or multi-camera setups. We propose a general model Weak Cube R-CNN, which can predict objects in 3D at inference time, requiring only 2D box annotations for training by exploiting the relationship between 2D projections of 3D cubes. Our proposed method utilizes pre-trained frozen foundation 2D models to estimate depth and orientation information on a training set. We use these estimated values as pseudo-ground truths during training. We design loss functions that avoid 3D labels by incorporating information from the external models into the loss. In this way, we aim to implicitly transfer knowledge from these large foundation 2D models without having access to 3D bounding box annotations. Experimental results on the SUN RGB-D dataset show increased performance in accuracy compared to an annotation time equalized Cube R-CNN baseline. While not precise for centimetre-level measurements, this method provides a strong foundation for further research.
<div id='section'>Paperid: <span id='pid'>1440, <a href='https://arxiv.org/pdf/2504.04435.pdf' target='_blank'>https://arxiv.org/pdf/2504.04435.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tatiana Merkulova, Bharani Jayakumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04435">Evaluation framework for Image Segmentation Algorithms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a comprehensive evaluation framework for image segmentation algorithms, encompassing naive methods, machine learning approaches, and deep learning techniques. We begin by introducing the fundamental concepts and importance of image segmentation, and the role of interactive segmentation in enhancing accuracy. A detailed background theory section explores various segmentation methods, including thresholding, edge detection, region growing, feature extraction, random forests, support vector machines, convolutional neural networks, U-Net, and Mask R-CNN. The implementation and experimental setup are thoroughly described, highlighting three primary approaches: algorithm assisting user, user assisting algorithm, and hybrid methods. Evaluation metrics such as Intersection over Union (IoU), computation time, and user interaction time are employed to measure performance. A comparative analysis presents detailed results, emphasizing the strengths, limitations, and trade-offs of each method. The paper concludes with insights into the practical applicability of these approaches across various scenarios and outlines future work, focusing on expanding datasets, developing more representative approaches, integrating real-time feedback, and exploring weakly supervised and self-supervised learning paradigms to enhance segmentation accuracy and efficiency. Keywords: Image Segmentation, Interactive Segmentation, Machine Learning, Deep Learning, Computer Vision
<div id='section'>Paperid: <span id='pid'>1441, <a href='https://arxiv.org/pdf/2503.23181.pdf' target='_blank'>https://arxiv.org/pdf/2503.23181.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sunoh Kim, Daeho Um
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.23181">Enhancing Weakly Supervised Video Grounding via Diverse Inference Strategies for Boundary and Prediction Selection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised video grounding aims to localize temporal boundaries relevant to a given query without explicit ground-truth temporal boundaries. While existing methods primarily use Gaussian-based proposals, they overlook the importance of (1) boundary prediction and (2) top-1 prediction selection during inference. In their boundary prediction, boundaries are simply set at half a standard deviation away from a Gaussian mean on both sides, which may not accurately capture the optimal boundaries. In the top-1 prediction process, these existing methods rely heavily on intersections with other proposals, without considering the varying quality of each proposal. To address these issues, we explore various inference strategies by introducing (1) novel boundary prediction methods to capture diverse boundaries from multiple Gaussians and (2) new selection methods that take proposal quality into account. Extensive experiments on the ActivityNet Captions and Charades-STA datasets validate the effectiveness of our inference strategies, demonstrating performance improvements without requiring additional training.
<div id='section'>Paperid: <span id='pid'>1442, <a href='https://arxiv.org/pdf/2503.22856.pdf' target='_blank'>https://arxiv.org/pdf/2503.22856.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shanshan Bai, Anna Kruspe, Xiaoxiang Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22856">Generating Synthetic Oracle Datasets to Analyze Noise Impact: A Study on Building Function Classification Using Tweets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tweets provides valuable semantic context for earth observation tasks and serves as a complementary modality to remote sensing imagery. In building function classification (BFC), tweets are often collected using geographic heuristics and labeled via external databases, an inherently weakly supervised process that introduces both label noise and sentence level feature noise (e.g., irrelevant or uninformative tweets). While label noise has been widely studied, the impact of sentence level feature noise remains underexplored, largely due to the lack of clean benchmark datasets for controlled analysis. In this work, we propose a method for generating a synthetic oracle dataset using LLM, designed to contain only tweets that are both correctly labeled and semantically relevant to their associated buildings. This oracle dataset enables systematic investigation of noise impacts that are otherwise difficult to isolate in real-world data. To assess its utility, we compare model performance using Naive Bayes and mBERT classifiers under three configurations: real vs. synthetic training data, and cross-domain generalization. Results show that noise in real tweets significantly degrades the contextual learning capacity of mBERT, reducing its performance to that of a simple keyword-based model. In contrast, the clean synthetic dataset allows mBERT to learn effectively, outperforming Naive Bayes Bayes by a large margin. These findings highlight that addressing feature noise is more critical than model complexity in this task. Our synthetic dataset offers a novel experimental environment for future noise injection studies and is publicly available on GitHub.
<div id='section'>Paperid: <span id='pid'>1443, <a href='https://arxiv.org/pdf/2503.13925.pdf' target='_blank'>https://arxiv.org/pdf/2503.13925.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Da Kuang, Guanwen Qiu, Junhyong Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13925">Reconstructing Cell Lineage Trees from Phenotypic Features with Metric Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>How a single fertilized cell gives rise to a complex array of specialized cell types in development is a central question in biology. The cells grow, divide, and acquire differentiated characteristics through poorly understood molecular processes. A key approach to studying developmental processes is to infer the tree graph of cell lineage division and differentiation histories, providing an analytical framework for dissecting individual cells' molecular decisions during replication and differentiation. Although genetically engineered lineage-tracing methods have advanced the field, they are either infeasible or ethically constrained in many organisms. In contrast, modern single-cell technologies can measure high-content molecular profiles (e.g., transcriptomes) in a wide range of biological systems.
  Here, we introduce CellTreeQM, a novel deep learning method based on transformer architectures that learns an embedding space with geometric properties optimized for tree-graph inference. By formulating lineage reconstruction as a tree-metric learning problem, we have systematically explored supervised, weakly supervised, and unsupervised training settings and present a Lineage Reconstruction Benchmark to facilitate comprehensive evaluation of our learning method. We benchmarked the method on (1) synthetic data modeled via Brownian motion with independent noise and spurious signals and (2) lineage-resolved single-cell RNA sequencing datasets. Experimental results show that CellTreeQM recovers lineage structures with minimal supervision and limited data, offering a scalable framework for uncovering cell lineage relationships in challenging animal models. To our knowledge, this is the first method to cast cell lineage inference explicitly as a metric learning task, paving the way for future computational models aimed at uncovering the molecular dynamics of cell lineage.
<div id='section'>Paperid: <span id='pid'>1444, <a href='https://arxiv.org/pdf/2503.04342.pdf' target='_blank'>https://arxiv.org/pdf/2503.04342.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ivan Oleksiyuk, Svyatoslav Voloshynovskiy, Tobias Golling
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04342">TRANSIT your events into a new mass: Fast background interpolation for weakly-supervised anomaly searches</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a new model for conditional and continuous data morphing called TRansport Adversarial Network for Smooth InTerpolation (TRANSIT). We apply it to create a background data template for weakly-supervised searches at the LHC. The method smoothly transforms sideband events to match signal region mass distributions. We demonstrate the performance of TRANSIT using the LHC Olympics R\&D dataset. The model captures non-linear mass correlations of features and produces a template that offers a competitive anomaly sensitivity compared to state-of-the-art transport-based template generators. Moreover, the computational training time required for TRANSIT is an order of magnitude lower than that of competing deep learning methods. This makes it ideal for analyses that iterate over many signal regions and signal models. Unlike generative models, which must learn a full probability density distribution, i.e., the correlations between all the variables, the proposed transport model only has to learn a smooth conditional shift of the distribution. This allows for a simpler, more efficient residual architecture, enabling mass uncorrelated features to pass the network unchanged while the mass correlated features are adjusted accordingly. Furthermore, we show that the latent space of the model provides a set of mass decorrelated features useful for anomaly detection without background sculpting.
<div id='section'>Paperid: <span id='pid'>1445, <a href='https://arxiv.org/pdf/2502.00629.pdf' target='_blank'>https://arxiv.org/pdf/2502.00629.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Wu, Hideki Nakayama
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.00629">Advanced Weakly-Supervised Formula Exploration for Neuro-Symbolic Mathematical Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, neuro-symbolic methods have become a popular and powerful approach that augments artificial intelligence systems with the capability to perform abstract, logical, and quantitative deductions with enhanced precision and controllability. Recent studies successfully performed symbolic reasoning by leveraging various machine learning models to explicitly or implicitly predict intermediate labels that provide symbolic instructions. However, these intermediate labels are not always prepared for every task as a part of training data, and pre-trained models, represented by Large Language Models (LLMs), also do not consistently generate valid symbolic instructions with their intrinsic knowledge. On the other hand, existing work developed alternative learning techniques that allow the learning system to autonomously uncover optimal symbolic instructions. Nevertheless, their performance also exhibits limitations when faced with relatively huge search spaces or more challenging reasoning problems. In view of this, in this work, we put forward an advanced practice for neuro-symbolic reasoning systems to explore the intermediate labels with weak supervision from problem inputs and final outputs. Our experiments on the Mathematics dataset illustrated the effectiveness of our proposals from multiple aspects.
<div id='section'>Paperid: <span id='pid'>1446, <a href='https://arxiv.org/pdf/2501.03891.pdf' target='_blank'>https://arxiv.org/pdf/2501.03891.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyi Wu, Hong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.03891">Superpixel Boundary Correction for Weakly-Supervised Semantic Segmentation on Histopathology Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancement of deep learning, computational pathology has made significant progress in cancer diagnosis and subtyping. Tissue segmentation is a core challenge, essential for prognosis and treatment decisions. Weakly supervised semantic segmentation (WSSS) reduces the annotation requirement by using image-level labels instead of pixel-level ones. However, Class Activation Map (CAM)-based methods still suffer from low spatial resolution and unclear boundaries. To address these issues, we propose a multi-level superpixel correction algorithm that refines CAM boundaries using superpixel clustering and floodfill. Experimental results show that our method achieves great performance on breast cancer segmentation dataset with mIoU of 71.08%, significantly improving tumor microenvironment boundary delineation.
<div id='section'>Paperid: <span id='pid'>1447, <a href='https://arxiv.org/pdf/2501.01658.pdf' target='_blank'>https://arxiv.org/pdf/2501.01658.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wang Lituan, Zhang Lei, Wang Yan, Wang Zhenbin, Zhang Zhenwei, Zhang Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01658">EAUWSeg: Eliminating annotation uncertainty in weakly-supervised medical image segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised medical image segmentation is gaining traction as it requires only rough annotations rather than accurate pixel-to-pixel labels, thereby reducing the workload for specialists. Although some progress has been made, there is still a considerable performance gap between the label-efficient methods and fully-supervised one, which can be attributed to the uncertainty nature of these weak labels. To address this issue, we propose a novel weak annotation method coupled with its learning framework EAUWSeg to eliminate the annotation uncertainty. Specifically, we first propose the Bounded Polygon Annotation (BPAnno) by simply labeling two polygons for a lesion. Then, the tailored learning mechanism that explicitly treat bounded polygons as two separated annotations is proposed to learn invariant feature by providing adversarial supervision signal for model training. Subsequently, a confidence-auxiliary consistency learner incorporates with a classification-guided confidence generator is designed to provide reliable supervision signal for pixels in uncertain region by leveraging the feature presentation consistency across pixels within the same category as well as class-specific information encapsulated in bounded polygons annotation. Experimental results demonstrate that EAUWSeg outperforms existing weakly-supervised segmentation methods. Furthermore, compared to fully-supervised counterparts, the proposed method not only delivers superior performance but also costs much less annotation workload. This underscores the superiority and effectiveness of our approach.
<div id='section'>Paperid: <span id='pid'>1448, <a href='https://arxiv.org/pdf/2412.19563.pdf' target='_blank'>https://arxiv.org/pdf/2412.19563.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongbiao Gao, Xiangcheng Sun, Guohua Lv, Deng Yu, Sijiu Niu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19563">Reinforced Label Denoising for Weakly-Supervised Audio-Visual Video Parsing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-visual video parsing (AVVP) aims to recognize audio and visual event labels with precise temporal boundaries, which is quite challenging since audio or visual modality might include only one event label with only the overall video labels available. Existing label denoising models often treat the denoising process as a separate preprocessing step, leading to a disconnect between label denoising and AVVP tasks. To bridge this gap, we present a novel joint reinforcement learning-based label denoising approach (RLLD). This approach enables simultaneous training of both label denoising and video parsing models through a joint optimization strategy. We introduce a novel AVVP-validation and soft inter-reward feedback mechanism that directly guides the learning of label denoising policy. Extensive experiments on AVVP tasks demonstrate the superior performance of our proposed method compared to label denoising techniques. Furthermore, by incorporating our label denoising method into other AVVP models, we find that it can further enhance parsing results.
<div id='section'>Paperid: <span id='pid'>1449, <a href='https://arxiv.org/pdf/2412.14870.pdf' target='_blank'>https://arxiv.org/pdf/2412.14870.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Isabelle Tingzon, Utku Can Ozturk, Ivan Dotu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14870">Large-scale School Mapping using Weakly Supervised Deep Learning for Universal School Connectivity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Improving global school connectivity is critical for ensuring inclusive and equitable quality education. To reliably estimate the cost of connecting schools, governments and connectivity providers require complete and accurate school location data - a resource that is often scarce in many low- and middle-income countries. To address this challenge, we propose a cost-effective, scalable approach to locating schools in high-resolution satellite images using weakly supervised deep learning techniques. Our best models, which combine vision transformers and convolutional neural networks, achieve AUPRC values above 0.96 across 10 pilot African countries. Leveraging explainable AI techniques, our approach can approximate the precise geographical coordinates of the school locations using only low-cost, classification-level annotations. To demonstrate the scalability of our method, we generate nationwide maps of school location predictions in African countries and present a detailed analysis of our results, using Senegal as our case study. Finally, we demonstrate the immediate usability of our work by introducing an interactive web mapping tool to streamline human-in-the-loop model validation efforts by government partners. This work successfully showcases the real-world utility of deep learning and satellite images for planning regional infrastructure and accelerating universal school connectivity.
<div id='section'>Paperid: <span id='pid'>1450, <a href='https://arxiv.org/pdf/2412.11237.pdf' target='_blank'>https://arxiv.org/pdf/2412.11237.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Max Riffi-Aslett, Christina Fell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11237">On the Generalizability of Iterative Patch Selection for Memory-Efficient High-Resolution Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Classifying large images with small or tiny regions of interest (ROI) is challenging due to computational and memory constraints. Weakly supervised memory-efficient patch selectors have achieved results comparable with strongly supervised methods. However, low signal-to-noise ratios and low entropy attention still cause overfitting. We explore these issues using a novel testbed on a memory-efficient cross-attention transformer with Iterative Patch Selection (IPS) as the patch selection module. Our testbed extends the megapixel MNIST benchmark to four smaller O2I (object-to-image) ratios ranging from 0.01% to 0.14% while keeping the canvas size fixed and introducing a noise generation component based on BÃ©zier curves. Experimental results generalize the observations made on CNNs to IPS whereby the O2I threshold below which the classifier fails to generalize is affected by the training dataset size. We further observe that the magnitude of this interaction differs for each task of the Megapixel MNIST. For tasks "Maj" and "Top", the rate is at its highest, followed by tasks "Max" and "Multi" where in the latter, this rate is almost at 0. Moreover, results show that in a low data setting, tuning the patch size to be smaller relative to the ROI improves generalization, resulting in an improvement of + 15% for the megapixel MNIST and + 5% for the Swedish traffic signs dataset compared to the original object-to-patch ratios in IPS. Further outcomes indicate that the similarity between the thickness of the noise component and the digits in the megapixel MNIST gradually causes IPS to fail to generalize, contributing to previous suspicions.
<div id='section'>Paperid: <span id='pid'>1451, <a href='https://arxiv.org/pdf/2412.00077.pdf' target='_blank'>https://arxiv.org/pdf/2412.00077.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nima Sedaghat, Tanawan Chatchadanoraset, Colin Orion Chandler, Ashish Mahabal, Maryam Eslami
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.00077">Selfish Evolution: Making Discoveries in Extreme Label Noise with the Help of Overfitting Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motivated by the scarcity of proper labels in an astrophysical application, we have developed a novel technique, called Selfish Evolution, which allows for the detection and correction of corrupted labels in a weakly supervised fashion. Unlike methods based on early stopping, we let the model train on the noisy dataset. Only then do we intervene and allow the model to overfit to individual samples. The ``evolution'' of the model during this process reveals patterns with enough information about the noisiness of the label, as well as its correct version. We train a secondary network on these spatiotemporal ``evolution cubes'' to correct potentially corrupted labels. We incorporate the technique in a closed-loop fashion, allowing for automatic convergence towards a mostly clean dataset, without presumptions about the state of the network in which we intervene. We evaluate on the main task of the Supernova-hunting dataset but also demonstrate efficiency on the more standard MNIST dataset.
<div id='section'>Paperid: <span id='pid'>1452, <a href='https://arxiv.org/pdf/2411.19547.pdf' target='_blank'>https://arxiv.org/pdf/2411.19547.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dihong Gong, Pu Lu, Zelong Wang, Meng Zhou, Xiuqiang He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19547">Training Agents with Weakly Supervised Feedback from Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) offer a promising basis for creating agents that can tackle complex tasks through iterative environmental interaction. Existing methods either require these agents to mimic expert-provided trajectories or rely on definitive environmental feedback for reinforcement learning which limits their application to specific scenarios like gaming or code generation. This paper introduces a novel training method for LLM-based agents using weakly supervised signals from a critic LLM, bypassing the need for expert trajectories or definitive feedback. Our agents are trained in iterative manner, where they initially generate trajectories through environmental interaction. Subsequently, a critic LLM selects a subset of good trajectories, which are then used to update the agents, enabling them to generate improved trajectories in the next iteration. Extensive tests on the API-bank dataset show consistent improvement in our agents' capabilities and comparable performance to GPT-4, despite using open-source models with much fewer parameters.
<div id='section'>Paperid: <span id='pid'>1453, <a href='https://arxiv.org/pdf/2411.18915.pdf' target='_blank'>https://arxiv.org/pdf/2411.18915.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vishnou Vinayagame, Gregory Senay, Luis MartÃ­
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18915">MATATA: Weakly Supervised End-to-End MAthematical Tool-Augmented Reasoning for Tabular Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Business documents often contain substantial tabular and textual information with numerical values, requiring mathematical reasoning for effective document understanding. While Small Language Models (SLMs) still struggle at this task, tool-augmented multi-step agents perform better, at the cost of relying on closed-source or larger models, external data, or extensive prompt-engineering. This work introduces MATATA, a novel weakly supervised end-to-end approach to train multi-step reasoning language agents for document tabular applications. MATATA presents an annotation-free paradigm for each agent to enhance 3.8B/8B SLMs. During its two-stage training, MATATA uses the final outcome of the multi-step reasoning chain as weak supervision. This approach avoids having to individually supervise each intermediate agent in the reasoning chain. By employing an adaptive planner and shared tools across different datasets, MATATA shows robust performance. Experiments demonstrate that MATATA achieves state-of-the-art on FinQA, and on TAT-QA among reasoning methods based on open-source SLMs. Although being SLM-based, MATATA closely matches GPT-4-based frameworks on TabMWP. This novel weakly supervised approach enables training an end-to-end multi-step reasoning agent without intermediate supervision, supporting future developments of cost-effective powerful agentic systems.
<div id='section'>Paperid: <span id='pid'>1454, <a href='https://arxiv.org/pdf/2411.08755.pdf' target='_blank'>https://arxiv.org/pdf/2411.08755.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sareh Soltani Nejad, Anwar Haque
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08755">Weakly-Supervised Anomaly Detection in Surveillance Videos Based on Two-Stream I3D Convolution Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The widespread implementation of urban surveillance systems has necessitated more sophisticated techniques for anomaly detection to ensure enhanced public safety. This paper presents a significant advancement in the field of anomaly detection through the application of Two-Stream Inflated 3D (I3D) Convolutional Networks. These networks substantially outperform traditional 3D Convolutional Networks (C3D) by more effectively extracting spatial and temporal features from surveillance videos, thus improving the precision of anomaly detection. Our research advances the field by implementing a weakly supervised learning framework based on Multiple Instance Learning (MIL), which uniquely conceptualizes surveillance videos as collections of 'bags' that contain instances (video clips). Each instance is innovatively processed through a ranking mechanism that prioritizes clips based on their potential to display anomalies. This novel strategy not only enhances the accuracy and precision of anomaly detection but also significantly diminishes the dependency on extensive manual annotations. Moreover, through meticulous optimization of model settings, including the choice of optimizer, our approach not only establishes new benchmarks in the performance of anomaly detection systems but also offers a scalable and efficient solution for real-world surveillance applications. This paper contributes significantly to the field of computer vision by delivering a more adaptable, efficient, and context-aware anomaly detection system, which is poised to redefine practices in urban surveillance.
<div id='section'>Paperid: <span id='pid'>1455, <a href='https://arxiv.org/pdf/2411.03082.pdf' target='_blank'>https://arxiv.org/pdf/2411.03082.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Irum Mehboob, Li Sun, Alireza Astegarpanah, Rustam Stolkin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03082">Self-supervised cross-modality learning for uncertainty-aware object detection and recognition in applications which lack pre-labelled training data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper shows how an uncertainty-aware, deep neural network can be trained to detect, recognise and localise objects in 2D RGB images, in applications lacking annotated train-ng datasets. We propose a self-supervising teacher-student pipeline, in which a relatively simple teacher classifier, trained with only a few labelled 2D thumbnails, automatically processes a larger body of unlabelled RGB-D data to teach a student network based on a modified YOLOv3 architecture. Firstly, 3D object detection with back projection is used to automatically extract and teach 2D detection and localisation information to the student network. Secondly, a weakly supervised 2D thumbnail classifier, with minimal training on a small number of hand-labelled images, is used to teach object category recognition. Thirdly, we use a Gaussian Process GP to encode and teach a robust uncertainty estimation functionality, so that the student can output confidence scores with each categorization. The resulting student significantly outperforms the same YOLO architecture trained directly on the same amount of labelled data. Our GP-based approach yields robust and meaningful uncertainty estimations for complex industrial object classifications. The end-to-end network is also capable of real-time processing, needed for robotics applications. Our method can be applied to many important industrial tasks, where labelled datasets are typically unavailable. In this paper, we demonstrate an example of detection, localisation, and object category recognition of nuclear mixed-waste materials in highly cluttered and unstructured scenes. This is critical for robotic sorting and handling of legacy nuclear waste, which poses complex environmental remediation challenges in many nuclearised nations.
<div id='section'>Paperid: <span id='pid'>1456, <a href='https://arxiv.org/pdf/2410.07460.pdf' target='_blank'>https://arxiv.org/pdf/2410.07460.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Wen, Evgenia Roussinova, Olivier Brina, Paolo Machi, Mohamed Bouri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07460">Generalizing Segmentation Foundation Model Under Sim-to-real Domain-shift for Guidewire Segmentation in X-ray Fluoroscopy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Guidewire segmentation during endovascular interventions holds the potential to significantly enhance procedural accuracy, improving visualization and providing critical feedback that can support both physicians and robotic systems in navigating complex vascular pathways. Unlike supervised segmentation networks, which need many expensive expert-annotated labels, sim-to-real domain adaptation approaches utilize synthetic data from simulations, offering a cost-effective solution. The success of models like Segment-Anything (SAM) has driven advancements in image segmentation foundation models with strong zero/few-shot generalization through prompt engineering. However, they struggle with medical images like X-ray fluoroscopy and the domain-shifts of the data. Given the challenges of acquiring annotation and the accessibility of labeled simulation data, we propose a sim-to-real domain adaption framework with a coarse-to-fine strategy to adapt SAM to X-ray fluoroscopy guidewire segmentation without any annotation on the target domain. We first generate the pseudo-labels by utilizing a simple source image style transfer technique that preserves the guidewire structure. Then, we develop a weakly supervised self-training architecture to fine-tune an end-to-end student SAM with the coarse labels by imposing consistency regularization and supervision from the teacher SAM network. We validate the effectiveness of the proposed method on a publicly available Cardiac dataset and an in-house Neurovascular dataset, where our method surpasses both pre-trained SAM and many state-of-the-art domain adaptation techniques by a large margin. Our code will be made public on GitHub soon.
<div id='section'>Paperid: <span id='pid'>1457, <a href='https://arxiv.org/pdf/2409.01330.pdf' target='_blank'>https://arxiv.org/pdf/2409.01330.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Iulian Emil Tampu, Per Nyman, Christoforos Spyretos, Ida Blystad, Alia Shamikh, Gabriela Prochazka, Teresita DÃ­az de StÃ¥hl, Johanna Sandgren, Peter Lundberg, Neda Haj-Hosseini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01330">Pediatric brain tumor classification using digital histopathology and deep learning: evaluation of SOTA methods on a multi-center Swedish cohort</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Brain tumors are the most common solid tumors in children and young adults, but the scarcity of large histopathology datasets has limited the application of computational pathology in this group. This study implements two weakly supervised multiple-instance learning (MIL) approaches on patch-features obtained from state-of-the-art histology-specific foundation models to classify pediatric brain tumors in hematoxylin and eosin whole slide images (WSIs) from a multi-center Swedish cohort. WSIs from 540 subjects (age 8.5$\pm$4.9 years) diagnosed with brain tumor were gathered from the six Swedish university hospitals. Instance (patch)-level features were obtained from WSIs using three pre-trained feature extractors: ResNet50, UNI, and CONCH. Instances were aggregated using attention-based MIL (ABMIL) or clustering-constrained attention MIL (CLAM) for patient-level classification. Models were evaluated on three classification tasks based on the hierarchical classification of pediatric brain tumors: tumor category, family, and type. Model generalization was assessed by training on data from two of the centers and testing on data from four other centers. Model interpretability was evaluated through attention mapping. The highest classification performance was achieved using UNI features and ABMIL aggregation, with Matthew's correlation coefficient of 0.76$\pm$0.04, 0.63$\pm$0.04, and 0.60$\pm$0.05 for tumor category, family, and type classification, respectively. When evaluating generalization, models utilizing UNI and CONCH features outperformed those using ResNet50. However, the drop in performance from the in-site to out-of-site testing was similar across feature extractors. These results show the potential of state-of-the-art computational pathology methods in diagnosing pediatric brain tumors at different hierarchical levels with fair generalizability on a multi-center national dataset.
<div id='section'>Paperid: <span id='pid'>1458, <a href='https://arxiv.org/pdf/2407.20600.pdf' target='_blank'>https://arxiv.org/pdf/2407.20600.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunfeng Zhao, Huiyu Zhou, Fei Wu, Xifeng Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.20600">Categorical Knowledge Fused Recognition: Fusing Hierarchical Knowledge with Image Classification through Aligning and Deep Metric Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image classification is a fundamental computer vision task and an important baseline for deep metric learning. In decades efforts have been made on enhancing image classification accuracy by using deep learning models while less attention has been paid on the reasoning aspect of the recognition, i.e., predictions could be made because of background or other surrounding objects rather than the target object. Hierarchical knowledge about image categories depicts inter-class similarities or dissimilarities. Effective fusion of such knowledge with deep learning image classification models is promising in improving target object identification and enhancing the reasoning aspect of the recognition. In this paper, we propose a novel deep metric learning based method to effectively fuse prior knowledge about image categories with mainstream backbone image classification models and enhance the reasoning aspect of the recognition in an end-to-end manner. Existing deep metric learning incorporated image classification methods mainly focus on whether sampled images are from the same class. A new triplet loss function term that aligns distances in the model latent space with those in knowledge space is presented and incorporated in the proposed method to facilitate the dual-modality fusion. Extensive experiments on the CIFAR-10, CIFAR-100, Mini-ImageNet, and ImageNet-1K datasets evaluated the proposed method, and results indicate that the proposed method is effective in enhancing the reasoning aspect of image recognition in terms of weakly-supervised object localization performance.
<div id='section'>Paperid: <span id='pid'>1459, <a href='https://arxiv.org/pdf/2407.15170.pdf' target='_blank'>https://arxiv.org/pdf/2407.15170.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhu Huang, Gang Pan, Chao Kang, YaoZhi Lv
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.15170">Semi-Supervised Pipe Video Temporal Defect Interval Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In sewer pipe Closed-Circuit Television (CCTV) inspection, accurate temporal defect localization is essential for effective defect classification, detection, segmentation and quantification. Industry standards typically do not require time-interval annotations, even though they are more informative than time-point annotations for defect localization, resulting in additional annotation costs when fully supervised methods are used. Additionally, differences in scene types and camera motion patterns between pipe inspections and Temporal Action Localization (TAL) hinder the effective transfer of point-supervised TAL methods. Therefore, this study introduces a Semi-supervised multi-Prototype-based method incorporating visual Odometry for enhanced attention guidance (PipeSPO). PipeSPO fully leverages unlabeled data through unsupervised pretext tasks and utilizes time-point annotated data with a weakly supervised multi-prototype-based method, relying on visual odometry features to capture camera pose information. Experiments on real-world datasets demonstrate that PipeSPO achieves 41.89% average precision across Intersection over Union (IoU) thresholds of 0.1-0.7, improving by 8.14% over current state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1460, <a href='https://arxiv.org/pdf/2407.13292.pdf' target='_blank'>https://arxiv.org/pdf/2407.13292.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lukuan Dong, Donghong Qin, Fengbo Bai, Fanhua Song, Yan Liu, Chen Xu, Zhijian Ou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.13292">Low-Resourced Speech Recognition for Iu Mien Language via Weakly-Supervised Phoneme-based Multilingual Pre-training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The mainstream automatic speech recognition (ASR) technology usually requires hundreds to thousands of hours of annotated speech data. Three approaches to low-resourced ASR are phoneme or subword based supervised pre-training, and self-supervised pre-training over multilingual data. The Iu Mien language is the main ethnic language of the Yao ethnic group in China and is low-resourced in the sense that the annotated speech is very limited. With less than 10 hours of transcribed Iu Mien language, this paper investigates and compares the three approaches for Iu Mien speech recognition. Our experiments are based on the recently released, three backbone models pretrained over the 10 languages from the CommonVoice dataset (CV-Lang10), which correspond to the three approaches for low-resourced ASR. It is found that phoneme supervision can achieve better results compared to subword supervision and self-supervision, thereby providing higher data-efficiency. Particularly, the Whistle models, i.e., obtained by the weakly-supervised phoneme-based multilingual pre-training, obtain the most competitive results.
<div id='section'>Paperid: <span id='pid'>1461, <a href='https://arxiv.org/pdf/2406.05794.pdf' target='_blank'>https://arxiv.org/pdf/2406.05794.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kiseung Kim, Jay-Yoon Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.05794">RE-RAG: Improving Open-Domain QA Performance and Interpretability with Relevance Estimator in Retrieval-Augmented Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Retrieval Augmented Generation (RAG) framework utilizes a combination of parametric knowledge and external knowledge to demonstrate state-of-the-art performance on open-domain question answering tasks. However, the RAG framework suffers from performance degradation when the query is accompanied by irrelevant contexts. In this work, we propose the RE-RAG framework, which introduces a relevance estimator (RE) that not only provides relative relevance between contexts as previous rerankers did, but also provides confidence, which can be used to classify whether given context is useful for answering the given question. We propose a weakly supervised method for training the RE simply utilizing question-answer data without any labels for correct contexts. We show that RE trained with a small generator (sLM) can not only improve the sLM fine-tuned together with RE but also improve previously unreferenced large language models (LLMs). Furthermore, we investigate new decoding strategies that utilize the proposed confidence measured by RE such as choosing to let the user know that it is "unanswerable" to answer the question given the retrieved contexts or choosing to rely on LLM's parametric knowledge rather than unrelated contexts.
<div id='section'>Paperid: <span id='pid'>1462, <a href='https://arxiv.org/pdf/2406.02653.pdf' target='_blank'>https://arxiv.org/pdf/2406.02653.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Reza Babaei, Samuel Cheng, Theresa Thai, Shangqing Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.02653">Pancreatic Tumor Segmentation as Anomaly Detection in CT Images Using Denoising Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the advances in medicine, cancer has remained a formidable challenge. Particularly in the case of pancreatic tumors, characterized by their diversity and late diagnosis, early detection poses a significant challenge crucial for effective treatment. The advancement of deep learning techniques, particularly supervised algorithms, has significantly propelled pancreatic tumor detection in the medical field. However, supervised deep learning approaches necessitate extensive labeled medical images for training, yet acquiring such annotations is both limited and costly. Conversely, weakly supervised anomaly detection methods, requiring only image-level annotations, have garnered interest. Existing methodologies predominantly hinge on generative adversarial networks (GANs) or autoencoder models, which can pose complexity in training and, these models may face difficulties in accurately preserving fine image details. This research presents a novel approach to pancreatic tumor detection, employing weak supervision anomaly detection through denoising diffusion algorithms. By incorporating a deterministic iterative process of adding and removing noise along with classifier guidance, the method enables seamless translation of images between diseased and healthy subjects, resulting in detailed anomaly maps without requiring complex training protocols and segmentation masks. This study explores denoising diffusion models as a recent advancement over traditional generative models like GANs, contributing to the field of pancreatic tumor detection. Recognizing the low survival rates of pancreatic cancer, this study emphasizes the need for continued research to leverage diffusion models' efficiency in medical segmentation tasks.
<div id='section'>Paperid: <span id='pid'>1463, <a href='https://arxiv.org/pdf/2405.12850.pdf' target='_blank'>https://arxiv.org/pdf/2405.12850.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jjahao Zhang, Yin Gu, Deyu Sun, Yuhua Gao, Ming Gao, Ming Cui, Teng Zhang, He Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.12850">Weakly supervised alignment and registration of MR-CT for cervical cancer radiotherapy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cervical cancer is one of the leading causes of death in women, and brachytherapy is currently the primary treatment method. However, it is important to precisely define the extent of paracervical tissue invasion to improve cancer diagnosis and treatment options. The fusion of the information characteristics of both computed tomography (CT) and magnetic resonance imaging(MRI) modalities may be useful in achieving a precise outline of the extent of paracervical tissue invasion. Registration is the initial step in information fusion. However, when aligning multimodal images with varying depths, manual alignment is prone to large errors and is time-consuming. Furthermore, the variations in the size of the Region of Interest (ROI) and the shape of multimodal images pose a significant challenge for achieving accurate registration.In this paper, we propose a preliminary spatial alignment algorithm and a weakly supervised multimodal registration network. The spatial position alignment algorithm efficiently utilizes the limited annotation information in the two modal images provided by the doctor to automatically align multimodal images with varying depths. By utilizing aligned multimodal images for weakly supervised registration and incorporating pyramidal features and cost volume to estimate the optical flow, the results indicate that the proposed method outperforms traditional volume rendering alignment methods and registration networks in various evaluation metrics. This demonstrates the effectiveness of our model in multimodal image registration.
<div id='section'>Paperid: <span id='pid'>1464, <a href='https://arxiv.org/pdf/2405.04093.pdf' target='_blank'>https://arxiv.org/pdf/2405.04093.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Da Fu, Mingfei Rong, Eun-Hu Kim, Hao Huang, Witold Pedrycz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04093">DCNN: Dual Cross-current Neural Networks Realized Using An Interactive Deep Learning Discriminator for Fine-grained Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate classification of fine-grained images remains a challenge in backbones based on convolutional operations or self-attention mechanisms. This study proposes novel dual-current neural networks (DCNN), which combine the advantages of convolutional operations and self-attention mechanisms to improve the accuracy of fine-grained image classification. The main novel design features for constructing a weakly supervised learning backbone model DCNN include (a) extracting heterogeneous data, (b) keeping the feature map resolution unchanged, (c) expanding the receptive field, and (d) fusing global representations and local features. Experimental results demonstrated that using DCNN as the backbone network for classifying certain fine-grained benchmark datasets achieved performance advantage improvements of 13.5--19.5% and 2.2--12.9%, respectively, compared to other advanced convolution or attention-based fine-grained backbones.
<div id='section'>Paperid: <span id='pid'>1465, <a href='https://arxiv.org/pdf/2404.16474.pdf' target='_blank'>https://arxiv.org/pdf/2404.16474.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhihao Shuai, Yinan Chen, Shunqiang Mao, Yihan Zho, Xiaohong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.16474">DiffSeg: A Segmentation Model for Skin Lesions Based on Diffusion Difference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised medical image segmentation (MIS) using generative models is crucial for clinical diagnosis. However, the accuracy of the segmentation results is often limited by insufficient supervision and the complex nature of medical imaging. Existing models also only provide a single outcome, which does not allow for the measurement of uncertainty. In this paper, we introduce DiffSeg, a segmentation model for skin lesions based on diffusion difference which exploits diffusion model principles to ex-tract noise-based features from images with diverse semantic information. By discerning difference between these noise features, the model identifies diseased areas. Moreover, its multi-output capability mimics doctors' annotation behavior, facilitating the visualization of segmentation result consistency and ambiguity. Additionally, it quantifies output uncertainty using Generalized Energy Distance (GED), aiding interpretability and decision-making for physicians. Finally, the model integrates outputs through the Dense Conditional Random Field (DenseCRF) algorithm to refine the segmentation boundaries by considering inter-pixel correlations, which improves the accuracy and optimizes the segmentation results. We demonstrate the effectiveness of DiffSeg on the ISIC 2018 Challenge dataset, outperforming state-of-the-art U-Net-based methods.
<div id='section'>Paperid: <span id='pid'>1466, <a href='https://arxiv.org/pdf/2404.13103.pdf' target='_blank'>https://arxiv.org/pdf/2404.13103.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marius Schmidt-Mengin, Alexis Benichoux, Shibeshih Belachew, Nikos Komodakis, Nikos Paragios
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.13103">ToNNO: Tomographic Reconstruction of a Neural Network's Output for Weakly Supervised Segmentation of 3D Medical Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Annotating lots of 3D medical images for training segmentation models is time-consuming. The goal of weakly supervised semantic segmentation is to train segmentation models without using any ground truth segmentation masks. Our work addresses the case where only image-level categorical labels, indicating the presence or absence of a particular region of interest (such as tumours or lesions), are available. Most existing methods rely on class activation mapping (CAM). We propose a novel approach, ToNNO, which is based on the Tomographic reconstruction of a Neural Network's Output. Our technique extracts stacks of slices with different angles from the input 3D volume, feeds these slices to a 2D encoder, and applies the inverse Radon transform in order to reconstruct a 3D heatmap of the encoder's predictions. This generic method allows to perform dense prediction tasks on 3D volumes using any 2D image encoder. We apply it to weakly supervised medical image segmentation by training the 2D encoder to output high values for slices containing the regions of interest. We test it on four large scale medical image datasets and outperform 2D CAM methods. We then extend ToNNO by combining tomographic reconstruction with CAM methods, proposing Averaged CAM and Tomographic CAM, which obtain even better results.
<div id='section'>Paperid: <span id='pid'>1467, <a href='https://arxiv.org/pdf/2404.07594.pdf' target='_blank'>https://arxiv.org/pdf/2404.07594.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Olatunji Mumini Omisore, Toluwanimi Akinyemi, Anh Nguyen, Lei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.07594">Weakly-Supervised Learning via Multi-Lateral Decoder Branching for Tool Segmentation in Robot-Assisted Cardiovascular Catheterization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot-assisted catheterization has garnered a good attention for its potentials in treating cardiovascular diseases. However, advancing surgeon-robot collaboration still requires further research, particularly on task-specific automation. For instance, automated tool segmentation can assist surgeons in visualizing and tracking of endovascular tools during cardiac procedures. While learning-based models have demonstrated state-of-the-art segmentation performances, generating ground-truth labels for fully-supervised methods is both labor-intensive time consuming, and costly. In this study, we propose a weakly-supervised learning method with multi-lateral pseudo labeling for tool segmentation in cardiovascular angiogram datasets. The method utilizes a modified U-Net architecture featuring one encoder and multiple laterally branched decoders. The decoders generate diverse pseudo labels under different perturbations, augmenting available partial labels. The pseudo labels are self-generated using a mixed loss function with shared consistency across the decoders. The weakly-supervised model was trained end-to-end and validated using partially annotated angiogram data from three cardiovascular catheterization procedures. Validation results show that the model could perform closer to fully-supervised models. Also, the proposed weakly-supervised multi-lateral method outperforms three well known methods used for weakly-supervised learning, offering the highest segmentation performance across the three angiogram datasets. Furthermore, numerous ablation studies confirmed the model's consistent performance under different parameters. Finally, the model was applied for tool segmentation in a robot-assisted catheterization experiments. The model enhanced visualization with high connectivity indices for guidewire and catheter, and a mean processing time of 35 ms per frame.
<div id='section'>Paperid: <span id='pid'>1468, <a href='https://arxiv.org/pdf/2404.03394.pdf' target='_blank'>https://arxiv.org/pdf/2404.03394.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Izumi Fujimori, Masaki Oono, Masami Shishibori
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.03394">Background Noise Reduction of Attention Map for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In weakly-supervised semantic segmentation (WSSS) using only image-level class labels, a problem with CNN-based Class Activation Maps (CAM) is that they tend to activate the most discriminative local regions of objects. On the other hand, methods based on Transformers learn global features but suffer from the issue of background noise contamination. This paper focuses on addressing the issue of background noise in attention weights within the existing WSSS method based on Conformer, known as TransCAM. The proposed method successfully reduces background noise, leading to improved accuracy of pseudo labels. Experimental results demonstrate that our model achieves segmentation performance of 70.5% on the PASCAL VOC 2012 validation data, 71.1% on the test data, and 45.9% on MS COCO 2014 data, outperforming TransCAM in terms of segmentation performance.
<div id='section'>Paperid: <span id='pid'>1469, <a href='https://arxiv.org/pdf/2403.19306.pdf' target='_blank'>https://arxiv.org/pdf/2403.19306.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuyang Shang, Tian Ma, Wanzhu Ren, Yuancheng Li, Jiayi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.19306">Sparse Generation: Making Pseudo Labels Sparse for Point Weakly Supervised Object Detection on Low Data Volume</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing pseudo label generation methods for point weakly supervised object detection are inadequate in low data volume and dense object detection tasks. We consider the generation of weakly supervised pseudo labels as the model's sparse output, and propose Sparse Generation as a solution to make pseudo labels sparse. The method employs three processing stages (Mapping, Mask, Regression), constructs dense tensors through the relationship between data and detector model, optimizes three of its parameters, and obtains a sparse tensor, thereby indirectly obtaining higher quality pseudo labels, and addresses the model's density problem on low data volume. Additionally, we propose perspective-based matching, which provides more rational pseudo boxes for prediction missed on instances. In comparison to the SOTA method, on four datasets (MS COCO-val, RSOD, SIMD, Bullet-Hole), the experimental results demonstrated a significant advantage.
<div id='section'>Paperid: <span id='pid'>1470, <a href='https://arxiv.org/pdf/2403.19225.pdf' target='_blank'>https://arxiv.org/pdf/2403.19225.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Angchi Xu, Wei-Shi Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.19225">Efficient and Effective Weakly-Supervised Action Segmentation via Action-Transition-Aware Boundary Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised action segmentation is a task of learning to partition a long video into several action segments, where training videos are only accompanied by transcripts (ordered list of actions). Most of existing methods need to infer pseudo segmentation for training by serial alignment between all frames and the transcript, which is time-consuming and hard to be parallelized while training. In this work, we aim to escape from this inefficient alignment with massive but redundant frames, and instead to directly localize a few action transitions for pseudo segmentation generation, where a transition refers to the change from an action segment to its next adjacent one in the transcript. As the true transitions are submerged in noisy boundaries due to intra-segment visual variation, we propose a novel Action-Transition-Aware Boundary Alignment (ATBA) framework to efficiently and effectively filter out noisy boundaries and detect transitions. In addition, to boost the semantic learning in the case that noise is inevitably present in the pseudo segmentation, we also introduce video-level losses to utilize the trusted video-level supervision. Extensive experiments show the effectiveness of our approach on both performance and training speed.
<div id='section'>Paperid: <span id='pid'>1471, <a href='https://arxiv.org/pdf/2403.18134.pdf' target='_blank'>https://arxiv.org/pdf/2403.18134.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhan Shi, Jingwei Zhang, Jun Kong, Fusheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.18134">Integrative Graph-Transformer Framework for Histopathology Whole Slide Image Representation and Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In digital pathology, the multiple instance learning (MIL) strategy is widely used in the weakly supervised histopathology whole slide image (WSI) classification task where giga-pixel WSIs are only labeled at the slide level. However, existing attention-based MIL approaches often overlook contextual information and intrinsic spatial relationships between neighboring tissue tiles, while graph-based MIL frameworks have limited power to recognize the long-range dependencies. In this paper, we introduce the integrative graph-transformer framework that simultaneously captures the context-aware relational features and global WSI representations through a novel Graph Transformer Integration (GTI) block. Specifically, each GTI block consists of a Graph Convolutional Network (GCN) layer modeling neighboring relations at the local instance level and an efficient global attention model capturing comprehensive global information from extensive feature embeddings. Extensive experiments on three publicly available WSI datasets: TCGA-NSCLC, TCGA-RCC and BRIGHT, demonstrate the superiority of our approach over current state-of-the-art MIL methods, achieving an improvement of 1.0% to 2.6% in accuracy and 0.7%-1.6% in AUROC.
<div id='section'>Paperid: <span id='pid'>1472, <a href='https://arxiv.org/pdf/2403.14829.pdf' target='_blank'>https://arxiv.org/pdf/2403.14829.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>F. M. Castro-MacÃ­as, P. Morales-Ãlvarez, Y. Wu, R. Molina, A. K. Katsaggelos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.14829">Hyperbolic Secant representation of the logistic function: Application to probabilistic Multiple Instance Learning for CT intracranial hemorrhage detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple Instance Learning (MIL) is a weakly supervised paradigm that has been successfully applied to many different scientific areas and is particularly well suited to medical imaging. Probabilistic MIL methods, and more specifically Gaussian Processes (GPs), have achieved excellent results due to their high expressiveness and uncertainty quantification capabilities. One of the most successful GP-based MIL methods, VGPMIL, resorts to a variational bound to handle the intractability of the logistic function. Here, we formulate VGPMIL using PÃ³lya-Gamma random variables. This approach yields the same variational posterior approximations as the original VGPMIL, which is a consequence of the two representations that the Hyperbolic Secant distribution admits. This leads us to propose a general GP-based MIL method that takes different forms by simply leveraging distributions other than the Hyperbolic Secant one. Using the Gamma distribution we arrive at a new approach that obtains competitive or superior predictive performance and efficiency. This is validated in a comprehensive experimental study including one synthetic MIL dataset, two well-known MIL benchmarks, and a real-world medical problem. We expect that this work provides useful ideas beyond MIL that can foster further research in the field.
<div id='section'>Paperid: <span id='pid'>1473, <a href='https://arxiv.org/pdf/2403.13429.pdf' target='_blank'>https://arxiv.org/pdf/2403.13429.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaushalya Kularatnam, Tania Stathaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13429">Detecting and Triaging Spoofing using Temporal Convolutional Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As algorithmic trading and electronic markets continue to transform the landscape of financial markets, detecting and deterring rogue agents to maintain a fair and efficient marketplace is crucial. The explosion of large datasets and the continually changing tricks of the trade make it difficult to adapt to new market conditions and detect bad actors. To that end, we propose a framework that can be adapted easily to various problems in the space of detecting market manipulation. Our approach entails initially employing a labelling algorithm which we use to create a training set to learn a weakly supervised model to identify potentially suspicious sequences of order book states. The main goal here is to learn a representation of the order book that can be used to easily compare future events. Subsequently, we posit the incorporation of expert assessment to scrutinize specific flagged order book states. In the event of an expert's unavailability, recourse is taken to the application of a more complex algorithm on the identified suspicious order book states. We then conduct a similarity search between any new representation of the order book against the expert labelled representations to rank the results of the weak learner. We show some preliminary results that are promising to explore further in this direction
<div id='section'>Paperid: <span id='pid'>1474, <a href='https://arxiv.org/pdf/2403.12212.pdf' target='_blank'>https://arxiv.org/pdf/2403.12212.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ramon Abilio, Guilherme Palermo Coelho, Ana Estela Antunes da Silva
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.12212">Evaluating Named Entity Recognition: A comparative analysis of mono- and multilingual transformer models on a novel Brazilian corporate earnings call transcripts dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Since 2018, when the Transformer architecture was introduced, Natural Language Processing has gained significant momentum with pre-trained Transformer-based models that can be fine-tuned for various tasks. Most models are pre-trained on large English corpora, making them less applicable to other languages, such as Brazilian Portuguese. In our research, we identified two models pre-trained in Brazilian Portuguese (BERTimbau and PTT5) and two multilingual models (mBERT and mT5). BERTimbau and mBERT use only the Encoder module, while PTT5 and mT5 use both the Encoder and Decoder. Our study aimed to evaluate their performance on a financial Named Entity Recognition (NER) task and determine the computational requirements for fine-tuning and inference. To this end, we developed the Brazilian Financial NER (BraFiNER) dataset, comprising sentences from Brazilian banks' earnings calls transcripts annotated using a weakly supervised approach. Additionally, we introduced a novel approach that reframes the token classification task as a text generation problem. After fine-tuning the models, we evaluated them using performance and error metrics. Our findings reveal that BERT-based models consistently outperform T5-based models. While the multilingual models exhibit comparable macro F1-scores, BERTimbau demonstrates superior performance over PTT5. In terms of error metrics, BERTimbau outperforms the other models. We also observed that PTT5 and mT5 generated sentences with changes in monetary and percentage values, highlighting the importance of accuracy and consistency in the financial domain. Our findings provide insights into the differing performance of BERT- and T5-based models for the NER task.
<div id='section'>Paperid: <span id='pid'>1475, <a href='https://arxiv.org/pdf/2403.01811.pdf' target='_blank'>https://arxiv.org/pdf/2403.01811.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Felix KÃ¼nnecke, Anna Filighera, Colin Leong, Tim Steuer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.01811">Enhancing Multi-Domain Automatic Short Answer Grading through an Explainable Neuro-Symbolic Pipeline</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Grading short answer questions automatically with interpretable reasoning behind the grading decision is a challenging goal for current transformer approaches. Justification cue detection, in combination with logical reasoners, has shown a promising direction for neuro-symbolic architectures in ASAG. But, one of the main challenges is the requirement of annotated justification cues in the students' responses, which only exist for a few ASAG datasets. To overcome this challenge, we contribute (1) a weakly supervised annotation procedure for justification cues in ASAG datasets, and (2) a neuro-symbolic model for explainable ASAG based on justification cues. Our approach improves upon the RMSE by 0.24 to 0.3 compared to the state-of-the-art on the Short Answer Feedback dataset in a bilingual, multi-domain, and multi-question training setup. This result shows that our approach provides a promising direction for generating high-quality grades and accompanying explanations for future research in ASAG and educational NLP.
<div id='section'>Paperid: <span id='pid'>1476, <a href='https://arxiv.org/pdf/2402.17792.pdf' target='_blank'>https://arxiv.org/pdf/2402.17792.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Leite, Alisson Silva, Gabriella Casalino, Arnab Sharma, Danielle Fortunato, Axel-Cyrille Ngomo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17792">EGNN-C+: Interpretable Evolving Granular Neural Network and Application in Classification of Weakly-Supervised EEG Data Streams</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a modified incremental learning algorithm for evolving Granular Neural Network Classifiers (eGNN-C+). We use double-boundary hyper-boxes to represent granules, and customize the adaptation procedures to enhance the robustness of outer boxes for data coverage and noise suppression, while ensuring that inner boxes remain flexible to capture drifts. The classifier evolves from scratch, incorporates new classes on the fly, and performs local incremental feature weighting. As an application, we focus on the classification of emotion-related patterns within electroencephalogram (EEG) signals. Emotion recognition is crucial for enhancing the realism and interactivity of computer systems. We extract features from the Fourier spectrum of EEG signals obtained from 28 individuals engaged in playing computer games -- a public dataset. Each game elicits a different predominant emotion: boredom, calmness, horror, or joy. We analyze individual electrodes, time window lengths, and frequency bands to assess the accuracy and interpretability of resulting user-independent neural models. The findings indicate that both brain hemispheres assist classification, especially electrodes on the temporal (T8) and parietal (P7) areas, alongside contributions from frontal and occipital electrodes. While patterns may manifest in any band, the Alpha (8-13Hz), Delta (1-4Hz), and Theta (4-8Hz) bands, in this order, exhibited higher correspondence with the emotion classes. The eGNN-C+ demonstrates effectiveness in learning EEG data. It achieves an accuracy of 81.7% and a 0.0029 II interpretability using 10-second time windows, even in face of a highly-stochastic time-varying 4-class classification problem.
<div id='section'>Paperid: <span id='pid'>1477, <a href='https://arxiv.org/pdf/2401.14074.pdf' target='_blank'>https://arxiv.org/pdf/2401.14074.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Y. Liu, L. Lin, K. K. Y. Wong, X. Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.14074">ProCNS: Progressive Prototype Calibration and Noise Suppression for Weakly-Supervised Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised segmentation (WSS) has emerged as a solution to mitigate the conflict between annotation cost and model performance by adopting sparse annotation formats (e.g., point, scribble, block, etc.). Typical approaches attempt to exploit anatomy and topology priors to directly expand sparse annotations into pseudo-labels. However, due to a lack of attention to the ambiguous edges in medical images and insufficient exploration of sparse supervision, existing approaches tend to generate erroneous and overconfident pseudo proposals in noisy regions, leading to cumulative model error and performance degradation. In this work, we propose a novel WSS approach, named ProCNS, encompassing two synergistic modules devised with the principles of progressive prototype calibration and noise suppression. Specifically, we design a Prototype-based Regional Spatial Affinity (PRSA) loss to maximize the pair-wise affinities between spatial and semantic elements, providing our model of interest with more reliable guidance. The affinities are derived from the input images and the prototype-refined predictions. Meanwhile, we propose an Adaptive Noise Perception and Masking (ANPM) module to obtain more enriched and representative prototype representations, which adaptively identifies and masks noisy regions within the pseudo proposals, reducing potential erroneous interference during prototype computation. Furthermore, we generate specialized soft pseudo-labels for the noisy regions identified by ANPM, providing supplementary supervision. Extensive experiments on six medical image segmentation tasks involving different modalities demonstrate that the proposed framework significantly outperforms representative state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1478, <a href='https://arxiv.org/pdf/2401.03312.pdf' target='_blank'>https://arxiv.org/pdf/2401.03312.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arjun Bhalla, Daniel Levenson, Jan Bernhard, Anton Abilov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.03312">Exploiting Data Hierarchy as a New Modality for Contrastive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work investigates how hierarchically structured data can help neural networks learn conceptual representations of cathedrals. The underlying WikiScenes dataset provides a spatially organized hierarchical structure of cathedral components. We propose a novel hierarchical contrastive training approach that leverages a triplet margin loss to represent the data's spatial hierarchy in the encoder's latent space. As such, the proposed approach investigates if the dataset structure provides valuable information for self-supervised learning. We apply t-SNE to visualize the resultant latent space and evaluate the proposed approach by comparing it with other dataset-specific contrastive learning methods using a common downstream classification task. The proposed method outperforms the comparable weakly-supervised and baseline methods. Our findings suggest that dataset structure is a valuable modality for weakly-supervised learning.
<div id='section'>Paperid: <span id='pid'>1479, <a href='https://arxiv.org/pdf/2312.17429.pdf' target='_blank'>https://arxiv.org/pdf/2312.17429.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meghana Holla, Ismini Lourentzou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.17429">Commonsense for Zero-Shot Natural Language Video Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Zero-shot Natural Language-Video Localization (NLVL) methods have exhibited promising results in training NLVL models exclusively with raw video data by dynamically generating video segments and pseudo-query annotations. However, existing pseudo-queries often lack grounding in the source video, resulting in unstructured and disjointed content. In this paper, we investigate the effectiveness of commonsense reasoning in zero-shot NLVL. Specifically, we present CORONET, a zero-shot NLVL framework that leverages commonsense to bridge the gap between videos and generated pseudo-queries via a commonsense enhancement module. CORONET employs Graph Convolution Networks (GCN) to encode commonsense information extracted from a knowledge graph, conditioned on the video, and cross-attention mechanisms to enhance the encoded video and pseudo-query representations prior to localization. Through empirical evaluations on two benchmark datasets, we demonstrate that CORONET surpasses both zero-shot and weakly supervised baselines, achieving improvements up to 32.13% across various recall thresholds and up to 6.33% in mIoU. These results underscore the significance of leveraging commonsense reasoning for zero-shot NLVL.
<div id='section'>Paperid: <span id='pid'>1480, <a href='https://arxiv.org/pdf/2312.15526.pdf' target='_blank'>https://arxiv.org/pdf/2312.15526.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kalpa Subbaih, Bharath Kumar Bolla
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.15526">Aspect category learning and sentimental analysis using weakly supervised learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The surge of e-commerce reviews has presented a challenge in manually annotating the vast volume of reviews to comprehend their underlying aspects and sentiments. This research focused on leveraging weakly supervised learning to tackle aspect category learning and the sentiment classification of reviews. Our approach involves the generation of labels for both aspects and sentiments, employing the Snorkel framework of WSL, which incorporates aspect terms, review sentiment scores, and review ratings as sources of weak signals. This innovative strategy significantly reduces the laborious labeling efforts required for processing such extensive datasets. In this study, we deployed hybrid models, namely BiLSTM, CNN-BiLSTM, and CNN-LSTM, which harness multiple inputs, including review text, aspect terms, and ratings. Our proposed model employs two distinct loss functions: Binary Cross Entropy with Sigmoid Activation for Multi-Label Classification, enabling us to learn aspect Labels such as Quality, Usability, Service, Size, and Price, and Categorical Cross Entropy with Softmax Activations for Multi-Class Classification. Subsequently, we meticulously evaluate the performance metrics of these three implemented models, including Macro F1 score and Macro Precision. CNN & Bi-LSTM model attained 0.78 and 0.79 F1 scores on aspect and sentiment identification, respectively. The outcomes of this research are poised to make a substantial contribution to e-commerce platforms, offering an efficient and automated means to label and analyze vast troves of user reviews.
<div id='section'>Paperid: <span id='pid'>1481, <a href='https://arxiv.org/pdf/2312.15162.pdf' target='_blank'>https://arxiv.org/pdf/2312.15162.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ning Wang, Jiajun Deng, Mingbo Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.15162">Cycle-Consistency Learning for Captioning and Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present that visual grounding and image captioning, which perform as two mutually inverse processes, can be bridged together for collaborative training by careful designs. By consolidating this idea, we introduce CyCo, a cyclic-consistent learning framework to ameliorate the independent training pipelines of visual grounding and image captioning. The proposed framework (1) allows the semi-weakly supervised training of visual grounding; (2) improves the performance of fully supervised visual grounding; (3) yields a general captioning model that can describe arbitrary image regions. Extensive experiments show that our fully supervised grounding model achieves state-of-the-art performance, and the semi-weakly supervised one also exhibits competitive performance compared to the fully supervised counterparts. Our image captioning model has the capability to freely describe image regions and meanwhile shows impressive performance on prevalent captioning benchmarks.
<div id='section'>Paperid: <span id='pid'>1482, <a href='https://arxiv.org/pdf/2312.14812.pdf' target='_blank'>https://arxiv.org/pdf/2312.14812.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David de la Rosa, Antonio J Rivera, MarÃ­a J del Jesus, Francisco Charte
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.14812">PARDINUS: Weakly supervised discarding of photo-trapping empty images based on autoencoders</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Photo-trapping cameras are widely employed for wildlife monitoring. Those cameras take photographs when motion is detected to capture images where animals appear. A significant portion of these images are empty - no wildlife appears in the image. Filtering out those images is not a trivial task since it requires hours of manual work from biologists. Therefore, there is a notable interest in automating this task. Automatic discarding of empty photo-trapping images is still an open field in the area of Machine Learning. Existing solutions often rely on state-of-the-art supervised convolutional neural networks that require the annotation of the images in the training phase. PARDINUS (Weakly suPervised discARDINg of photo-trapping empty images based on aUtoencoderS) is constructed on the foundation of weakly supervised learning and proves that this approach equals or even surpasses other fully supervised methods that require further labeling work.
<div id='section'>Paperid: <span id='pid'>1483, <a href='https://arxiv.org/pdf/2311.16042.pdf' target='_blank'>https://arxiv.org/pdf/2311.16042.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jane Wu, Diego Thomas, Ronald Fedkiw
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.16042">Weakly-Supervised 3D Reconstruction of Clothed Humans via Normal Maps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel deep learning-based approach to the 3D reconstruction of clothed humans using weak supervision via 2D normal maps. Given a single RGB image or multiview images, our network infers a signed distance function (SDF) discretized on a tetrahedral mesh surrounding the body in a rest pose. Subsequently, inferred pose and camera parameters are used to generate a normal map from the SDF. A key aspect of our approach is the use of Marching Tetrahedra to (uniquely) compute a triangulated surface from the SDF on the tetrahedral mesh, facilitating straightforward differentiation (and thus backpropagation). Thus, given only ground truth normal maps (with no volumetric information ground truth information), we can train the network to produce SDF values from corresponding RGB images. Optionally, an additional multiview loss leads to improved results. We demonstrate the efficacy of our approach for both network inference and 3D reconstruction.
<div id='section'>Paperid: <span id='pid'>1484, <a href='https://arxiv.org/pdf/2311.15080.pdf' target='_blank'>https://arxiv.org/pdf/2311.15080.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shentong Mo, Bhiksha Raj
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.15080">Weakly-Supervised Audio-Visual Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-visual segmentation is a challenging task that aims to predict pixel-level masks for sound sources in a video. Previous work applied a comprehensive manually designed architecture with countless pixel-wise accurate masks as supervision. However, these pixel-level masks are expensive and not available in all cases. In this work, we aim to simplify the supervision as the instance-level annotation, i.e., weakly-supervised audio-visual segmentation. We present a novel Weakly-Supervised Audio-Visual Segmentation framework, namely WS-AVS, that can learn multi-scale audio-visual alignment with multi-scale multiple-instance contrastive learning for audio-visual segmentation. Extensive experiments on AVSBench demonstrate the effectiveness of our WS-AVS in the weakly-supervised audio-visual segmentation of single-source and multi-source scenarios.
<div id='section'>Paperid: <span id='pid'>1485, <a href='https://arxiv.org/pdf/2311.11214.pdf' target='_blank'>https://arxiv.org/pdf/2311.11214.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anjali Sharma, Priya Banerjee, Nikhil Singh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.11214">Infrared image identification method of substation equipment fault under weak supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study presents a weakly supervised method for identifying faults in infrared images of substation equipment. It utilizes the Faster RCNN model for equipment identification, enhancing detection accuracy through modifications to the model's network structure and parameters. The method is exemplified through the analysis of infrared images captured by inspection robots at substations. Performance is validated against manually marked results, demonstrating that the proposed algorithm significantly enhances the accuracy of fault identification across various equipment types.
<div id='section'>Paperid: <span id='pid'>1486, <a href='https://arxiv.org/pdf/2311.04678.pdf' target='_blank'>https://arxiv.org/pdf/2311.04678.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Watkinson Gabriel, Cohen Ethan, Bourriez Nicolas, Bendidi Ihab, Bollot Guillaume, Genovesio Auguste
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.04678">Weakly supervised cross-modal learning in high-content screening</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the surge in available data from various modalities, there is a growing need to bridge the gap between different data types. In this work, we introduce a novel approach to learn cross-modal representations between image data and molecular representations for drug discovery. We propose EMM and IMM, two innovative loss functions built on top of CLIP that leverage weak supervision and cross sites replicates in High-Content Screening. Evaluating our model against known baseline on cross-modal retrieval, we show that our proposed approach allows to learn better representations and mitigate batch effect. In addition, we also present a preprocessing method for the JUMP-CP dataset that effectively reduce the required space from 85Tb to a mere usable 7Tb size, still retaining all perturbations and most of the information content.
<div id='section'>Paperid: <span id='pid'>1487, <a href='https://arxiv.org/pdf/2309.16627.pdf' target='_blank'>https://arxiv.org/pdf/2309.16627.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shreyas H Ramananda, Vaanathi Sundaresan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.16627">Class Activation Map-based Weakly supervised Hemorrhage Segmentation using Resnet-LSTM in Non-Contrast Computed Tomography images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In clinical settings, intracranial hemorrhages (ICH) are routinely diagnosed using non-contrast CT (NCCT) for severity assessment. Accurate automated segmentation of ICH lesions is the initial and essential step, immensely useful for such assessment. However, compared to other structural imaging modalities such as MRI, in NCCT images ICH appears with very low contrast and poor SNR. Over recent years, deep learning (DL)-based methods have shown great potential, however, training them requires a huge amount of manually annotated lesion-level labels, with sufficient diversity to capture the characteristics of ICH. In this work, we propose a novel weakly supervised DL method for ICH segmentation on NCCT scans, using image-level binary classification labels, which are less time-consuming and labor-efficient when compared to the manual labeling of individual ICH lesions. Our method initially determines the approximate location of ICH using class activation maps from a classification network, which is trained to learn dependencies across contiguous slices. We further refine the ICH segmentation using pseudo-ICH masks obtained in an unsupervised manner. The method is flexible and uses a computationally light architecture during testing. On evaluating our method on the validation data of the MICCAI 2022 INSTANCE challenge, our method achieves a Dice value of 0.55, comparable with those of existing weakly supervised method (Dice value of 0.47), despite training on a much smaller training data.
<div id='section'>Paperid: <span id='pid'>1488, <a href='https://arxiv.org/pdf/2309.16309.pdf' target='_blank'>https://arxiv.org/pdf/2309.16309.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yidan Fan, Yongxin Yu, Wenhuan Lu, Yahong Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.16309">Weakly-Supervised Video Anomaly Detection with Snippet Anomalous Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With a focus on abnormal events contained within untrimmed videos, there is increasing interest among researchers in video anomaly detection. Among different video anomaly detection scenarios, weakly-supervised video anomaly detection poses a significant challenge as it lacks frame-wise labels during the training stage, only relying on video-level labels as coarse supervision. Previous methods have made attempts to either learn discriminative features in an end-to-end manner or employ a twostage self-training strategy to generate snippet-level pseudo labels. However, both approaches have certain limitations. The former tends to overlook informative features at the snippet level, while the latter can be susceptible to noises. In this paper, we propose an Anomalous Attention mechanism for weakly-supervised anomaly detection to tackle the aforementioned problems. Our approach takes into account snippet-level encoded features without the supervision of pseudo labels. Specifically, our approach first generates snippet-level anomalous attention and then feeds it together with original anomaly scores into a Multi-branch Supervision Module. The module learns different areas of the video, including areas that are challenging to detect, and also assists the attention optimization. Experiments on benchmark datasets XDViolence and UCF-Crime verify the effectiveness of our method. Besides, thanks to the proposed snippet-level attention, we obtain a more precise anomaly localization.
<div id='section'>Paperid: <span id='pid'>1489, <a href='https://arxiv.org/pdf/2309.13264.pdf' target='_blank'>https://arxiv.org/pdf/2309.13264.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Javaria Farooq, Nayyer Aafaq, M Khizer Ali Khan, Ammar Saleem, M Ibraheem Siddiqui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.13264">Randomize to Generalize: Domain Randomization for Runway FOD Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tiny Object Detection is challenging due to small size, low resolution, occlusion, background clutter, lighting conditions and small object-to-image ratio. Further, object detection methodologies often make underlying assumption that both training and testing data remain congruent. However, this presumption often leads to decline in performance when model is applied to out-of-domain(unseen) data. Techniques like synthetic image generation are employed to improve model performance by leveraging variations in input data. Such an approach typically presumes access to 3D-rendered datasets. In contrast, we propose a novel two-stage methodology Synthetic Randomized Image Augmentation (SRIA), carefully devised to enhance generalization capabilities of models encountering 2D datasets, particularly with lower resolution which is more practical in real-world scenarios. The first stage employs a weakly supervised technique to generate pixel-level segmentation masks. Subsequently, the second stage generates a batch-wise synthesis of artificial images, carefully designed with an array of diverse augmentations. The efficacy of proposed technique is illustrated on challenging foreign object debris (FOD) detection. We compare our results with several SOTA models including CenterNet, SSD, YOLOv3, YOLOv4, YOLOv5, and Outer Vit on a publicly available FOD-A dataset. We also construct an out-of-distribution test set encompassing 800 annotated images featuring a corpus of ten common categories. Notably, by harnessing merely 1.81% of objects from source training data and amalgamating with 29 runway background images, we generate 2227 synthetic images. Subsequent model retraining via transfer learning, utilizing enriched dataset generated by domain randomization, demonstrates significant improvement in detection accuracy. We report that detection accuracy improved from an initial 41% to 92% for OOD test set.
<div id='section'>Paperid: <span id='pid'>1490, <a href='https://arxiv.org/pdf/2309.13072.pdf' target='_blank'>https://arxiv.org/pdf/2309.13072.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xianggen Liu, Zhengdong Lu, Lili Mou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.13072">Weakly Supervised Reasoning by Neuro-Symbolic Approaches</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning has largely improved the performance of various natural language processing (NLP) tasks. However, most deep learning models are black-box machinery, and lack explicit interpretation. In this chapter, we will introduce our recent progress on neuro-symbolic approaches to NLP, which combines different schools of AI, namely, symbolism and connectionism. Generally, we will design a neural system with symbolic latent structures for an NLP task, and apply reinforcement learning or its relaxation to perform weakly supervised reasoning in the downstream task. Our framework has been successfully applied to various tasks, including table query reasoning, syntactic structure reasoning, information extraction reasoning, and rule reasoning. For each application, we will introduce the background, our approach, and experimental results.
<div id='section'>Paperid: <span id='pid'>1491, <a href='https://arxiv.org/pdf/2309.10421.pdf' target='_blank'>https://arxiv.org/pdf/2309.10421.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maarten Burger, Rob Wijnhoven, Shaodi You
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.10421">Exploring Different Levels of Supervision for Detecting and Localizing Solar Panels on Remote Sensing Imagery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study investigates object presence detection and localization in remote sensing imagery, focusing on solar panel recognition. We explore different levels of supervision, evaluating three models: a fully supervised object detector, a weakly supervised image classifier with CAM-based localization, and a minimally supervised anomaly detector. The classifier excels in binary presence detection (0.79 F1-score), while the object detector (0.72) offers precise localization. The anomaly detector requires more data for viable performance. Fusion of model results shows potential accuracy gains. CAM impacts localization modestly, with GradCAM, GradCAM++, and HiResCAM yielding superior results. Notably, the classifier remains robust with less data, in contrast to the object detector.
<div id='section'>Paperid: <span id='pid'>1492, <a href='https://arxiv.org/pdf/2309.09060.pdf' target='_blank'>https://arxiv.org/pdf/2309.09060.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yueyang Li, Yonghong Hou, Wanqing Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.09060">Sub-action Prototype Learning for Point-level Weakly-supervised Temporal Action Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Point-level weakly-supervised temporal action localization (PWTAL) aims to localize actions with only a single timestamp annotation for each action instance. Existing methods tend to mine dense pseudo labels to alleviate the label sparsity, but overlook the potential sub-action temporal structures, resulting in inferior performance. To tackle this problem, we propose a novel sub-action prototype learning framework (SPL-Loc) which comprises Sub-action Prototype Clustering (SPC) and Ordered Prototype Alignment (OPA). SPC adaptively extracts representative sub-action prototypes which are capable to perceive the temporal scale and spatial content variation of action instances. OPA selects relevant prototypes to provide completeness clue for pseudo label generation by applying a temporal alignment loss. As a result, pseudo labels are derived from alignment results to improve action boundary prediction. Extensive experiments on three popular benchmarks demonstrate that the proposed SPL-Loc significantly outperforms existing SOTA PWTAL methods.
<div id='section'>Paperid: <span id='pid'>1493, <a href='https://arxiv.org/pdf/2309.07823.pdf' target='_blank'>https://arxiv.org/pdf/2309.07823.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiqiao Meng, Zonglin Di, Siwei Yang, Yin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.07823">Large-scale Weakly Supervised Learning for Road Extraction from Satellite Imagery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic road extraction from satellite imagery using deep learning is a viable alternative to traditional manual mapping. Therefore it has received considerable attention recently. However, most of the existing methods are supervised and require pixel-level labeling, which is tedious and error-prone. To make matters worse, the earth has a diverse range of terrain, vegetation, and man-made objects. It is well known that models trained in one area generalize poorly to other areas. Various shooting conditions such as light and angel, as well as different image processing techniques further complicate the issue. It is impractical to develop training data to cover all image styles. This paper proposes to leverage OpenStreetMap road data as weak labels and large scale satellite imagery to pre-train semantic segmentation models. Our extensive experimental results show that the prediction accuracy increases with the amount of the weakly labeled data, as well as the road density in the areas chosen for training. Using as much as 100 times more data than the widely used DeepGlobe road dataset, our model with the D-LinkNet architecture and the ResNet-50 backbone exceeds the top performer of the current DeepGlobe leaderboard. Furthermore, due to large-scale pre-training, our model generalizes much better than those trained with only the curated datasets, implying great application potential.
<div id='section'>Paperid: <span id='pid'>1494, <a href='https://arxiv.org/pdf/2309.04105.pdf' target='_blank'>https://arxiv.org/pdf/2309.04105.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zuojin Tang, Bo Sun, Tongwei Ma, Daosheng Li, Zhenhui Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.04105">Weakly Supervised Point Clouds Transformer for 3D Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The annotation of 3D datasets is required for semantic-segmentation and object detection in scene understanding. In this paper we present a framework for the weakly supervision of a point clouds transformer that is used for 3D object detection. The aim is to decrease the required amount of supervision needed for training, as a result of the high cost of annotating a 3D datasets. We propose an Unsupervised Voting Proposal Module, which learns randomly preset anchor points and uses voting network to select prepared anchor points of high quality. Then it distills information into student and teacher network. In terms of student network, we apply ResNet network to efficiently extract local characteristics. However, it also can lose much global information. To provide the input which incorporates the global and local information as the input of student networks, we adopt the self-attention mechanism of transformer to extract global features, and the ResNet layers to extract region proposals. The teacher network supervises the classification and regression of the student network using the pre-trained model on ImageNet. On the challenging KITTI datasets, the experimental results have achieved the highest level of average precision compared with the most recent weakly supervised 3D object detectors.
<div id='section'>Paperid: <span id='pid'>1495, <a href='https://arxiv.org/pdf/2308.15960.pdf' target='_blank'>https://arxiv.org/pdf/2308.15960.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Harshith Mohan Kumar, Sean Lawrence
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.15960">Fusing Pseudo Labels with Weak Supervision for Dynamic Traffic Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advanced Driver Assistance Systems (ADAS) have made significant strides, capitalizing on computer vision to enhance perception and decision-making capabilities. Nonetheless, the adaptation of these systems to diverse traffic scenarios poses challenges due to shifts in data distribution stemming from factors such as location, weather, and road infrastructure. To tackle this, we introduce a weakly-supervised label unification pipeline that amalgamates pseudo labels from a multitude of object detection models trained on heterogeneous datasets. Our pipeline engenders a unified label space through the amalgamation of labels from disparate datasets, rectifying bias and enhancing generalization. We fine-tune multiple object detection models on individual datasets, subsequently crafting a unified dataset featuring pseudo labels, meticulously validated for precision. Following this, we retrain a solitary object detection model using the merged label space, culminating in a resilient model proficient in dynamic traffic scenarios. We put forth a comprehensive evaluation of our approach, employing diverse datasets originating from varied Asian countries, effectively demonstrating its efficacy in challenging road conditions. Notably, our method yields substantial enhancements in object detection performance, culminating in a model with heightened resistance against domain shifts.
<div id='section'>Paperid: <span id='pid'>1496, <a href='https://arxiv.org/pdf/2308.06199.pdf' target='_blank'>https://arxiv.org/pdf/2308.06199.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anna-Grace Linton, Vania Dimitrova, Amy Downing, Richard Wagland, Adam Glaser
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.06199">Weakly Supervised Text Classification on Free Text Comments in Patient-Reported Outcome Measures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Free text comments (FTC) in patient-reported outcome measures (PROMs) data are typically analysed using manual methods, such as content analysis, which is labour-intensive and time-consuming. Machine learning analysis methods are largely unsupervised, necessitating post-analysis interpretation. Weakly supervised text classification (WSTC) can be a valuable method of analysis to classify domain-specific text data in which there is limited labelled data. In this paper, we apply five WSTC techniques to FTC in PROMs data to identify health-related quality of life (HRQoL) themes reported by colorectal cancer patients. The WSTC methods label all the themes mentioned in the FTC. The results showed moderate performance on the PROMs data, mainly due to the precision of the models, and variation between themes. Evaluation of the classification performance illustrated the potential and limitations of keyword based WSTC to label PROMs FTC when labelled data is limited.
<div id='section'>Paperid: <span id='pid'>1497, <a href='https://arxiv.org/pdf/2308.04687.pdf' target='_blank'>https://arxiv.org/pdf/2308.04687.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abhishek Kushwaha, Sarthak Gupta, Anish Bhanushali, Tathagato Rai Dastidar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.04687">Rapid Training Data Creation by Synthesizing Medical Images for Classification and Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While the use of artificial intelligence (AI) for medical image analysis is gaining wide acceptance, the expertise, time and cost required to generate annotated data in the medical field are significantly high, due to limited availability of both data and expert annotation. Strongly supervised object localization models require data that is exhaustively annotated, meaning all objects of interest in an image are identified. This is difficult to achieve and verify for medical images. We present a method for the transformation of real data to train any Deep Neural Network to solve the above problems. We show the efficacy of this approach on both a weakly supervised localization model and a strongly supervised localization model. For the weakly supervised model, we show that the localization accuracy increases significantly using the generated data. For the strongly supervised model, this approach overcomes the need for exhaustive annotation on real images. In the latter model, we show that the accuracy, when trained with generated images, closely parallels the accuracy when trained with exhaustively annotated real images. The results are demonstrated on images of human urine samples obtained using microscopy.
<div id='section'>Paperid: <span id='pid'>1498, <a href='https://arxiv.org/pdf/2308.03486.pdf' target='_blank'>https://arxiv.org/pdf/2308.03486.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vicente Sampaio, Filipe R. Cordeiro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.03486">Improving Mass Detection in Mammography Images: A Study of Weakly Supervised Learning and Class Activation Map Methods</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, weakly supervised models have aided in mass detection using mammography images, decreasing the need for pixel-level annotations. However, most existing models in the literature rely on Class Activation Maps (CAM) as the activation method, overlooking the potential benefits of exploring other activation techniques. This work presents a study that explores and compares different activation maps in conjunction with state-of-the-art methods for weakly supervised training in mammography images. Specifically, we investigate CAM, GradCAM, GradCAM++, XGradCAM, and LayerCAM methods within the framework of the GMIC model for mass detection in mammography images. The evaluation is conducted on the VinDr-Mammo dataset, utilizing the metrics Accuracy, True Positive Rate (TPR), False Negative Rate (FNR), and False Positive Per Image (FPPI). Results show that using different strategies of activation maps during training and test stages leads to an improvement of the model. With this strategy, we improve the results of the GMIC method, decreasing the FPPI value and increasing TPR.
<div id='section'>Paperid: <span id='pid'>1499, <a href='https://arxiv.org/pdf/2307.14889.pdf' target='_blank'>https://arxiv.org/pdf/2307.14889.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peter Bauer, Arij Bouazizi, Ulrich Kressel, Fabian B. Flohr
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.14889">Weakly Supervised Multi-Modal 3D Human Body Pose Estimation for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate 3D human pose estimation (3D HPE) is crucial for enabling autonomous vehicles (AVs) to make informed decisions and respond proactively in critical road scenarios. Promising results of 3D HPE have been gained in several domains such as human-computer interaction, robotics, sports and medical analytics, often based on data collected in well-controlled laboratory environments. Nevertheless, the transfer of 3D HPE methods to AVs has received limited research attention, due to the challenges posed by obtaining accurate 3D pose annotations and the limited suitability of data from other domains.
  We present a simple yet efficient weakly supervised approach for 3D HPE in the AV context by employing a high-level sensor fusion between camera and LiDAR data. The weakly supervised setting enables training on the target datasets without any 2D/3D keypoint labels by using an off-the-shelf 2D joint extractor and pseudo labels generated from LiDAR to image projections. Our approach outperforms state-of-the-art results by up to $\sim$ 13% on the Waymo Open Dataset in the weakly supervised setting and achieves state-of-the-art results in the supervised setting.
<div id='section'>Paperid: <span id='pid'>1500, <a href='https://arxiv.org/pdf/2307.07341.pdf' target='_blank'>https://arxiv.org/pdf/2307.07341.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixin Guo, Tzu-Jui Julius Wang, Selen Pehlivan, Abduljalil Radman, Jorma Laaksonen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.07341">PiTL: Cross-modal Retrieval with Weakly-supervised Vision-language Pre-training via Prompting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language (VL) Pre-training (VLP) has shown to well generalize VL models over a wide range of VL downstream tasks, especially for cross-modal retrieval. However, it hinges on a huge amount of image-text pairs, which requires tedious and costly curation. On the contrary, weakly-supervised VLP (W-VLP) explores means with object tags generated by a pre-trained object detector (OD) from images. Yet, they still require paired information, i.e. images and object-level annotations, as supervision to train an OD.
  To further reduce the amount of supervision, we propose Prompts-in-The-Loop (PiTL) that prompts knowledge from large language models (LLMs) to describe images. Concretely, given a category label of an image, e.g. refinery, the knowledge, e.g. a refinery could be seen with large storage tanks, pipework, and ..., extracted by LLMs is used as the language counterpart. The knowledge supplements, e.g. the common relations among entities most likely appearing in a scene. We create IN14K, a new VL dataset of 9M images and 1M descriptions of 14K categories from ImageNet21K with PiTL. Empirically, the VL models pre-trained with PiTL-generated pairs are strongly favored over other W-VLP works on image-to-text (I2T) and text-to-image (T2I) retrieval tasks, with less supervision. The results reveal the effectiveness of PiTL-generated pairs for VLP.
<div id='section'>Paperid: <span id='pid'>1501, <a href='https://arxiv.org/pdf/2307.05201.pdf' target='_blank'>https://arxiv.org/pdf/2307.05201.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao Wang, Zheng Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.05201">The Staged Knowledge Distillation in Video Classification: Harmonizing Student Progress by a Complementary Weakly Supervised Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the context of label-efficient learning on video data, the distillation method and the structural design of the teacher-student architecture have a significant impact on knowledge distillation. However, the relationship between these factors has been overlooked in previous research. To address this gap, we propose a new weakly supervised learning framework for knowledge distillation in video classification that is designed to improve the efficiency and accuracy of the student model. Our approach leverages the concept of substage-based learning to distill knowledge based on the combination of student substages and the correlation of corresponding substages. We also employ the progressive cascade training method to address the accuracy loss caused by the large capacity gap between the teacher and the student. Additionally, we propose a pseudo-label optimization strategy to improve the initial data label. To optimize the loss functions of different distillation substages during the training process, we introduce a new loss method based on feature distribution. We conduct extensive experiments on both real and simulated data sets, demonstrating that our proposed approach outperforms existing distillation methods in terms of knowledge distillation for video classification tasks. Our proposed substage-based distillation approach has the potential to inform future research on label-efficient learning for video data.
<div id='section'>Paperid: <span id='pid'>1502, <a href='https://arxiv.org/pdf/2307.04870.pdf' target='_blank'>https://arxiv.org/pdf/2307.04870.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Woojoo Na, Abiy Tasissa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.04870">RACH-Space: Reconstructing Adaptive Convex Hull Space with Applications in Weak Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce RACH-Space, an algorithm for labelling unlabelled data in weakly supervised learning, given incomplete, noisy information about the labels. RACH-Space offers simplicity in implementation without requiring hard assumptions on data or the sources of weak supervision, and is well suited for practical applications where fully labelled data is not available. Our method is built upon a geometrical interpretation of the space spanned by the set of weak signals. We also analyze the theoretical properties underlying the relationship between the convex hulls in this space and the accuracy of our output labels, bridging geometry with machine learning. Empirical results demonstrate that RACH-Space works well in practice and compares favorably to the best existing label models for weakly supervised learning.
<div id='section'>Paperid: <span id='pid'>1503, <a href='https://arxiv.org/pdf/2307.00562.pdf' target='_blank'>https://arxiv.org/pdf/2307.00562.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Silas Santiago Lopes Pereira, JosÃ© Everardo Bessa Maia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.00562">A MIL Approach for Anomaly Detection in Surveillance Videos from Multiple Camera Views</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Occlusion and clutter are two scene states that make it difficult to detect anomalies in surveillance video. Furthermore, anomaly events are rare and, as a consequence, class imbalance and lack of labeled anomaly data are also key features of this task. Therefore, weakly supervised methods are heavily researched for this application. In this paper, we tackle these typical problems of anomaly detection in surveillance video by combining Multiple Instance Learning (MIL) to deal with the lack of labels and Multiple Camera Views (MC) to reduce occlusion and clutter effects. In the resulting MC-MIL algorithm we apply a multiple camera combined loss function to train a regression network with Sultani's MIL ranking function. To evaluate the MC-MIL algorithm first proposed here, the multiple camera PETS-2009 benchmark dataset was re-labeled for the anomaly detection task from multiple camera views. The result shows a significant performance improvement in F1 score compared to the single-camera configuration.
<div id='section'>Paperid: <span id='pid'>1504, <a href='https://arxiv.org/pdf/2306.13830.pdf' target='_blank'>https://arxiv.org/pdf/2306.13830.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyu Gao, Dimitri N. Mavris
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.13830">Improved Aircraft Environmental Impact Segmentation via Metric Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate modeling of aircraft environmental impact is pivotal to the design of operational procedures and policies to mitigate negative aviation environmental impact. Aircraft environmental impact segmentation is a process which clusters aircraft types that have similar environmental impact characteristics based on a set of aircraft features. This practice helps model a large population of aircraft types with insufficient aircraft noise and performance models and contributes to better understanding of aviation environmental impact. Through measuring the similarity between aircraft types, distance metric is the kernel of aircraft segmentation. Traditional ways of aircraft segmentation use plain distance metrics and assign equal weight to all features in an unsupervised clustering process. In this work, we utilize weakly-supervised metric learning and partial information on aircraft fuel burn, emissions, and noise to learn weighted distance metrics for aircraft environmental impact segmentation. We show in a comprehensive case study that the tailored distance metrics can indeed make aircraft segmentation better reflect the actual environmental impact of aircraft. The metric learning approach can help refine a number of similar data-driven analytical studies in aviation.
<div id='section'>Paperid: <span id='pid'>1505, <a href='https://arxiv.org/pdf/2306.11928.pdf' target='_blank'>https://arxiv.org/pdf/2306.11928.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vivien Cabannes, Carles Domingo-Enrich
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.11928">Open Problem: Learning with Variational Objectives on Measures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The theory of statistical learning has focused on variational objectives expressed on functions. In this note, we discuss motivations to write similar objectives on measures, in particular to discuss out-of-distribution generalization and weakly-supervised learning. It raises a natural question: can one cast usual statistical learning results to objectives expressed on measures? Does the resulting construction lead to new algorithms of practical interest?
<div id='section'>Paperid: <span id='pid'>1506, <a href='https://arxiv.org/pdf/2305.18430.pdf' target='_blank'>https://arxiv.org/pdf/2305.18430.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liam Toran, Cory Van Der Walt, Alan Sammarone, Alex Keller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.18430">Scalable and Weakly Supervised Bank Transaction Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper aims to categorize bank transactions using weak supervision, natural language processing, and deep neural network techniques. Our approach minimizes the reliance on expensive and difficult-to-obtain manual annotations by leveraging heuristics and domain knowledge to train accurate transaction classifiers. We present an effective and scalable end-to-end data pipeline, including data preprocessing, transaction text embedding, anchoring, label generation, discriminative neural network training, and an overview of the system architecture. We demonstrate the effectiveness of our method by showing it outperforms existing market-leading solutions, achieves accurate categorization, and can be quickly extended to novel and composite use cases. This can in turn unlock many financial applications such as financial health reporting and credit risk assessment.
<div id='section'>Paperid: <span id='pid'>1507, <a href='https://arxiv.org/pdf/2305.17193.pdf' target='_blank'>https://arxiv.org/pdf/2305.17193.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ivan R. Nabi, Ben Cardoen, Ismail M. Khater, Guang Gao, Timothy H. Wong, Ghassan Hamarneh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.17193">AI-based analysis of super-resolution microscopy: Biological discovery in the absence of ground truth</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Super-resolution microscopy, or nanoscopy, enables the use of fluorescent-based molecular localization tools to study molecular structure at the nanoscale level in the intact cell, bridging the mesoscale gap to classical structural biology methodologies. Analysis of super-resolution data by artificial intelligence (AI), such as machine learning, offers tremendous potential for discovery of new biology, that, by definition, is not known and lacks ground truth. Herein, we describe the application of weakly supervised paradigms to super-resolution microscopy and its potential to enable the accelerated exploration of the nanoscale architecture of subcellular macromolecules and organelles.
<div id='section'>Paperid: <span id='pid'>1508, <a href='https://arxiv.org/pdf/2305.06912.pdf' target='_blank'>https://arxiv.org/pdf/2305.06912.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hugo Oliveira, Pedro H. T. Gama, Isabelle Bloch, Roberto Marcondes Cesar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.06912">Meta-Learners for Few-Shot Weakly-Supervised Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most uses of Meta-Learning in visual recognition are very often applied to image classification, with a relative lack of works in other tasks {such} as segmentation and detection. We propose a generic Meta-Learning framework for few-shot weakly-supervised segmentation in medical imaging domains. We conduct a comparative analysis of meta-learners from distinct paradigms adapted to few-shot image segmentation in different sparsely annotated radiological tasks. The imaging modalities include 2D chest, mammographic and dental X-rays, as well as 2D slices of volumetric tomography and resonance images. Our experiments consider a total of 9 meta-learners, 4 backbones and multiple target organ segmentation tasks. We explore small-data scenarios in radiology with varying weak annotation styles and densities. Our analysis shows that metric-based meta-learning approaches achieve better segmentation results in tasks with smaller domain shifts in comparison to the meta-training datasets, while some gradient- and fusion-based meta-learners are more generalizable to larger domain shifts.
<div id='section'>Paperid: <span id='pid'>1509, <a href='https://arxiv.org/pdf/2305.05748.pdf' target='_blank'>https://arxiv.org/pdf/2305.05748.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Paolo Tirotta, Akira Yuasa, Masashi Morita
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.05748">Multilevel Sentence Embeddings for Personality Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Representing text into a multidimensional space can be done with sentence embedding models such as Sentence-BERT (SBERT). However, training these models when the data has a complex multilevel structure requires individually trained class-specific models, which increases time and computing costs. We propose a two step approach which enables us to map sentences according to their hierarchical memberships and polarity. At first we teach the upper level sentence space through an AdaCos loss function and then finetune with a novel loss function mainly based on the cosine similarity of intra-level pairs. We apply this method to three different datasets: two weakly supervised Big Five personality dataset obtained from English and Japanese Twitter data and the benchmark MNLI dataset. We show that our single model approach performs better than multiple class-specific classification models.
<div id='section'>Paperid: <span id='pid'>1510, <a href='https://arxiv.org/pdf/2305.05525.pdf' target='_blank'>https://arxiv.org/pdf/2305.05525.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Min Hun Lee, Yi Jing Choy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.05525">Exploring a Gradient-based Explainable AI Technique for Time-Series Data: A Case Study of Assessing Stroke Rehabilitation Exercises</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Explainable artificial intelligence (AI) techniques are increasingly being explored to provide insights into why AI and machine learning (ML) models provide a certain outcome in various applications. However, there has been limited exploration of explainable AI techniques on time-series data, especially in the healthcare context. In this paper, we describe a threshold-based method that utilizes a weakly supervised model and a gradient-based explainable AI technique (i.e. saliency map) and explore its feasibility to identify salient frames of time-series data. Using the dataset from 15 post-stroke survivors performing three upper-limb exercises and labels on whether a compensatory motion is observed or not, we implemented a feed-forward neural network model and utilized gradients of each input on model outcomes to identify salient frames that involve compensatory motions. According to the evaluation using frame-level annotations, our approach achieved a recall of 0.96 and an F2-score of 0.91. Our results demonstrated the potential of a gradient-based explainable AI technique (e.g. saliency map) for time-series data, such as highlighting the frames of a video that therapists should focus on reviewing and reducing the efforts on frame-level labeling for model training.
<div id='section'>Paperid: <span id='pid'>1511, <a href='https://arxiv.org/pdf/2304.14114.pdf' target='_blank'>https://arxiv.org/pdf/2304.14114.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Lai, ChiMan Vong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.14114">Towards Precise Weakly Supervised Object Detection via Interactive Contrastive Learning of Context Information</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised object detection (WSOD) aims at learning precise object detectors with only image-level tags. In spite of intensive research on deep learning (DL) approaches over the past few years, there is still a significant performance gap between WSOD and fully supervised object detection. In fact, most existing WSOD methods only consider the visual appearance of each region proposal but ignore employing the useful context information in the image. To this end, this paper proposes an interactive end-to-end WSDO framework called JLWSOD with two innovations: i) two types of WSOD-specific context information (i.e., instance-wise correlation andsemantic-wise correlation) are proposed and introduced into WSOD framework; ii) an interactive graph contrastive learning (iGCL) mechanism is designed to jointly optimize the visual appearance and context information for better WSOD performance. Specifically, the iGCL mechanism takes full advantage of the complementary interpretations of the WSOD, namely instance-wise detection and semantic-wise prediction tasks, forming a more comprehensive solution. Extensive experiments on the widely used PASCAL VOC and MS COCO benchmarks verify the superiority of JLWSOD over alternative state-of-the-art approaches and baseline models (improvement of 3.6%~23.3% on mAP and 3.4%~19.7% on CorLoc, respectively).
<div id='section'>Paperid: <span id='pid'>1512, <a href='https://arxiv.org/pdf/2304.08912.pdf' target='_blank'>https://arxiv.org/pdf/2304.08912.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yen-Chieh Lien, Hamed Zamani, W. Bruce Croft
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.08912">Generalized Weak Supervision for Neural Information Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural ranking models (NRMs) have demonstrated effective performance in several information retrieval (IR) tasks. However, training NRMs often requires large-scale training data, which is difficult and expensive to obtain. To address this issue, one can train NRMs via weak supervision, where a large dataset is automatically generated using an existing ranking model (called the weak labeler) for training NRMs. Weakly supervised NRMs can generalize from the observed data and significantly outperform the weak labeler. This paper generalizes this idea through an iterative re-labeling process, demonstrating that weakly supervised models can iteratively play the role of weak labeler and significantly improve ranking performance without using manually labeled data. The proposed Generalized Weak Supervision (GWS) solution is generic and orthogonal to the ranking model architecture. This paper offers four implementations of GWS: self-labeling, cross-labeling, joint cross- and self-labeling, and greedy multi-labeling. GWS also benefits from a query importance weighting mechanism based on query performance prediction methods to reduce noise in the generated training data. We further draw a theoretical connection between self-labeling and Expectation-Maximization. Our experiments on two passage retrieval benchmarks suggest that all implementations of GWS lead to substantial improvements compared to weak supervision in all cases.
<div id='section'>Paperid: <span id='pid'>1513, <a href='https://arxiv.org/pdf/2304.08120.pdf' target='_blank'>https://arxiv.org/pdf/2304.08120.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sacha Lapins, Antony Butcher, J. -Michael Kendall, Thomas S. Hudson, Anna L. Stork, Maximilian J. Werner, Jemma Gunning, Alex M. Brisbourne
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.08120">DAS-N2N: Machine learning Distributed Acoustic Sensing (DAS) signal denoising without clean data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This article presents a weakly supervised machine learning method, which we call DAS-N2N, for suppressing strong random noise in distributed acoustic sensing (DAS) recordings. DAS-N2N requires no manually produced labels (i.e., pre-determined examples of clean event signals or sections of noise) for training and aims to map random noise processes to a chosen summary statistic, such as the distribution mean, median or mode, whilst retaining the true underlying signal. This is achieved by splicing (joining together) two fibres hosted within a single optical cable, recording two noisy copies of the same underlying signal corrupted by different independent realizations of random observational noise. A deep learning model can then be trained using only these two noisy copies of the data to produce a near fully-denoised copy. Once the model is trained, only noisy data from a single fibre is required. Using a dataset from a DAS array deployed on the surface of the Rutford Ice Stream in Antarctica, we demonstrate that DAS-N2N greatly suppresses incoherent noise and enhances the signal-to-noise ratios (SNR) of natural microseismic icequake events. We further show that this approach is inherently more efficient and effective than standard stop/pass band and white noise (e.g., Wiener) filtering routines, as well as a comparable self-supervised learning method based on masking individual DAS channels. Our preferred model for this task is lightweight, processing 30 seconds of data recorded at a sampling frequency of 1000 Hz over 985 channels (approx. 1 km of fiber) in $<$1 s. Due to the high noise levels in DAS recordings, efficient data-driven denoising methods, such as DAS-N2N, will prove essential to time-critical DAS earthquake detection, particularly in the case of microseismic monitoring.
<div id='section'>Paperid: <span id='pid'>1514, <a href='https://arxiv.org/pdf/2304.07470.pdf' target='_blank'>https://arxiv.org/pdf/2304.07470.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rahul Kale, Vrizlynn L. L. Thing
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.07470">Few-shot Weakly-supervised Cybersecurity Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With increased reliance on Internet based technologies, cyberattacks compromising users' sensitive data are becoming more prevalent. The scale and frequency of these attacks are escalating rapidly, affecting systems and devices connected to the Internet. The traditional defense mechanisms may not be sufficiently equipped to handle the complex and ever-changing new threats. The significant breakthroughs in the machine learning methods including deep learning, had attracted interests from the cybersecurity research community for further enhancements in the existing anomaly detection methods. Unfortunately, collecting labelled anomaly data for all new evolving and sophisticated attacks is not practical. Training and tuning the machine learning model for anomaly detection using only a handful of labelled data samples is a pragmatic approach. Therefore, few-shot weakly supervised anomaly detection is an encouraging research direction. In this paper, we propose an enhancement to an existing few-shot weakly-supervised deep learning anomaly detection framework. This framework incorporates data augmentation, representation learning and ordinal regression. We then evaluated and showed the performance of our implemented framework on three benchmark datasets: NSL-KDD, CIC-IDS2018, and TON_IoT.
<div id='section'>Paperid: <span id='pid'>1515, <a href='https://arxiv.org/pdf/2304.06841.pdf' target='_blank'>https://arxiv.org/pdf/2304.06841.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Niloufar Fakhfour, Mohammad ShahverdiKondori, Sajjad Hashembeiki, Mohammadjavad Norouzi, Hoda Mohammadzade
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.06841">Video alignment using unsupervised learning of local and global features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we tackle the problem of video alignment, the process of matching the frames of a pair of videos containing similar actions. The main challenge in video alignment is that accurate correspondence should be established despite the differences in the execution processes and appearances between the two videos. We introduce an unsupervised method for alignment that uses global and local features of the frames. In particular, we introduce effective features for each video frame by means of three machine vision tools: person detection, pose estimation, and VGG network. Then the features are processed and combined to construct a multidimensional time series that represent the video. The resulting time series are used to align videos of the same actions using a novel version of dynamic time warping named Diagonalized Dynamic Time Warping(DDTW). The main advantage of our approach is that no training is required, which makes it applicable for any new type of action without any need to collect training samples for it. Additionally, our approach can be used for framewise labeling of action phases in a dataset with only a few labeled videos. For evaluation, we considered video synchronization and phase classification tasks on the Penn action and subset of UCF101 datasets. Also, for an effective evaluation of the video synchronization task, we present a new metric called Enclosed Area Error(EAE). The results show that our method outperforms previous state-of-the-art methods, such as TCC, and other self-supervised and weakly supervised methods.
<div id='section'>Paperid: <span id='pid'>1516, <a href='https://arxiv.org/pdf/2303.02961.pdf' target='_blank'>https://arxiv.org/pdf/2303.02961.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hui Liu, Xiaojun Wan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.02961">Models See Hallucinations: Evaluating the Factuality in Video Captioning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video captioning aims to describe events in a video with natural language. In recent years, many works have focused on improving captioning models' performance. However, like other text generation tasks, it risks introducing factual errors not supported by the input video. These factual errors can seriously affect the quality of the generated text, sometimes making it completely unusable. Although factual consistency has received much research attention in text-to-text tasks (e.g., summarization), it is less studied in the context of vision-based text generation. In this work, we conduct a detailed human evaluation of the factuality in video captioning and collect two annotated factuality datasets. We find that 57.0% of the model-generated sentences have factual errors, indicating it is a severe problem in this field. However, existing evaluation metrics are mainly based on n-gram matching and show little correlation with human factuality annotation. We further propose a weakly-supervised, model-based factuality metric FactVC, which outperforms previous metrics on factuality evaluation of video captioning. The datasets and metrics will be released to promote future research for video captioning.
<div id='section'>Paperid: <span id='pid'>1517, <a href='https://arxiv.org/pdf/2303.02857.pdf' target='_blank'>https://arxiv.org/pdf/2303.02857.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fateme Bahri, Nilanjan Ray
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.02857">Weakly Supervised Realtime Dynamic Background Subtraction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Background subtraction is a fundamental task in computer vision with numerous real-world applications, ranging from object tracking to video surveillance. Dynamic backgrounds poses a significant challenge here. Supervised deep learning-based techniques are currently considered state-of-the-art for this task. However, these methods require pixel-wise ground-truth labels, which can be time-consuming and expensive. In this work, we propose a weakly supervised framework that can perform background subtraction without requiring per-pixel ground-truth labels. Our framework is trained on a moving object-free sequence of images and comprises two networks. The first network is an autoencoder that generates background images and prepares dynamic background images for training the second network. The dynamic background images are obtained by thresholding the background-subtracted images. The second network is a U-Net that uses the same object-free video for training and the dynamic background images as pixel-wise ground-truth labels. During the test phase, the input images are processed by the autoencoder and U-Net, which generate background and dynamic background images, respectively. The dynamic background image helps remove dynamic motion from the background-subtracted image, enabling us to obtain a foreground image that is free of dynamic artifacts. To demonstrate the effectiveness of our method, we conducted experiments on selected categories of the CDnet 2014 dataset and the I2R dataset. Our method outperformed all top-ranked unsupervised methods. We also achieved better results than one of the two existing weakly supervised methods, and our performance was similar to the other. Our proposed method is online, real-time, efficient, and requires minimal frame-level annotation, making it suitable for a wide range of real-world applications.
<div id='section'>Paperid: <span id='pid'>1518, <a href='https://arxiv.org/pdf/2302.11252.pdf' target='_blank'>https://arxiv.org/pdf/2302.11252.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Viet-Quoc Pham, Nao Mishima
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.11252">Focusing On Targets For Improving Weakly Supervised Visual Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised visual grounding aims to predict the region in an image that corresponds to a specific linguistic query, where the mapping between the target object and query is unknown in the training stage. The state-of-the-art method uses a vision language pre-training model to acquire heatmaps from Grad-CAM, which matches every query word with an image region, and uses the combined heatmap to rank the region proposals. In this paper, we propose two simple but efficient methods for improving this approach. First, we propose a target-aware cropping approach to encourage the model to learn both object and scene level semantic representations. Second, we apply dependency parsing to extract words related to the target object, and then put emphasis on these words in the heatmap combination. Our method surpasses the previous SOTA methods on RefCOCO, RefCOCO+, and RefCOCOg by a notable margin.
<div id='section'>Paperid: <span id='pid'>1519, <a href='https://arxiv.org/pdf/2302.10275.pdf' target='_blank'>https://arxiv.org/pdf/2302.10275.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hui Wang, Yueyang li, Haichi Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.10275">Semantic Feature Integration network for Fine-grained Visual Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fine-Grained Visual Classification (FGVC) is known as a challenging task due to subtle differences among subordinate categories. Many current FGVC approaches focus on identifying and locating discriminative regions by using the attention mechanism, but neglect the presence of unnecessary features that hinder the understanding of object structure. These unnecessary features, including 1) ambiguous parts resulting from the visual similarity in object appearances and 2) noninformative parts (e.g., background noise), can have a significant adverse impact on classification results. In this paper, we propose the Semantic Feature Integration network (SFI-Net) to address the above difficulties. By eliminating unnecessary features and reconstructing the semantic relations among discriminative features, our SFI-Net has achieved satisfying performance. The network consists of two modules: 1) the multi-level feature filter (MFF) module is proposed to remove unnecessary features with different receptive field, and then concatenate the preserved features on pixel level for subsequent disposal; 2) the semantic information reconstitution (SIR) module is presented to further establish semantic relations among discriminative features obtained from the MFF module. These two modules are carefully designed to be light-weighted and can be trained end-to-end in a weakly-supervised way. Extensive experiments on four challenging fine-grained benchmarks demonstrate that our proposed SFI-Net achieves the state-of-the-arts performance. Especially, the classification accuracy of our model on CUB-200-2011 and Stanford Dogs reaches 92.64% and 93.03%, respectively.
<div id='section'>Paperid: <span id='pid'>1520, <a href='https://arxiv.org/pdf/2302.09649.pdf' target='_blank'>https://arxiv.org/pdf/2302.09649.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>You Lu, Wenzhuo Song, Chidubem Arachie, Bert Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.09649">Weakly Supervised Label Learning Flows</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Supervised learning usually requires a large amount of labelled data. However, attaining ground-truth labels is costly for many tasks. Alternatively, weakly supervised methods learn with cheap weak signals that only approximately label some data. Many existing weakly supervised learning methods learn a deterministic function that estimates labels given the input data and weak signals. In this paper, we develop label learning flows (LLF), a general framework for weakly supervised learning problems. Our method is a generative model based on normalizing flows. The main idea of LLF is to optimize the conditional likelihoods of all possible labelings of the data within a constrained space defined by weak signals. We develop a training method for LLF that trains the conditional flow inversely and avoids estimating the labels. Once a model is trained, we can make predictions with a sampling algorithm. We apply LLF to three weakly supervised learning problems. Experiment results show that our method outperforms many baselines we compare against.
<div id='section'>Paperid: <span id='pid'>1521, <a href='https://arxiv.org/pdf/2301.10371.pdf' target='_blank'>https://arxiv.org/pdf/2301.10371.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adrian Benton, Tianze Shi, Ozan Ä°rsoy, Igor Malioutov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.10371">Weakly Supervised Headline Dependency Parsing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>English news headlines form a register with unique syntactic properties that have been documented in linguistics literature since the 1930s. However, headlines have received surprisingly little attention from the NLP syntactic parsing community. We aim to bridge this gap by providing the first news headline corpus of Universal Dependencies annotated syntactic dependency trees, which enables us to evaluate existing state-of-the-art dependency parsers on news headlines. To improve English news headline parsing accuracies, we develop a projection method to bootstrap silver training data from unlabeled news headline-article lead sentence pairs. Models trained on silver headline parses demonstrate significant improvements in performance over models trained solely on gold-annotated long-form texts. Ultimately, we find that, although projected silver training data improves parser performance across different news outlets, the improvement is moderated by constructions idiosyncratic to outlet.
<div id='section'>Paperid: <span id='pid'>1522, <a href='https://arxiv.org/pdf/2301.04882.pdf' target='_blank'>https://arxiv.org/pdf/2301.04882.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ke Zhang, Xiahai Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.04882">ZScribbleSeg: Zen and the Art of Scribble Supervised Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Curating a large scale fully-annotated dataset can be both labour-intensive and expertise-demanding, especially for medical images. To alleviate this problem, we propose to utilize solely scribble annotations for weakly supervised segmentation. Existing solutions mainly leverage selective losses computed solely on annotated areas and generate pseudo gold standard segmentation by propagating labels to adjacent areas. However, these methods could suffer from the inaccurate and sometimes unrealistic pseudo segmentation due to the insufficient supervision and incomplete shape features. Different from previous efforts, we first investigate the principle of ''good scribble annotations'', which leads to efficient scribble forms via supervision maximization and randomness simulation. Furthermore, we introduce regularization terms to encode the spatial relationship and shape prior, where a new formulation is developed to estimate the mixture ratios of label classes. These ratios are critical in identifying the unlabeled pixels for each class and correcting erroneous predictions, thus the accurate estimation lays the foundation for the incorporation of spatial prior. Finally, we integrate the efficient scribble supervision with the prior into a unified framework, denoted as ZScribbleSeg, and apply the method to multiple scenarios. Leveraging only scribble annotations, ZScribbleSeg set new state-of-the-arts on four segmentation tasks using ACDC, MSCMRseg, MyoPS and PPSS datasets.
<div id='section'>Paperid: <span id='pid'>1523, <a href='https://arxiv.org/pdf/2301.01835.pdf' target='_blank'>https://arxiv.org/pdf/2301.01835.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin Habib, Elvin Isufi, Ward van Breda, Arjen Jongepier, Jochen L. Cremer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.01835">Deep Statistical Solver for Distribution System State Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Implementing accurate Distribution System State Estimation (DSSE) faces several challenges, among which the lack of observability and the high density of the distribution system. While data-driven alternatives based on Machine Learning models could be a choice, they suffer in DSSE because of the lack of labeled data. In fact, measurements in the distribution system are often noisy, corrupted, and unavailable. To address these issues, we propose the Deep Statistical Solver for Distribution System State Estimation (DSS$^2$), a deep learning model based on graph neural networks (GNNs) that accounts for the network structure of the distribution system and for the physical governing power flow equations. DSS$^2$ leverages hypergraphs to represent the heterogeneous components of the distribution systems and updates their latent representations via a node-centric message-passing scheme. A weakly supervised learning approach is put forth to train the DSS$^2$ in a learning-to-optimize fashion w.r.t. the Weighted Least Squares loss with noisy measurements and pseudomeasurements. By enforcing the GNN output into the power flow equations and the latter into the loss function, we force the DSS$^2$ to respect the physics of the distribution system. This strategy enables learning from noisy measurements, acting as an implicit denoiser, and alleviating the need for ideal labeled data. Extensive experiments with case studies on the IEEE 14-bus, 70-bus, and 179-bus networks showed the DSS$^2$ outperforms by a margin the conventional Weighted Least Squares algorithm in accuracy, convergence, and computational time, while being more robust to noisy, erroneous, and missing measurements. The DSS$^2$ achieves a competing, yet lower, performance compared with the supervised models that rely on the unrealistic assumption of having all the true labels.
<div id='section'>Paperid: <span id='pid'>1524, <a href='https://arxiv.org/pdf/2211.02533.pdf' target='_blank'>https://arxiv.org/pdf/2211.02533.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenting Ye, Hongfei Yang, Shuai Zhao, Haoyang Fang, Xingjian Shi, Naveen Neppalli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.02533">A Transformer-Based Substitute Recommendation Model Incorporating Weakly Supervised Customer Behavior Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The substitute-based recommendation is widely used in E-commerce to provide better alternatives to customers. However, existing research typically uses the customer behavior signals like co-view and view-but-purchase-another to capture the substitute relationship. Despite its intuitive soundness, we find that such an approach might ignore the functionality and characteristics of products. In this paper, we adapt substitute recommendation into language matching problem by taking product title description as model input to consider product functionality. We design a new transformation method to de-noise the signals derived from production data. In addition, we consider multilingual support from the engineering point of view. Our proposed end-to-end transformer-based model achieves both successes from offline and online experiments. The proposed model has been deployed in a large-scale E-commerce website for 11 marketplaces in 6 languages. Our proposed model is demonstrated to increase revenue by 19% based on an online A/B experiment.
<div id='section'>Paperid: <span id='pid'>1525, <a href='https://arxiv.org/pdf/2210.11806.pdf' target='_blank'>https://arxiv.org/pdf/2210.11806.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Li Chong, Denghao Ma, Yueguo Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.11806">Multi-view Semantic Matching of Question retrieval using Fine-grained Semantic Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As a key task of question answering, question retrieval has attracted much attention from the communities of academia and industry. Previous solutions mainly focus on the translation model, topic model, and deep learning techniques. Distinct from the previous solutions, we propose to construct fine-grained semantic representations of a question by a learned importance score assigned to each keyword, so that we can achieve a fine-grained question matching solution with these semantic representations of different lengths. Accordingly, we propose a multi-view semantic matching model by reusing the important keywords in multiple semantic representations.
  As a key of constructing fine-grained semantic representations, we are the first to use a cross-task weakly supervised extraction model that applies question-question labelled signals to supervise the keyword extraction process (i.e. to learn the keyword importance). The extraction model integrates the deep semantic representation and lexical matching information with statistical features to estimate the importance of keywords. We conduct extensive experiments on three public datasets and the experimental results show that our proposed model significantly outperforms the state-of-the-art solutions.
<div id='section'>Paperid: <span id='pid'>1526, <a href='https://arxiv.org/pdf/2201.11808.pdf' target='_blank'>https://arxiv.org/pdf/2201.11808.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rassa Ghavami Modegh, Ahmad Salimi, Alireza Dizaji, Hamid R. Rabiee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2201.11808">LAP: An Attention-Based Module for Concept Based Self-Interpretation and Knowledge Injection in Convolutional Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the state-of-the-art performance of deep convolutional neural networks, they are susceptible to bias and malfunction in unseen situations. Moreover, the complex computation behind their reasoning is not human-understandable to develop trust. External explainer methods have tried to interpret network decisions in a human-understandable way, but they are accused of fallacies due to their assumptions and simplifications. On the other side, the inherent self-interpretability of models, while being more robust to the mentioned fallacies, cannot be applied to the already trained models. In this work, we propose a new attention-based pooling layer, called Local Attention Pooling (LAP), that accomplishes self-interpretability and the possibility for knowledge injection without performance loss. The module is easily pluggable into any convolutional neural network, even the already trained ones. We have defined a weakly supervised training scheme to learn the distinguishing features in decision-making without depending on experts' annotations. We verified our claims by evaluating several LAP-extended models on two datasets, including ImageNet. The proposed framework offers more valid human-understandable and faithful-to-the-model interpretations than the commonly used white-box explainer methods.
<div id='section'>Paperid: <span id='pid'>1527, <a href='https://arxiv.org/pdf/2111.04699.pdf' target='_blank'>https://arxiv.org/pdf/2111.04699.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrea Bandini, Sana Smaoui, Catriona M. Steele
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2111.04699">Automated pharyngeal phase detection and bolus localization in videofluoroscopic swallowing study: Killing two birds with one stone?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The videofluoroscopic swallowing study (VFSS) is a gold-standard imaging technique for assessing swallowing, but analysis and rating of VFSS recordings is time consuming and requires specialized training and expertise. Researchers have recently demonstrated that it is possible to automatically detect the pharyngeal phase of swallowing and to localize the bolus in VFSS recordings via computer vision, fostering the development of novel techniques for automatic VFSS analysis. However, training of algorithms to perform these tasks requires large amounts of annotated data that are seldom available. We demonstrate that the challenges of pharyngeal phase detection and bolus localization can be solved together using a single approach. We propose a deep-learning framework that jointly tackles pharyngeal phase detection and bolus localization in a weakly-supervised manner, requiring only the initial and final frames of the pharyngeal phase as ground truth annotations for the training. Our approach stems from the observation that bolus presence in the pharynx is the most prominent visual feature upon which to infer whether individual VFSS frames belong to the pharyngeal phase. We conducted extensive experiments with multiple convolutional neural networks (CNNs) on a dataset of 1245 bolus-level clips from 59 healthy subjects. We demonstrated that the pharyngeal phase can be detected with an F1-score higher than 0.9. Moreover, by processing the class activation maps of the CNNs, we were able to localize the bolus with promising results, obtaining correlations with ground truth trajectories higher than 0.9, without any manual annotations of bolus location used for training purposes. Once validated on a larger sample of participants with swallowing disorders, our framework will pave the way for the development of intelligent tools for VFSS analysis to support clinicians in swallowing assessment.
<div id='section'>Paperid: <span id='pid'>1528, <a href='https://arxiv.org/pdf/2109.00456.pdf' target='_blank'>https://arxiv.org/pdf/2109.00456.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jacob KÃ¶nig, Mark Jenkins, Mike Mannion, Peter Barrie, Gordon Morison
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2109.00456">Weakly-Supervised Surface Crack Segmentation by Generating Pseudo-Labels using Localization with a Classifier and Thresholding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Surface cracks are a common sight on public infrastructure nowadays. Recent work has been addressing this problem by supporting structural maintenance measures using machine learning methods. Those methods are used to segment surface cracks from their background, making them easier to localize. However, a common issue is that to create a well-functioning algorithm, the training data needs to have detailed annotations of pixels that belong to cracks. Our work proposes a weakly supervised approach that leverages a CNN classifier in a novel way to create surface crack pseudo labels. First, we use the classifier to create a rough crack localization map by using its class activation maps and a patch based classification approach and fuse this with a thresholding based approach to segment the mostly darker crack pixels. The classifier assists in suppressing noise from the background regions, which commonly are incorrectly highlighted as cracks by standard thresholding methods. Then, the pseudo labels can be used in an end-to-end approach when training a standard CNN for surface crack segmentation. Our method is shown to yield sufficiently accurate pseudo labels. Those labels, incorporated into segmentation CNN training using multiple recent crack segmentation architectures, achieve comparable performance to fully supervised methods on four popular crack segmentation datasets.
<div id='section'>Paperid: <span id='pid'>1529, <a href='https://arxiv.org/pdf/2107.11851.pdf' target='_blank'>https://arxiv.org/pdf/2107.11851.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Xiong, Fabian Caba Heilbron, Dahua Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2107.11851">Transcript to Video: Efficient Clip Sequencing from Texts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Among numerous videos shared on the web, well-edited ones always attract more attention. However, it is difficult for inexperienced users to make well-edited videos because it requires professional expertise and immense manual labor. To meet the demands for non-experts, we present Transcript-to-Video -- a weakly-supervised framework that uses texts as input to automatically create video sequences from an extensive collection of shots. Specifically, we propose a Content Retrieval Module and a Temporal Coherent Module to learn visual-language representations and model shot sequencing styles, respectively. For fast inference, we introduce an efficient search strategy for real-time video clip sequencing. Quantitative results and user studies demonstrate empirically that the proposed learning framework can retrieve content-relevant shots while creating plausible video sequences in terms of style. Besides, the run-time performance analysis shows that our framework can support real-world applications.
<div id='section'>Paperid: <span id='pid'>1530, <a href='https://arxiv.org/pdf/2105.01306.pdf' target='_blank'>https://arxiv.org/pdf/2105.01306.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Youngseo Son, Vasudha Varadarajan, H Andrew Schwartz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2105.01306">Discourse Relation Embeddings: Representing the Relations between Discourse Segments in Social Media</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Discourse relations are typically modeled as a discrete class that characterizes the relation between segments of text (e.g. causal explanations, expansions). However, such predefined discrete classes limits the universe of potential relationships and their nuanced differences. Analogous to contextual word embeddings, we propose representing discourse relations as points in high dimensional continuous space. However, unlike words, discourse relations often have no surface form (relations are between two segments, often with no word or phrase in that gap) which presents a challenge for existing embedding techniques. We present a novel method for automatically creating discourse relation embeddings (DiscRE), addressing the embedding challenge through a weakly supervised, multitask approach to learn diverse and nuanced relations between discourse segments in social media. Results show DiscRE can: (1) obtain the best performance on Twitter discourse relation classification task (macro F1=0.76) (2) improve the state of the art in social media causality prediction (from F1=.79 to .81), (3) perform beyond modern sentence and contextual word embeddings at traditional discourse relation classification, and (4) capture novel nuanced relations (e.g. relations semantically at the intersection of causal explanations and counterfactuals).
<div id='section'>Paperid: <span id='pid'>1531, <a href='https://arxiv.org/pdf/2006.09235.pdf' target='_blank'>https://arxiv.org/pdf/2006.09235.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Liang, Wenya Wang, Fengmao Lv
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2006.09235">Weakly-supervised Domain Adaption for Aspect Extraction via Multi-level Interaction Transfer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fine-grained aspect extraction is an essential sub-task in aspect based opinion analysis. It aims to identify the aspect terms (a.k.a. opinion targets) of a product or service in each sentence. However, expensive annotation process is usually involved to acquire sufficient token-level labels for each domain. To address this limitation, some previous works propose domain adaptation strategies to transfer knowledge from a sufficiently labeled source domain to unlabeled target domains. But due to both the difficulty of fine-grained prediction problems and the large domain gap between domains, the performance remains unsatisfactory. This work conducts a pioneer study on leveraging sentence-level aspect category labels that can be usually available in commercial services like review sites to promote token-level transfer for the extraction purpose. Specifically, the aspect category information is used to construct pivot knowledge for transfer with assumption that the interactions between sentence-level aspect category and token-level aspect terms are invariant across domains. To this end, we propose a novel multi-level reconstruction mechanism that aligns both the fine-grained and coarse-grained information in multiple levels of abstractions. Comprehensive experiments demonstrate that our approach can fully utilize sentence-level aspect category labels to improve cross-domain aspect extraction with a large performance gain.
<div id='section'>Paperid: <span id='pid'>1532, <a href='https://arxiv.org/pdf/2003.13648.pdf' target='_blank'>https://arxiv.org/pdf/2003.13648.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sheng Sun, Armando Marino, Wenze Shui, Zhongwen Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2003.13648">Weakly-supervised land classification for coastal zone based on deep convolutional neural networks by incorporating dual-polarimetric characteristics into training dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work we explore the performance of DCNNs on semantic segmentation using spaceborne polarimetric synthetic aperture radar (PolSAR) datasets. The semantic segmentation task using PolSAR data can be categorized as weakly supervised learning when the characteristics of SAR data and data annotating procedures are factored in. Datasets are initially analyzed for selecting feasible pre-training images. Then the differences between spaceborne and airborne datasets are examined in terms of spatial resolution and viewing geometry. In this study we used two dual-polarimetric images acquired by TerraSAR-X DLR. A novel method to produce training dataset with more supervised information is developed. Specifically, a series of typical classified images as well as intensity images serve as training datasets. A field survey is conducted for an area of about 20 square kilometers to obtain a ground truth dataset used for accuracy evaluation. Several transfer learning strategies are made for aforementioned training datasets which will be combined in a practicable order. Three DCNN models, including SegNet, U-Net, and LinkNet, are implemented next.
<div id='section'>Paperid: <span id='pid'>1533, <a href='https://arxiv.org/pdf/2003.11846.pdf' target='_blank'>https://arxiv.org/pdf/2003.11846.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lu Wang, Dong-xue Liang, Xiao-lei Yin, Jing Qiu, Zhi-yun Yang, Jun-hui Xing, Jian-zeng Dong, Zhao-yuan Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2003.11846">Weakly-supervised 3D coronary artery reconstruction from two-view angiographic images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The reconstruction of three-dimensional models of coronary arteries is of great significance for the localization, evaluation and diagnosis of stenosis and plaque in the arteries, as well as for the assisted navigation of interventional surgery. In the clinical practice, physicians use a few angles of coronary angiography to capture arterial images, so it is of great practical value to perform 3D reconstruction directly from coronary angiography images. However, this is a very difficult computer vision task due to the complex shape of coronary blood vessels, as well as the lack of data set and key point labeling. With the rise of deep learning, more and more work is being done to reconstruct 3D models of human organs from medical images using deep neural networks. We propose an adversarial and generative way to reconstruct three dimensional coronary artery models, from two different views of angiographic images of coronary arteries. With 3D fully supervised learning and 2D weakly supervised learning schemes, we obtained reconstruction accuracies that outperform state-of-art techniques.
<div id='section'>Paperid: <span id='pid'>1534, <a href='https://arxiv.org/pdf/1803.10699.pdf' target='_blank'>https://arxiv.org/pdf/1803.10699.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Li Ding, Chenliang Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1803.10699">Weakly-Supervised Action Segmentation with Iterative Soft Boundary Assignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we address the task of weakly-supervised human action segmentation in long, untrimmed videos. Recent methods have relied on expensive learning models, such as Recurrent Neural Networks (RNN) and Hidden Markov Models (HMM). However, these methods suffer from expensive computational cost, thus are unable to be deployed in large scale. To overcome the limitations, the keys to our design are efficiency and scalability. We propose a novel action modeling framework, which consists of a new temporal convolutional network, named Temporal Convolutional Feature Pyramid Network (TCFPN), for predicting frame-wise action labels, and a novel training strategy for weakly-supervised sequence modeling, named Iterative Soft Boundary Assignment (ISBA), to align action sequences and update the network in an iterative fashion. The proposed framework is evaluated on two benchmark datasets, Breakfast and Hollywood Extended, with four different evaluation metrics. Extensive experimental results show that our methods achieve competitive or superior performance to state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1535, <a href='https://arxiv.org/pdf/2512.01145.pdf' target='_blank'>https://arxiv.org/pdf/2512.01145.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Riyadh Mohammed Almushrafy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.01145">Weakly Supervised Continuous Micro-Expression Intensity Estimation Using Temporal Deep Neural Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Micro-facial expressions are brief and involuntary facial movements that reflect genuine emotional states. While most prior work focuses on classifying discrete micro-expression categories, far fewer studies address the continuous evolution of intensity over time. Progress in this direction is limited by the lack of frame-level intensity labels, which makes fully supervised regression impractical. We propose a unified framework for continuous micro-expression intensity estimation using only weak temporal labels (onset, apex, offset). A simple triangular prior converts sparse temporal landmarks into dense pseudo-intensity trajectories, and a lightweight temporal regression model that combines a ResNet18 encoder with a bidirectional GRU predicts frame-wise intensity directly from image sequences. The method requires no frame-level annotation effort and is applied consistently across datasets through a single preprocessing and temporal alignment pipeline. Experiments on SAMM and CASME II show strong temporal agreement with the pseudo-intensity trajectories. On SAMM, the model reaches a Spearman correlation of 0.9014 and a Kendall correlation of 0.7999, outperforming a frame-wise baseline. On CASME II, it achieves up to 0.9116 and 0.8168, respectively, when trained without the apex-ranking term. Ablation studies confirm that temporal modeling and structured pseudo labels are central to capturing the rise-apex-fall dynamics of micro-facial movements. To our knowledge, this is the first unified approach for continuous micro-expression intensity estimation using only sparse temporal annotations.
<div id='section'>Paperid: <span id='pid'>1536, <a href='https://arxiv.org/pdf/2511.07429.pdf' target='_blank'>https://arxiv.org/pdf/2511.07429.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hari Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.07429">Knowledge-Guided Textual Reasoning for Explainable Video Anomaly Detection via LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Text-based Explainable Video Anomaly Detection (TbVAD), a language-driven framework for weakly supervised video anomaly detection that performs anomaly detection and explanation entirely within the textual domain. Unlike conventional WSVAD models that rely on explicit visual features, TbVAD represents video semantics through language, enabling interpretable and knowledge-grounded reasoning. The framework operates in three stages: (1) transforming video content into fine-grained captions using a vision-language model, (2) constructing structured knowledge by organizing the captions into four semantic slots (action, object, context, environment), and (3) generating slot-wise explanations that reveal which semantic factors contribute most to the anomaly decision. We evaluate TbVAD on two public benchmarks, UCF-Crime and XD-Violence, demonstrating that textual knowledge reasoning provides interpretable and reliable anomaly detection for real-world surveillance scenarios.
<div id='section'>Paperid: <span id='pid'>1537, <a href='https://arxiv.org/pdf/2510.25075.pdf' target='_blank'>https://arxiv.org/pdf/2510.25075.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Keisuke Imoto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.25075">Joint Analysis of Acoustic Scenes and Sound Events Based on Semi-Supervised Training of Sound Events With Partial Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Annotating time boundaries of sound events is labor-intensive, limiting the scalability of strongly supervised learning in audio detection. To reduce annotation costs, weakly-supervised learning with only clip-level labels has been widely adopted. As an alternative, partial label learning offers a cost-effective approach, where a set of possible labels is provided instead of exact weak annotations. However, partial label learning for audio analysis remains largely unexplored. Motivated by the observation that acoustic scenes provide contextual information for constructing a set of possible sound events, we utilize acoustic scene information to construct partial labels of sound events. On the basis of this idea, in this paper, we propose a multitask learning framework that jointly performs acoustic scene classification and sound event detection with partial labels of sound events. While reducing annotation costs, weakly-supervised and partial label learning often suffer from decreased detection performance due to lacking the precise event set and their temporal annotations. To better balance between annotation cost and detection performance, we also explore a semi-supervised framework that leverages both strong and partial labels. Moreover, to refine partial labels and achieve better model training, we propose a label refinement method based on self-distillation for the proposed approach with partial labels.
<div id='section'>Paperid: <span id='pid'>1538, <a href='https://arxiv.org/pdf/2510.03606.pdf' target='_blank'>https://arxiv.org/pdf/2510.03606.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mattia Scardecchia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03606">Unsupervised Transformer Pre-Training for Images: Self-Distillation, Mean Teachers, and Random Crops</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in self-supervised learning (SSL) have made it possible to learn general-purpose visual features that capture both the high-level semantics and the fine-grained spatial structure of images. Most notably, the recent DINOv2 has established a new state of the art by surpassing weakly supervised methods (WSL) like OpenCLIP on most benchmarks. In this survey, we examine the core ideas behind its approach, multi-crop view augmentation and self-distillation with a mean teacher, and trace their development in previous work. We then compare the performance of DINO and DINOv2 with other SSL and WSL methods across various downstream tasks, and highlight some remarkable emergent properties of their learned features with transformer backbones. We conclude by briefly discussing DINOv2's limitations, its impact, and future research directions.
<div id='section'>Paperid: <span id='pid'>1539, <a href='https://arxiv.org/pdf/2509.26145.pdf' target='_blank'>https://arxiv.org/pdf/2509.26145.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yukun Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26145">LMILAtt: A Deep Learning Model for Depression Detection from Social Media Users Enhanced by Multi-Instance Learning Based on Attention Mechanism</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Depression is a major global public health challenge and its early identification is crucial. Social media data provides a new perspective for depression detection, but existing methods face limitations such as insufficient accuracy, insufficient utilization of time series features, and high annotation costs. To this end, this study proposes the LMILAtt model, which innovatively integrates Long Short-Term Memory autoencoders and attention mechanisms: firstly, the temporal dynamic features of user tweets (such as depressive tendency evolution patterns) are extracted through unsupervised LSTM autoencoders. Secondly, the attention mechanism is used to dynamically weight key texts (such as early depression signals) and construct a multi-example learning architecture to improve the accuracy of user-level detection. Finally, the performance was verified on the WU3D dataset labeled by professional medicine. Experiments show that the model is significantly better than the baseline model in terms of accuracy, recall and F1 score. In addition, the weakly supervised learning strategy significantly reduces the cost of labeling and provides an efficient solution for large-scale social media depression screening.
<div id='section'>Paperid: <span id='pid'>1540, <a href='https://arxiv.org/pdf/2509.14554.pdf' target='_blank'>https://arxiv.org/pdf/2509.14554.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoming Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14554">Generative Large Language Models for Knowledge Representation: A Systematic Review of Concept Map Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rise of generative large language models (LLMs) has opened new opportunities for automating knowledge representation through concept maps, a long-standing pedagogical tool valued for fostering meaningful learning and higher-order thinking. Traditional construction of concept maps is labor-intensive, requiring significant expertise and time, limiting their scalability in education. This review systematically synthesizes the emerging body of research on LLM-enabled concept map generation, focusing on two guiding questions: (a) What methods and technical features of LLMs are employed to construct concept maps? (b) What empirical evidence exists to validate their educational utility? Through a comprehensive search across major databases and AI-in-education conference proceedings, 28 studies meeting rigorous inclusion criteria were analyzed using thematic synthesis. Findings reveal six major methodological categories: human-in-the-loop systems, weakly supervised learning models, fine-tuned domain-specific LLMs, pre-trained LLMs with prompt engineering, hybrid systems integrating knowledge bases, and modular frameworks combining symbolic and statistical tools. Validation strategies ranged from quantitative metrics (precision, recall, F1-score, semantic similarity) to qualitative evaluations (expert review, learner feedback). Results indicate LLM-generated maps hold promise for scalable, adaptive, and pedagogically relevant knowledge visualization, though challenges remain regarding validity, interpretability, multilingual adaptability, and classroom integration. Future research should prioritize interdisciplinary co-design, empirical classroom trials, and alignment with instructional practices to realize their full educational potential.
<div id='section'>Paperid: <span id='pid'>1541, <a href='https://arxiv.org/pdf/2509.08698.pdf' target='_blank'>https://arxiv.org/pdf/2509.08698.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thorsten Wittkopp
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08698">A layered architecture for log analysis in complex IT systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the evolving IT landscape, stability and reliability of systems are essential, yet their growing complexity challenges DevOps teams in implementation and maintenance. Log analysis, a core element of AIOps, provides critical insights into complex behaviors and failures. This dissertation introduces a three-layered architecture to support DevOps in failure resolution. The first layer, Log Investigation, performs autonomous log labeling and anomaly classification. We propose a method that labels log data without manual effort, enabling supervised training and precise evaluation of anomaly detection. Additionally, we define a taxonomy that groups anomalies into three categories, ensuring appropriate method selection. The second layer, Anomaly Detection, detects behaviors deviating from the norm. We propose a flexible Anomaly Detection method adaptable to unsupervised, weakly supervised, and supervised training. Evaluations on public and industry datasets show F1-scores between 0.98 and 1.0, ensuring reliable anomaly detection. The third layer, Root Cause Analysis, identifies minimal log sets describing failures, their origin, and event sequences. By balancing training data and identifying key services, our Root Cause Analysis method consistently detects 90-98% of root cause log lines within the top 10 candidates, providing actionable insights for mitigation. Our research addresses how log analysis methods can be designed and optimized to help DevOps resolve failures efficiently. By integrating these three layers, the architecture equips teams with robust methods to enhance IT system reliability.
<div id='section'>Paperid: <span id='pid'>1542, <a href='https://arxiv.org/pdf/2508.15973.pdf' target='_blank'>https://arxiv.org/pdf/2508.15973.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minh-Tan Pham
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15973">Contributions to Label-Efficient Learning in Computer Vision and Remote Sensing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This manuscript presents a series of my selected contributions to the topic of label-efficient learning in computer vision and remote sensing. The central focus of this research is to develop and adapt methods that can learn effectively from limited or partially annotated data, and can leverage abundant unlabeled data in real-world applications. The contributions span both methodological developments and domain-specific adaptations, in particular addressing challenges unique to Earth observation data such as multi-modality, spatial resolution variability, and scene heterogeneity. The manuscript is organized around four main axes including (1) weakly supervised learning for object discovery and detection based on anomaly-aware representations learned from large amounts of background images; (2) multi-task learning that jointly trains on multiple datasets with disjoint annotations to improve performance on object detection and semantic segmentation; (3) self-supervised and supervised contrastive learning with multimodal data to enhance scene classification in remote sensing; and (4) few-shot learning for hierarchical scene classification using both explicit and implicit modeling of class hierarchies. These contributions are supported by extensive experimental results across natural and remote sensing datasets, reflecting the outcomes of several collaborative research projects. The manuscript concludes by outlining ongoing and future research directions focused on scaling and enhancing label-efficient learning for real-world applications.
<div id='section'>Paperid: <span id='pid'>1543, <a href='https://arxiv.org/pdf/2507.00297.pdf' target='_blank'>https://arxiv.org/pdf/2507.00297.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Ifeoluwa Adelani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00297">Natural language processing for African languages</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in word embeddings and language models use large-scale, unlabelled data and self-supervised learning to boost NLP performance. Multilingual models, often trained on web-sourced data like Wikipedia, face challenges: few low-resource languages are included, their data is often noisy, and lack of labeled datasets makes it hard to evaluate performance outside high-resource languages like English. In this dissertation, we focus on languages spoken in Sub-Saharan Africa where all the indigenous languages in this region can be regarded as low-resourced in terms of the availability of labelled data for NLP tasks and unlabelled data found on the web. We analyse the noise in the publicly available corpora, and curate a high-quality corpus, demonstrating that the quality of semantic representations learned in word embeddings does not only depend on the amount of data but on the quality of pre-training data. We demonstrate empirically the limitations of word embeddings, and the opportunities the multilingual pre-trained language model (PLM) offers especially for languages unseen during pre-training and low-resource scenarios. We further study how to adapt and specialize multilingual PLMs to unseen African languages using a small amount of monolingual texts. To address the under-representation of the African languages in NLP research, we developed large scale human-annotated labelled datasets for 21 African languages in two impactful NLP tasks: named entity recognition and machine translation. We conduct an extensive empirical evaluation using state-of-the-art methods across supervised, weakly-supervised, and transfer learning settings.
<div id='section'>Paperid: <span id='pid'>1544, <a href='https://arxiv.org/pdf/2505.09129.pdf' target='_blank'>https://arxiv.org/pdf/2505.09129.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09129">WSCIF: A Weakly-Supervised Color Intelligence Framework for Tactical Anomaly Detection in Surveillance Keyframes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The deployment of traditional deep learning models in high-risk security tasks in an unlabeled, data-non-exploitable video intelligence environment faces significant challenges. In this paper, we propose a lightweight anomaly detection framework based on color features for surveillance video clips in a high sensitivity tactical mission, aiming to quickly identify and interpret potential threat events under resource-constrained and data-sensitive conditions. The method fuses unsupervised KMeans clustering with RGB channel histogram modeling to achieve composite detection of structural anomalies and color mutation signals in key frames. The experiment takes an operation surveillance video occurring in an African country as a research sample, and successfully identifies multiple highly anomalous frames related to high-energy light sources, target presence, and reflective interference under the condition of no access to the original data. The results show that this method can be effectively used for tactical assassination warning, suspicious object screening and environmental drastic change monitoring with strong deployability and tactical interpretation value. The study emphasizes the importance of color features as low semantic battlefield signal carriers, and its battlefield intelligent perception capability will be further extended by combining graph neural networks and temporal modeling in the future.
<div id='section'>Paperid: <span id='pid'>1545, <a href='https://arxiv.org/pdf/2505.09083.pdf' target='_blank'>https://arxiv.org/pdf/2505.09083.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dominic Zaun Eu Jones
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09083">Ornithologist: Towards Trustworthy "Reasoning" about Central Bank Communications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>I develop Ornithologist, a weakly-supervised textual classification system and measure the hawkishness and dovishness of central bank text. Ornithologist uses ``taxonomy-guided reasoning'', guiding a large language model with human-authored decision trees. This increases the transparency and explainability of the system and makes it accessible to non-experts. It also reduces hallucination risk. Since it requires less supervision than traditional classification systems, it can more easily be applied to other problems or sources of text (e.g. news) without much modification. Ornithologist measurements of hawkishness and dovishness of RBA communication carry information about the future of the cash rate path and of market expectations.
<div id='section'>Paperid: <span id='pid'>1546, <a href='https://arxiv.org/pdf/2501.02021.pdf' target='_blank'>https://arxiv.org/pdf/2501.02021.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditya Prakash
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.02021">Weakly Supervised Learning on Large Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph classification plays a pivotal role in various domains, including pathology, where images can be represented as graphs. In this domain, images can be represented as graphs, where nodes might represent individual nuclei, and edges capture the spatial or functional relationships between them. Often, the overall label of the graph, such as a cancer type or disease state, is determined by patterns within smaller, localized regions of the image. This work introduces a weakly-supervised graph classification framework leveraging two subgraph extraction techniques: (1) Sliding-window approach (2) BFS-based approach. Subgraphs are processed using a Graph Attention Network (GAT), which employs attention mechanisms to identify the most informative subgraphs for classification. Weak supervision is achieved by propagating graph-level labels to subgraphs, eliminating the need for detailed subgraph annotations.
<div id='section'>Paperid: <span id='pid'>1547, <a href='https://arxiv.org/pdf/2406.11014.pdf' target='_blank'>https://arxiv.org/pdf/2406.11014.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luca Moschella
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.11014">Latent Communication in Artificial Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As NNs permeate various scientific and industrial domains, understanding the universality and reusability of their representations becomes crucial. At their core, these networks create intermediate neural representations, indicated as latent spaces, of the input data and subsequently leverage them to perform specific downstream tasks. This dissertation focuses on the universality and reusability of neural representations. Do the latent representations crafted by a NN remain exclusive to a particular trained instance, or can they generalize across models, adapting to factors such as randomness during training, model architecture, or even data domain? This adaptive quality introduces the notion of Latent Communication -- a phenomenon that describes when representations can be unified or reused across neural spaces. A salient observation from our research is the emergence of similarities in latent representations, even when these originate from distinct or seemingly unrelated NNs. By exploiting a partial correspondence between the two data distributions that establishes a semantic link, we found that these representations can either be projected into a universal representation, coined as Relative Representation, or be directly translated from one space to another. Latent Communication allows for a bridge between independently trained NN, irrespective of their training regimen, architecture, or the data modality they were trained on -- as long as the data semantic content stays the same (e.g., images and their captions). This holds true for both generation, classification and retrieval downstream tasks; in supervised, weakly supervised, and unsupervised settings; and spans various data modalities including images, text, audio, and graphs -- showcasing the universality of the Latent Communication phenomenon. [...]
<div id='section'>Paperid: <span id='pid'>1548, <a href='https://arxiv.org/pdf/2406.05605.pdf' target='_blank'>https://arxiv.org/pdf/2406.05605.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sayan Mandal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.05605">Deep Learning to Predict Glaucoma Progression using Structural Changes in the Eye</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Glaucoma is a chronic eye disease characterized by optic neuropathy, leading to irreversible vision loss. It progresses gradually, often remaining undiagnosed until advanced stages. Early detection is crucial to monitor atrophy and develop treatment strategies to prevent further vision impairment. Data-centric methods have enabled computer-aided algorithms for precise glaucoma diagnosis.
  In this study, we use deep learning models to identify complex disease traits and progression criteria, detecting subtle changes indicative of glaucoma. We explore the structure-function relationship in glaucoma progression and predict functional impairment from structural eye deterioration. We analyze statistical and machine learning methods, including deep learning techniques with optical coherence tomography (OCT) scans for accurate progression prediction.
  Addressing challenges like age variability, data imbalances, and noisy labels, we develop novel semi-supervised time-series algorithms:
  1. Weakly-Supervised Time-Series Learning: We create a CNN-LSTM model to encode spatiotemporal features from OCT scans. This approach uses age-related progression and positive-unlabeled data to establish robust pseudo-progression criteria, bypassing gold-standard labels.
  2. Semi-Supervised Time-Series Learning: Using labels from Guided Progression Analysis (GPA) in a contrastive learning scheme, the CNN-LSTM architecture learns from potentially mislabeled data to improve prediction accuracy.
  Our methods outperform conventional and state-of-the-art techniques.
<div id='section'>Paperid: <span id='pid'>1549, <a href='https://arxiv.org/pdf/2405.17444.pdf' target='_blank'>https://arxiv.org/pdf/2405.17444.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Min Hun Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.17444">Towards Gradient-based Time-Series Explanations through a SpatioTemporal Attention Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we explore the feasibility of using a transformer-based, spatiotemporal attention network (STAN) for gradient-based time-series explanations. First, we trained the STAN model for video classifications using the global and local views of data and weakly supervised labels on time-series data (i.e. the type of an activity). We then leveraged a gradient-based XAI technique (e.g. saliency map) to identify salient frames of time-series data. According to the experiments using the datasets of four medically relevant activities, the STAN model demonstrated its potential to identify important frames of videos.
<div id='section'>Paperid: <span id='pid'>1550, <a href='https://arxiv.org/pdf/2405.01583.pdf' target='_blank'>https://arxiv.org/pdf/2405.01583.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nadia Saeed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.01583">MediFact at MEDIQA-M3G 2024: Medical Question Answering in Dermatology with Multimodal Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The MEDIQA-M3G 2024 challenge necessitates novel solutions for Multilingual & Multimodal Medical Answer Generation in dermatology (wai Yim et al., 2024a). This paper addresses the limitations of traditional methods by proposing a weakly supervised learning approach for open-ended medical question-answering (QA). Our system leverages readily available MEDIQA-M3G images via a VGG16-CNN-SVM model, enabling multilingual (English, Chinese, Spanish) learning of informative skin condition representations. Using pre-trained QA models, we further bridge the gap between visual and textual information through multimodal fusion. This approach tackles complex, open-ended questions even without predefined answer choices. We empower the generation of comprehensive answers by feeding the ViT-CLIP model with multiple responses alongside images. This work advances medical QA research, paving the way for clinical decision support systems and ultimately improving healthcare delivery.
<div id='section'>Paperid: <span id='pid'>1551, <a href='https://arxiv.org/pdf/2403.14678.pdf' target='_blank'>https://arxiv.org/pdf/2403.14678.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Romeo Valentin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.14678">Towards a Framework for Deep Learning Certification in Safety-Critical Applications Using Inherently Safe Design and Run-Time Error Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although an ever-growing number of applications employ deep learning based systems for prediction, decision-making, or state estimation, almost no certification processes have been established that would allow such systems to be deployed in safety-critical applications. In this work we consider real-world problems arising in aviation and other safety-critical areas, and investigate their requirements for a certified model. To this end, we investigate methodologies from the machine learning research community aimed towards verifying robustness and reliability of deep learning systems, and evaluate these methodologies with regard to their applicability to real-world problems. Then, we establish a new framework towards deep learning certification based on (i) inherently safe design, and (ii) run-time error detection. Using a concrete use case from aviation, we show how deep learning models can recover disentangled variables through the use of weakly-supervised representation learning. We argue that such a system design is inherently less prone to common model failures, and can be verified to encode underlying mechanisms governing the data. Then, we investigate four techniques related to the run-time safety of a model, namely (i) uncertainty quantification, (ii) out-of-distribution detection, (iii) feature collapse, and (iv) adversarial attacks. We evaluate each for their applicability and formulate a set of desiderata that a certified model should fulfill. Finally, we propose a novel model structure that exhibits all desired properties discussed in this work, and is able to make regression and uncertainty predictions, as well as detect out-of-distribution inputs, while requiring no regression labels to train. We conclude with a discussion of the current state and expected future progress of deep learning certification, and its industrial and social implications.
<div id='section'>Paperid: <span id='pid'>1552, <a href='https://arxiv.org/pdf/2312.01262.pdf' target='_blank'>https://arxiv.org/pdf/2312.01262.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kangcheng Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.01262">A Review and A Robust Framework of Data-Efficient 3D Scene Parsing with Traditional/Learned 3D Descriptors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing state-of-the-art 3D point cloud understanding methods merely perform well in a fully supervised manner. To the best of our knowledge, there exists no unified framework that simultaneously solves the downstream high-level understanding tasks including both segmentation and detection, especially when labels are extremely limited. This work presents a general and simple framework to tackle point cloud understanding when labels are limited. The first contribution is that we have done extensive methodology comparisons of traditional and learned 3D descriptors for the task of weakly supervised 3D scene understanding, and validated that our adapted traditional PFH-based 3D descriptors show excellent generalization ability across different domains. The second contribution is that we proposed a learning-based region merging strategy based on the affinity provided by both the traditional/learned 3D descriptors and learned semantics. The merging process takes both low-level geometric and high-level semantic feature correlations into consideration. Experimental results demonstrate that our framework has the best performance among the three most important weakly supervised point clouds understanding tasks including semantic segmentation, instance segmentation, and object detection even when very limited number of points are labeled. Our method, termed Region Merging 3D (RM3D), has superior performance on ScanNet data-efficient learning online benchmarks and other four large-scale 3D understanding benchmarks under various experimental settings, outperforming current arts by a margin for various 3D understanding tasks without complicated learning strategies such as active learning.
<div id='section'>Paperid: <span id='pid'>1553, <a href='https://arxiv.org/pdf/2311.06567.pdf' target='_blank'>https://arxiv.org/pdf/2311.06567.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Heejeong Nam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.06567">SCADI: Self-supervised Causal Disentanglement in Latent Variable Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Causal disentanglement has great potential for capturing complex situations. However, there is a lack of practical and efficient approaches. It is already known that most unsupervised disentangling methods are unable to produce identifiable results without additional information, often leading to randomly disentangled output. Therefore, most existing models for disentangling are weakly supervised, providing information about intrinsic factors, which incurs excessive costs. Therefore, we propose a novel model, SCADI(SElf-supervised CAusal DIsentanglement), that enables the model to discover semantic factors and learn their causal relationships without any supervision. This model combines a masked structural causal model (SCM) with a pseudo-label generator for causal disentanglement, aiming to provide a new direction for self-supervised causal disentanglement models.
<div id='section'>Paperid: <span id='pid'>1554, <a href='https://arxiv.org/pdf/2311.02992.pdf' target='_blank'>https://arxiv.org/pdf/2311.02992.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David A. Wood
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.02992">NEURO HAND: A weakly supervised Hierarchical Attention Network for interpretable neuroimaging abnormality Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Clinical neuroimaging data is naturally hierarchical. Different magnetic resonance imaging (MRI) sequences within a series, different slices covering the head, and different regions within each slice all confer different information. In this work we present a hierarchical attention network for abnormality detection using MRI scans obtained in a clinical hospital setting. The proposed network is suitable for non-volumetric data (i.e. stacks of high-resolution MRI slices), and can be trained from binary examination-level labels. We show that this hierarchical approach leads to improved classification, while providing interpretability through either coarse inter- and intra-slice abnormality localisation, or giving importance scores for different slices and sequences, making our model suitable for use as an automated triaging system in radiology departments.
<div id='section'>Paperid: <span id='pid'>1555, <a href='https://arxiv.org/pdf/2307.06385.pdf' target='_blank'>https://arxiv.org/pdf/2307.06385.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kalyan Ramakrishnan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.06385">Temporal Label-Refinement for Weakly-Supervised Audio-Visual Event Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-Visual Event Localization (AVEL) is the task of temporally localizing and classifying \emph{audio-visual events}, i.e., events simultaneously visible and audible in a video. In this paper, we solve AVEL in a weakly-supervised setting, where only video-level event labels (their presence/absence, but not their locations in time) are available as supervision for training. Our idea is to use a base model to estimate labels on the training data at a finer temporal resolution than at the video level and re-train the model with these labels. I.e., we determine the subset of labels for each \emph{slice} of frames in a training video by (i) replacing the frames outside the slice with those from a second video having no overlap in video-level labels, and (ii) feeding this synthetic video into the base model to extract labels for just the slice in question. To handle the out-of-distribution nature of our synthetic videos, we propose an auxiliary objective for the base model that induces more reliable predictions of the localized event labels as desired. Our three-stage pipeline outperforms several existing AVEL methods with no architectural changes and improves performance on a related weakly-supervised task as well.
<div id='section'>Paperid: <span id='pid'>1556, <a href='https://arxiv.org/pdf/2306.06149.pdf' target='_blank'>https://arxiv.org/pdf/2306.06149.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eduardo Hugo Sanchez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.06149">Read, look and detect: Bounding box annotation from image-caption pairs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Various methods have been proposed to detect objects while reducing the cost of data annotation. For instance, weakly supervised object detection (WSOD) methods rely only on image-level annotations during training. Unfortunately, data annotation remains expensive since annotators must provide the categories describing the content of each image and labeling is restricted to a fixed set of categories. In this paper, we propose a method to locate and label objects in an image by using a form of weaker supervision: image-caption pairs. By leveraging recent advances in vision-language (VL) models and self-supervised vision transformers (ViTs), our method is able to perform phrase grounding and object detection in a weakly supervised manner. Our experiments demonstrate the effectiveness of our approach by achieving a 47.51% recall@1 score in phrase grounding on Flickr30k Entities and establishing a new state-of-the-art in object detection by achieving 21.1 mAP 50 and 10.5 mAP 50:95 on MS COCO when exclusively relying on image-caption pairs.
<div id='section'>Paperid: <span id='pid'>1557, <a href='https://arxiv.org/pdf/2302.01440.pdf' target='_blank'>https://arxiv.org/pdf/2302.01440.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengyu Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.01440">Generalized Uncertainty of Deep Neural Networks: Taxonomy and Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks have seen enormous success in various real-world applications. Beyond their predictions as point estimates, increasing attention has been focused on quantifying the uncertainty of their predictions. In this review, we show that the uncertainty of deep neural networks is not only important in a sense of interpretability and transparency, but also crucial in further advancing their performance, particularly in learning systems seeking robustness and efficiency. We will generalize the definition of the uncertainty of deep neural networks to any number or vector that is associated with an input or an input-label pair, and catalog existing methods on ``mining'' such uncertainty from a deep model. We will include those methods from the classic field of uncertainty quantification as well as those methods that are specific to deep neural networks. We then show a wide spectrum of applications of such generalized uncertainty in realistic learning tasks including robust learning such as noisy learning, adversarially robust learning; data-efficient learning such as semi-supervised and weakly-supervised learning; and model-efficient learning such as model compression and knowledge distillation.
<div id='section'>Paperid: <span id='pid'>1558, <a href='https://arxiv.org/pdf/2301.06021.pdf' target='_blank'>https://arxiv.org/pdf/2301.06021.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.06021">Interpretable and Scalable Graphical Models for Complex Spatio-temporal Processes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This thesis focuses on data that has complex spatio-temporal structure and on probabilistic graphical models that learn the structure in an interpretable and scalable manner. We target two research areas of interest: Gaussian graphical models for tensor-variate data and summarization of complex time-varying texts using topic models. This work advances the state-of-the-art in several directions. First, it introduces a new class of tensor-variate Gaussian graphical models via the Sylvester tensor equation. Second, it develops an optimization technique based on a fast-converging proximal alternating linearized minimization method, which scales tensor-variate Gaussian graphical model estimations to modern big-data settings. Third, it connects Kronecker-structured (inverse) covariance models with spatio-temporal partial differential equations (PDEs) and introduces a new framework for ensemble Kalman filtering that is capable of tracking chaotic physical systems. Fourth, it proposes a modular and interpretable framework for unsupervised and weakly-supervised probabilistic topic modeling of time-varying data that combines generative statistical models with computational geometric methods. Throughout, practical applications of the methodology are considered using real datasets. This includes brain-connectivity analysis using EEG data, space weather forecasting using solar imaging data, longitudinal analysis of public opinions using Twitter data, and mining of mental health related issues using TalkLife data. We show in each case that the graphical modeling framework introduced here leads to improved interpretability, accuracy, and scalability.
<div id='section'>Paperid: <span id='pid'>1559, <a href='https://arxiv.org/pdf/2108.01487.pdf' target='_blank'>https://arxiv.org/pdf/2108.01487.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anant Khandelwal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2108.01487">WeaSuL: Weakly Supervised Dialogue Policy Learning: Reward Estimation for Multi-turn Dialogue</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>An intelligent dialogue system in a multi-turn setting should not only generate the responses which are of good quality, but it should also generate the responses which can lead to long-term success of the dialogue. Although, the current approaches improved the response quality, but they over-look the training signals present in the dialogue data. We can leverage these signals to generate the weakly supervised training data for learning dialog policy and reward estimator, and make the policy take actions (generates responses) which can foresee the future direction for a successful (rewarding) conversation. We simulate the dialogue between an agent and a user (modelled similar to an agent with supervised learning objective) to interact with each other. The agent uses dynamic blocking to generate ranked diverse responses and exploration-exploitation to select among the Top-K responses. Each simulated state-action pair is evaluated (works as a weak annotation) with three quality modules: Semantic Relevant, Semantic Coherence and Consistent Flow. Empirical studies with two benchmarks indicate that our model can significantly out-perform the response quality and lead to a successful conversation on both automatic evaluation and human judgement.
<div id='section'>Paperid: <span id='pid'>1560, <a href='https://arxiv.org/pdf/2105.00815.pdf' target='_blank'>https://arxiv.org/pdf/2105.00815.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2105.00815">Representation Learning for Weakly Supervised Relation Extraction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent years have seen rapid development in Information Extraction, as well as its subtask, Relation Extraction. Relation Extraction is able to detect semantic relations between entities in sentences. Currently, many efficient approaches have been applied to relation extraction tasks. Supervised learning approaches especially have good performance. However, there are still many difficult challenges. One of the most serious problems is that manually labeled data is difficult to acquire. In most cases, limited data for supervised approaches equals lousy performance. Thus here, under the situation with only limited training data, we focus on how to improve the performance of our supervised baseline system with unsupervised pre-training. Feature is one of the key components in improving the supervised approaches. Traditional approaches usually apply hand-crafted features, which require expert knowledge and expensive human labor. However, this type of feature might suffer from data sparsity: when the training set size is small, the model parameters might be poorly estimated. In this thesis, we present several novel unsupervised pre-training models to learn the distributed text representation features, which are encoded with rich syntactic-semantic patterns of relation expressions. The experiments have demonstrated that this type of feature, combine with the traditional hand-crafted features, could improve the performance of the logistic classification model for relation extraction, especially on the classification of relations with only minor training instances.
<div id='section'>Paperid: <span id='pid'>1561, <a href='https://arxiv.org/pdf/2008.11945.pdf' target='_blank'>https://arxiv.org/pdf/2008.11945.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongquan Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2008.11945">Moderately Supervised Learning: Definition, Framework and Generality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning with supervision has achieved remarkable success in numerous artificial intelligence (AI) applications. In the current literature, by referring to the properties of the labels prepared for the training dataset, learning with supervision is categorized as supervised learning (SL) and weakly supervised learning (WSL). SL concerns the situation where the training data set is assigned with ideal (complete, exact and accurate) labels, while WSL concerns the situation where the training data set is assigned with non-ideal (incomplete, inexact or inaccurate) labels. However, various solutions for SL tasks have shown that the given labels are not always easy to learn, and the transformation from the given labels to easy-to-learn targets can significantly affect the performance of the final SL solutions. Without considering the properties of the transformation from the given labels to easy-to-learn targets, the definition of SL conceals some details that can be critical to building the appropriate solutions for specific SL tasks. Thus, for engineers in the AI application field, it is desirable to reveal these details systematically. This article attempts to achieve this goal by expanding the categorization of SL and investigating the sub-type moderately supervised learning (MSL) that concerns the situation where the given labels are ideal, but due to the simplicity in annotation, careful designs are required to transform the given labels into easy-to-learn targets. From the perspectives of the definition, framework and generality, we conceptualize MSL to present a complete fundamental basis to systematically analyse MSL tasks. At meantime, revealing the relation between the conceptualization of MSL and the mathematicians' vision, this paper as well establishes a tutorial for AI application engineers to refer to viewing a problem to be solved from the mathematicians' vision.
