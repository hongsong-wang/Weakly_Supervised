<!DOCTYPE html>
<html>
<head>
<title>arXiv Papers of Weakly Supervised Learning</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">


/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 10px 20px 10px 20px;
  max-width: 960px;
  margin: 0 auto;
}

span#pid {
  color:red;
  
}
span#filename{
  font-style: oblique;
}

span#title{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: italic;
  font-size: 20px;
  border:1px solid #B50;
}
span#abs{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: oblique;
  font-size: 18px;
}
</style>
</head>
<body><div id='title' style='font-size:1.3em; font-weight:bold;'>arXiv Papers of Weakly Supervised Learning</div><br>
<div id='section'>Paperid: <span id='pid'>1, <a href='https://arxiv.org/pdf/2601.21688.pdf' target='_blank'>https://arxiv.org/pdf/2601.21688.pdf</a></span>   <span><a href='https://github.com/ICML26-anon/XFactors' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/ICML26-anon/XFactors' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexandre Myara, Nicolas Bourriez, Thomas Boyer, Thomas Lemercier, Ihab Bendidi, Auguste Genovesio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.21688">XFACTORS: Disentangled Information Bottleneck via Contrastive Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Disentangled representation learning aims to map independent factors of variation to independent representation components. On one hand, purely unsupervised approaches have proven successful on fully disentangled synthetic data, but fail to recover semantic factors from real data without strong inductive biases. On the other hand, supervised approaches are unstable and hard to scale to large attribute sets because they rely on adversarial objectives or auxiliary classifiers. We introduce \textsc{XFactors}, a weakly-supervised VAE framework that disentangles and provides explicit control over a chosen set of factors. Building on the Disentangled Information Bottleneck perspective, we decompose the representation into a residual subspace $\mathcal{S}$ and factor-specific subspaces $\mathcal{T}_1,\ldots,\mathcal{T}_K$ and a residual subspace $\mathcal{S}$. Each target factor is encoded in its assigned $\mathcal{T}_i$ through contrastive supervision: an InfoNCE loss pulls together latents sharing the same factor value and pushes apart mismatched pairs. In parallel, KL regularization imposes a Gaussian structure on both $\mathcal{S}$ and the aggregated factor subspaces, organizing the geometry without additional supervision for non-targeted factors and avoiding adversarial training and classifiers. Across multiple datasets, with constant hyperparameters, \textsc{XFactors} achieves state-of-the-art disentanglement scores and yields consistent qualitative factor alignment in the corresponding subspaces, enabling controlled factor swapping via latent replacement. We further demonstrate that our method scales correctly with increasing latent capacity and evaluate it on the real-world dataset CelebA. Our code is available at \href{https://github.com/ICML26-anon/XFactors}{github.com/ICML26-anon/XFactors}.
<div id='section'>Paperid: <span id='pid'>2, <a href='https://arxiv.org/pdf/2601.08192.pdf' target='_blank'>https://arxiv.org/pdf/2601.08192.pdf</a></span>   <span><a href='https://github.com/faiyazabdullah/MultimodalMedAgent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Md. Faiyaz Abdullah Sayeedi, Rashedur Rahman, Siam Tahsin Bhuiyan, Sefatul Wasi, Ashraful Islam, Saadia Binte Alam, AKM Mahbubur Rahman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.08192">Route, Retrieve, Reflect, Repair: Self-Improving Agentic Framework for Visual Detection and Linguistic Reasoning in Medical Imaging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical image analysis increasingly relies on large vision-language models (VLMs), yet most systems remain single-pass black boxes that offer limited control over reasoning, safety, and spatial grounding. We propose R^4, an agentic framework that decomposes medical imaging workflows into four coordinated agents: a Router that configures task- and specialization-aware prompts from the image, patient history, and metadata; a Retriever that uses exemplar memory and pass@k sampling to jointly generate free-text reports and bounding boxes; a Reflector that critiques each draft-box pair for key clinical error modes (negation, laterality, unsupported claims, contradictions, missing findings, and localization errors); and a Repairer that iteratively revises both narrative and spatial outputs under targeted constraints while curating high-quality exemplars for future cases. Instantiated on chest X-ray analysis with multiple modern VLM backbones and evaluated on report generation and weakly supervised detection, R^4 consistently boosts LLM-as-a-Judge scores by roughly +1.7-+2.5 points and mAP50 by +2.5-+3.5 absolute points over strong single-VLM baselines, without any gradient-based fine-tuning. These results show that agentic routing, reflection, and repair can turn strong but brittle VLMs into more reliable and better grounded tools for clinical image interpretation. Our code can be found at: https://github.com/faiyazabdullah/MultimodalMedAgent
<div id='section'>Paperid: <span id='pid'>3, <a href='https://arxiv.org/pdf/2512.22188.pdf' target='_blank'>https://arxiv.org/pdf/2512.22188.pdf</a></span>   <span><a href='https://github.com/lingxitong/HookMIL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xitong Ling, Minxi Ouyang, Xiaoxiao Li, Jiawen Li, Ying Chen, Yuxuan Sun, Xinrui Chen, Tian Guan, Xiaoping Liu, Yonghong He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.22188">HookMIL: Revisiting Context Modeling in Multiple Instance Learning for Computational Pathology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple Instance Learning (MIL) has enabled weakly supervised analysis of whole-slide images (WSIs) in computational pathology. However, traditional MIL approaches often lose crucial contextual information, while transformer-based variants, though more expressive, suffer from quadratic complexity and redundant computations. To address these limitations, we propose HookMIL, a context-aware and computationally efficient MIL framework that leverages compact, learnable hook tokens for structured contextual aggregation. These tokens can be initialized from (i) key-patch visual features, (ii) text embeddings from vision-language pathology models, and (iii) spatially grounded features from spatial transcriptomics-vision models. This multimodal initialization enables Hook Tokens to incorporate rich textual and spatial priors, accelerating convergence and enhancing representation quality. During training, Hook tokens interact with instances through bidirectional attention with linear complexity. To further promote specialization, we introduce a Hook Diversity Loss that encourages each token to focus on distinct histopathological patterns. Additionally, a hook-to-hook communication mechanism refines contextual interactions while minimizing redundancy. Extensive experiments on four public pathology datasets demonstrate that HookMIL achieves state-of-the-art performance, with improved computational efficiency and interpretability. Codes are available at https://github.com/lingxitong/HookMIL.
<div id='section'>Paperid: <span id='pid'>4, <a href='https://arxiv.org/pdf/2512.19990.pdf' target='_blank'>https://arxiv.org/pdf/2512.19990.pdf</a></span>   <span><a href='https://github.com/gpgpgp123/DDTM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Gao, Ke Li, Di Wang, Yongshan Zhu, Yiming Zhang, Xuemei Luo, Yifeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.19990">A Dual-Branch Local-Global Framework for Cross-Resolution Land Cover Mapping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-resolution land cover mapping aims to produce high-resolution semantic predictions from coarse or low-resolution supervision, yet the severe resolution mismatch makes effective learning highly challenging. Existing weakly supervised approaches often struggle to align fine-grained spatial structures with coarse labels, leading to noisy supervision and degraded mapping accuracy. To tackle this problem, we propose DDTM, a dual-branch weakly supervised framework that explicitly decouples local semantic refinement from global contextual reasoning. Specifically, DDTM introduces a diffusion-based branch to progressively refine fine-scale local semantics under coarse supervision, while a transformer-based branch enforces long-range contextual consistency across large spatial extents. In addition, we design a pseudo-label confidence evaluation module to mitigate noise induced by cross-resolution inconsistencies and to selectively exploit reliable supervisory signals. Extensive experiments demonstrate that DDTM establishes a new state-of-the-art on the Chesapeake Bay benchmark, achieving 66.52\% mIoU and substantially outperforming prior weakly supervised methods. The code is available at https://github.com/gpgpgp123/DDTM.
<div id='section'>Paperid: <span id='pid'>5, <a href='https://arxiv.org/pdf/2512.10353.pdf' target='_blank'>https://arxiv.org/pdf/2512.10353.pdf</a></span>   <span><a href='https://github.com/YihengLyu/TranSamba' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiheng Lyu, Lian Xu, Mohammed Bennamoun, Farid Boussaid, Coen Arrow, Girish Dwivedi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.10353">Hybrid Transformer-Mamba Architecture for Weakly Supervised Volumetric Medical Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation offers a label-efficient solution to train segmentation models for volumetric medical imaging. However, existing approaches often rely on 2D encoders that neglect the inherent volumetric nature of the data. We propose TranSamba, a hybrid Transformer-Mamba architecture designed to capture 3D context for weakly supervised volumetric medical segmentation. TranSamba augments a standard Vision Transformer backbone with Cross-Plane Mamba blocks, which leverage the linear complexity of state space models for efficient information exchange across neighboring slices. The information exchange enhances the pairwise self-attention within slices computed by the Transformer blocks, directly contributing to the attention maps for object localization. TranSamba achieves effective volumetric modeling with time complexity that scales linearly with the input volume depth and maintains constant memory usage for batch processing. Extensive experiments on three datasets demonstrate that TranSamba establishes new state-of-the-art performance, consistently outperforming existing methods across diverse modalities and pathologies. Our source code and trained models are openly accessible at: https://github.com/YihengLyu/TranSamba.
<div id='section'>Paperid: <span id='pid'>6, <a href='https://arxiv.org/pdf/2512.10031.pdf' target='_blank'>https://arxiv.org/pdf/2512.10031.pdf</a></span>   <span><a href='https://kaist-viclab.github.io/ABBSPO_site/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Woojin Lee, Hyugjae Chang, Jaeho Moon, Jaehyup Lee, Munchurl Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.10031">ABBSPO: Adaptive Bounding Box Scaling and Symmetric Prior based Orientation Prediction for Detecting Aerial Image Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised oriented object detection (WS-OOD) has gained attention as a cost-effective alternative to fully supervised methods, providing both efficiency and high accuracy. Among weakly supervised approaches, horizontal bounding box (HBox)-supervised OOD stands out for its ability to directly leverage existing HBox annotations while achieving the highest accuracy under weak supervision settings. This paper introduces adaptive bounding box scaling and symmetry-prior-based orientation prediction, called ABBSPO, a framework for WS-OOD. Our ABBSPO addresses limitations of previous HBox-supervised OOD methods, which compare ground truth (GT) HBoxes directly with the minimum circumscribed rectangles of predicted RBoxes, often leading to inaccurate scale estimation. To overcome this, we propose: (i) Adaptive Bounding Box Scaling (ABBS), which appropriately scales GT HBoxes to optimize for the size of each predicted RBox, ensuring more accurate scale prediction; and (ii) a Symmetric Prior Angle (SPA) loss that exploits inherent symmetry of aerial objects for self-supervised learning, resolving issues in previous methods where learning collapses when predictions for all three augmented views (original, rotated, and flipped) are consistently incorrect. Extensive experimental results demonstrate that ABBSPO achieves state-of-the-art performance, outperforming existing methods.
<div id='section'>Paperid: <span id='pid'>7, <a href='https://arxiv.org/pdf/2512.05996.pdf' target='_blank'>https://arxiv.org/pdf/2512.05996.pdf</a></span>   <span><a href='https://umfieldrobotics.github.io/FishDetector-R1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Liu, Jingyu Song, Vedanth Kallakuri, Katherine A. Skinner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.05996">FishDetector-R1: Unified MLLM-Based Framework with Reinforcement Fine-Tuning for Weakly Supervised Fish Detection, Segmentation, and Counting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Analyzing underwater fish imagery is critical for ecological monitoring but remains difficult due to visual degradation and costly annotations. We introduce FishDetector-R1, a unified MLLM-based framework for fish detection, segmentation, and counting under weak supervision. On the DeepFish dataset, our framework achieves substantial gains over baselines, improving AP by 20% and mIoU by 10%, while reducing MAE by 30% and GAME by 35%. These improvements stem from two key components: a novel detect-to-count prompt that enforces spatially consistent detections and counts, and Reinforcement Learning from Verifiable Reward (RLVR) with a complementary scalable paradigm leveraging sparse point labels. Ablation studies further validate the effectiveness of this reward design. Moreover, the improvement generalizes well to other underwater datasets, confirming strong cross-domain robustness. Overall, FishDetector-R1 provides a reliable and scalable solution for accurate marine visual understanding via weak supervision. The project page for FishDetector-R1 is https://umfieldrobotics.github.io/FishDetector-R1.
<div id='section'>Paperid: <span id='pid'>8, <a href='https://arxiv.org/pdf/2512.05113.pdf' target='_blank'>https://arxiv.org/pdf/2512.05113.pdf</a></span>   <span><a href='https://chien90190.github.io/splannequin/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao-Jen Chien, Yi-Chuan Huang, Chung-Ho Wu, Wei-Lun Chao, Yu-Lun Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.05113">Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthesizing high-fidelity frozen 3D scenes from monocular Mannequin-Challenge (MC) videos is a unique problem distinct from standard dynamic scene reconstruction. Instead of focusing on modeling motion, our goal is to create a frozen scene while strategically preserving subtle dynamics to enable user-controlled instant selection. To achieve this, we introduce a novel application of dynamic Gaussian splatting: the scene is modeled dynamically, which retains nearby temporal variation, and a static scene is rendered by fixing the model's time parameter. However, under this usage, monocular capture with sparse temporal supervision introduces artifacts like ghosting and blur for Gaussians that become unobserved or occluded at weakly supervised timestamps. We propose Splannequin, an architecture-agnostic regularization that detects two states of Gaussian primitives, hidden and defective, and applies temporal anchoring. Under predominantly forward camera motion, hidden states are anchored to their recent well-observed past states, while defective states are anchored to future states with stronger supervision. Our method integrates into existing dynamic Gaussian pipelines via simple loss terms, requires no architectural changes, and adds zero inference overhead. This results in markedly improved visual quality, enabling high-fidelity, user-selectable frozen-time renderings, validated by a 96% user preference. Project page: https://chien90190.github.io/splannequin/
<div id='section'>Paperid: <span id='pid'>9, <a href='https://arxiv.org/pdf/2512.02359.pdf' target='_blank'>https://arxiv.org/pdf/2512.02359.pdf</a></span>   <span><a href='https://github.com/zqyq/Weakly-MVCC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bin Li, Daijie Chen, Qi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.02359">WSCF-MVCC: Weakly-supervised Calibration-free Multi-view Crowd Counting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-view crowd counting can effectively mitigate occlusion issues that commonly arise in single-image crowd counting. Existing deep-learning multi-view crowd counting methods project different camera view images onto a common space to obtain ground-plane density maps, requiring abundant and costly crowd annotations and camera calibrations. Hence, calibration-free methods are proposed that do not require camera calibrations and scene-level crowd annotations. However, existing calibration-free methods still require expensive image-level crowd annotations for training the single-view counting module. Thus, in this paper, we propose a weakly-supervised calibration-free multi-view crowd counting method (WSCF-MVCC), directly using crowd count as supervision for the single-view counting module rather than density maps constructed from crowd annotations. Instead, a self-supervised ranking loss that leverages multi-scale priors is utilized to enhance the model's perceptual ability without additional annotation costs. What's more, the proposed model leverages semantic information to achieve a more accurate view matching and, consequently, a more precise scene-level crowd count estimation. The proposed method outperforms the state-of-the-art methods on three widely used multi-view counting datasets under weakly supervised settings, indicating that it is more suitable for practical deployment compared with calibrated methods. Code is released in https://github.com/zqyq/Weakly-MVCC.
<div id='section'>Paperid: <span id='pid'>10, <a href='https://arxiv.org/pdf/2512.01178.pdf' target='_blank'>https://arxiv.org/pdf/2512.01178.pdf</a></span>   <span><a href='https://github.com/Magicboomliu/VSRD_plus_plus' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihua Liu, Hiroki Sakuma, Masatoshi Okutomi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.01178">VSRD++: Autolabeling for 3D Object Detection via Instance-Aware Volumetric Silhouette Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monocular 3D object detection is a fundamental yet challenging task in 3D scene understanding. Existing approaches heavily depend on supervised learning with extensive 3D annotations, which are often acquired from LiDAR point clouds through labor-intensive labeling processes. To tackle this problem, we propose VSRD++, a novel weakly supervised framework for monocular 3D object detection that eliminates the reliance on 3D annotations and leverages neural-field-based volumetric rendering with weak 2D supervision. VSRD++ consists of a two-stage pipeline: multi-view 3D autolabeling and subsequent monocular 3D detector training. In the multi-view autolabeling stage, object surfaces are represented as signed distance fields (SDFs) and rendered as instance masks via the proposed instance-aware volumetric silhouette rendering. To optimize 3D bounding boxes, we decompose each instance's SDF into a cuboid SDF and a residual distance field (RDF) that captures deviations from the cuboid. To address the geometry inconsistency commonly observed in volume rendering methods applied to dynamic objects, we model the dynamic objects by including velocity into bounding box attributes as well as assigning confidence to each pseudo-label. Moreover, we also employ a 3D attribute initialization module to initialize the dynamic bounding box parameters. In the monocular 3D object detection phase, the optimized 3D bounding boxes serve as pseudo labels for training monocular 3D object detectors. Extensive experiments on the KITTI-360 dataset demonstrate that VSRD++ significantly outperforms existing weakly supervised approaches for monocular 3D object detection on both static and dynamic scenes. Code is available at https://github.com/Magicboomliu/VSRD_plus_plus
<div id='section'>Paperid: <span id='pid'>11, <a href='https://arxiv.org/pdf/2512.00130.pdf' target='_blank'>https://arxiv.org/pdf/2512.00130.pdf</a></span>   <span><a href='https://github.com/DanielaPlusPlus/LGCOAMix' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fadi Dornaika, Danyang Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.00130">Local and Global Context-and-Object-part-Aware Superpixel-based Data Augmentation for Deep Visual Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cutmix-based data augmentation, which uses a cut-and-paste strategy, has shown remarkable generalization capabilities in deep learning. However, existing methods primarily consider global semantics with image-level constraints, which excessively reduces attention to the discriminative local context of the class and leads to a performance improvement bottleneck. Moreover, existing methods for generating augmented samples usually involve cutting and pasting rectangular or square regions, resulting in a loss of object part information. To mitigate the problem of inconsistency between the augmented image and the generated mixed label, existing methods usually require double forward propagation or rely on an external pre-trained network for object centering, which is inefficient. To overcome the above limitations, we propose LGCOAMix, an efficient context-aware and object-part-aware superpixel-based grid blending method for data augmentation. To the best of our knowledge, this is the first time that a label mixing strategy using a superpixel attention approach has been proposed for cutmix-based data augmentation. It is the first instance of learning local features from discriminative superpixel-wise regions and cross-image superpixel contrasts. Extensive experiments on various benchmark datasets show that LGCOAMix outperforms state-of-the-art cutmix-based data augmentation methods on classification tasks, {and weakly supervised object location on CUB200-2011.} We have demonstrated the effectiveness of LGCOAMix not only for CNN networks, but also for Transformer networks. Source codes are available at https://github.com/DanielaPlusPlus/LGCOAMix.
<div id='section'>Paperid: <span id='pid'>12, <a href='https://arxiv.org/pdf/2511.14639.pdf' target='_blank'>https://arxiv.org/pdf/2511.14639.pdf</a></span>   <span><a href='https://github.com/Ace95/SLAM-AGS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Marco Acerbis, Swarnadip Chatterjee, Christophe Avenel, Joakim Lindblad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.14639">SLAM-AGS: Slide-Label Aware Multi-Task Pretraining Using Adaptive Gradient Surgery in Computational Cytology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computational cytology faces two major challenges: i) instance-level labels are unreliable and prohibitively costly to obtain, ii) witness rates are extremely low. We propose SLAM-AGS, a Slide-Label-Aware Multitask pretraining framework that jointly optimizes (i) a weakly supervised similarity objective on slide-negative patches and (ii) a self-supervised contrastive objective on slide-positive patches, yielding stronger performance on downstream tasks. To stabilize learning, we apply Adaptive Gradient Surgery to tackle conflicting task gradients and prevent model collapse. We integrate the pretrained encoder into an attention-based Multiple Instance Learning aggregator for bag-level prediction and attention-guided retrieval of the most abnormal instances in a bag. On a publicly available bone-marrow cytology dataset, with simulated witness rates from 10% down to 0.5%, SLAM-AGS improves bag-level F1-Score and Top 400 positive cell retrieval over other pretraining methods, with the largest gains at low witness rates, showing that resolving gradient interference enables stable pretraining and better performance on downstream tasks. To facilitate reproducibility, we share our complete implementation and evaluation framework as open source: https://github.com/Ace95/SLAM-AGS.
<div id='section'>Paperid: <span id='pid'>13, <a href='https://arxiv.org/pdf/2511.14250.pdf' target='_blank'>https://arxiv.org/pdf/2511.14250.pdf</a></span>   <span><a href='https://yoni-yaffe.github.io/count-the-notes' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonathan Yaffe, Ben Maman, Meinard Müller, Amit H. Bermano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.14250">Count The Notes: Histogram-Based Supervision for Automatic Music Transcription</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic Music Transcription (AMT) converts audio recordings into symbolic musical representations. Training deep neural networks (DNNs) for AMT typically requires strongly aligned training pairs with precise frame-level annotations. Since creating such datasets is costly and impractical for many musical contexts, weakly aligned approaches using segment-level annotations have gained traction. However, existing methods often rely on Dynamic Time Warping (DTW) or soft alignment loss functions, both of which still require local semantic correspondences, making them error-prone and computationally expensive. In this article, we introduce CountEM, a novel AMT framework that eliminates the need for explicit local alignment by leveraging note event histograms as supervision, enabling lighter computations and greater flexibility. Using an Expectation-Maximization (EM) approach, CountEM iteratively refines predictions based solely on note occurrence counts, significantly reducing annotation efforts while maintaining high transcription accuracy. Experiments on piano, guitar, and multi-instrument datasets demonstrate that CountEM matches or surpasses existing weakly supervised methods, improving AMT's robustness, scalability, and efficiency. Our project page is available at https://yoni-yaffe.github.io/count-the-notes.
<div id='section'>Paperid: <span id='pid'>14, <a href='https://arxiv.org/pdf/2511.13863.pdf' target='_blank'>https://arxiv.org/pdf/2511.13863.pdf</a></span>   <span><a href='https://krantiparida.github.io/projects/cs3.html' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kranti Kumar Parida, Omar Emara, Hazel Doughty, Dima Damen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13863">Segmenting Collision Sound Sources in Egocentric Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans excel at multisensory perception and can often recognise object properties from the sound of their interactions. Inspired by this, we propose the novel task of Collision Sound Source Segmentation (CS3), where we aim to segment the objects responsible for a collision sound in visual input (i.e. video frames from the collision clip), conditioned on the audio. This task presents unique challenges. Unlike isolated sound events, a collision sound arises from interactions between two objects, and the acoustic signature of the collision depends on both. We focus on egocentric video, where sounds are often clear, but the visual scene is cluttered, objects are small, and interactions are brief. To address these challenges, we propose a weakly-supervised method for audio-conditioned segmentation, utilising foundation models (CLIP and SAM2). We also incorporate egocentric cues, i.e. objects in hands, to find acting objects that can potentially be collision sound sources. Our approach outperforms competitive baselines by $3\times$ and $4.7\times$ in mIoU on two benchmarks we introduce for the CS3 task: EPIC-CS3 and Ego4D-CS3.
<div id='section'>Paperid: <span id='pid'>15, <a href='https://arxiv.org/pdf/2511.10334.pdf' target='_blank'>https://arxiv.org/pdf/2511.10334.pdf</a></span>   <span><a href='https://github.com/lessiYin/DSANet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenti Yin, Huaxin Zhang, Xiang Wang, Yuqing Lu, Yicheng Zhang, Bingquan Gong, Jialong Zuo, Li Yu, Changxin Gao, Nong Sang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.10334">Learning to Tell Apart: Weakly Supervised Video Anomaly Detection via Disentangled Semantic Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in weakly-supervised video anomaly detection have achieved remarkable performance by applying the multiple instance learning paradigm based on multimodal foundation models such as CLIP to highlight anomalous instances and classify categories. However, their objectives may tend to detect the most salient response segments, while neglecting to mine diverse normal patterns separated from anomalies, and are prone to category confusion due to similar appearance, leading to unsatisfactory fine-grained classification results. Therefore, we propose a novel Disentangled Semantic Alignment Network (DSANet) to explicitly separate abnormal and normal features from coarse-grained and fine-grained aspects, enhancing the distinguishability. Specifically, at the coarse-grained level, we introduce a self-guided normality modeling branch that reconstructs input video features under the guidance of learned normal prototypes, encouraging the model to exploit normality cues inherent in the video, thereby improving the temporal separation of normal patterns and anomalous events. At the fine-grained level, we present a decoupled contrastive semantic alignment mechanism, which first temporally decomposes each video into event-centric and background-centric components using frame-level anomaly scores and then applies visual-language contrastive learning to enhance class-discriminative representations. Comprehensive experiments on two standard benchmarks, namely XD-Violence and UCF-Crime, demonstrate that DSANet outperforms existing state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>16, <a href='https://arxiv.org/pdf/2511.10003.pdf' target='_blank'>https://arxiv.org/pdf/2511.10003.pdf</a></span>   <span><a href='https://github.com/liuxuexun/DBGroup' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuexun Liu, Xiaoxu Xu, Qiudan Zhang, Lin Ma, Xu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.10003">DBGroup: Dual-Branch Point Grouping for Weakly Supervised 3D Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised 3D instance segmentation is essential for 3D scene understanding, especially as the growing scale of data and high annotation costs associated with fully supervised approaches. Existing methods primarily rely on two forms of weak supervision: one-thing-one-click annotations and bounding box annotations, both of which aim to reduce labeling efforts. However, these approaches still encounter limitations, including labor-intensive annotation processes, high complexity, and reliance on expert annotators. To address these challenges, we propose \textbf{DBGroup}, a two-stage weakly supervised 3D instance segmentation framework that leverages scene-level annotations as a more efficient and scalable alternative. In the first stage, we introduce a Dual-Branch Point Grouping module to generate pseudo labels guided by semantic and mask cues extracted from multi-view images. To further improve label quality, we develop two refinement strategies: Granularity-Aware Instance Merging and Semantic Selection and Propagation. The second stage involves multi-round self-training on an end-to-end instance segmentation network using the refined pseudo-labels. Additionally, we introduce an Instance Mask Filter strategy to address inconsistencies within the pseudo labels. Extensive experiments demonstrate that DBGroup achieves competitive performance compared to sparse-point-level supervised 3D instance segmentation methods, while surpassing state-of-the-art scene-level supervised 3D semantic segmentation approaches. Code is available at https://github.com/liuxuexun/DBGroup.
<div id='section'>Paperid: <span id='pid'>17, <a href='https://arxiv.org/pdf/2511.05833.pdf' target='_blank'>https://arxiv.org/pdf/2511.05833.pdf</a></span>   <span><a href='https://github.com/Taixi-CHEN/TYrPPG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Taixi Chen, Yiu-ming Cheung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.05833">TYrPPG: Uncomplicated and Enhanced Learning Capability rPPG for Remote Heart Rate Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Remote photoplethysmography (rPPG) can remotely extract physiological signals from RGB video, which has many advantages in detecting heart rate, such as low cost and no invasion to patients. The existing rPPG model is usually based on the transformer module, which has low computation efficiency. Recently, the Mamba model has garnered increasing attention due to its efficient performance in natural language processing tasks, demonstrating potential as a substitute for transformer-based algorithms. However, the Mambaout model and its variants prove that the SSM module, which is the core component of the Mamba model, is unnecessary for the vision task. Therefore, we hope to prove the feasibility of using the Mambaout-based module to remotely learn the heart rate. Specifically, we propose a novel rPPG algorithm called uncomplicated and enhanced learning capability rPPG (TYrPPG). This paper introduces an innovative gated video understanding block (GVB) designed for efficient analysis of RGB videos. Based on the Mambaout structure, this block integrates 2D-CNN and 3D-CNN to enhance video understanding for analysis. In addition, we propose a comprehensive supervised loss function (CSL) to improve the model's learning capability, along with its weakly supervised variants. The experiments show that our TYrPPG can achieve state-of-the-art performance in commonly used datasets, indicating its prospects and superiority in remote heart rate estimation. The source code is available at https://github.com/Taixi-CHEN/TYrPPG.
<div id='section'>Paperid: <span id='pid'>18, <a href='https://arxiv.org/pdf/2511.00456.pdf' target='_blank'>https://arxiv.org/pdf/2511.00456.pdf</a></span>   <span><a href='https://github.com/kiranshahi/pneumonia-analysis' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kiran Shahi, Anup Bagale
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.00456">Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study proposes a weakly supervised deep learning framework for pneumonia classification and localization from chest X-rays, utilizing Grad-CAM explanations. Instead of costly pixel-level annotations, our approach uses image-level labels to generate clinically meaningful heatmaps that highlight regions affected by pneumonia. We evaluate seven pre-trained architectures and the Vision Transformer under identical training conditions, using focal loss and patient-wise splits to prevent data leakage. Experimental results suggest that all models achieved high accuracy (96-98%), with ResNet-18 and EfficientNet-B0 showing the best overall performance and MobileNet-V2 providing an efficient lightweight alternative. Grad-CAM heatmap visualizations confirm that the proposed models focus on clinically relevant lung regions, supporting the use of interpretable AI for radiological diagnostics. This work highlights the potential of weakly supervised, explainable models that enhance the transparency of pneumonia screening and clinical trust in AI-assisted screening.
<div id='section'>Paperid: <span id='pid'>19, <a href='https://arxiv.org/pdf/2510.22055.pdf' target='_blank'>https://arxiv.org/pdf/2510.22055.pdf</a></span>   <span><a href='https://github.com/VenkteshV/QuanTemp_Plus' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>V Venktesh, Deepali Prabhu, Avishek Anand
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.22055">A Benchmark for Open-Domain Numerical Fact-Checking Enhanced by Claim Decomposition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fact-checking numerical claims is critical as the presence of numbers provide mirage of veracity despite being fake potentially causing catastrophic impacts on society. The prior works in automatic fact verification do not primarily focus on natural numerical claims. A typical human fact-checker first retrieves relevant evidence addressing the different numerical aspects of the claim and then reasons about them to predict the veracity of the claim. Hence, the search process of a human fact-checker is a crucial skill that forms the foundation of the verification process. Emulating a real-world setting is essential to aid in the development of automated methods that encompass such skills. However, existing benchmarks employ heuristic claim decomposition approaches augmented with weakly supervised web search to collect evidences for verifying claims. This sometimes results in less relevant evidences and noisy sources with temporal leakage rendering a less realistic retrieval setting for claim verification. Hence, we introduce QuanTemp++: a dataset consisting of natural numerical claims, an open domain corpus, with the corresponding relevant evidence for each claim. The evidences are collected through a claim decomposition process approximately emulating the approach of human fact-checker and veracity labels ensuring there is no temporal leakage. Given this dataset, we also characterize the retrieval performance of key claim decomposition paradigms. Finally, we observe their effect on the outcome of the verification pipeline and draw insights. The code for data pipeline along with link to data can be found at https://github.com/VenkteshV/QuanTemp_Plus
<div id='section'>Paperid: <span id='pid'>20, <a href='https://arxiv.org/pdf/2510.21532.pdf' target='_blank'>https://arxiv.org/pdf/2510.21532.pdf</a></span>   <span><a href='https://github.com/rohban-lab/FrameShield' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mojtaba Nafez, Mobina Poulaei, Nikan Vasei, Bardia Soltani Moakhar, Mohammad Sabokrou, MohammadHossein Rohban
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.21532">FrameShield: Adversarially Robust Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Video Anomaly Detection (WSVAD) has achieved notable advancements, yet existing models remain vulnerable to adversarial attacks, limiting their reliability. Due to the inherent constraints of weak supervision, where only video-level labels are provided despite the need for frame-level predictions, traditional adversarial defense mechanisms, such as adversarial training, are not effective since video-level adversarial perturbations are typically weak and inadequate. To address this limitation, pseudo-labels generated directly from the model can enable frame-level adversarial training; however, these pseudo-labels are inherently noisy, significantly degrading performance. We therefore introduce a novel Pseudo-Anomaly Generation method called Spatiotemporal Region Distortion (SRD), which creates synthetic anomalies by applying severe augmentations to localized regions in normal videos while preserving temporal consistency. Integrating these precisely annotated synthetic anomalies with the noisy pseudo-labels substantially reduces label noise, enabling effective adversarial training. Extensive experiments demonstrate that our method significantly enhances the robustness of WSVAD models against adversarial attacks, outperforming state-of-the-art methods by an average of 71.0\% in overall AUROC performance across multiple benchmarks. The implementation and code are publicly available at https://github.com/rohban-lab/FrameShield.
<div id='section'>Paperid: <span id='pid'>21, <a href='https://arxiv.org/pdf/2510.21449.pdf' target='_blank'>https://arxiv.org/pdf/2510.21449.pdf</a></span>   <span><a href='https://github.com/YsTvT/MoniTor' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengtian Yang, Yue Feng, Yingshi Liu, Jingrou Zhang, Jie Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.21449">MoniTor: Exploiting Large Language Models with Instruction for Online Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Anomaly Detection (VAD) aims to locate unusual activities or behaviors within videos. Recently, offline VAD has garnered substantial research attention, which has been invigorated by the progress in large language models (LLMs) and vision-language models (VLMs), offering the potential for a more nuanced understanding of anomalies. However, online VAD has seldom received attention due to real-time constraints and computational intensity. In this paper, we introduce a novel Memory-based online scoring queue scheme for Training-free VAD (MoniTor), to address the inherent complexities in online VAD. Specifically, MoniTor applies a streaming input to VLMs, leveraging the capabilities of pre-trained large-scale models. To capture temporal dependencies more effectively, we incorporate a novel prediction mechanism inspired by Long Short-Term Memory (LSTM) networks. This ensures the model can effectively model past states and leverage previous predictions to identify anomalous behaviors. Thereby, it better understands the current frame. Moreover, we design a scoring queue and an anomaly prior to dynamically store recent scores and cover all anomalies in the monitoring scenario, providing guidance for LLMs to distinguish between normal and abnormal behaviors over time. We evaluate MoniTor on two large datasets (i.e., UCF-Crime and XD-Violence) containing various surveillance and real-world scenarios. The results demonstrate that MoniTor outperforms state-of-the-art methods and is competitive with weakly supervised methods without training. Code is available at https://github.com/YsTvT/MoniTor.
<div id='section'>Paperid: <span id='pid'>22, <a href='https://arxiv.org/pdf/2510.12385.pdf' target='_blank'>https://arxiv.org/pdf/2510.12385.pdf</a></span>   <span><a href='https://timschoonbeek.github.io/stormpsr' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tim J. Schoonbeek, Shao-Hsuan Hung, Dan Lehman, Hans Onvlee, Jacek Kustra, Peter H. N. de With, Fons van der Sommen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.12385">Learning to Recognize Correctly Completed Procedure Steps in Egocentric Assembly Videos through Spatio-Temporal Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Procedure step recognition (PSR) aims to identify all correctly completed steps and their sequential order in videos of procedural tasks. The existing state-of-the-art models rely solely on detecting assembly object states in individual video frames. By neglecting temporal features, model robustness and accuracy are limited, especially when objects are partially occluded. To overcome these limitations, we propose Spatio-Temporal Occlusion-Resilient Modeling for Procedure Step Recognition (STORM-PSR), a dual-stream framework for PSR that leverages both spatial and temporal features. The assembly state detection stream operates effectively with unobstructed views of the object, while the spatio-temporal stream captures both spatial and temporal features to recognize step completions even under partial occlusion. This stream includes a spatial encoder, pre-trained using a novel weakly supervised approach to capture meaningful spatial representations, and a transformer-based temporal encoder that learns how these spatial features relate over time. STORM-PSR is evaluated on the MECCANO and IndustReal datasets, reducing the average delay between actual and predicted assembly step completions by 11.2% and 26.1%, respectively, compared to prior methods. We demonstrate that this reduction in delay is driven by the spatio-temporal stream, which does not rely on unobstructed views of the object to infer completed steps. The code for STORM-PSR, along with the newly annotated MECCANO labels, is made publicly available at https://timschoonbeek.github.io/stormpsr .
<div id='section'>Paperid: <span id='pid'>23, <a href='https://arxiv.org/pdf/2510.10564.pdf' target='_blank'>https://arxiv.org/pdf/2510.10564.pdf</a></span>   <span><a href='https://github.com/lalunex/MGSD-WSS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liang Li, Zhou Yang, Xiaofei Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10564">Multi-Granularity Sequence Denoising with Weakly Supervised Signal for Sequential Recommendation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sequential recommendation aims to predict the next item based on user interests in historical interaction sequences. Historical interaction sequences often contain irrelevant noisy items, which significantly hinders the performance of recommendation systems. Existing research employs unsupervised methods that indirectly identify item-granularity irrelevant noise by predicting the ground truth item. Since these methods lack explicit noise labels, they are prone to misidentify users' interested items as noise. Additionally, while these methods focus on removing item-granularity noise driven by the ground truth item, they overlook interest-granularity noise, limiting their ability to perform broader denoising based on user interests. To address these issues, we propose Multi-Granularity Sequence Denoising with Weakly Supervised Signal for Sequential Recommendation(MGSD-WSS). MGSD-WSS first introduces the Multiple Gaussian Kernel Perceptron module to map the original and enhance sequence into a common representation space and utilizes weakly supervised signals to accurately identify noisy items in the historical interaction sequence. Subsequently, it employs the item-granularity denoising module with noise-weighted contrastive learning to obtain denoised item representations. Then, it extracts target interest representations from the ground truth item and applies noise-weighted contrastive learning to obtain denoised interest representations. Finally, based on the denoised item and interest representations, MGSD-WSS predicts the next item. Extensive experiments on five datasets demonstrate that the proposed method significantly outperforms state-of-the-art sequence recommendation and denoising models. Our code is available at https://github.com/lalunex/MGSD-WSS.
<div id='section'>Paperid: <span id='pid'>24, <a href='https://arxiv.org/pdf/2510.08052.pdf' target='_blank'>https://arxiv.org/pdf/2510.08052.pdf</a></span>   <span><a href='https://github.com/BheeshmSharma/RASALoRE-BMVC-2025/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bheeshm Sharma, Karthikeyan Jaganathan, Balamurugan Palaniappan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08052">RASALoRE: Region Aware Spatial Attention with Location-based Random Embeddings for Weakly Supervised Anomaly Detection in Brain MRI Scans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Anomaly detection (WSAD) in brain MRI scans is an important challenge useful to obtain quick and accurate detection of brain anomalies when precise pixel-level anomaly annotations are unavailable and only weak labels (e.g., slice-level) are available. In this work, we propose RASALoRE: Region Aware Spatial Attention with Location-based Random Embeddings, a novel two-stage WSAD framework. In the first stage, we introduce a Discriminative Dual Prompt Tuning (DDPT) mechanism that generates high-quality pseudo weak masks based on slice-level labels, serving as coarse localization cues. In the second stage, we propose a segmentation network with a region-aware spatial attention mechanism that relies on fixed location-based random embeddings. This design enables the model to effectively focus on anomalous regions. Our approach achieves state-of-the-art anomaly detection performance, significantly outperforming existing WSAD methods while utilizing less than 8 million parameters. Extensive evaluations on the BraTS20, BraTS21, BraTS23, and MSD datasets demonstrate a substantial performance improvement coupled with a significant reduction in computational complexity. Code is available at: https://github.com/BheeshmSharma/RASALoRE-BMVC-2025/.
<div id='section'>Paperid: <span id='pid'>25, <a href='https://arxiv.org/pdf/2510.05899.pdf' target='_blank'>https://arxiv.org/pdf/2510.05899.pdf</a></span>   <span><a href='https://github.com/jiesihu/Weak-ICL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiesi Hu, Yanwu Yang, Zhiyu Ye, Jinyan Zhou, Jianfeng Cao, Hanyang Peng, Ting Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05899">Efficient Universal Models for Medical Image Segmentation via Weakly Supervised In-Context Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Universal models for medical image segmentation, such as interactive and in-context learning (ICL) models, offer strong generalization but require extensive annotations. Interactive models need repeated user prompts for each image, while ICL relies on dense, pixel-level labels. To address this, we propose Weakly Supervised In-Context Learning (WS-ICL), a new ICL paradigm that leverages weak prompts (e.g., bounding boxes or points) instead of dense labels for context. This approach significantly reduces annotation effort by eliminating the need for fine-grained masks and repeated user prompting for all images. We evaluated the proposed WS-ICL model on three held-out benchmarks. Experimental results demonstrate that WS-ICL achieves performance comparable to regular ICL models at a significantly lower annotation cost. In addition, WS-ICL is highly competitive even under the interactive paradigm. These findings establish WS-ICL as a promising step toward more efficient and unified universal models for medical image segmentation. Our code and model are publicly available at https://github.com/jiesihu/Weak-ICL.
<div id='section'>Paperid: <span id='pid'>26, <a href='https://arxiv.org/pdf/2509.10308.pdf' target='_blank'>https://arxiv.org/pdf/2509.10308.pdf</a></span>   <span><a href='https://github.com/riskaudit/GraphCSVAE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Joshua Dimasaka, Christian GeiÃ, Robert Muir-Wood, Emily So
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10308">GraphCSVAE: Graph Categorical Structured Variational Autoencoder for Spatiotemporal Auditing of Physical Vulnerability Towards Sustainable Post-Disaster Risk Reduction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the aftermath of disasters, many institutions worldwide face challenges in continually monitoring changes in disaster risk, limiting the ability of key decision-makers to assess progress towards the UN Sendai Framework for Disaster Risk Reduction 2015-2030. While numerous efforts have substantially advanced the large-scale modeling of hazard and exposure through Earth observation and data-driven methods, progress remains limited in modeling another equally important yet challenging element of the risk equation: physical vulnerability. To address this gap, we introduce Graph Categorical Structured Variational Autoencoder (GraphCSVAE), a novel probabilistic data-driven framework for modeling physical vulnerability by integrating deep learning, graph representation, and categorical probabilistic inference, using time-series satellite-derived datasets and prior expert belief systems. We introduce a weakly supervised first-order transition matrix that reflects the changes in the spatiotemporal distribution of physical vulnerability in two disaster-stricken and socioeconomically disadvantaged areas: (1) the cyclone-impacted coastal Khurushkul community in Bangladesh and (2) the mudslide-affected city of Freetown in Sierra Leone. Our work reveals post-disaster regional dynamics in physical vulnerability, offering valuable insights into localized spatiotemporal auditing and sustainable strategies for post-disaster risk reduction.
<div id='section'>Paperid: <span id='pid'>27, <a href='https://arxiv.org/pdf/2509.08991.pdf' target='_blank'>https://arxiv.org/pdf/2509.08991.pdf</a></span>   <span><a href='https://github.com/magdalena-wysocki/ultron' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Magdalena Wysocki, Felix Duelmer, Ananya Bal, Nassir Navab, Mohammad Farid Azampour
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08991">UltrON: Ultrasound Occupancy Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In free-hand ultrasound imaging, sonographers rely on expertise to mentally integrate partial 2D views into 3D anatomical shapes. Shape reconstruction can assist clinicians in this process. Central to this task is the choice of shape representation, as it determines how accurately and efficiently the structure can be visualized, analyzed, and interpreted. Implicit representations, such as SDF and occupancy function, offer a powerful alternative to traditional voxel- or mesh-based methods by modeling continuous, smooth surfaces with compact storage, avoiding explicit discretization. Recent studies demonstrate that SDF can be effectively optimized using annotations derived from segmented B-mode ultrasound images. Yet, these approaches hinge on precise annotations, overlooking the rich acoustic information embedded in B-mode intensity. Moreover, implicit representation approaches struggle with the ultrasound's view-dependent nature and acoustic shadowing artifacts, which impair reconstruction. To address the problems resulting from occlusions and annotation dependency, we propose an occupancy-based representation and introduce \gls{UltrON} that leverages acoustic features to improve geometric consistency in weakly-supervised optimization regime. We show that these features can be obtained from B-mode images without additional annotation cost. Moreover, we propose a novel loss function that compensates for view-dependency in the B-mode images and facilitates occupancy optimization from multiview ultrasound. By incorporating acoustic properties, \gls{UltrON} generalizes to shapes of the same anatomy. We show that \gls{UltrON} mitigates the limitations of occlusions and sparse labeling and paves the way for more accurate 3D reconstruction. Code and dataset will be available at https://github.com/magdalena-wysocki/ultron.
<div id='section'>Paperid: <span id='pid'>28, <a href='https://arxiv.org/pdf/2509.08289.pdf' target='_blank'>https://arxiv.org/pdf/2509.08289.pdf</a></span>   <span><a href='https://github.com/gyl2565309278/DTH-CP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuelin Guo, Haoyu He, Zhiyuan Chen, Zitong Huang, Renhao Lu, Lu Shi, Zejun Wang, Weizhe Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08289">Dual-Thresholding Heatmaps to Cluster Proposals for Weakly Supervised Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised object detection (WSOD) has attracted significant attention in recent years, as it does not require box-level annotations. State-of-the-art methods generally adopt a multi-module network, which employs WSDDN as the multiple instance detection network module and multiple instance refinement modules to refine performance. However, these approaches suffer from three key limitations. First, existing methods tend to generate pseudo GT boxes that either focus only on discriminative parts, failing to capture the whole object, or cover the entire object but fail to distinguish between adjacent intra-class instances. Second, the foundational WSDDN architecture lacks a crucial background class representation for each proposal and exhibits a large semantic gap between its branches. Third, prior methods discard ignored proposals during optimization, leading to slow convergence. To address these challenges, we first design a heatmap-guided proposal selector (HGPS) algorithm, which utilizes dual thresholds on heatmaps to pre-select proposals, enabling pseudo GT boxes to both capture the full object extent and distinguish between adjacent intra-class instances. We then present a weakly supervised basic detection network (WSBDN), which augments each proposal with a background class representation and uses heatmaps for pre-supervision to bridge the semantic gap between matrices. At last, we introduce a negative certainty supervision loss on ignored proposals to accelerate convergence. Extensive experiments on the challenging PASCAL VOC 2007 and 2012 datasets demonstrate the effectiveness of our framework. We achieve mAP/mCorLoc scores of 58.5%/81.8% on VOC 2007 and 55.6%/80.5% on VOC 2012, performing favorably against the state-of-the-art WSOD methods. Our code is publicly available at https://github.com/gyl2565309278/DTH-CP.
<div id='section'>Paperid: <span id='pid'>29, <a href='https://arxiv.org/pdf/2509.07507.pdf' target='_blank'>https://arxiv.org/pdf/2509.07507.pdf</a></span>   <span><a href='https://github.com/CEA-LIST/MVAT' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/CEA-LIST/MVAT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Saad Lahlali, Alexandre Fournier Montgieux, Nicolas Granger, HervÃ© Le Borgne, Quoc Cuong Pham
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07507">MVAT: Multi-View Aware Teacher for Weakly Supervised 3D Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Annotating 3D data remains a costly bottleneck for 3D object detection, motivating the development of weakly supervised annotation methods that rely on more accessible 2D box annotations. However, relying solely on 2D boxes introduces projection ambiguities since a single 2D box can correspond to multiple valid 3D poses. Furthermore, partial object visibility under a single viewpoint setting makes accurate 3D box estimation difficult. We propose MVAT, a novel framework that leverages temporal multi-view present in sequential data to address these challenges. Our approach aggregates object-centric point clouds across time to build 3D object representations as dense and complete as possible. A Teacher-Student distillation paradigm is employed: The Teacher network learns from single viewpoints but targets are derived from temporally aggregated static objects. Then the Teacher generates high quality pseudo-labels that the Student learns to predict from a single viewpoint for both static and moving objects. The whole framework incorporates a multi-view 2D projection loss to enforce consistency between predicted 3D boxes and all available 2D annotations. Experiments on the nuScenes and Waymo Open datasets demonstrate that MVAT achieves state-of-the-art performance for weakly supervised 3D object detection, significantly narrowing the gap with fully supervised methods without requiring any 3D box annotations. % \footnote{Code available upon acceptance} Our code is available in our public repository (\href{https://github.com/CEA-LIST/MVAT}{code}).
<div id='section'>Paperid: <span id='pid'>30, <a href='https://arxiv.org/pdf/2508.19060.pdf' target='_blank'>https://arxiv.org/pdf/2508.19060.pdf</a></span>   <span><a href='https://github.com/blaz-r/SuperSimpleNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>BlaÅ¾ Rolih, Matic FuÄka, Danijel SkoÄaj
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19060">No Label Left Behind: A Unified Surface Defect Detection Model for all Supervision Regimes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Surface defect detection is a critical task across numerous industries, aimed at efficiently identifying and localising imperfections or irregularities on manufactured components. While numerous methods have been proposed, many fail to meet industrial demands for high performance, efficiency, and adaptability. Existing approaches are often constrained to specific supervision scenarios and struggle to adapt to the diverse data annotations encountered in real-world manufacturing processes, such as unsupervised, weakly supervised, mixed supervision, and fully supervised settings. To address these challenges, we propose SuperSimpleNet, a highly efficient and adaptable discriminative model built on the foundation of SimpleNet. SuperSimpleNet incorporates a novel synthetic anomaly generation process, an enhanced classification head, and an improved learning procedure, enabling efficient training in all four supervision scenarios, making it the first model capable of fully leveraging all available data annotations. SuperSimpleNet sets a new standard for performance across all scenarios, as demonstrated by its results on four challenging benchmark datasets. Beyond accuracy, it is very fast, achieving an inference time below 10 ms. With its ability to unify diverse supervision paradigms while maintaining outstanding speed and reliability, SuperSimpleNet represents a promising step forward in addressing real-world manufacturing challenges and bridging the gap between academic research and industrial applications. Code: https://github.com/blaz-r/SuperSimpleNet
<div id='section'>Paperid: <span id='pid'>31, <a href='https://arxiv.org/pdf/2508.17186.pdf' target='_blank'>https://arxiv.org/pdf/2508.17186.pdf</a></span>   <span><a href='https://github.com/zhenghuizhao/AdvCP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenghui Zhao, Chen Wu, Di Wang, Hongruixuan Chen, Cuiqun Chen, Zhuo Zheng, Bo Du, Liangpei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17186">Advancing Weakly-Supervised Change Detection in Satellite Images via Adversarial Class Prompting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-Supervised Change Detection (WSCD) aims to distinguish specific object changes (e.g., objects appearing or disappearing) from background variations (e.g., environmental changes due to light, weather, or seasonal shifts) in paired satellite images, relying only on paired image (i.e., image-level) classification labels. This technique significantly reduces the need for dense annotations required in fully-supervised change detection. However, as image-level supervision only indicates whether objects have changed in a scene, WSCD methods often misclassify background variations as object changes, especially in complex remote-sensing scenarios. In this work, we propose an Adversarial Class Prompting (AdvCP) method to address this co-occurring noise problem, including two phases: a) Adversarial Prompt Mining: After each training iteration, we introduce adversarial prompting perturbations, using incorrect one-hot image-level labels to activate erroneous feature mappings. This process reveals co-occurring adversarial samples under weak supervision, namely background variation features that are likely to be misclassified as object changes. b) Adversarial Sample Rectification: We integrate these adversarially prompt-activated pixel samples into training by constructing an online global prototype. This prototype is built from an exponentially weighted moving average of the current batch and all historical training data. Our AdvCP can be seamlessly integrated into current WSCD methods without adding additional inference cost. Experiments on ConvNet, Transformer, and Segment Anything Model (SAM)-based baselines demonstrate significant performance enhancements. Furthermore, we demonstrate the generalizability of AdvCP to other multi-class weakly-supervised dense prediction scenarios. Code is available at https://github.com/zhenghuizhao/AdvCP
<div id='section'>Paperid: <span id='pid'>32, <a href='https://arxiv.org/pdf/2508.16159.pdf' target='_blank'>https://arxiv.org/pdf/2508.16159.pdf</a></span>   <span><a href='https://github.com/jarch-ma/TLG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Ma, Guo-Sen Xie, Fang Zhao, Zechao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16159">Through the Looking Glass: A Dual Perspective on Weakly-Supervised Few-Shot Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Meta-learning aims to uniformly sample homogeneous support-query pairs, characterized by the same categories and similar attributes, and extract useful inductive biases through identical network architectures. However, this identical network design results in over-semantic homogenization. To address this, we propose a novel homologous but heterogeneous network. By treating support-query pairs as dual perspectives, we introduce heterogeneous visual aggregation (HA) modules to enhance complementarity while preserving semantic commonality. To further reduce semantic noise and amplify the uniqueness of heterogeneous semantics, we design a heterogeneous transfer (HT) module. Finally, we propose heterogeneous CLIP (HC) textual information to enhance the generalization capability of multimodal models. In the weakly-supervised few-shot semantic segmentation (WFSS) task, with only 1/24 of the parameters of existing state-of-the-art models, TLG achieves a 13.2\% improvement on Pascal-5\textsuperscript{i} and a 9.7\% improvement on COCO-20\textsuperscript{i}. To the best of our knowledge, TLG is also the first weakly supervised (image-level) model that outperforms fully supervised (pixel-level) models under the same backbone architectures. The code is available at https://github.com/jarch-ma/TLG.
<div id='section'>Paperid: <span id='pid'>33, <a href='https://arxiv.org/pdf/2508.12023.pdf' target='_blank'>https://arxiv.org/pdf/2508.12023.pdf</a></span>   <span><a href='https://github.com/SFI-Visual-Intelligence/wiselvam.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Durgesh Kumar Singh, Qing Cao, Sarina Thomas, AhcÃ¨ne Boubekki, Robert Jenssen, Michael Kampffmeyer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12023">WiseLVAM: A Novel Framework For Left Ventricle Automatic Measurements</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Clinical guidelines recommend performing left ventricular (LV) linear measurements in B-mode echocardiographic images at the basal level -- typically at the mitral valve leaflet tips -- and aligned perpendicular to the LV long axis along a virtual scanline (SL). However, most automated methods estimate landmarks directly from B-mode images for the measurement task, where even small shifts in predicted points along the LV walls can lead to significant measurement errors, reducing their clinical reliability. A recent semi-automatic method, EnLVAM, addresses this limitation by constraining landmark prediction to a clinician-defined SL and training on generated Anatomical Motion Mode (AMM) images to predict LV landmarks along the same. To enable full automation, a contour-aware SL placement approach is proposed in this work, in which the LV contour is estimated using a weakly supervised B-mode landmark detector. SL placement is then performed by inferring the LV long axis and the basal level- mimicking clinical guidelines. Building on this foundation, we introduce \textit{WiseLVAM} -- a novel, fully automated yet manually adaptable framework for automatically placing the SL and then automatically performing the LV linear measurements in the AMM mode. \textit{WiseLVAM} utilizes the structure-awareness from B-mode images and the motion-awareness from AMM mode to enhance robustness and accuracy with the potential to provide a practical solution for the routine clinical application. The source code is publicly available at https://github.com/SFI-Visual-Intelligence/wiselvam.git.
<div id='section'>Paperid: <span id='pid'>34, <a href='https://arxiv.org/pdf/2508.10256.pdf' target='_blank'>https://arxiv.org/pdf/2508.10256.pdf</a></span>   <span><a href='https://github.com/nantonzhang/Awesome-Crack-Detection' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinan Zhang, Haolin Wang, Yung-An Hsieh, Zhongyu Yang, Anthony Yezzi, Yi-Chang Tsai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10256">Deep Learning for Crack Detection: A Review of Learning Paradigms, Generalizability, and Datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Crack detection plays a crucial role in civil infrastructures, including inspection of pavements, buildings, etc., and deep learning has significantly advanced this field in recent years. While numerous technical and review papers exist in this domain, emerging trends are reshaping the landscape. These shifts include transitions in learning paradigms (from fully supervised learning to semi-supervised, weakly-supervised, unsupervised, few-shot, domain adaptation and fine-tuning foundation models), improvements in generalizability (from single-dataset performance to cross-dataset evaluation), and diversification in dataset acquisition (from RGB images to specialized sensor-based data). In this review, we systematically analyze these trends and highlight representative works. Additionally, we introduce a new annotated dataset collected with 3D laser scans, 3DCrack, to support future research and conduct extensive benchmarking experiments to establish baselines for commonly used deep learning methodologies, including recent foundation models. Our findings provide insights into the evolving methodologies and future directions in deep learning-based crack detection. Project page: https://github.com/nantonzhang/Awesome-Crack-Detection
<div id='section'>Paperid: <span id='pid'>35, <a href='https://arxiv.org/pdf/2508.07298.pdf' target='_blank'>https://arxiv.org/pdf/2508.07298.pdf</a></span>   <span><a href='https://github.com/Senyh/SynMatch' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiqiang Shen, Peng Cao, Xiaoli Liu, Jinzhu Yang, Osmar R. Zaiane
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07298">SynMatch: Rethinking Consistency in Medical Image Segmentation with Sparse Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Label scarcity remains a major challenge in deep learning-based medical image segmentation. Recent studies use strong-weak pseudo supervision to leverage unlabeled data. However, performance is often hindered by inconsistencies between pseudo labels and their corresponding unlabeled images. In this work, we propose \textbf{SynMatch}, a novel framework that sidesteps the need for improving pseudo labels by synthesizing images to match them instead. Specifically, SynMatch synthesizes images using texture and shape features extracted from the same segmentation model that generates the corresponding pseudo labels for unlabeled images. This design enables the generation of highly consistent synthesized-image-pseudo-label pairs without requiring any training parameters for image synthesis. We extensively evaluate SynMatch across diverse medical image segmentation tasks under semi-supervised learning (SSL), weakly-supervised learning (WSL), and barely-supervised learning (BSL) settings with increasingly limited annotations. The results demonstrate that SynMatch achieves superior performance, especially in the most challenging BSL setting. For example, it outperforms the recent strong-weak pseudo supervision-based method by 29.71\% and 10.05\% on the polyp segmentation task with 5\% and 10\% scribble annotations, respectively. The code will be released at https://github.com/Senyh/SynMatch.
<div id='section'>Paperid: <span id='pid'>36, <a href='https://arxiv.org/pdf/2508.06485.pdf' target='_blank'>https://arxiv.org/pdf/2508.06485.pdf</a></span>   <span><a href='https://github.com/Sofianebouaziz1/WGAST.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sofiane Bouaziz, Adel Hafiane, Raphael Canals, Rachid Nedjai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06485">WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Urbanization, climate change, and agricultural stress are increasing the demand for precise and timely environmental monitoring. Land Surface Temperature (LST) is a key variable in this context and is retrieved from remote sensing satellites. However, these systems face a trade-off between spatial and temporal resolution. While spatio-temporal fusion methods offer promising solutions, few have addressed the estimation of daily LST at 10 m resolution. In this study, we present WGAST, a Weakly-Supervised Generative Network for Daily 10 m LST Estimation via Spatio-Temporal Fusion of Terra MODIS, Landsat 8, and Sentinel-2. WGAST is the first end-to-end deep learning framework designed for this task. It adopts a conditional generative adversarial architecture, with a generator composed of four stages: feature extraction, fusion, LST reconstruction, and noise suppression. The first stage employs a set of encoders to extract multi-level latent representations from the inputs, which are then fused in the second stage using cosine similarity, normalization, and temporal attention mechanisms. The third stage decodes the fused features into high-resolution LST, followed by a Gaussian filter to suppress high-frequency noise. Training follows a weakly supervised strategy based on physical averaging principles and reinforced by a PatchGAN discriminator. Experiments demonstrate that WGAST outperforms existing methods in both quantitative and qualitative evaluations. Compared to the best-performing baseline, on average, WGAST reduces RMSE by 17.18% and improves SSIM by 11.00%. Furthermore, WGAST is robust to cloud-induced LST and effectively captures fine-scale thermal patterns, as validated against 33 ground-based sensors. The code is available at https://github.com/Sofianebouaziz1/WGAST.git.
<div id='section'>Paperid: <span id='pid'>37, <a href='https://arxiv.org/pdf/2508.04943.pdf' target='_blank'>https://arxiv.org/pdf/2508.04943.pdf</a></span>   <span><a href='https://github.com/XZPKU/TRKT.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhu Xu, Ting Lei, Zhimin Li, Guan Wang, Qingchao Chen, Yuxin Peng, Yang liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04943">TRKT: Weakly Supervised Dynamic Scene Graph Generation with Temporal-enhanced Relation-aware Knowledge Transferring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic Scene Graph Generation (DSGG) aims to create a scene graph for each video frame by detecting objects and predicting their relationships. Weakly Supervised DSGG (WS-DSGG) reduces annotation workload by using an unlocalized scene graph from a single frame per video for training. Existing WS-DSGG methods depend on an off-the-shelf external object detector to generate pseudo labels for subsequent DSGG training. However, detectors trained on static, object-centric images struggle in dynamic, relation-aware scenarios required for DSGG, leading to inaccurate localization and low-confidence proposals. To address the challenges posed by external object detectors in WS-DSGG, we propose a Temporal-enhanced Relation-aware Knowledge Transferring (TRKT) method, which leverages knowledge to enhance detection in relation-aware dynamic scenarios. TRKT is built on two key components:(1)Relation-aware knowledge mining: we first employ object and relation class decoders that generate category-specific attention maps to highlight both object regions and interactive areas. Then we propose an Inter-frame Attention Augmentation strategy that exploits optical flow for neighboring frames to enhance the attention maps, making them motion-aware and robust to motion blur. This step yields relation- and motion-aware knowledge mining for WS-DSGG. (2) we introduce a Dual-stream Fusion Module that integrates category-specific attention maps into external detections to refine object localization and boost confidence scores for object proposals. Extensive experiments demonstrate that TRKT achieves state-of-the-art performance on Action Genome dataset. Our code is avaliable at https://github.com/XZPKU/TRKT.git.
<div id='section'>Paperid: <span id='pid'>38, <a href='https://arxiv.org/pdf/2508.03201.pdf' target='_blank'>https://arxiv.org/pdf/2508.03201.pdf</a></span>   <span><a href='https://github.com/I2-Multimedia-Lab/AlignCAT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yidan Wang, Chenyi Zhuang, Wutao Liu, Pan Gao, Nicu Sebe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03201">AlignCAT: Visual-Linguistic Alignment of Category and Attributefor Weakly Supervised Visual Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised visual grounding (VG) aims to locate objects in images based on text descriptions. Despite significant progress, existing methods lack strong cross-modal reasoning to distinguish subtle semantic differences in text expressions due to category-based and attribute-based ambiguity. To address these challenges, we introduce AlignCAT, a novel query-based semantic matching framework for weakly supervised VG. To enhance visual-linguistic alignment, we propose a coarse-grained alignment module that utilizes category information and global context, effectively mitigating interference from category-inconsistent objects. Subsequently, a fine-grained alignment module leverages descriptive information and captures word-level text features to achieve attribute consistency. By exploiting linguistic cues to their fullest extent, our proposed AlignCAT progressively filters out misaligned visual queries and enhances contrastive learning efficiency. Extensive experiments on three VG benchmarks, namely RefCOCO, RefCOCO+, and RefCOCOg, verify the superiority of AlignCAT against existing weakly supervised methods on two VG tasks. Our code is available at: https://github.com/I2-Multimedia-Lab/AlignCAT.
<div id='section'>Paperid: <span id='pid'>39, <a href='https://arxiv.org/pdf/2508.01310.pdf' target='_blank'>https://arxiv.org/pdf/2508.01310.pdf</a></span>   <span><a href='https://github.com/riskaudit/GraphVSSM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Joshua Dimasaka, Christian GeiÃ, Emily So
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01310">GraphVSSM: Graph Variational State-Space Model for Probabilistic Spatiotemporal Inference of Dynamic Exposure and Vulnerability for Regional Disaster Resilience Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Regional disaster resilience quantifies the changing nature of physical risks to inform policy instruments ranging from local immediate recovery to international sustainable development. While many existing state-of-practice methods have greatly advanced the dynamic mapping of exposure and hazard, our understanding of large-scale physical vulnerability has remained static, costly, limited, region-specific, coarse-grained, overly aggregated, and inadequately calibrated. With the significant growth in the availability of time-series satellite imagery and derived products for exposure and hazard, we focus our work on the equally important yet challenging element of the risk equation: physical vulnerability. We leverage machine learning methods that flexibly capture spatial contextual relationships, limited temporal observations, and uncertainty in a unified probabilistic spatiotemporal inference framework. We therefore introduce Graph Variational State-Space Model (GraphVSSM), a novel modular spatiotemporal approach that uniquely integrates graph deep learning, state-space modeling, and variational inference using time-series data and prior expert belief systems in a weakly supervised or coarse-to-fine-grained manner. We present three major results: a city-wide demonstration in Quezon City, Philippines; an investigation of sudden changes in the cyclone-impacted coastal Khurushkul community (Bangladesh) and mudslide-affected Freetown (Sierra Leone); and an open geospatial dataset, METEOR 2.5D, that spatiotemporally enhances the existing global static dataset for UN Least Developed Countries (2020). Beyond advancing regional disaster resilience assessment and improving our understanding global disaster risk reduction progress, our method also offers a probabilistic deep learning approach, contributing to broader urban studies that require compositional data analysis in weak supervision.
<div id='section'>Paperid: <span id='pid'>40, <a href='https://arxiv.org/pdf/2508.01250.pdf' target='_blank'>https://arxiv.org/pdf/2508.01250.pdf</a></span>   <span><a href='https://github.com/CVI-SZU/DisFaceRep' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/CVI-SZU/DisFaceRep' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoqin Wang, Xianxu Hou, Meidan Ding, Junliang Chen, Kaijun Deng, Jinheng Xie, Linlin Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01250">DisFaceRep: Representation Disentanglement for Co-occurring Facial Components in Weakly Supervised Face Parsing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face parsing aims to segment facial images into key components such as eyes, lips, and eyebrows. While existing methods rely on dense pixel-level annotations, such annotations are expensive and labor-intensive to obtain. To reduce annotation cost, we introduce Weakly Supervised Face Parsing (WSFP), a new task setting that performs dense facial component segmentation using only weak supervision, such as image-level labels and natural language descriptions. WSFP introduces unique challenges due to the high co-occurrence and visual similarity of facial components, which lead to ambiguous activations and degraded parsing performance. To address this, we propose DisFaceRep, a representation disentanglement framework designed to separate co-occurring facial components through both explicit and implicit mechanisms. Specifically, we introduce a co-occurring component disentanglement strategy to explicitly reduce dataset-level bias, and a text-guided component disentanglement loss to guide component separation using language supervision implicitly. Extensive experiments on CelebAMask-HQ, LaPa, and Helen demonstrate the difficulty of WSFP and the effectiveness of DisFaceRep, which significantly outperforms existing weakly supervised semantic segmentation methods. The code will be released at \href{https://github.com/CVI-SZU/DisFaceRep}{\textcolor{cyan}{https://github.com/CVI-SZU/DisFaceRep}}.
<div id='section'>Paperid: <span id='pid'>41, <a href='https://arxiv.org/pdf/2508.00312.pdf' target='_blank'>https://arxiv.org/pdf/2508.00312.pdf</a></span>   <span><a href='https://github.com/Sumutan/GV-VAD.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Suhang Cai, Xiaohao Peng, Chong Wang, Xiaojie Cai, Jiangbo Qian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00312">GV-VAD : Exploring Video Generation for Weakly-Supervised Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video anomaly detection (VAD) plays a critical role in public safety applications such as intelligent surveillance. However, the rarity, unpredictability, and high annotation cost of real-world anomalies make it difficult to scale VAD datasets, which limits the performance and generalization ability of existing models. To address this challenge, we propose a generative video-enhanced weakly-supervised video anomaly detection (GV-VAD) framework that leverages text-conditioned video generation models to produce semantically controllable and physically plausible synthetic videos. These virtual videos are used to augment training data at low cost. In addition, a synthetic sample loss scaling strategy is utilized to control the influence of generated synthetic samples for efficient training. The experiments show that the proposed framework outperforms state-of-the-art methods on UCF-Crime datasets. The code is available at https://github.com/Sumutan/GV-VAD.git.
<div id='section'>Paperid: <span id='pid'>42, <a href='https://arxiv.org/pdf/2507.20976.pdf' target='_blank'>https://arxiv.org/pdf/2507.20976.pdf</a></span>   <span><a href='https://humansensinglab.github.io/AGenDA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Fang, Minhyek Jeon, Zheyang Qin, Stanislav Panev, Celso de Melo, Shuowen Hu, Shayok Chakraborty, Fernando De la Torre
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20976">Adapting Vehicle Detectors for Aerial Imagery to Unseen Domains with Weak Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detecting vehicles in aerial imagery is a critical task with applications in traffic monitoring, urban planning, and defense intelligence. Deep learning methods have provided state-of-the-art (SOTA) results for this application. However, a significant challenge arises when models trained on data from one geographic region fail to generalize effectively to other areas. Variability in factors such as environmental conditions, urban layouts, road networks, vehicle types, and image acquisition parameters (e.g., resolution, lighting, and angle) leads to domain shifts that degrade model performance. This paper proposes a novel method that uses generative AI to synthesize high-quality aerial images and their labels, improving detector training through data augmentation. Our key contribution is the development of a multi-stage, multi-modal knowledge transfer framework utilizing fine-tuned latent diffusion models (LDMs) to mitigate the distribution gap between the source and target environments. Extensive experiments across diverse aerial imagery domains show consistent performance improvements in AP50 over supervised learning on source domain data, weakly supervised adaptation methods, unsupervised domain adaptation methods, and open-set object detectors by 4-23%, 6-10%, 7-40%, and more than 50%, respectively. Furthermore, we introduce two newly annotated aerial datasets from New Zealand and Utah to support further research in this field. Project page is available at: https://humansensinglab.github.io/AGenDA
<div id='section'>Paperid: <span id='pid'>43, <a href='https://arxiv.org/pdf/2507.18677.pdf' target='_blank'>https://arxiv.org/pdf/2507.18677.pdf</a></span>   <span><a href='https://github.com/SiyuMU/Loaded2UnNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyu Mu, Wei Xuan Chan, Choon Hwai Yap
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18677">HeartUnloadNet: A Weakly-Supervised Cycle-Consistent Graph Network for Predicting Unloaded Cardiac Geometry from Diastolic States</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The unloaded cardiac geometry (i.e., the state of the heart devoid of luminal pressure) serves as a valuable zero-stress and zero-strain reference and is critical for personalized biomechanical modeling of cardiac function, to understand both healthy and diseased physiology and to predict the effects of cardiac interventions. However, estimating the unloaded geometry from clinical images remains a challenging task. Traditional approaches rely on inverse finite element (FE) solvers that require iterative optimization and are computationally expensive. In this work, we introduce HeartUnloadNet, a deep learning framework that predicts the unloaded left ventricular (LV) shape directly from the end diastolic (ED) mesh while explicitly incorporating biophysical priors. The network accepts a mesh of arbitrary size along with physiological parameters such as ED pressure, myocardial stiffness scale, and fiber helix orientation, and outputs the corresponding unloaded mesh. It adopts a graph attention architecture and employs a cycle-consistency strategy to enable bidirectional (loading and unloading) prediction, allowing for partial self-supervision that improves accuracy and reduces the need for large training datasets. Trained and tested on 20,700 FE simulations across diverse LV geometries and physiological conditions, HeartUnloadNet achieves sub-millimeter accuracy, with an average DSC of 0.986 and HD of 0.083 cm, while reducing inference time to just 0.02 seconds per case, over 10^5 times faster and significantly more accurate than traditional inverse FE solvers. Ablation studies confirm the effectiveness of the architecture. Notably, the cycle-consistent design enables the model to maintain a DSC of 97% even with as few as 200 training samples. This work thus presents a scalable and accurate surrogate for inverse FE solvers, supporting real-time clinical applications in the future.
<div id='section'>Paperid: <span id='pid'>44, <a href='https://arxiv.org/pdf/2507.10935.pdf' target='_blank'>https://arxiv.org/pdf/2507.10935.pdf</a></span>   <span><a href='https://github.com/tongshw/GeoDistill' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaowen Tong, Zimin Xia, Alexandre Alahi, Xuming He, Yujiao Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10935">GeoDistill: Geometry-Guided Self-Distillation for Weakly Supervised Cross-View Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-view localization, the task of estimating a camera's 3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with satellite images, is crucial for large-scale outdoor applications like autonomous navigation and augmented reality. Existing methods often rely on fully supervised learning, which requires costly ground-truth pose annotations. In this work, we propose GeoDistill, a Geometry guided weakly supervised self distillation framework that uses teacher-student learning with Field-of-View (FoV)-based masking to enhance local feature learning for robust cross-view localization. In GeoDistill, the teacher model localizes a panoramic image, while the student model predicts locations from a limited FoV counterpart created by FoV-based masking. By aligning the student's predictions with those of the teacher, the student focuses on key features like lane lines and ignores textureless regions, such as roads. This results in more accurate predictions and reduced uncertainty, regardless of whether the query images are panoramas or limited FoV images. Our experiments show that GeoDistill significantly improves localization performance across different frameworks. Additionally, we introduce a novel orientation estimation network that predicts relative orientation without requiring precise planar position ground truth. GeoDistill provides a scalable and efficient solution for real-world cross-view localization challenges. Code and model can be found at https://github.com/tongshw/GeoDistill.
<div id='section'>Paperid: <span id='pid'>45, <a href='https://arxiv.org/pdf/2507.07578.pdf' target='_blank'>https://arxiv.org/pdf/2507.07578.pdf</a></span>   <span><a href='https://github.com/ChunyanWang1/DGKD-WLSS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunyan Wang, Dong Zhang, Jinhui Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07578">Diffusion-Guided Knowledge Distillation for Weakly-Supervised Low-Light Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised semantic segmentation aims to assign category labels to each pixel using weak annotations, significantly reducing manual annotation costs. Although existing methods have achieved remarkable progress in well-lit scenarios, their performance significantly degrades in low-light environments due to two fundamental limitations: severe image quality degradation (e.g., low contrast, noise, and color distortion) and the inherent constraints of weak supervision. These factors collectively lead to unreliable class activation maps and semantically ambiguous pseudo-labels, ultimately compromising the model's ability to learn discriminative feature representations. To address these problems, we propose Diffusion-Guided Knowledge Distillation for Weakly-Supervised Low-light Semantic Segmentation (DGKD-WLSS), a novel framework that synergistically combines Diffusion-Guided Knowledge Distillation (DGKD) with Depth-Guided Feature Fusion (DGF2). DGKD aligns normal-light and low-light features via diffusion-based denoising and knowledge distillation, while DGF2 integrates depth maps as illumination-invariant geometric priors to enhance structural feature learning. Extensive experiments demonstrate the effectiveness of DGKD-WLSS, which achieves state-of-the-art performance in weakly supervised semantic segmentation tasks under low-light conditions. The source codes have been released at:https://github.com/ChunyanWang1/DGKD-WLSS.
<div id='section'>Paperid: <span id='pid'>46, <a href='https://arxiv.org/pdf/2507.04839.pdf' target='_blank'>https://arxiv.org/pdf/2507.04839.pdf</a></span>   <span><a href='https://github.com/fraunhoferhhi/RIPE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Johannes KÃ¼nzel, Anna Hilsmann, Peter Eisert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04839">RIPE: Reinforcement Learning on Unlabeled Image Pairs for Robust Keypoint Extraction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce RIPE, an innovative reinforcement learning-based framework for weakly-supervised training of a keypoint extractor that excels in both detection and description tasks. In contrast to conventional training regimes that depend heavily on artificial transformations, pre-generated models, or 3D data, RIPE requires only a binary label indicating whether paired images represent the same scene. This minimal supervision significantly expands the pool of training data, enabling the creation of a highly generalized and robust keypoint extractor.
  RIPE utilizes the encoder's intermediate layers for the description of the keypoints with a hyper-column approach to integrate information from different scales. Additionally, we propose an auxiliary loss to enhance the discriminative capability of the learned descriptors.
  Comprehensive evaluations on standard benchmarks demonstrate that RIPE simplifies data preparation while achieving competitive performance compared to state-of-the-art techniques, marking a significant advancement in robust keypoint extraction and description. To support further research, we have made our code publicly available at https://github.com/fraunhoferhhi/RIPE.
<div id='section'>Paperid: <span id='pid'>47, <a href='https://arxiv.org/pdf/2507.02751.pdf' target='_blank'>https://arxiv.org/pdf/2507.02751.pdf</a></span>   <span><a href='https://github.com/VisionXLab/PWOOD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingxin Liu, Peiyuan Zhang, Yuan Liu, Wei Zhang, Yue Zhou, Ning Liao, Ziyang Gong, Junwei Luo, Zhirui Wang, Yi Yu, Xue Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02751">Partial Weakly-Supervised Oriented Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing demand for oriented object detection (OOD) across various domains has driven significant research in this area. However, the high cost of dataset annotation remains a major concern. Current mainstream OOD algorithms can be mainly categorized into three types: (1) fully supervised methods using complete oriented bounding box (OBB) annotations, (2) semi-supervised methods using partial OBB annotations, and (3) weakly supervised methods using weak annotations such as horizontal boxes or points. However, these algorithms inevitably increase the cost of models in terms of annotation speed or annotation cost. To address this issue, we propose:(1) the first Partial Weakly-Supervised Oriented Object Detection (PWOOD) framework based on partially weak annotations (horizontal boxes or single points), which can efficiently leverage large amounts of unlabeled data, significantly outperforming weakly supervised algorithms trained with partially weak annotations, also offers a lower cost solution; (2) Orientation-and-Scale-aware Student (OS-Student) model capable of learning orientation and scale information with only a small amount of orientation-agnostic or scale-agnostic weak annotations; and (3) Class-Agnostic Pseudo-Label Filtering strategy (CPF) to reduce the model's sensitivity to static filtering thresholds. Comprehensive experiments on DOTA-v1.0/v1.5/v2.0 and DIOR datasets demonstrate that our PWOOD framework performs comparably to, or even surpasses, traditional semi-supervised algorithms.
<div id='section'>Paperid: <span id='pid'>48, <a href='https://arxiv.org/pdf/2507.02743.pdf' target='_blank'>https://arxiv.org/pdf/2507.02743.pdf</a></span>   <span><a href='https://github.com/Minimel/box-prompt-learning-VFM.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>MÃ©lanie Gaillochet, Mehrdad Noori, Sahar Dastani, Christian Desrosiers, HervÃ© Lombaert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02743">Prompt learning with bounding box constraints for medical image segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pixel-wise annotations are notoriously labourious and costly to obtain in the medical domain. To mitigate this burden, weakly supervised approaches based on bounding box annotations-much easier to acquire-offer a practical alternative. Vision foundation models have recently shown noteworthy segmentation performance when provided with prompts such as points or bounding boxes. Prompt learning exploits these models by adapting them to downstream tasks and automating segmentation, thereby reducing user intervention. However, existing prompt learning approaches depend on fully annotated segmentation masks. This paper proposes a novel framework that combines the representational power of foundation models with the annotation efficiency of weakly supervised segmentation. More specifically, our approach automates prompt generation for foundation models using only bounding box annotations. Our proposed optimization scheme integrates multiple constraints derived from box annotations with pseudo-labels generated by the prompted foundation model. Extensive experiments across multimodal datasets reveal that our weakly supervised method achieves an average Dice score of 84.90% in a limited data setting, outperforming existing fully-supervised and weakly-supervised approaches. The code is available at https://github.com/Minimel/box-prompt-learning-VFM.git
<div id='section'>Paperid: <span id='pid'>49, <a href='https://arxiv.org/pdf/2507.01384.pdf' target='_blank'>https://arxiv.org/pdf/2507.01384.pdf</a></span>   <span><a href='https://github.com/WangLY136/MUG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Langyu Wang, Bingke Zhu, Yingying Chen, Yiyuan Zhang, Ming Tang, Jinqiao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01384">MUG: Pseudo Labeling Augmented Audio-Visual Mamba Network for Audio-Visual Video Parsing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The weakly-supervised audio-visual video parsing (AVVP) aims to predict all modality-specific events and locate their temporal boundaries. Despite significant progress, due to the limitations of the weakly-supervised and the deficiencies of the model architecture, existing methods are lacking in simultaneously improving both the segment-level prediction and the event-level prediction. In this work, we propose a audio-visual Mamba network with pseudo labeling aUGmentation (MUG) for emphasising the uniqueness of each segment and excluding the noise interference from the alternate modalities. Specifically, we annotate some of the pseudo-labels based on previous work. Using unimodal pseudo-labels, we perform cross-modal random combinations to generate new data, which can enhance the model's ability to parse various segment-level event combinations. For feature processing and interaction, we employ a audio-visual mamba network. The AV-Mamba enhances the ability to perceive different segments and excludes additional modal noise while sharing similar modal information. Our extensive experiments demonstrate that MUG improves state-of-the-art results on LLP dataset in all metrics (e.g,, gains of 2.1% and 1.2% in terms of visual Segment-level and audio Segment-level metrics). Our code is available at https://github.com/WangLY136/MUG.
<div id='section'>Paperid: <span id='pid'>50, <a href='https://arxiv.org/pdf/2506.23648.pdf' target='_blank'>https://arxiv.org/pdf/2506.23648.pdf</a></span>   <span><a href='https://github.com/cskdstz/MReg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Liu, Yuhao Huang, Lian Liu, Chengrui Zhang, Haotian Lin, Tong Han, Zhiyuan Zhu, Yanlin Chen, Yuerui Chen, Dong Ni, Zhongshan Gou, Xin Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23648">MReg: A Novel Regression Model with MoE-based Video Feature Mining for Mitral Regurgitation Diagnosis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Color Doppler echocardiography is a crucial tool for diagnosing mitral regurgitation (MR). Recent studies have explored intelligent methods for MR diagnosis to minimize user dependence and improve accuracy. However, these approaches often fail to align with clinical workflow and may lead to suboptimal accuracy and interpretability. In this study, we introduce an automated MR diagnosis model (MReg) developed on the 4-chamber cardiac color Doppler echocardiography video (A4C-CDV). It follows comprehensive feature mining strategies to detect MR and assess its severity, considering clinical realities. Our contribution is threefold. First, we formulate the MR diagnosis as a regression task to capture the continuity and ordinal relationships between categories. Second, we design a feature selection and amplification mechanism to imitate the sonographer's diagnostic logic for accurate MR grading. Third, inspired by the Mixture-of-Experts concept, we introduce a feature summary module to extract the category-level features, enhancing the representational capacity for more accurate grading. We trained and evaluated our proposed MReg on a large in-house A4C-CDV dataset comprising 1868 cases with three graded regurgitation labels. Compared to other weakly supervised video anomaly detection and supervised classification methods, MReg demonstrated superior performance in MR diagnosis. Our code is available at: https://github.com/cskdstz/MReg.
<div id='section'>Paperid: <span id='pid'>51, <a href='https://arxiv.org/pdf/2506.22505.pdf' target='_blank'>https://arxiv.org/pdf/2506.22505.pdf</a></span>   <span><a href='GitHub' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/bakerhassan/WSOS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hassan Baker, Matthew S. Emigh, Austin J. Brockmeier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22505">Weakly Supervised Object Segmentation by Background Conditional Divergence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As a computer vision task, automatic object segmentation remains challenging in specialized image domains without massive labeled data, such as synthetic aperture sonar images, remote sensing, biomedical imaging, etc. In any domain, obtaining pixel-wise segmentation masks is expensive. In this work, we propose a method for training a masking network to perform binary object segmentation using weak supervision in the form of image-wise presence or absence of an object of interest, which provides less information but may be obtained more quickly from manual or automatic labeling. A key step in our method is that the segmented objects can be placed into background-only images to create realistic, images of the objects with counterfactual backgrounds. To create a contrast between the original and counterfactual background images, we propose to first cluster the background-only images, and then during learning create counterfactual images that blend objects segmented from their original source backgrounds to backgrounds chosen from a targeted cluster. One term in the training loss is the divergence between these counterfactual images and the real object images with backgrounds of the target cluster. The other term is a supervised loss for background-only images. While an adversarial critic could provide the divergence, we use sample-based divergences. We conduct experiments on side-scan and synthetic aperture sonar in which our approach succeeds compared to previous unsupervised segmentation baselines that were only tested on natural images. Furthermore, to show generality we extend our experiments to natural images, obtaining reasonable performance with our method that avoids pretrained networks, generative networks, and adversarial critics. The basecode for this work can be found at \href{GitHub}{https://github.com/bakerhassan/WSOS}.
<div id='section'>Paperid: <span id='pid'>52, <a href='https://arxiv.org/pdf/2506.09022.pdf' target='_blank'>https://arxiv.org/pdf/2506.09022.pdf</a></span>   <span><a href='https://github.com/mahmoodlab/MIL-Lab' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Shao, Richard J. Chen, Andrew H. Song, Joel Runevic, Ming Y. Lu, Tong Ding, Faisal Mahmood
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09022">Do Multiple Instance Learning Models Transfer?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple Instance Learning (MIL) is a cornerstone approach in computational pathology (CPath) for generating clinically meaningful slide-level embeddings from gigapixel tissue images. However, MIL often struggles with small, weakly supervised clinical datasets. In contrast to fields such as NLP and conventional computer vision, where transfer learning is widely used to address data scarcity, the transferability of MIL models remains poorly understood. In this study, we systematically evaluate the transfer learning capabilities of pretrained MIL models by assessing 11 models across 21 pretraining tasks for morphological and molecular subtype prediction. Our results show that pretrained MIL models, even when trained on different organs than the target task, consistently outperform models trained from scratch. Moreover, pretraining on pancancer datasets enables strong generalization across organs and tasks, outperforming slide foundation models while using substantially less pretraining data. These findings highlight the robust adaptability of MIL models and demonstrate the benefits of leveraging transfer learning to boost performance in CPath. Lastly, we provide a resource which standardizes the implementation of MIL models and collection of pretrained model weights on popular CPath tasks, available at https://github.com/mahmoodlab/MIL-Lab
<div id='section'>Paperid: <span id='pid'>53, <a href='https://arxiv.org/pdf/2506.06589.pdf' target='_blank'>https://arxiv.org/pdf/2506.06589.pdf</a></span>   <span><a href='https://github.com/jacqueline-he/precise-information-control' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jacqueline He, Howard Yen, Margaret Li, Shuyue Stella Li, Zhiyuan Zeng, Weijia Shi, Yulia Tsvetkov, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06589">Precise Information Control in Long-Form Text Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A central challenge in modern language models (LMs) is intrinsic hallucination: the generation of information that is plausible but unsubstantiated relative to input context. To study this problem, we propose Precise Information Control (PIC), a new task formulation that requires models to generate long-form outputs grounded in a provided set of short self-contained statements, known as verifiable claims, without adding any unsupported ones. For comprehensiveness, PIC includes a full setting that tests a model's ability to include exactly all input claims, and a partial setting that requires the model to selectively incorporate only relevant claims. We present PIC-Bench, a benchmark of eight long-form generation tasks (e.g., summarization, biography generation) adapted to the PIC setting, where LMs are supplied with well-formed, verifiable input claims. Our evaluation of a range of open and proprietary LMs on PIC-Bench reveals that, surprisingly, state-of-the-art LMs still intrinsically hallucinate in over 70% of outputs. To alleviate this lack of faithfulness, we introduce a post-training framework, using a weakly supervised preference data construction method, to train an 8B PIC-LM with stronger PIC ability--improving from 69.1% to 91.0% F1 in the full PIC setting. When integrated into end-to-end factual generation pipelines, PIC-LM improves exact match recall by 17.1% on ambiguous QA with retrieval, and factual precision by 30.5% on a birthplace verification task, underscoring the potential of precisely grounded generation.
<div id='section'>Paperid: <span id='pid'>54, <a href='https://arxiv.org/pdf/2506.05895.pdf' target='_blank'>https://arxiv.org/pdf/2506.05895.pdf</a></span>   <span><a href='https://github.com/adrienpetralia/CamAL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Adrien Petralia, Paul Boniol, Philippe Charpentier, Themis Palpanas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05895">Few Labels are all you need: A Weakly Supervised Framework for Appliance Localization in Smart-Meter Series</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Improving smart grid system management is crucial in the fight against climate change, and enabling consumers to play an active role in this effort is a significant challenge for electricity suppliers. In this regard, millions of smart meters have been deployed worldwide in the last decade, recording the main electricity power consumed in individual households. This data produces valuable information that can help them reduce their electricity footprint; nevertheless, the collected signal aggregates the consumption of the different appliances running simultaneously in the house, making it difficult to apprehend. Non-Intrusive Load Monitoring (NILM) refers to the challenge of estimating the power consumption, pattern, or on/off state activation of individual appliances using the main smart meter signal. Recent methods proposed to tackle this task are based on a fully supervised deep-learning approach that requires both the aggregate signal and the ground truth of individual appliance power. However, such labels are expensive to collect and extremely scarce in practice, as they require conducting intrusive surveys in households to monitor each appliance. In this paper, we introduce CamAL, a weakly supervised approach for appliance pattern localization that only requires information on the presence of an appliance in a household to be trained. CamAL merges an ensemble of deep-learning classifiers combined with an explainable classification method to be able to localize appliance patterns. Our experimental evaluation, conducted on 4 real-world datasets, demonstrates that CamAL significantly outperforms existing weakly supervised baselines and that current SotA fully supervised NILM approaches require significantly more labels to reach CamAL performances. The source of our experiments is available at: https://github.com/adrienpetralia/CamAL. This paper appeared in ICDE 2025.
<div id='section'>Paperid: <span id='pid'>55, <a href='https://arxiv.org/pdf/2505.24824.pdf' target='_blank'>https://arxiv.org/pdf/2505.24824.pdf</a></span>   <span><a href='https://github.com/Archiel19/FRAx4.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Marta LÃ³pez-Rauhut, Hongyu Zhou, Mathieu Aubry, Loic Landrieu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24824">Segmenting France Across Four Centuries</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Historical maps offer an invaluable perspective into territory evolution across past centuries--long before satellite or remote sensing technologies existed. Deep learning methods have shown promising results in segmenting historical maps, but publicly available datasets typically focus on a single map type or period, require extensive and costly annotations, and are not suited for nationwide, long-term analyses. In this paper, we introduce a new dataset of historical maps tailored for analyzing large-scale, long-term land use and land cover evolution with limited annotations. Spanning metropolitan France (548,305 km^2), our dataset contains three map collections from the 18th, 19th, and 20th centuries. We provide both comprehensive modern labels and 22,878 km^2 of manually annotated historical labels for the 18th and 19th century maps. Our dataset illustrates the complexity of the segmentation task, featuring stylistic inconsistencies, interpretive ambiguities, and significant landscape changes (e.g., marshlands disappearing in favor of forests). We assess the difficulty of these challenges by benchmarking three approaches: a fully-supervised model trained with historical labels, and two weakly-supervised models that rely only on modern annotations. The latter either use the modern labels directly or first perform image-to-image translation to address the stylistic gap between historical and contemporary maps. Finally, we discuss how these methods can support long-term environment monitoring, offering insights into centuries of landscape transformation. Our official project repository is publicly available at https://github.com/Archiel19/FRAx4.git.
<div id='section'>Paperid: <span id='pid'>56, <a href='https://arxiv.org/pdf/2505.24103.pdf' target='_blank'>https://arxiv.org/pdf/2505.24103.pdf</a></span>   <span><a href='https://github.com/woyut/WSAG-PLSP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Peiran Xu, Yadong Mu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24103">Weakly-Supervised Affordance Grounding Guided by Part-Level Semantic Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we focus on the task of weakly supervised affordance grounding, where a model is trained to identify affordance regions on objects using human-object interaction images and egocentric object images without dense labels. Previous works are mostly built upon class activation maps, which are effective for semantic segmentation but may not be suitable for locating actions and functions. Leveraging recent advanced foundation models, we develop a supervised training pipeline based on pseudo labels. The pseudo labels are generated from an off-the-shelf part segmentation model, guided by a mapping from affordance to part names. Furthermore, we introduce three key enhancements to the baseline model: a label refining stage, a fine-grained feature alignment process, and a lightweight reasoning module. These techniques harness the semantic knowledge of static objects embedded in off-the-shelf foundation models to improve affordance learning, effectively bridging the gap between objects and actions. Extensive experiments demonstrate that the performance of the proposed model has achieved a breakthrough improvement over existing methods. Our codes are available at https://github.com/woyut/WSAG-PLSP .
<div id='section'>Paperid: <span id='pid'>57, <a href='https://arxiv.org/pdf/2505.22028.pdf' target='_blank'>https://arxiv.org/pdf/2505.22028.pdf</a></span>   <span><a href='https://github.com/Speechless-10308/WSC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zi-Hao Zhou, Jun-Jie Wang, Tong Wei, Min-Ling Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22028">Weakly-Supervised Contrastive Learning for Imprecise Class Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contrastive learning has achieved remarkable success in learning effective representations, with supervised contrastive learning often outperforming self-supervised approaches. However, in real-world scenarios, data annotations are often ambiguous or inaccurate, meaning that class labels may not reliably indicate whether two examples belong to the same class. This limitation restricts the applicability of supervised contrastive learning. To address this challenge, we introduce the concept of ``continuous semantic similarity'' to define positive and negative pairs. Instead of directly relying on imprecise class labels, we measure the semantic similarity between example pairs, which quantifies how closely they belong to the same category by iteratively refining weak supervisory signals. Based on this concept, we propose a graph-theoretic framework for weakly-supervised contrastive learning, where semantic similarity serves as the graph weights. Our framework is highly versatile and can be applied to many weakly-supervised learning scenarios. We demonstrate its effectiveness through experiments in two common settings, i.e., noisy label and partial label learning, where existing methods can be easily integrated to significantly improve performance. Theoretically, we establish an error bound for our approach, showing that it can approximate supervised contrastive learning under mild conditions. The implementation code is available at https://github.com/Speechless-10308/WSC.
<div id='section'>Paperid: <span id='pid'>58, <a href='https://arxiv.org/pdf/2505.19190.pdf' target='_blank'>https://arxiv.org/pdf/2505.19190.pdf</a></span>   <span><a href='https://github.com/Raina-Xin/I2MoE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayi Xin, Sukwon Yun, Jie Peng, Inyoung Choi, Jenna L. Ballard, Tianlong Chen, Qi Long
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19190">I2MoE: Interpretable Multimodal Interaction-aware Mixture-of-Experts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modality fusion is a cornerstone of multimodal learning, enabling information integration from diverse data sources. However, vanilla fusion methods are limited by (1) inability to account for heterogeneous interactions between modalities and (2) lack of interpretability in uncovering the multimodal interactions inherent in the data. To this end, we propose I2MoE (Interpretable Multimodal Interaction-aware Mixture of Experts), an end-to-end MoE framework designed to enhance modality fusion by explicitly modeling diverse multimodal interactions, as well as providing interpretation on a local and global level. First, I2MoE utilizes different interaction experts with weakly supervised interaction losses to learn multimodal interactions in a data-driven way. Second, I2MoE deploys a reweighting model that assigns importance scores for the output of each interaction expert, which offers sample-level and dataset-level interpretation. Extensive evaluation of medical and general multimodal datasets shows that I2MoE is flexible enough to be combined with different fusion techniques, consistently improves task performance, and provides interpretation across various real-world scenarios. Code is available at https://github.com/Raina-Xin/I2MoE.
<div id='section'>Paperid: <span id='pid'>59, <a href='https://arxiv.org/pdf/2505.19022.pdf' target='_blank'>https://arxiv.org/pdf/2505.19022.pdf</a></span>   <span><a href='https://github.com/Kamino666/RethinkingVAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihao Liu, Xiaoyu Wu, Wenna Li, Linlin Yang, Shengjin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19022">Rethinking Metrics and Benchmarks of Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Anomaly Detection (VAD), which aims to detect anomalies that deviate from expectation, has attracted increasing attention in recent years. Existing advancements in VAD primarily focus on model architectures and training strategies, while devoting insufficient attention to evaluation metrics and benchmarks. In this paper, we rethink VAD evaluation methods through comprehensive analyses, revealing three critical limitations in current practices: 1) existing metrics are significantly influenced by single annotation bias; 2) current metrics fail to reward early detection of anomalies; 3) available benchmarks lack the capability to evaluate scene overfitting of fully/weakly-supervised algorithms. To address these limitations, we propose three novel evaluation methods: first, we establish probabilistic AUC/AP (Prob-AUC/AP) metrics utlizing multi-round annotations to mitigate single annotation bias; second, we develop a Latency-aware Average Precision (LaAP) metric that rewards early and accurate anomaly detection; and finally, we introduce two hard normal benchmarks (UCF-HN, MSAD-HN) with videos specifically designed to evaluate scene overfitting. We report performance comparisons of ten state-of-the-art VAD approaches using our proposed evaluation methods, providing novel perspectives for future VAD model development. We release our data and code in https://github.com/Kamino666/RethinkingVAD.
<div id='section'>Paperid: <span id='pid'>60, <a href='https://arxiv.org/pdf/2505.18686.pdf' target='_blank'>https://arxiv.org/pdf/2505.18686.pdf</a></span>   <span><a href='https://github.com/MRUIL/WeakMCN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Liu, Silin Cheng, Xinwei He, Sebastien Ourselin, Lei Tan, Gen Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18686">WeakMCN: Multi-task Collaborative Network for Weakly Supervised Referring Expression Comprehension and Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised referring expression comprehension(WREC) and segmentation(WRES) aim to learn object grounding based on a given expression using weak supervision signals like image-text pairs. While these tasks have traditionally been modeled separately, we argue that they can benefit from joint learning in a multi-task framework. To this end, we propose WeakMCN, a novel multi-task collaborative network that effectively combines WREC and WRES with a dual-branch architecture. Specifically, the WREC branch is formulated as anchor-based contrastive learning, which also acts as a teacher to supervise the WRES branch. In WeakMCN, we propose two innovative designs to facilitate multi-task collaboration, namely Dynamic Visual Feature Enhancement(DVFE) and Collaborative Consistency Module(CCM). DVFE dynamically combines various pre-trained visual knowledge to meet different task requirements, while CCM promotes cross-task consistency from the perspective of optimization. Extensive experimental results on three popular REC and RES benchmarks, i.e., RefCOCO, RefCOCO+, and RefCOCOg, consistently demonstrate performance gains of WeakMCN over state-of-the-art single-task alternatives, e.g., up to 3.91% and 13.11% on RefCOCO for WREC and WRES tasks, respectively. Furthermore, experiments also validate the strong generalization ability of WeakMCN in both semi-supervised REC and RES settings against existing methods, e.g., +8.94% for semi-REC and +7.71% for semi-RES on 1% RefCOCO. The code is publicly available at https://github.com/MRUIL/WeakMCN.
<div id='section'>Paperid: <span id='pid'>61, <a href='https://arxiv.org/pdf/2505.17982.pdf' target='_blank'>https://arxiv.org/pdf/2505.17982.pdf</a></span>   <span><a href='https://github.com/bryanwong17/HiVE-MIL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bryan Wong, Jong Woo Kim, Huazhu Fu, Mun Yong Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17982">Few-Shot Learning from Gigapixel Images via Hierarchical Vision-Language Alignment and Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language models (VLMs) have recently been integrated into multiple instance learning (MIL) frameworks to address the challenge of few-shot, weakly supervised classification of whole slide images (WSIs). A key trend involves leveraging multi-scale information to better represent hierarchical tissue structures. However, existing methods often face two key limitations: (1) insufficient modeling of interactions within the same modalities across scales (e.g., 5x and 20x) and (2) inadequate alignment between visual and textual modalities on the same scale. To address these gaps, we propose HiVE-MIL, a hierarchical vision-language framework that constructs a unified graph consisting of (1) parent-child links between coarse (5x) and fine (20x) visual/textual nodes to capture hierarchical relationships, and (2) heterogeneous intra-scale edges linking visual and textual nodes on the same scale. To further enhance semantic consistency, HiVE-MIL incorporates a two-stage, text-guided dynamic filtering mechanism that removes weakly correlated patch-text pairs, and introduces a hierarchical contrastive loss to align textual semantics across scales. Extensive experiments on TCGA breast, lung, and kidney cancer datasets demonstrate that HiVE-MIL consistently outperforms both traditional MIL and recent VLM-based MIL approaches, achieving gains of up to 4.1% in macro F1 under 16-shot settings. Our results demonstrate the value of jointly modeling hierarchical structure and multimodal alignment for efficient and scalable learning from limited pathology data. The code is available at https://github.com/bryanwong17/HiVE-MIL
<div id='section'>Paperid: <span id='pid'>62, <a href='https://arxiv.org/pdf/2505.16399.pdf' target='_blank'>https://arxiv.org/pdf/2505.16399.pdf</a></span>   <span><a href='https://github.com/dengq7/Sketchy-3DIS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qian Deng, Le Hui, Jin Xie, Jian Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16399">Sketchy Bounding-box Supervision for 3D Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Bounding box supervision has gained considerable attention in weakly supervised 3D instance segmentation. While this approach alleviates the need for extensive point-level annotations, obtaining accurate bounding boxes in practical applications remains challenging. To this end, we explore the inaccurate bounding box, named sketchy bounding box, which is imitated through perturbing ground truth bounding box by adding scaling, translation, and rotation. In this paper, we propose Sketchy-3DIS, a novel weakly 3D instance segmentation framework, which jointly learns pseudo labeler and segmentator to improve the performance under the sketchy bounding-box supervisions. Specifically, we first propose an adaptive box-to-point pseudo labeler that adaptively learns to assign points located in the overlapped parts between two sketchy bounding boxes to the correct instance, resulting in compact and pure pseudo instance labels. Then, we present a coarse-to-fine instance segmentator that first predicts coarse instances from the entire point cloud and then learns fine instances based on the region of coarse instances. Finally, by using the pseudo instance labels to supervise the instance segmentator, we can gradually generate high-quality instances through joint training. Extensive experiments show that our method achieves state-of-the-art performance on both the ScanNetV2 and S3DIS benchmarks, and even outperforms several fully supervised methods using sketchy bounding boxes. Code is available at https://github.com/dengq7/Sketchy-3DIS.
<div id='section'>Paperid: <span id='pid'>63, <a href='https://arxiv.org/pdf/2505.13905.pdf' target='_blank'>https://arxiv.org/pdf/2505.13905.pdf</a></span>   <span><a href='https://github.com/CLASS-Lab/4D-ROLLS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruihan Liu, Xiaoyi Wu, Xijun Chen, Liang Hu, Yunjiang Lou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13905">4D-ROLLS: 4D Radar Occupancy Learning via LiDAR Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A comprehensive understanding of 3D scenes is essential for autonomous vehicles (AVs), and among various perception tasks, occupancy estimation plays a central role by providing a general representation of drivable and occupied space. However, most existing occupancy estimation methods rely on LiDAR or cameras, which perform poorly in degraded environments such as smoke, rain, snow, and fog. In this paper, we propose 4D-ROLLS, the first weakly supervised occupancy estimation method for 4D radar using the LiDAR point cloud as the supervisory signal. Specifically, we introduce a method for generating pseudo-LiDAR labels, including occupancy queries and LiDAR height maps, as multi-stage supervision to train the 4D radar occupancy estimation model. Then the model is aligned with the occupancy map produced by LiDAR, fine-tuning its accuracy in occupancy estimation. Extensive comparative experiments validate the exceptional performance of 4D-ROLLS. Its robustness in degraded environments and effectiveness in cross-dataset training are qualitatively demonstrated. The model is also seamlessly transferred to downstream tasks BEV segmentation and point cloud occupancy prediction, highlighting its potential for broader applications. The lightweight network enables 4D-ROLLS model to achieve fast inference speeds at about 30 Hz on a 4060 GPU. The code of 4D-ROLLS will be made available at https://github.com/CLASS-Lab/4D-ROLLS.
<div id='section'>Paperid: <span id='pid'>64, <a href='https://arxiv.org/pdf/2505.12911.pdf' target='_blank'>https://arxiv.org/pdf/2505.12911.pdf</a></span>   <span><a href='https://github.com/sapeirone/hiero' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Simone Alberto Peirone, Francesca Pistilli, Giuseppe Averta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12911">HiERO: understanding the hierarchy of human behavior enhances reasoning on egocentric videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human activities are particularly complex and variable, and this makes challenging for deep learning models to reason about them. However, we note that such variability does have an underlying structure, composed of a hierarchy of patterns of related actions. We argue that such structure can emerge naturally from unscripted videos of human activities, and can be leveraged to better reason about their content. We present HiERO, a weakly-supervised method to enrich video segments features with the corresponding hierarchical activity threads. By aligning video clips with their narrated descriptions, HiERO infers contextual, semantic and temporal reasoning with an hierarchical architecture. We prove the potential of our enriched features with multiple video-text alignment benchmarks (EgoMCQ, EgoNLQ) with minimal additional training, and in zero-shot for procedure learning tasks (EgoProceL and Ego4D Goal-Step). Notably, HiERO achieves state-of-the-art performance in all the benchmarks, and for procedure learning tasks it outperforms fully-supervised methods by a large margin (+12.5% F1 on EgoProceL) in zero shot. Our results prove the relevance of using knowledge of the hierarchy of human activities for multiple reasoning tasks in egocentric vision.
<div id='section'>Paperid: <span id='pid'>65, <a href='https://arxiv.org/pdf/2505.09263.pdf' target='_blank'>https://arxiv.org/pdf/2505.09263.pdf</a></span>   <span><a href='https://github.com/gaobb/AnoGen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guan Gui, Bin-Bin Gao, Jun Liu, Chengjie Wang, Yunsheng Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09263">Few-Shot Anomaly-Driven Generation for Anomaly Classification and Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anomaly detection is a practical and challenging task due to the scarcity of anomaly samples in industrial inspection. Some existing anomaly detection methods address this issue by synthesizing anomalies with noise or external data. However, there is always a large semantic gap between synthetic and real-world anomalies, resulting in weak performance in anomaly detection. To solve the problem, we propose a few-shot Anomaly-driven Generation (AnoGen) method, which guides the diffusion model to generate realistic and diverse anomalies with only a few real anomalies, thereby benefiting training anomaly detection models. Specifically, our work is divided into three stages. In the first stage, we learn the anomaly distribution based on a few given real anomalies and inject the learned knowledge into an embedding. In the second stage, we use the embedding and given bounding boxes to guide the diffusion model to generate realistic and diverse anomalies on specific objects (or textures). In the final stage, we propose a weakly-supervised anomaly detection method to train a more powerful model with generated anomalies. Our method builds upon DRAEM and DesTSeg as the foundation model and conducts experiments on the commonly used industrial anomaly detection dataset, MVTec. The experiments demonstrate that our generated anomalies effectively improve the model performance of both anomaly classification and segmentation tasks simultaneously, \eg, DRAEM and DseTSeg achieved a 5.8\% and 1.5\% improvement in AU-PR metric on segmentation task, respectively. The code and generated anomalous data are available at https://github.com/gaobb/AnoGen.
<div id='section'>Paperid: <span id='pid'>66, <a href='https://arxiv.org/pdf/2505.07817.pdf' target='_blank'>https://arxiv.org/pdf/2505.07817.pdf</a></span>   <span><a href='https://kahnchana.github.io/LangToMo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kanchana Ranasinghe, Xiang Li, E-Ro Nguyen, Cristina Mata, Jongwoo Park, Michael S Ryoo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07817">Pixel Motion as Universal Representation for Robot Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present LangToMo, a vision-language-action framework structured as a dual-system architecture that uses pixel motion forecasts as intermediate representations. Our high-level System 2, an image diffusion model, generates text-conditioned pixel motion sequences from a single frame to guide robot control. Pixel motion-a universal, interpretable, and motion-centric representation-can be extracted from videos in a weakly-supervised manner, enabling diffusion model training on any video-caption data. Treating generated pixel motion as learned universal representations, our low level System 1 module translates these into robot actions via motion-to-action mapping functions, which can be either hand-crafted or learned with minimal supervision. System 2 operates as a high-level policy applied at sparse temporal intervals, while System 1 acts as a low-level policy at dense temporal intervals. This hierarchical decoupling enables flexible, scalable, and generalizable robot control under both unsupervised and supervised settings, bridging the gap between language, motion, and action. Checkout https://kahnchana.github.io/LangToMo
<div id='section'>Paperid: <span id='pid'>67, <a href='https://arxiv.org/pdf/2505.03207.pdf' target='_blank'>https://arxiv.org/pdf/2505.03207.pdf</a></span>   <span><a href='https://github.com/xyt-ml/PLC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yutong Xie, Fuchao Yang, Yuheng Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03207">Partial Label Clustering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partial label learning (PLL) is a significant weakly supervised learning framework, where each training example corresponds to a set of candidate labels and only one label is the ground-truth label. For the first time, this paper investigates the partial label clustering problem, which takes advantage of the limited available partial labels to improve the clustering performance. Specifically, we first construct a weight matrix of examples based on their relationships in the feature space and disambiguate the candidate labels to estimate the ground-truth label based on the weight matrix. Then, we construct a set of must-link and cannot-link constraints based on the disambiguation results. Moreover, we propagate the initial must-link and cannot-link constraints based on an adversarial prior promoted dual-graph learning approach. Finally, we integrate weight matrix construction, label disambiguation, and pairwise constraints propagation into a joint model to achieve mutual enhancement. We also theoretically prove that a better disambiguated label matrix can help improve clustering performance. Comprehensive experiments demonstrate our method realizes superior performance when comparing with state-of-the-art constrained clustering methods, and outperforms PLL and semi-supervised PLL methods when only limited samples are annotated. The code is publicly available at https://github.com/xyt-ml/PLC.
<div id='section'>Paperid: <span id='pid'>68, <a href='https://arxiv.org/pdf/2505.02179.pdf' target='_blank'>https://arxiv.org/pdf/2505.02179.pdf</a></span>   <span><a href='https://github.com/modadundun/ProDisc-VAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Zhu, Qi Yu, Xinru Dong, Shiyu Li, Yue Liu, Jinlong Jiang, Lei Shu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02179">ProDisc-VAD: An Efficient System for Weakly-Supervised Anomaly Detection in Video Surveillance Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised video anomaly detection (WS-VAD) using Multiple Instance Learning (MIL) suffers from label ambiguity, hindering discriminative feature learning. We propose ProDisc-VAD, an efficient framework tackling this via two synergistic components. The Prototype Interaction Layer (PIL) provides controlled normality modeling using a small set of learnable prototypes, establishing a robust baseline without being overwhelmed by dominant normal data. The Pseudo-Instance Discriminative Enhancement (PIDE) loss boosts separability by applying targeted contrastive learning exclusively to the most reliable extreme-scoring instances (highest/lowest scores). ProDisc-VAD achieves strong AUCs (97.98% ShanghaiTech, 87.12% UCF-Crime) using only 0.4M parameters, over 800x fewer than recent ViT-based methods like VadCLIP. Code is available at https://github.com/modadundun/ProDisc-VAD.
<div id='section'>Paperid: <span id='pid'>69, <a href='https://arxiv.org/pdf/2504.19357.pdf' target='_blank'>https://arxiv.org/pdf/2504.19357.pdf</a></span>   <span><a href='https://github.com/diku-dk/credanno' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Lu, Chong Yin, Silvia Ingala, Kenny Erleben, Michael Bachmann Nielsen, Sune Darkner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.19357">MERA: Multimodal and Multiscale Self-Explanatory Model with Considerably Reduced Annotation for Lung Nodule Diagnosis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Lung cancer, a leading cause of cancer-related deaths globally, emphasises the importance of early detection for better patient outcomes. Pulmonary nodules, often early indicators of lung cancer, necessitate accurate, timely diagnosis. Despite Explainable Artificial Intelligence (XAI) advances, many existing systems struggle providing clear, comprehensive explanations, especially with limited labelled data. This study introduces MERA, a Multimodal and Multiscale self-Explanatory model designed for lung nodule diagnosis with considerably Reduced Annotation requirements. MERA integrates unsupervised and weakly supervised learning strategies (self-supervised learning techniques and Vision Transformer architecture for unsupervised feature extraction) and a hierarchical prediction mechanism leveraging sparse annotations via semi-supervised active learning in the learned latent space. MERA explains its decisions on multiple levels: model-level global explanations via semantic latent space clustering, instance-level case-based explanations showing similar instances, local visual explanations via attention maps, and concept explanations using critical nodule attributes. Evaluations on the public LIDC dataset show MERA's superior diagnostic accuracy and self-explainability. With only 1% annotated samples, MERA achieves diagnostic accuracy comparable to or exceeding state-of-the-art methods requiring full annotation. The model's inherent design delivers comprehensive, robust, multilevel explanations aligned closely with clinical practice, enhancing trustworthiness and transparency. Demonstrated viability of unsupervised and weakly supervised learning lowers the barrier to deploying diagnostic AI in broader medical domains. Our complete code is open-source available: https://github.com/diku-dk/credanno.
<div id='section'>Paperid: <span id='pid'>70, <a href='https://arxiv.org/pdf/2504.17379.pdf' target='_blank'>https://arxiv.org/pdf/2504.17379.pdf</a></span>   <span><a href='https://github.com/tueimage/GABMIL' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/tueimage/GABMIL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hassan Keshvarikhojasteh, Mihail Tifrea, Sibylle Hess, Josien P. W. Pluim, Mitko Veta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.17379">A Spatially-Aware Multiple Instance Learning Framework for Digital Pathology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple instance learning (MIL) is a promising approach for weakly supervised classification in pathology using whole slide images (WSIs). However, conventional MIL methods such as Attention-Based Deep Multiple Instance Learning (ABMIL) typically disregard spatial interactions among patches that are crucial to pathological diagnosis. Recent advancements, such as Transformer based MIL (TransMIL), have incorporated spatial context and inter-patch relationships. However, it remains unclear whether explicitly modeling patch relationships yields similar performance gains in ABMIL, which relies solely on Multi-Layer Perceptrons (MLPs). In contrast, TransMIL employs Transformer-based layers, introducing a fundamental architectural shift at the cost of substantially increased computational complexity. In this work, we enhance the ABMIL framework by integrating interaction-aware representations to address this question. Our proposed model, Global ABMIL (GABMIL), explicitly captures inter-instance dependencies while preserving computational efficiency. Experimental results on two publicly available datasets for tumor subtyping in breast and lung cancers demonstrate that GABMIL achieves up to a 7 percentage point improvement in AUPRC and a 5 percentage point increase in the Kappa score over ABMIL, with minimal or no additional computational overhead. These findings underscore the importance of incorporating patch interactions within MIL frameworks. Our code is available at \href{https://github.com/tueimage/GABMIL}{\texttt{GABMIL}}.
<div id='section'>Paperid: <span id='pid'>71, <a href='https://arxiv.org/pdf/2504.14783.pdf' target='_blank'>https://arxiv.org/pdf/2504.14783.pdf</a></span>   <span><a href='https://github.com/ChongQingNoSubway/MILDropout' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhui Zhu, Peijie Qiu, Xiwen Chen, Zhangsihao Yang, Aristeidis Sotiras, Abolfazl Razi, Yalin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14783">How Effective Can Dropout Be in Multiple Instance Learning ?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple Instance Learning (MIL) is a popular weakly-supervised method for various applications, with a particular interest in histological whole slide image (WSI) classification. Due to the gigapixel resolution of WSI, applications of MIL in WSI typically necessitate a two-stage training scheme: first, extract features from the pre-trained backbone and then perform MIL aggregation. However, it is well-known that this suboptimal training scheme suffers from "noisy" feature embeddings from the backbone and inherent weak supervision, hindering MIL from learning rich and generalizable features. However, the most commonly used technique (i.e., dropout) for mitigating this issue has yet to be explored in MIL. In this paper, we empirically explore how effective the dropout can be in MIL. Interestingly, we observe that dropping the top-k most important instances within a bag leads to better performance and generalization even under noise attack. Based on this key observation, we propose a novel MIL-specific dropout method, termed MIL-Dropout, which systematically determines which instances to drop. Experiments on five MIL benchmark datasets and two WSI datasets demonstrate that MIL-Dropout boosts the performance of current MIL methods with a negligible computational cost. The code is available at https://github.com/ChongQingNoSubway/MILDropout.
<div id='section'>Paperid: <span id='pid'>72, <a href='https://arxiv.org/pdf/2504.11368.pdf' target='_blank'>https://arxiv.org/pdf/2504.11368.pdf</a></span>   <span><a href='https://github.com/jingkunchen/FGI.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingkun Chen, Haoran Duan, Xiao Zhang, Boyan Gao, Tao Tan, Vicente Grau, Jungong Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11368">From Gaze to Insight: Bridging Human Visual Attention and Vision Language Model Explanation for Weakly-Supervised Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical image segmentation remains challenging due to the high cost of pixel-level annotations for training. In the context of weak supervision, clinician gaze data captures regions of diagnostic interest; however, its sparsity limits its use for segmentation. In contrast, vision-language models (VLMs) provide semantic context through textual descriptions but lack the explanation precision required. Recognizing that neither source alone suffices, we propose a teacher-student framework that integrates both gaze and language supervision, leveraging their complementary strengths. Our key insight is that gaze data indicates where clinicians focus during diagnosis, while VLMs explain why those regions are significant. To implement this, the teacher model first learns from gaze points enhanced by VLM-generated descriptions of lesion morphology, establishing a foundation for guiding the student model. The teacher then directs the student through three strategies: (1) Multi-scale feature alignment to fuse visual cues with textual semantics; (2) Confidence-weighted consistency constraints to focus on reliable predictions; (3) Adaptive masking to limit error propagation in uncertain areas. Experiments on the Kvasir-SEG, NCI-ISBI, and ISIC datasets show that our method achieves Dice scores of 80.78%, 80.53%, and 84.22%, respectively-improving 3-5% over gaze baselines without increasing the annotation burden. By preserving correlations among predictions, gaze data, and lesion descriptions, our framework also maintains clinical interpretability. This work illustrates how integrating human visual attention with AI-generated semantic context can effectively overcome the limitations of individual weak supervision signals, thereby advancing the development of deployable, annotation-efficient medical AI systems. Code is available at: https://github.com/jingkunchen/FGI.git.
<div id='section'>Paperid: <span id='pid'>73, <a href='https://arxiv.org/pdf/2504.11014.pdf' target='_blank'>https://arxiv.org/pdf/2504.11014.pdf</a></span>   <span><a href='https://ies0411.github.io/GATE3D/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Eunsoo Im, Changhyun Jee, Jung Kwon Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11014">GATE3D: Generalized Attention-based Task-synergized Estimation in 3D*</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emerging trend in computer vision emphasizes developing universal models capable of simultaneously addressing multiple diverse tasks. Such universality typically requires joint training across multi-domain datasets to ensure effective generalization. However, monocular 3D object detection presents unique challenges in multi-domain training due to the scarcity of datasets annotated with accurate 3D ground-truth labels, especially beyond typical road-based autonomous driving contexts. To address this challenge, we introduce a novel weakly supervised framework leveraging pseudo-labels. Current pretrained models often struggle to accurately detect pedestrians in non-road environments due to inherent dataset biases. Unlike generalized image-based 2D object detection models, achieving similar generalization in monocular 3D detection remains largely unexplored. In this paper, we propose GATE3D, a novel framework designed specifically for generalized monocular 3D object detection via weak supervision. GATE3D effectively bridges domain gaps by employing consistency losses between 2D and 3D predictions. Remarkably, our model achieves competitive performance on the KITTI benchmark as well as on an indoor-office dataset collected by us to evaluate the generalization capabilities of our framework. Our results demonstrate that GATE3D significantly accelerates learning from limited annotated data through effective pre-training strategies, highlighting substantial potential for broader impacts in robotics, augmented reality, and virtual reality applications. Project page: https://ies0411.github.io/GATE3D/
<div id='section'>Paperid: <span id='pid'>74, <a href='https://arxiv.org/pdf/2504.09881.pdf' target='_blank'>https://arxiv.org/pdf/2504.09881.pdf</a></span>   <span><a href='https://github.com/chenshunpeng/FoL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Changwei Wang, Shunpeng Chen, Yukun Song, Rongtao Xu, Zherui Zhang, Jiguang Zhang, Haoran Yang, Yu Zhang, Kexue Fu, Shide Du, Zhiwei Xu, Longxiang Gao, Li Guo, Shibiao Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09881">Focus on Local: Finding Reliable Discriminative Regions for Visual Place Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Place Recognition (VPR) is aimed at predicting the location of a query image by referencing a database of geotagged images. For VPR task, often fewer discriminative local regions in an image produce important effects while mundane background regions do not contribute or even cause perceptual aliasing because of easy overlap. However, existing methods lack precisely modeling and full exploitation of these discriminative regions. In this paper, we propose the Focus on Local (FoL) approach to stimulate the performance of image retrieval and re-ranking in VPR simultaneously by mining and exploiting reliable discriminative local regions in images and introducing pseudo-correlation supervision. First, we design two losses, Extraction-Aggregation Spatial Alignment Loss (SAL) and Foreground-Background Contrast Enhancement Loss (CEL), to explicitly model reliable discriminative local regions and use them to guide the generation of global representations and efficient re-ranking. Second, we introduce a weakly-supervised local feature training strategy based on pseudo-correspondences obtained from aggregating global features to alleviate the lack of local correspondences ground truth for the VPR task. Third, we suggest an efficient re-ranking pipeline that is efficiently and precisely based on discriminative region guidance. Finally, experimental results show that our FoL achieves the state-of-the-art on multiple VPR benchmarks in both image retrieval and re-ranking stages and also significantly outperforms existing two-stage VPR methods in terms of computational efficiency. Code and models are available at https://github.com/chenshunpeng/FoL
<div id='section'>Paperid: <span id='pid'>75, <a href='https://arxiv.org/pdf/2504.03096.pdf' target='_blank'>https://arxiv.org/pdf/2504.03096.pdf</a></span>   <span><a href='https://siatheindochinese.github.io/sia_act_page/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhen Hao Sia, Yogesh Singh Rawat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03096">Scaling Open-Vocabulary Action Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we focus on scaling open-vocabulary action detection. Existing approaches for action detection are predominantly limited to closed-set scenarios and rely on complex, parameter-heavy architectures. Extending these models to the open-vocabulary setting poses two key challenges: (1) the lack of large-scale datasets with many action classes for robust training, and (2) parameter-heavy adaptations to a pretrained vision-language contrastive model to convert it for detection, risking overfitting the additional non-pretrained parameters to base action classes. Firstly, we introduce an encoder-only multimodal model for video action detection, reducing the reliance on parameter-heavy additions for video action detection. Secondly, we introduce a simple weakly supervised training strategy to exploit an existing closed-set action detection dataset for pretraining. Finally, we depart from the ill-posed base-to-novel benchmark used by prior works in open-vocabulary action detection and devise a new benchmark to evaluate on existing closed-set action detection datasets without ever using them for training, showing novel results to serve as baselines for future work. Our code is available at https://siatheindochinese.github.io/sia_act_page/ .
<div id='section'>Paperid: <span id='pid'>76, <a href='https://arxiv.org/pdf/2503.11439.pdf' target='_blank'>https://arxiv.org/pdf/2503.11439.pdf</a></span>   <span><a href='https://github.com/shjo-april/COIN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sanghyun Jo, Seo Jin Lee, Seungwoo Lee, Seohyung Hong, Hyungseok Seo, Kyungsu Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11439">COIN: Confidence Score-Guided Distillation for Annotation-Free Cell Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cell instance segmentation (CIS) is crucial for identifying individual cell morphologies in histopathological images, providing valuable insights for biological and medical research. While unsupervised CIS (UCIS) models aim to reduce the heavy reliance on labor-intensive image annotations, they fail to accurately capture cell boundaries, causing missed detections and poor performance. Recognizing the absence of error-free instances as a key limitation, we present COIN (COnfidence score-guided INstance distillation), a novel annotation-free framework with three key steps: (1) Increasing the sensitivity for the presence of error-free instances via unsupervised semantic segmentation with optimal transport, leveraging its ability to discriminate spatially minor instances, (2) Instance-level confidence scoring to measure the consistency between model prediction and refined mask and identify highly confident instances, offering an alternative to ground truth annotations, and (3) Progressive expansion of confidence with recursive self-distillation. Extensive experiments across six datasets show COIN outperforming existing UCIS methods, even surpassing semi- and weakly-supervised approaches across all metrics on the MoNuSeg and TNBC datasets. The code is available at https://github.com/shjo-april/COIN.
<div id='section'>Paperid: <span id='pid'>77, <a href='https://arxiv.org/pdf/2503.11032.pdf' target='_blank'>https://arxiv.org/pdf/2503.11032.pdf</a></span>   <span><a href='https://github.com/zhang-lilin/WSCAT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lilin Zhang, Chengpei Wu, Ning Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11032">Weakly Supervised Contrastive Adversarial Training for Learning Robust Features from Semi-supervised Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing adversarial training (AT) methods often suffer from incomplete perturbation, meaning that not all non-robust features are perturbed when generating adversarial examples (AEs). This results in residual correlations between non-robust features and labels, leading to suboptimal learning of robust features. However, achieving complete perturbation, i.e., perturbing as many non-robust features as possible, is challenging due to the difficulty in distinguishing robust and non-robust features and the sparsity of labeled data. To address these challenges, we propose a novel approach called Weakly Supervised Contrastive Adversarial Training (WSCAT). WSCAT ensures complete perturbation for improved learning of robust features by disrupting correlations between non-robust features and labels through complete AE generation over partially labeled data, grounded in information theory. Extensive theoretical analysis and comprehensive experiments on widely adopted benchmarks validate the superiority of WSCAT. Our code is available at https://github.com/zhang-lilin/WSCAT.
<div id='section'>Paperid: <span id='pid'>78, <a href='https://arxiv.org/pdf/2503.07982.pdf' target='_blank'>https://arxiv.org/pdf/2503.07982.pdf</a></span>   <span><a href='https://github.com/shjo-april/DiffEGG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sanghyun Jo, Ziseok Lee, Wooyeol Lee, Kyungsu Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.07982">DiffEGG: Diffusion-Driven Edge Generation as a Pixel-Annotation-Free Alternative for Instance Annotation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving precise panoptic segmentation relies on pixel-wise instance annotations, but obtaining such datasets is costly. Unsupervised instance segmentation (UIS) eliminates annotation requirements but struggles with adjacent instance merging and single-instance fragmentation, largely due to the limitations of DINO-based backbones which lack strong instance separation cues. Weakly-supervised panoptic segmentation (WPS) reduces annotation costs using sparse labels (e.g., points, boxes), yet these annotations remain expensive and introduce human bias and boundary errors. To address these challenges, we propose DiffEGG (Diffusion-Driven EdGe Generation), a fully annotation-free method that extracts instance-aware features from pretrained diffusion models to generate precise instance edge maps. Unlike DINO-based UIS methods, diffusion models inherently capture fine-grained, instance-aware features, enabling more precise boundary delineation. For WPS, DiffEGG eliminates annotation costs and human bias by operating without any form of manual supervision, addressing the key limitations of prior best methods. Additionally, we introduce RIP, a post-processing technique that fuses DiffEGG's edge maps with segmentation masks in a task-agnostic manner. RIP allows DiffEGG to be seamlessly integrated into various segmentation frameworks. When applied to UIS, DiffEGG and RIP achieve an average $+4.4\text{ AP}$ improvement over prior best UIS methods. When combined with weakly-supervised semantic segmentation (WSS), DiffEGG enables WPS without instance annotations, outperforming prior best point-supervised WPS methods by $+1.7\text{ PQ}$. These results demonstrate that DiffEGG's edge maps serve as a cost-effective, annotation-free alternative to instance annotations, significantly improving segmentation without human intervention. Code is available at https://github.com/shjo-april/DiffEGG.
<div id='section'>Paperid: <span id='pid'>79, <a href='https://arxiv.org/pdf/2503.07635.pdf' target='_blank'>https://arxiv.org/pdf/2503.07635.pdf</a></span>   <span><a href='https://github.com/WissingChen/CRA-GQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weixing Chen, Yang Liu, Binglin Chen, Jiandong Su, Yongsen Zheng, Liang Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.07635">Cross-modal Causal Relation Alignment for Video Question Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video question grounding (VideoQG) requires models to answer the questions and simultaneously infer the relevant video segments to support the answers. However, existing VideoQG methods usually suffer from spurious cross-modal correlations, leading to a failure to identify the dominant visual scenes that align with the intended question. Moreover, vision-language models exhibit unfaithful generalization performance and lack robustness on challenging downstream tasks such as VideoQG. In this work, we propose a novel VideoQG framework named Cross-modal Causal Relation Alignment (CRA), to eliminate spurious correlations and improve the causal consistency between question-answering and video temporal grounding. Our CRA involves three essential components: i) Gaussian Smoothing Grounding (GSG) module for estimating the time interval via cross-modal attention, which is de-noised by an adaptive Gaussian filter, ii) Cross-Modal Alignment (CMA) enhances the performance of weakly supervised VideoQG by leveraging bidirectional contrastive learning between estimated video segments and QA features, iii) Explicit Causal Intervention (ECI) module for multimodal deconfounding, which involves front-door intervention for vision and back-door intervention for language. Extensive experiments on two VideoQG datasets demonstrate the superiority of our CRA in discovering visually grounded content and achieving robust question reasoning. Codes are available at https://github.com/WissingChen/CRA-GQA.
<div id='section'>Paperid: <span id='pid'>80, <a href='https://arxiv.org/pdf/2503.04106.pdf' target='_blank'>https://arxiv.org/pdf/2503.04106.pdf</a></span>   <span><a href='https://github.com/wanghr64/WeakMedSAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Wang, Lian Huai, Wenbin Li, Lei Qi, Xingqun Jiang, Yinghuan Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04106">WeakMedSAM: Weakly-Supervised Medical Image Segmentation via SAM with Sub-Class Exploration and Prompt Affinity Mining</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We have witnessed remarkable progress in foundation models in vision tasks. Currently, several recent works have utilized the segmenting anything model (SAM) to boost the segmentation performance in medical images, where most of them focus on training an adaptor for fine-tuning a large amount of pixel-wise annotated medical images following a fully supervised manner. In this paper, to reduce the labeling cost, we investigate a novel weakly-supervised SAM-based segmentation model, namely WeakMedSAM. Specifically, our proposed WeakMedSAM contains two modules: 1) to mitigate severe co-occurrence in medical images, a sub-class exploration module is introduced to learn accurate feature representations. 2) to improve the quality of the class activation maps, our prompt affinity mining module utilizes the prompt capability of SAM to obtain an affinity map for random-walk refinement. Our method can be applied to any SAM-like backbone, and we conduct experiments with SAMUS and EfficientSAM. The experimental results on three popularly-used benchmark datasets, i.e., BraTS 2019, AbdomenCT-1K, and MSD Cardiac dataset, show the promising results of our proposed WeakMedSAM. Our code is available at https://github.com/wanghr64/WeakMedSAM.
<div id='section'>Paperid: <span id='pid'>81, <a href='https://arxiv.org/pdf/2503.03562.pdf' target='_blank'>https://arxiv.org/pdf/2503.03562.pdf</a></span>   <span><a href='https://guyao2023.github.io/Phys-AD/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenqiao Li, Yao Gu, Xintao Chen, Xiaohao Xu, Ming Hu, Xiaonan Huang, Yingna Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.03562">Towards Visual Discrimination and Reasoning of Real-World Physical Dynamics: Physics-Grounded Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans detect real-world object anomalies by perceiving, interacting, and reasoning based on object-conditioned physical knowledge. The long-term goal of Industrial Anomaly Detection (IAD) is to enable machines to autonomously replicate this skill. However, current IAD algorithms are largely developed and tested on static, semantically simple datasets, which diverge from real-world scenarios where physical understanding and reasoning are essential. To bridge this gap, we introduce the Physics Anomaly Detection (Phys-AD) dataset, the first large-scale, real-world, physics-grounded video dataset for industrial anomaly detection. Collected using a real robot arm and motor, Phys-AD provides a diverse set of dynamic, semantically rich scenarios. The dataset includes more than 6400 videos across 22 real-world object categories, interacting with robot arms and motors, and exhibits 47 types of anomalies. Anomaly detection in Phys-AD requires visual reasoning, combining both physical knowledge and video content to determine object abnormality. We benchmark state-of-the-art anomaly detection methods under three settings: unsupervised AD, weakly-supervised AD, and video-understanding AD, highlighting their limitations in handling physics-grounded anomalies. Additionally, we introduce the Physics Anomaly Explanation (PAEval) metric, designed to assess the ability of visual-language foundation models to not only detect anomalies but also provide accurate explanations for their underlying physical causes. Our project is available at https://guyao2023.github.io/Phys-AD/.
<div id='section'>Paperid: <span id='pid'>82, <a href='https://arxiv.org/pdf/2502.21109.pdf' target='_blank'>https://arxiv.org/pdf/2502.21109.pdf</a></span>   <span><a href='https://github.com/DIAGNijmegen/tumor-percentage-mil-regression' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Marina D'Amato, Jeroen van der Laak, Francesco Ciompi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.21109">"No negatives needed": weakly-supervised regression for interpretable tumor detection in whole-slide histopathology images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate tumor detection in digital pathology whole-slide images (WSIs) is crucial for cancer diagnosis and treatment planning. Multiple Instance Learning (MIL) has emerged as a widely used approach for weakly-supervised tumor detection with large-scale data without the need for manual annotations. However, traditional MIL methods often depend on classification tasks that require tumor-free cases as negative examples, which are challenging to obtain in real-world clinical workflows, especially for surgical resection specimens. We address this limitation by reformulating tumor detection as a regression task, estimating tumor percentages from WSIs, a clinically available target across multiple cancer types. In this paper, we provide an analysis of the proposed weakly-supervised regression framework by applying it to multiple organs, specimen types and clinical scenarios. We characterize the robustness of our framework to tumor percentage as a noisy regression target, and introduce a novel concept of amplification technique to improve tumor detection sensitivity when learning from small tumor regions. Finally, we provide interpretable insights into the model's predictions by analyzing visual attention and logit maps. Our code is available at https://github.com/DIAGNijmegen/tumor-percentage-mil-regression.
<div id='section'>Paperid: <span id='pid'>83, <a href='https://arxiv.org/pdf/2502.20838.pdf' target='_blank'>https://arxiv.org/pdf/2502.20838.pdf</a></span>   <span><a href='https://github.com/Ragib-Amin-Nihal/DSMIL-Loc' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ragib Amin Nihal, Benjamin Yen, Runwu Shi, Kazuhiro Nakadai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20838">Weakly Supervised Multiple Instance Learning for Whale Call Detection and Temporal Localization in Long-Duration Passive Acoustic Monitoring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Marine ecosystem monitoring via Passive Acoustic Monitoring (PAM) generates vast data, but deep learning often requires precise annotations and short segments. We introduce DSMIL-LocNet, a Multiple Instance Learning framework for whale call detection and localization using only bag-level labels. Our dual-stream model processes 2-30 minute audio segments, leveraging spectral and temporal features with attention-based instance selection. Tests on Antarctic whale data show longer contexts improve classification (F1: 0.8-0.9) while medium instances ensure localization precision (0.65-0.70). This suggests MIL can enhance scalable marine monitoring. Code: https://github.com/Ragib-Amin-Nihal/DSMIL-Loc
<div id='section'>Paperid: <span id='pid'>84, <a href='https://arxiv.org/pdf/2502.19707.pdf' target='_blank'>https://arxiv.org/pdf/2502.19707.pdf</a></span>   <span><a href='https://github.com/bluehenglee/MLI-MSC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianning Chi, Zelan Li, Geng Lin, MingYang Sun, Xiaosheng Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19707">Weakly Supervised Segmentation Framework for Thyroid Nodule Based on High-confidence Labels and High-rationality Losses</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised segmentation methods can delineate thyroid nodules in ultrasound images efficiently using training data with coarse labels, but suffer from: 1) low-confidence pseudo-labels that follow topological priors, introducing significant label noise, and 2) low-rationality loss functions that rigidly compare segmentation with labels, ignoring discriminative information for nodules with diverse and complex shapes. To solve these issues, we clarify the objective and references for weakly supervised ultrasound image segmentation, presenting a framework with high-confidence pseudo-labels to represent topological and anatomical information and high-rationality losses to capture multi-level discriminative features. Specifically, we fuse geometric transformations of four-point annotations and MedSAM model results prompted by specific annotations to generate high-confidence box, foreground, and background labels. Our high-rationality learning strategy includes: 1) Alignment loss measuring spatial consistency between segmentation and box label, and topological continuity within the foreground label, guiding the network to perceive nodule location; 2) Contrastive loss pulling features from labeled foreground regions while pushing features from labeled foreground and background regions, guiding the network to learn nodule and background feature distribution; 3) Prototype correlation loss measuring consistency between correlation maps derived by comparing features with foreground and background prototypes, refining uncertain regions to accurate nodule edges. Experimental results show that our method achieves state-of-the-art performance on the TN3K and DDTI datasets. The code is available at https://github.com/bluehenglee/MLI-MSC.
<div id='section'>Paperid: <span id='pid'>85, <a href='https://arxiv.org/pdf/2502.15885.pdf' target='_blank'>https://arxiv.org/pdf/2502.15885.pdf</a></span>   <span><a href='https://github.com/AIGeeksGroup/DOEI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongjie Zhu, Zeyu Zhang, Guansong Pang, Xu Wang, Shimin Wen, Yu Bai, Daji Ergu, Ying Cai, Yang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.15885">DOEI: Dual Optimization of Embedding Information for Attention-Enhanced Class Activation Maps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation (WSSS) typically utilizes limited semantic annotations to obtain initial Class Activation Maps (CAMs). However, due to the inadequate coupling between class activation responses and semantic information in high-dimensional space, the CAM is prone to object co-occurrence or under-activation, resulting in inferior recognition accuracy. To tackle this issue, we propose DOEI, Dual Optimization of Embedding Information, a novel approach that reconstructs embedding representations through semantic-aware attention weight matrices to optimize the expression capability of embedding information. Specifically, DOEI amplifies tokens with high confidence and suppresses those with low confidence during the class-to-patch interaction. This alignment of activation responses with semantic information strengthens the propagation and decoupling of target features, enabling the generated embeddings to more accurately represent target features in high-level semantic space. In addition, we propose a hybrid-feature alignment module in DOEI that combines RGB values, embedding-guided features, and self-attention weights to increase the reliability of candidate tokens. Comprehensive experiments show that DOEI is an effective plug-and-play module that empowers state-of-the-art visual transformer-based WSSS models to significantly improve the quality of CAMs and segmentation performance on popular benchmarks, including PASCAL VOC (+3.6%, +1.5%, +1.2% mIoU) and MS COCO (+1.2%, +1.6% mIoU). Code will be available at https://github.com/AIGeeksGroup/DOEI.
<div id='section'>Paperid: <span id='pid'>86, <a href='https://arxiv.org/pdf/2502.15152.pdf' target='_blank'>https://arxiv.org/pdf/2502.15152.pdf</a></span>   <span><a href='https://github.com/psychofict/CW-BASS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ebenezer Tarubinga, Jenifer Kalafatovich, Seong-Whan Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.15152">CW-BASS: Confidence-Weighted Boundary-Aware Learning for Semi-Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semi-supervised semantic segmentation (SSSS) aims to improve segmentation performance by utilizing large amounts of unlabeled data with limited labeled samples. Existing methods often suffer from coupling, where over-reliance on initial labeled data leads to suboptimal learning; confirmation bias, where incorrect predictions reinforce themselves repeatedly; and boundary blur caused by limited boundary-awareness and ambiguous edge cues. To address these issues, we propose CW-BASS, a novel framework for SSSS. In order to mitigate the impact of incorrect predictions, we assign confidence weights to pseudo-labels. Additionally, we leverage boundary-delineation techniques, which, despite being extensively explored in weakly-supervised semantic segmentation (WSSS), remain underutilized in SSSS. Specifically, our method: (1) reduces coupling via a confidence-weighted loss that adjusts pseudo-label influence based on their predicted confidence scores, (2) mitigates confirmation bias with a dynamic thresholding mechanism that learns to filter out pseudo-labels based on model performance, (3) tackles boundary blur using a boundary-aware module to refine segmentation near object edges, and (4) reduces label noise through a confidence decay strategy that progressively refines pseudo-labels during training. Extensive experiments on Pascal VOC 2012 and Cityscapes demonstrate that CW-BASS achieves state-of-the-art performance. Notably, CW-BASS achieves a 65.9% mIoU on Cityscapes under a challenging and underexplored 1/30 (3.3%) split (100 images), highlighting its effectiveness in limited-label settings. Our code is available at https://github.com/psychofict/CW-BASS.
<div id='section'>Paperid: <span id='pid'>87, <a href='https://arxiv.org/pdf/2502.10294.pdf' target='_blank'>https://arxiv.org/pdf/2502.10294.pdf</a></span>   <span><a href='https://github.com/anpc849/QMaxViT-Unet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Thien B. Nguyen-Tat, Hoang-An Vo, Phuoc-Sang Dang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10294">QMaxViT-Unet+: A Query-Based MaxViT-Unet with Edge Enhancement for Scribble-Supervised Segmentation of Medical Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The deployment of advanced deep learning models for medical image segmentation is often constrained by the requirement for extensively annotated datasets. Weakly-supervised learning, which allows less precise labels, has become a promising solution to this challenge. Building on this approach, we propose QMaxViT-Unet+, a novel framework for scribble-supervised medical image segmentation. This framework is built on the U-Net architecture, with the encoder and decoder replaced by Multi-Axis Vision Transformer (MaxViT) blocks. These blocks enhance the model's ability to learn local and global features efficiently. Additionally, our approach integrates a query-based Transformer decoder to refine features and an edge enhancement module to compensate for the limited boundary information in the scribble label. We evaluate the proposed QMaxViT-Unet+ on four public datasets focused on cardiac structures, colorectal polyps, and breast cancer: ACDC, MS-CMRSeg, SUN-SEG, and BUSI. Evaluation metrics include the Dice similarity coefficient (DSC) and the 95th percentile of Hausdorff distance (HD95). Experimental results show that QMaxViT-Unet+ achieves 89.1\% DSC and 1.316mm HD95 on ACDC, 88.4\% DSC and 2.226mm HD95 on MS-CMRSeg, 71.4\% DSC and 4.996mm HD95 on SUN-SEG, and 69.4\% DSC and 50.122mm HD95 on BUSI. These results demonstrate that our method outperforms existing approaches in terms of accuracy, robustness, and efficiency while remaining competitive with fully-supervised learning approaches. This makes it ideal for medical image analysis, where high-quality annotations are often scarce and require significant effort and expense. The code is available at: https://github.com/anpc849/QMaxViT-Unet
<div id='section'>Paperid: <span id='pid'>88, <a href='https://arxiv.org/pdf/2502.10263.pdf' target='_blank'>https://arxiv.org/pdf/2502.10263.pdf</a></span>   <span><a href='https://github.com/worldbank/ai4data-use' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aivin V. Solatorio, Rafael Macalaba, James Liounis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10263">Large Language Models and Synthetic Data for Monitoring Dataset Mentions in Research Papers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking how data is mentioned and used in research papers provides critical insights for improving data discoverability, quality, and production. However, manually identifying and classifying dataset mentions across vast academic literature is resource-intensive and not scalable. This paper presents a machine learning framework that automates dataset mention detection across research domains by leveraging large language models (LLMs), synthetic data, and a two-stage fine-tuning process. We employ zero-shot extraction from research papers, an LLM-as-a-Judge for quality assessment, and a reasoning agent for refinement to generate a weakly supervised synthetic dataset. The Phi-3.5-mini instruct model is pre-fine-tuned on this dataset, followed by fine-tuning on a manually annotated subset. At inference, a ModernBERT-based classifier efficiently filters dataset mentions, reducing computational overhead while maintaining high recall. Evaluated on a held-out manually annotated sample, our fine-tuned model outperforms NuExtract-v1.5 and GLiNER-large-v2.1 in dataset extraction accuracy. Our results highlight how LLM-generated synthetic data can effectively address training data scarcity, improving generalization in low-resource settings. This framework offers a pathway toward scalable monitoring of dataset usage, enhancing transparency, and supporting researchers, funders, and policymakers in identifying data gaps and strengthening data accessibility for informed decision-making.
<div id='section'>Paperid: <span id='pid'>89, <a href='https://arxiv.org/pdf/2502.09874.pdf' target='_blank'>https://arxiv.org/pdf/2502.09874.pdf</a></span>   <span><a href='https://github.com/LQY404/FrGNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Ling, Wenxiao Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09874">FrGNet: A fourier-guided weakly-supervised framework for nuclear instance segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nuclear instance segmentation has played a critical role in pathology image analysis. The main challenges arise from the difficulty in accurately segmenting instances and the high cost of precise mask-level annotations for fully-supervised training.In this work, we propose a fourier guidance framework for solving the weakly-supervised nuclear instance segmentation problem. In this framework, we construct a fourier guidance module to fuse the priori information into the training process of the model, which facilitates the model to capture the relevant features of the nuclear. Meanwhile, in order to further improve the model's ability to represent the features of nuclear, we propose the guide-based instance level contrastive module. This module makes full use of the framework's own properties and guide information to effectively enhance the representation features of nuclear. We show on two public datasets that our model can outperform current SOTA methods under fully-supervised design, and in weakly-supervised experiments, with only a small amount of labeling our model still maintains close to the performance under full supervision.In addition, we also perform generalization experiments on a private dataset, and without any labeling, our model is able to segment nuclear images that have not been seen during training quite effectively. As open science, all codes and pre-trained models are available at https://github.com/LQY404/FrGNet.
<div id='section'>Paperid: <span id='pid'>90, <a href='https://arxiv.org/pdf/2502.09471.pdf' target='_blank'>https://arxiv.org/pdf/2502.09471.pdf</a></span>   <span><a href='https://github.com/VisionXLab/whollywood' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/VisionXLab/whollywood-jittor' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Yu, Xue Yang, Yansheng Li, Zhenjun Han, Feipeng Da, Junchi Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09471">Wholly-WOOD: Wholly Leveraging Diversified-quality Labels for Weakly-supervised Oriented Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately estimating the orientation of visual objects with compact rotated bounding boxes (RBoxes) has become a prominent demand, which challenges existing object detection paradigms that only use horizontal bounding boxes (HBoxes). To equip the detectors with orientation awareness, supervised regression/classification modules have been introduced at the high cost of rotation annotation. Meanwhile, some existing datasets with oriented objects are already annotated with horizontal boxes or even single points. It becomes attractive yet remains open for effectively utilizing weaker single point and horizontal annotations to train an oriented object detector (OOD). We develop Wholly-WOOD, a weakly-supervised OOD framework, capable of wholly leveraging various labeling forms (Points, HBoxes, RBoxes, and their combination) in a unified fashion. By only using HBox for training, our Wholly-WOOD achieves performance very close to that of the RBox-trained counterpart on remote sensing and other areas, significantly reducing the tedious efforts on labor-intensive annotation for oriented objects. The source codes are available at https://github.com/VisionXLab/whollywood (PyTorch-based) and https://github.com/VisionXLab/whollywood-jittor (Jittor-based).
<div id='section'>Paperid: <span id='pid'>91, <a href='https://arxiv.org/pdf/2502.04268.pdf' target='_blank'>https://arxiv.org/pdf/2502.04268.pdf</a></span>   <span><a href='https://github.com/VisionXLab/point2rbox-v2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Yu, Botao Ren, Peiyuan Zhang, Mingxin Liu, Junwei Luo, Shaofeng Zhang, Feipeng Da, Junchi Yan, Xue Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04268">Point2RBox-v2: Rethinking Point-supervised Oriented Object Detection with Spatial Layout Among Instances</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapidly increasing demand for oriented object detection (OOD), recent research involving weakly-supervised detectors for learning OOD from point annotations has gained great attention. In this paper, we rethink this challenging task setting with the layout among instances and present Point2RBox-v2. At the core are three principles: 1) Gaussian overlap loss. It learns an upper bound for each instance by treating objects as 2D Gaussian distributions and minimizing their overlap. 2) Voronoi watershed loss. It learns a lower bound for each instance through watershed on Voronoi tessellation. 3) Consistency loss. It learns the size/rotation variation between two output sets with respect to an input image and its augmented view. Supplemented by a few devised techniques, e.g. edge loss and copy-paste, the detector is further enhanced. To our best knowledge, Point2RBox-v2 is the first approach to explore the spatial layout among instances for learning point-supervised OOD. Our solution is elegant and lightweight, yet it is expected to give a competitive performance especially in densely packed scenes: 62.61%/86.15%/34.71% on DOTA/HRSC/FAIR1M. Code is available at https://github.com/VisionXLab/point2rbox-v2.
<div id='section'>Paperid: <span id='pid'>92, <a href='https://arxiv.org/pdf/2502.03118.pdf' target='_blank'>https://arxiv.org/pdf/2502.03118.pdf</a></span>   <span><a href='https://github.com/yanwenCi/Tell2Reg.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wen Yan, Qianye Yang, Shiqi Huang, Yipei Wang, Shonit Punwani, Mark Emberton, Vasilis Stavrinides, Yipeng Hu, Dean Barratt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03118">Tell2Reg: Establishing spatial correspondence between images by the same language prompts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spatial correspondence can be represented by pairs of segmented regions, such that the image registration networks aim to segment corresponding regions rather than predicting displacement fields or transformation parameters. In this work, we show that such a corresponding region pair can be predicted by the same language prompt on two different images using the pre-trained large multimodal models based on GroundingDINO and SAM. This enables a fully automated and training-free registration algorithm, potentially generalisable to a wide range of image registration tasks. In this paper, we present experimental results using one of the challenging tasks, registering inter-subject prostate MR images, which involves both highly variable intensity and morphology between patients. Tell2Reg is training-free, eliminating the need for costly and time-consuming data curation and labelling that was previously required for this registration task. This approach outperforms unsupervised learning-based registration methods tested, and has a performance comparable to weakly-supervised methods. Additional qualitative results are also presented to suggest that, for the first time, there is a potential correlation between language semantics and spatial correspondence, including the spatial invariance in language-prompted regions and the difference in language prompts between the obtained local and global correspondences. Code is available at https://github.com/yanwenCi/Tell2Reg.git.
<div id='section'>Paperid: <span id='pid'>93, <a href='https://arxiv.org/pdf/2502.00240.pdf' target='_blank'>https://arxiv.org/pdf/2502.00240.pdf</a></span>   <span><a href='https://github.com/YasminZhang/ADCR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yasi Zhang, Oscar Leong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.00240">Learning Difference-of-Convex Regularizers for Inverse Problems: A Flexible Framework with Theoretical Guarantees</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning effective regularization is crucial for solving ill-posed inverse problems, which arise in a wide range of scientific and engineering applications. While data-driven methods that parameterize regularizers using deep neural networks have demonstrated strong empirical performance, they often result in highly nonconvex formulations that lack theoretical guarantees. Recent work has shown that incorporating structured nonconvexity into neural network-based regularizers, such as weak convexity, can strike a balance between empirical performance and theoretical tractability. In this paper, we demonstrate that a broader class of nonconvex functions, difference-of-convex (DC) functions, can yield improved empirical performance while retaining strong convergence guarantees. The DC structure enables the use of well-established optimization algorithms, such as the Difference-of-Convex Algorithm (DCA) and a Proximal Subgradient Method (PSM), which extend beyond standard gradient descent. Furthermore, we provide theoretical insights into the conditions under which optimal regularizers can be expressed as DC functions. Extensive experiments on computed tomography (CT) reconstruction tasks show that our approach achieves strong performance across sparse and limited-view settings, consistently outperforming other weakly supervised learned regularizers. Our code is available at \url{https://github.com/YasminZhang/ADCR}.
<div id='section'>Paperid: <span id='pid'>94, <a href='https://arxiv.org/pdf/2501.17053.pdf' target='_blank'>https://arxiv.org/pdf/2501.17053.pdf</a></span>   <span><a href='https://akash2907.github.io/cospal_webpage' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Akash Kumar, Zsolt Kira, Yogesh Singh Rawat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.17053">Contextual Self-paced Learning for Weakly Supervised Spatio-Temporal Video Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we focus on Weakly Supervised Spatio-Temporal Video Grounding (WSTVG). It is a multimodal task aimed at localizing specific subjects spatio-temporally based on textual queries without bounding box supervision. Motivated by recent advancements in multi-modal foundation models for grounding tasks, we first explore the potential of state-of-the-art object detection models for WSTVG. Despite their robust zero-shot capabilities, our adaptation reveals significant limitations, including inconsistent temporal predictions, inadequate understanding of complex queries, and challenges in adapting to difficult scenarios. We propose CoSPaL (Contextual Self-Paced Learning), a novel approach which is designed to overcome these limitations. CoSPaL integrates three core components: (1) Tubelet Phrase Grounding (TPG), which introduces spatio-temporal prediction by linking textual queries to tubelets; (2) Contextual Referral Grounding (CRG), which improves comprehension of complex queries by extracting contextual information to refine object identification over time; and (3) Self-Paced Scene Understanding (SPS), a training paradigm that progressively increases task difficulty, enabling the model to adapt to complex scenarios by transitioning from coarse to fine-grained understanding.
<div id='section'>Paperid: <span id='pid'>95, <a href='https://arxiv.org/pdf/2501.15326.pdf' target='_blank'>https://arxiv.org/pdf/2501.15326.pdf</a></span>   <span><a href='https://ntlm1686.github.io/raso' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiajie Li, Brian R Quaranto, Chenhui Xu, Ishan Mishra, Ruiyang Qin, Dancheng Liu, Peter C W Kim, Jinjun Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15326">Recognize Any Surgical Object: Unleashing the Power of Weakly-Supervised Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present RASO, a foundation model designed to Recognize Any Surgical Object, offering robust open-set recognition capabilities across a broad range of surgical procedures and object classes, in both surgical images and videos. RASO leverages a novel weakly-supervised learning framework that generates tag-image-text pairs automatically from large-scale unannotated surgical lecture videos, significantly reducing the need for manual annotations. Our scalable data generation pipeline gathers 2,200 surgical procedures and produces 3.6 million tag annotations across 2,066 unique surgical tags. Our experiments show that RASO achieves improvements of 2.9 mAP, 4.5 mAP, 10.6 mAP, and 7.2 mAP on four standard surgical benchmarks, respectively, in zero-shot settings, and surpasses state-of-the-art models in supervised surgical action recognition tasks. Code, model, and demo are available at https://ntlm1686.github.io/raso.
<div id='section'>Paperid: <span id='pid'>96, <a href='https://arxiv.org/pdf/2501.13426.pdf' target='_blank'>https://arxiv.org/pdf/2501.13426.pdf</a></span>   <span><a href='https://github.com/zxk688' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jian Wang, Xiaokang Zhang, Xianping Ma, Weikang Yu, Pedram Ghamisi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13426">Auto-Prompting SAM for Weakly Supervised Landslide Extraction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised landslide extraction aims to identify landslide regions from remote sensing data using models trained with weak labels, particularly image-level labels. However, it is often challenged by the imprecise boundaries of the extracted objects due to the lack of pixel-wise supervision and the properties of landslide objects. To tackle these issues, we propose a simple yet effective method by auto-prompting the Segment Anything Model (SAM), i.e., APSAM. Instead of depending on high-quality class activation maps (CAMs) for pseudo-labeling or fine-tuning SAM, our method directly yields fine-grained segmentation masks from SAM inference through prompt engineering. Specifically, it adaptively generates hybrid prompts from the CAMs obtained by an object localization network. To provide sufficient information for SAM prompting, an adaptive prompt generation (APG) algorithm is designed to fully leverage the visual patterns of CAMs, enabling the efficient generation of pseudo-masks for landslide extraction. These informative prompts are able to identify the extent of landslide areas (box prompts) and denote the centers of landslide objects (point prompts), guiding SAM in landslide segmentation. Experimental results on high-resolution aerial and satellite datasets demonstrate the effectiveness of our method, achieving improvements of at least 3.0\% in F1 score and 3.69\% in IoU compared to other state-of-the-art methods. The source codes and datasets will be available at https://github.com/zxk688.
<div id='section'>Paperid: <span id='pid'>97, <a href='https://arxiv.org/pdf/2501.07496.pdf' target='_blank'>https://arxiv.org/pdf/2501.07496.pdf</a></span>   <span><a href='https://github.com/xjpp2016/MAVD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenping Jin, Li Zhu, Jing Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07496">Aligning First, Then Fusing: A Novel Weakly Supervised Multimodal Violence Detection Method</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised violence detection refers to the technique of training models to identify violent segments in videos using only video-level labels. Among these approaches, multimodal violence detection, which integrates modalities such as audio and optical flow, holds great potential. Existing methods in this domain primarily focus on designing multimodal fusion models to address modality discrepancies. In contrast, we take a different approach; leveraging the inherent discrepancies across modalities in violence event representation to propose a novel multimodal semantic feature alignment method. This method sparsely maps the semantic features of local, transient, and less informative modalities ( such as audio and optical flow ) into the more informative RGB semantic feature space. Through an iterative process, the method identifies the suitable no-zero feature matching subspace and aligns the modality-specific event representations based on this subspace, enabling the full exploitation of information from all modalities during the subsequent modality fusion stage. Building on this, we design a new weakly supervised violence detection framework that consists of unimodal multiple-instance learning for extracting unimodal semantic features, multimodal alignment, multimodal fusion, and final detection. Experimental results on benchmark datasets demonstrate the effectiveness of our method, achieving an average precision (AP) of 86.07% on the XD-Violence dataset. Our code is available at https://github.com/xjpp2016/MAVD.
<div id='section'>Paperid: <span id='pid'>98, <a href='https://arxiv.org/pdf/2501.04934.pdf' target='_blank'>https://arxiv.org/pdf/2501.04934.pdf</a></span>   <span><a href='https://github.com/zhenghuizhao/Plug-and-Play-DISep-for-Change-Detection' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenghui Zhao, Chen Wu, Lixiang Ru, Di Wang, Hongruixuan Chen, Cuiqun Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04934">Plug-and-Play DISep: Separating Dense Instances for Scene-to-Pixel Weakly-Supervised Change Detection in High-Resolution Remote Sensing Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing Weakly-Supervised Change Detection (WSCD) methods often encounter the problem of "instance lumping" under scene-level supervision, particularly in scenarios with a dense distribution of changed instances (i.e., changed objects). In these scenarios, unchanged pixels between changed instances are also mistakenly identified as changed, causing multiple changes to be mistakenly viewed as one. In practical applications, this issue prevents the accurate quantification of the number of changes. To address this issue, we propose a Dense Instance Separation (DISep) method as a plug-and-play solution, refining pixel features from a unified instance perspective under scene-level supervision. Specifically, our DISep comprises a three-step iterative training process: 1) Instance Localization: We locate instance candidate regions for changed pixels using high-pass class activation maps. 2) Instance Retrieval: We identify and group these changed pixels into different instance IDs through connectivity searching. Then, based on the assigned instance IDs, we extract corresponding pixel-level features on a per-instance basis. 3) Instance Separation: We introduce a separation loss to enforce intra-instance pixel consistency in the embedding space, thereby ensuring separable instance feature representations. The proposed DISep adds only minimal training cost and no inference cost. It can be seamlessly integrated to enhance existing WSCD methods. We achieve state-of-the-art performance by enhancing {three Transformer-based and four ConvNet-based methods} on the LEVIR-CD, WHU-CD, DSIFN-CD, SYSU-CD, and CDD datasets. Additionally, our DISep can be used to improve fully-supervised change detection methods. Code is available at https://github.com/zhenghuizhao/Plug-and-Play-DISep-for-Change-Detection.
<div id='section'>Paperid: <span id='pid'>99, <a href='https://arxiv.org/pdf/2501.04440.pdf' target='_blank'>https://arxiv.org/pdf/2501.04440.pdf</a></span>   <span><a href='https://github.com/zhasion/RSAR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Zhang, Xue Yang, Yuxuan Li, Jian Yang, Ming-Ming Cheng, Xiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04440">RSAR: Restricted State Angle Resolver and Rotated SAR Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rotated object detection has made significant progress in the optical remote sensing. However, advancements in the Synthetic Aperture Radar (SAR) field are laggard behind, primarily due to the absence of a large-scale dataset. Annotating such a dataset is inefficient and costly. A promising solution is to employ a weakly supervised model (e.g., trained with available horizontal boxes only) to generate pseudo-rotated boxes for reference before manual calibration. Unfortunately, the existing weakly supervised models exhibit limited accuracy in predicting the object's angle. Previous works attempt to enhance angle prediction by using angle resolvers that decouple angles into cosine and sine encodings. In this work, we first reevaluate these resolvers from a unified perspective of dimension mapping and expose that they share the same shortcomings: these methods overlook the unit cycle constraint inherent in these encodings, easily leading to prediction biases. To address this issue, we propose the Unit Cycle Resolver, which incorporates a unit circle constraint loss to improve angle prediction accuracy. Our approach can effectively improve the performance of existing state-of-the-art weakly supervised methods and even surpasses fully supervised models on existing optical benchmarks (i.e., DOTA-v1.0 dataset). With the aid of UCR, we further annotate and introduce RSAR, the largest multi-class rotated SAR object detection dataset to date. Extensive experiments on both RSAR and optical datasets demonstrate that our UCR enhances angle prediction accuracy. Our dataset and code can be found at: https://github.com/zhasion/RSAR.
<div id='section'>Paperid: <span id='pid'>100, <a href='https://arxiv.org/pdf/2412.20924.pdf' target='_blank'>https://arxiv.org/pdf/2412.20924.pdf</a></span>   <span><a href='https://github.com/Vison307/HisynSeg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijie Fang, Yifeng Wang, Peizhang Xie, Zhi Wang, Yongbing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20924">HisynSeg: Weakly-Supervised Histopathological Image Segmentation via Image-Mixing Synthesis and Consistency Regularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tissue semantic segmentation is one of the key tasks in computational pathology. To avoid the expensive and laborious acquisition of pixel-level annotations, a wide range of studies attempt to adopt the class activation map (CAM), a weakly-supervised learning scheme, to achieve pixel-level tissue segmentation. However, CAM-based methods are prone to suffer from under-activation and over-activation issues, leading to poor segmentation performance. To address this problem, we propose a novel weakly-supervised semantic segmentation framework for histopathological images based on image-mixing synthesis and consistency regularization, dubbed HisynSeg. Specifically, synthesized histopathological images with pixel-level masks are generated for fully-supervised model training, where two synthesis strategies are proposed based on Mosaic transformation and BÃ©zier mask generation. Besides, an image filtering module is developed to guarantee the authenticity of the synthesized images. In order to further avoid the model overfitting to the occasional synthesis artifacts, we additionally propose a novel self-supervised consistency regularization, which enables the real images without segmentation masks to supervise the training of the segmentation model. By integrating the proposed techniques, the HisynSeg framework successfully transforms the weakly-supervised semantic segmentation problem into a fully-supervised one, greatly improving the segmentation accuracy. Experimental results on three datasets prove that the proposed method achieves a state-of-the-art performance. Code is available at https://github.com/Vison307/HisynSeg.
<div id='section'>Paperid: <span id='pid'>101, <a href='https://arxiv.org/pdf/2412.19418.pdf' target='_blank'>https://arxiv.org/pdf/2412.19418.pdf</a></span>   <span><a href='https://github.com/heyuanpengpku/GUEF/tree/main' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanpeng He, Lijian Li, Tianxiang Zhan, Wenpin Jiao, Chi-Man Pun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19418">Generalized Uncertainty-Based Evidential Fusion with Hybrid Multi-Head Attention for Weak-Supervised Temporal Action Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised temporal action localization (WS-TAL) is a task of targeting at localizing complete action instances and categorizing them with video-level labels. Action-background ambiguity, primarily caused by background noise resulting from aggregation and intra-action variation, is a significant challenge for existing WS-TAL methods. In this paper, we introduce a hybrid multi-head attention (HMHA) module and generalized uncertainty-based evidential fusion (GUEF) module to address the problem. The proposed HMHA effectively enhances RGB and optical flow features by filtering redundant information and adjusting their feature distribution to better align with the WS-TAL task. Additionally, the proposed GUEF adaptively eliminates the interference of background noise by fusing snippet-level evidences to refine uncertainty measurement and select superior foreground feature information, which enables the model to concentrate on integral action instances to achieve better action localization and classification performance. Experimental results conducted on the THUMOS14 dataset demonstrate that our method outperforms state-of-the-art methods. Our code is available in \url{https://github.com/heyuanpengpku/GUEF/tree/main}.
<div id='section'>Paperid: <span id='pid'>102, <a href='https://arxiv.org/pdf/2412.18738.pdf' target='_blank'>https://arxiv.org/pdf/2412.18738.pdf</a></span>   <span><a href='https://github.com/IPMI-NWU/HELPNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Zhang, Shaoxuan Wu, Peilin Zhang, Zhuo Jin, Xiaosong Xiong, Qirong Bu, Jingkun Chen, Jun Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.18738">HELPNet: Hierarchical Perturbations Consistency and Entropy-guided Ensemble for Scribble Supervised Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating fully annotated labels for medical image segmentation is prohibitively time-intensive and costly, emphasizing the necessity for innovative approaches that minimize reliance on detailed annotations. Scribble annotations offer a more cost-effective alternative, significantly reducing the expenses associated with full annotations. However, scribble annotations offer limited and imprecise information, failing to capture the detailed structural and boundary characteristics necessary for accurate organ delineation. To address these challenges, we propose HELPNet, a novel scribble-based weakly supervised segmentation framework, designed to bridge the gap between annotation efficiency and segmentation performance. HELPNet integrates three modules. The Hierarchical perturbations consistency (HPC) module enhances feature learning by employing density-controlled jigsaw perturbations across global, local, and focal views, enabling robust modeling of multi-scale structural representations. Building on this, the Entropy-guided pseudo-label (EGPL) module evaluates the confidence of segmentation predictions using entropy, generating high-quality pseudo-labels. Finally, the structural prior refinement (SPR) module incorporates connectivity and bounded priors to enhance the precision and reliability and pseudo-labels. Experimental results on three public datasets ACDC, MSCMRseg, and CHAOS show that HELPNet significantly outperforms state-of-the-art methods for scribble-based weakly supervised segmentation and achieves performance comparable to fully supervised methods. The code is available at https://github.com/IPMI-NWU/HELPNet.
<div id='section'>Paperid: <span id='pid'>103, <a href='https://arxiv.org/pdf/2412.17601.pdf' target='_blank'>https://arxiv.org/pdf/2412.17601.pdf</a></span>   <span><a href='https://github.com/jarch-ma/AFANet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Ma, Guo-Sen Xie, Fang Zhao, Zechao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.17601">AFANet: Adaptive Frequency-Aware Network for Weakly-Supervised Few-Shot Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Few-shot learning aims to recognize novel concepts by leveraging prior knowledge learned from a few samples. However, for visually intensive tasks such as few-shot semantic segmentation, pixel-level annotations are time-consuming and costly. Therefore, in this paper, we utilize the more challenging image-level annotations and propose an adaptive frequency-aware network (AFANet) for weakly-supervised few-shot semantic segmentation (WFSS). Specifically, we first propose a cross-granularity frequency-aware module (CFM) that decouples RGB images into high-frequency and low-frequency distributions and further optimizes semantic structural information by realigning them. Unlike most existing WFSS methods using the textual information from the multi-modal language-vision model, e.g., CLIP, in an offline learning manner, we further propose a CLIP-guided spatial-adapter module (CSM), which performs spatial domain adaptive transformation on textual information through online learning, thus providing enriched cross-modal semantic information for CFM. Extensive experiments on the Pascal-5\textsuperscript{i} and COCO-20\textsuperscript{i} datasets demonstrate that AFANet has achieved state-of-the-art performance. The code is available at https://github.com/jarch-ma/AFANet.
<div id='section'>Paperid: <span id='pid'>104, <a href='https://arxiv.org/pdf/2412.11076.pdf' target='_blank'>https://arxiv.org/pdf/2412.11076.pdf</a></span>   <span><a href='https://github.com/zwyang6/MoRe' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiwei Yang, Yucong Meng, Kexue Fu, Shuo Wang, Zhijian Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11076">MoRe: Class Patch Attention Needs Regularization for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Semantic Segmentation (WSSS) with image-level labels typically uses Class Activation Maps (CAM) to achieve dense predictions. Recently, Vision Transformer (ViT) has provided an alternative to generate localization maps from class-patch attention. However, due to insufficient constraints on modeling such attention, we observe that the Localization Attention Maps (LAM) often struggle with the artifact issue, i.e., patch regions with minimal semantic relevance are falsely activated by class tokens. In this work, we propose MoRe to address this issue and further explore the potential of LAM. Our findings suggest that imposing additional regularization on class-patch attention is necessary. To this end, we first view the attention as a novel directed graph and propose the Graph Category Representation module to implicitly regularize the interaction among class-patch entities. It ensures that class tokens dynamically condense the related patch information and suppress unrelated artifacts at a graph level. Second, motivated by the observation that CAM from classification weights maintains smooth localization of objects, we devise the Localization-informed Regularization module to explicitly regularize the class-patch attention. It directly mines the token relations from CAM and further supervises the consistency between class and patch tokens in a learnable manner. Extensive experiments are conducted on PASCAL VOC and MS COCO, validating that MoRe effectively addresses the artifact issue and achieves state-of-the-art performance, surpassing recent single-stage and even multi-stage methods. Code is available at https://github.com/zwyang6/MoRe.
<div id='section'>Paperid: <span id='pid'>105, <a href='https://arxiv.org/pdf/2412.06286.pdf' target='_blank'>https://arxiv.org/pdf/2412.06286.pdf</a></span>   <span><a href='https://github.com/patrick-john-ramos/nada' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Patrick Ramos, Nicolas Gonthier, Selina Khan, Yuta Nakashima, Noa Garcia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.06286">No Annotations for Object Detection in Art through Stable Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object detection in art is a valuable tool for the digital humanities, as it allows for faster identification of objects in artistic and historical images compared to humans. However, annotating such images poses significant challenges due to the need for specialized domain expertise. We present NADA (no annotations for detection in art), a pipeline that leverages diffusion models' art-related knowledge for object detection in paintings without the need for full bounding box supervision. Our method, which supports both weakly-supervised and zero-shot scenarios and does not require any fine-tuning of its pretrained components, consists of a class proposer based on large vision-language models and a class-conditioned detector based on Stable Diffusion. NADA is evaluated on two artwork datasets, ArtDL 2.0 and IconArt, outperforming prior work in weakly-supervised detection, while being the first work for zero-shot object detection in art. Code is available at https://github.com/patrick-john-ramos/nada
<div id='section'>Paperid: <span id='pid'>106, <a href='https://arxiv.org/pdf/2412.05876.pdf' target='_blank'>https://arxiv.org/pdf/2412.05876.pdf</a></span>   <span><a href='https://github.com/Xuefeng-Ni/MG-3D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuefeng Ni, Linshan Wu, Jiaxin Zhuang, Qiong Wang, Mingxiang Wu, Varut Vardhanabhuti, Lihai Zhang, Hanyu Gao, Hao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05876">MG-3D: Multi-Grained Knowledge-Enhanced 3D Medical Vision-Language Pre-training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D medical image analysis is pivotal in numerous clinical applications. However, the scarcity of labeled data and limited generalization capabilities hinder the advancement of AI-empowered models. Radiology reports are easily accessible and can serve as weakly-supervised signals. However, large-scale vision-language pre-training (VLP) remains underexplored in 3D medical image analysis. Specifically, the insufficient investigation into multi-grained radiology semantics and their correlations across patients leads to underutilization of large-scale volume-report data.
  Considering intra-patient cross-modal semantic consistency and inter-patient semantic correlations, we propose a multi-task VLP method, MG-3D, pre-trained on large-scale data (47.1K), addressing the challenges by the following two aspects: 1) Establishing the correspondence between volume semantics and multi-grained medical knowledge of each patient with cross-modal global alignment and complementary modality-guided local reconstruction, ensuring intra-patient features of different modalities cohesively represent the same semantic content; 2) Correlating inter-patient visual semantics based on fine-grained report correlations across patients, and keeping sensitivity to global individual differences via contrastive learning, enhancing the discriminative feature representation. Furthermore, we delve into the scaling law to explore potential performance improvements. Comprehensive evaluations across nine uni- and cross-modal clinical tasks are carried out to assess model efficacy. Extensive experiments on both internal and external datasets demonstrate the superior transferability, scalability, and generalization of MG-3D, showcasing its potential in advancing feature representation for 3D medical image analysis. Code will be available: https://github.com/Xuefeng-Ni/MG-3D.
<div id='section'>Paperid: <span id='pid'>107, <a href='https://arxiv.org/pdf/2412.04473.pdf' target='_blank'>https://arxiv.org/pdf/2412.04473.pdf</a></span>   <span><a href='https://github.com/woshixiaobai2019/nids-gpt.gi' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04473">Take Package as Language: Anomaly Detection Using Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Network data packet anomaly detection faces numerous challenges, including exploring new anomaly supervision signals, researching weakly supervised anomaly detection, and improving model interpretability. This paper proposes NIDS-GPT, a GPT-based causal language model for network intrusion detection. Unlike previous work, NIDS-GPT innovatively treats each number in the packet as an independent "word" rather than packet fields, enabling a more fine-grained data representation. We adopt an improved GPT-2 model and design special tokenizers and embedding layers to better capture the structure and semantics of network data. NIDS-GPT has good scalability, supports unsupervised pre-training, and enhances model interpretability through attention weight visualization. Experiments on the CICIDS2017 and car-hacking datasets show that NIDS-GPT achieves 100\% accuracy under extreme imbalance conditions, far surpassing traditional methods; it also achieves over 90\% accuracy in one-shot learning. These results demonstrate NIDS-GPT's excellent performance and potential in handling complex network anomaly detection tasks, especially in data-imbalanced and resource-constrained scenarios. The code is available at \url{https://github.com/woshixiaobai2019/nids-gpt.gi
<div id='section'>Paperid: <span id='pid'>108, <a href='https://arxiv.org/pdf/2412.03968.pdf' target='_blank'>https://arxiv.org/pdf/2412.03968.pdf</a></span>   <span><a href='https://github.com/MiSsU-HH/Exact' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Zhu, Yan Zhu, Jiayu Xiao, Tianxiang Xiao, Yike Ma, Yucheng Zhang, Feng Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.03968">Exact: Exploring Space-Time Perceptive Clues for Weakly Supervised Satellite Image Time Series Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automated crop mapping through Satellite Image Time Series (SITS) has emerged as a crucial avenue for agricultural monitoring and management. However, due to the low resolution and unclear parcel boundaries, annotating pixel-level masks is exceptionally complex and time-consuming in SITS. This paper embraces the weakly supervised paradigm (i.e., only image-level categories available) to liberate the crop mapping task from the exhaustive annotation burden. The unique characteristics of SITS give rise to several challenges in weakly supervised learning: (1) noise perturbation from spatially neighboring regions, and (2) erroneous semantic bias from anomalous temporal periods. To address the above difficulties, we propose a novel method, termed exploring space-time perceptive clues (Exact). First, we introduce a set of spatial clues to explicitly capture the representative patterns of different crops from the most class-relative regions. Besides, we leverage the temporal-to-class interaction of the model to emphasize the contributions of pivotal clips, thereby enhancing the model perception for crop regions. Build upon the space-time perceptive clues, we derive the clue-based CAMs to effectively supervise the SITS segmentation network. Our method demonstrates impressive performance on various SITS benchmarks. Remarkably, the segmentation network trained on Exact-generated masks achieves 95% of its fully supervised performance, showing the bright promise of weakly supervised paradigm in crop mapping scenario. Our code will be publicly available.
<div id='section'>Paperid: <span id='pid'>109, <a href='https://arxiv.org/pdf/2412.02012.pdf' target='_blank'>https://arxiv.org/pdf/2412.02012.pdf</a></span>   <span><a href='https://zhangdylan83.github.io/ewsmia/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenbo Zhang, Junyu Chen, Christopher Kanan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02012">INSIGHT: Explainable Weakly-Supervised Medical Image Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to their large sizes, volumetric scans and whole-slide pathology images (WSIs) are often processed by extracting embeddings from local regions and then an aggregator makes predictions from this set. However, current methods require post-hoc visualization techniques (e.g., Grad-CAM) and often fail to localize small yet clinically crucial details. To address these limitations, we introduce INSIGHT, a novel weakly-supervised aggregator that integrates heatmap generation as an inductive bias. Starting from pre-trained feature maps, INSIGHT employs a detection module with small convolutional kernels to capture fine details and a context module with a broader receptive field to suppress local false positives. The resulting internal heatmap highlights diagnostically relevant regions. On CT and WSI benchmarks, INSIGHT achieves state-of-the-art classification results and high weakly-labeled semantic segmentation performance. Project website and code are available at: https://zhangdylan83.github.io/ewsmia/
<div id='section'>Paperid: <span id='pid'>110, <a href='https://arxiv.org/pdf/2411.19290.pdf' target='_blank'>https://arxiv.org/pdf/2411.19290.pdf</a></span>   <span><a href='https://yunjinli.github.io/project-sadg' target='_blank'>  GitHub</a></span> <span><a href='https://yunjinli.github.io/project-sadg/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yun-Jin Li, Mariia Gladkova, Yan Xia, Daniel Cremers
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19290">TRASE: Tracking-free 4D Segmentation and Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding dynamic 3D scenes is crucial for extended reality (XR) and autonomous driving. Incorporating semantic information into 3D reconstruction enables holistic scene representations, unlocking immersive and interactive applications. To this end, we introduce TRASE, a novel tracking-free 4D segmentation method for dynamic scene understanding. TRASE learns a 4D segmentation feature field in a weakly-supervised manner, leveraging a soft-mined contrastive learning objective guided by SAM masks. The resulting feature space is semantically coherent and well-separated, and final object-level segmentation is obtained via unsupervised clustering. This enables fast editing, such as object removal, composition, and style transfer, by directly manipulating the scene's Gaussians. We evaluate TRASE on five dynamic benchmarks, demonstrating state-of-the-art segmentation performance from unseen viewpoints and its effectiveness across various interactive editing tasks. Our project page is available at: https://yunjinli.github.io/project-sadg/
<div id='section'>Paperid: <span id='pid'>111, <a href='https://arxiv.org/pdf/2411.19067.pdf' target='_blank'>https://arxiv.org/pdf/2411.19067.pdf</a></span>   <span><a href='https://github.com/naver-ai/maskris' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Minhyun Lee, Seungho Lee, Song Park, Dongyoon Han, Byeongho Heo, Hyunjung Shim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19067">MaskRIS: Semantic Distortion-aware Data Augmentation for Referring Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring Image Segmentation (RIS) is an advanced vision-language task that involves identifying and segmenting objects within an image as described by free-form text descriptions. While previous studies focused on aligning visual and language features, exploring training techniques, such as data augmentation, remains underexplored. In this work, we explore effective data augmentation for RIS and propose a novel training framework called Masked Referring Image Segmentation (MaskRIS). We observe that the conventional image augmentations fall short of RIS, leading to performance degradation, while simple random masking significantly enhances the performance of RIS. MaskRIS uses both image and text masking, followed by Distortion-aware Contextual Learning (DCL) to fully exploit the benefits of the masking strategy. This approach can improve the model's robustness to occlusions, incomplete information, and various linguistic complexities, resulting in a significant performance improvement. Experiments demonstrate that MaskRIS can easily be applied to various RIS models, outperforming existing methods in both fully supervised and weakly supervised settings. Finally, MaskRIS achieves new state-of-the-art performance on RefCOCO, RefCOCO+, and RefCOCOg datasets. Code is available at https://github.com/naver-ai/maskris.
<div id='section'>Paperid: <span id='pid'>112, <a href='https://arxiv.org/pdf/2411.17376.pdf' target='_blank'>https://arxiv.org/pdf/2411.17376.pdf</a></span>   <span><a href='https://fujiry0.github.io/RealTraj-project-page' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryo Fujii, Hideo Saito, Ryo Hachiuma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17376">RealTraj: Towards Real-World Pedestrian Trajectory Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper jointly addresses three key limitations in conventional pedestrian trajectory forecasting: pedestrian perception errors, real-world data collection costs, and person ID annotation costs. We propose a novel framework, RealTraj, that enhances the real-world applicability of trajectory forecasting. Our approach includes two training phases -- self-supervised pretraining on synthetic data and weakly-supervised fine-tuning with limited real-world data -- to minimize data collection efforts. To improve robustness to real-world errors, we focus on both model design and training objectives. Specifically, we present Det2TrajFormer, a trajectory forecasting model that remains invariant to tracking noise by using past detections as inputs. Additionally, we pretrain the model using multiple pretext tasks, which enhance robustness and improve forecasting performance based solely on detection data. Unlike previous trajectory forecasting methods, our approach fine-tunes the model using only ground-truth detections, reducing the need for costly person ID annotations. In the experiments, we comprehensively verify the effectiveness of the proposed method against the limitations, and the method outperforms state-of-the-art trajectory forecasting methods on multiple datasets. The code will be released at https://fujiry0.github.io/RealTraj-project-page.
<div id='section'>Paperid: <span id='pid'>113, <a href='https://arxiv.org/pdf/2411.16219.pdf' target='_blank'>https://arxiv.org/pdf/2411.16219.pdf</a></span>   <span><a href='https://github.com/manuelknott/banana-defect-segmentation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Manuel Knott, Divinefavour Odion, Sameer Sontakke, Anup Karwa, Thijs Defraeye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16219">Weakly Supervised Panoptic Segmentation for Defect-Based Grading of Fresh Produce</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual inspection for defect grading in agricultural supply chains is crucial but traditionally labor-intensive and error-prone. Automated computer vision methods typically require extensively annotated datasets, which are often unavailable in decentralized supply chains. We address this challenge by evaluating the Segment Anything Model (SAM) to generate dense panoptic segmentation masks from sparse annotations. These dense predictions are then used to train a supervised panoptic segmentation model. Focusing on banana surface defects (bruises and scars), we validate our approach using 476 field images annotated with 1440 defects. While SAM-generated masks generally align with human annotations, substantially reducing annotation effort, we explicitly identify failure cases associated with specific defect sizes and shapes. Despite these limitations, our approach offers practical estimates of defect number and relative size from panoptic masks, underscoring the potential and current boundaries of foundation models for defect quantification in low-data agricultural scenarios. GitHub: https://github.com/manuelknott/banana-defect-segmentation
<div id='section'>Paperid: <span id='pid'>114, <a href='https://arxiv.org/pdf/2411.15763.pdf' target='_blank'>https://arxiv.org/pdf/2411.15763.pdf</a></span>   <span><a href='https://github.com/arvindmvepa/al-seg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Arvind Murari Vepa, Zukang Yang, Andrew Choi, Jungseock Joo, Fabien Scalzo, Yizhou Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15763">Integrating Deep Metric Learning with Coreset for Active Learning in 3D Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning has seen remarkable advancements in machine learning, yet it often demands extensive annotated data. Tasks like 3D semantic segmentation impose a substantial annotation burden, especially in domains like medicine, where expert annotations drive up the cost. Active learning (AL) holds great potential to alleviate this annotation burden in 3D medical segmentation. The majority of existing AL methods, however, are not tailored to the medical domain. While weakly-supervised methods have been explored to reduce annotation burden, the fusion of AL with weak supervision remains unexplored, despite its potential to significantly reduce annotation costs. Additionally, there is little focus on slice-based AL for 3D segmentation, which can also significantly reduce costs in comparison to conventional volume-based AL. This paper introduces a novel metric learning method for Coreset to perform slice-based active learning in 3D medical segmentation. By merging contrastive learning with inherent data groupings in medical imaging, we learn a metric that emphasizes the relevant differences in samples for training 3D medical segmentation models. We perform comprehensive evaluations using both weak and full annotations across four datasets (medical and non-medical). Our findings demonstrate that our approach surpasses existing active learning techniques on both weak and full annotations and obtains superior performance with low-annotation budgets which is crucial in medical imaging. Source code for this project is available in the supplementary materials and on GitHub: https://github.com/arvindmvepa/al-seg.
<div id='section'>Paperid: <span id='pid'>115, <a href='https://arxiv.org/pdf/2411.14429.pdf' target='_blank'>https://arxiv.org/pdf/2411.14429.pdf</a></span>   <span><a href='https://github.com/rayleizhu/GLMix' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Zhu, Xinjiang Wang, Wayne Zhang, Rynson W. H. Lau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.14429">Revisiting the Integration of Convolution and Attention for Vision Backbone</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Convolutions (Convs) and multi-head self-attentions (MHSAs) are typically considered alternatives to each other for building vision backbones. Although some works try to integrate both, they apply the two operators simultaneously at the finest pixel granularity. With Convs responsible for per-pixel feature extraction already, the question is whether we still need to include the heavy MHSAs at such a fine-grained level. In fact, this is the root cause of the scalability issue w.r.t. the input resolution for vision transformers. To address this important problem, we propose in this work to use MSHAs and Convs in parallel \textbf{at different granularity levels} instead. Specifically, in each layer, we use two different ways to represent an image: a fine-grained regular grid and a coarse-grained set of semantic slots. We apply different operations to these two representations: Convs to the grid for local features, and MHSAs to the slots for global features. A pair of fully differentiable soft clustering and dispatching modules is introduced to bridge the grid and set representations, thus enabling local-global fusion. Through extensive experiments on various vision tasks, we empirically verify the potential of the proposed integration scheme, named \textit{GLMix}: by offloading the burden of fine-grained features to light-weight Convs, it is sufficient to use MHSAs in a few (e.g., 64) semantic slots to match the performance of recent state-of-the-art backbones, while being more efficient. Our visualization results also demonstrate that the soft clustering module produces a meaningful semantic grouping effect with only IN1k classification supervision, which may induce better interpretability and inspire new weakly-supervised semantic segmentation approaches. Code will be available at \url{https://github.com/rayleizhu/GLMix}.
<div id='section'>Paperid: <span id='pid'>116, <a href='https://arxiv.org/pdf/2411.10364.pdf' target='_blank'>https://arxiv.org/pdf/2411.10364.pdf</a></span>   <span><a href='https://github.com/TianhaoMa5/LLP-AHIL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianhao Ma, Han Chen, Juncheng Hu, Yungang Zhu, Ximing Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10364">Forming Auxiliary High-confident Instance-level Loss to Promote Learning from Label Proportions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning from label proportions (LLP), i.e., a challenging weakly-supervised learning task, aims to train a classifier by using bags of instances and the proportions of classes within bags, rather than annotated labels for each instance. Beyond the traditional bag-level loss, the mainstream methodology of LLP is to incorporate an auxiliary instance-level loss with pseudo-labels formed by predictions. Unfortunately, we empirically observed that the pseudo-labels are are often inaccurate due to over-smoothing, especially for the scenarios with large bag sizes, hurting the classifier induction. To alleviate this problem, we suggest a novel LLP method, namely Learning from Label Proportions with Auxiliary High-confident Instance-level Loss (L^2P-AHIL). Specifically, we propose a dual entropy-based weight (DEW) method to adaptively measure the confidences of pseudo-labels. It simultaneously emphasizes accurate predictions at the bag level and avoids overly smoothed predictions. We then form high-confident instance-level loss with DEW, and jointly optimize it with the bag-level loss in a self-training manner. The experimental results on benchmark datasets show that L^2P-AHIL can surpass the existing baseline methods, and the performance gain can be more significant as the bag size increases. The implementation of our method is available at https://github.com/TianhaoMa5/LLP-AHIL.
<div id='section'>Paperid: <span id='pid'>117, <a href='https://arxiv.org/pdf/2411.06652.pdf' target='_blank'>https://arxiv.org/pdf/2411.06652.pdf</a></span>   <span><a href='https://github.com/liuzywen/LFScribble' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengyi Liu, Longzhen Wang, Xianyong Fang, Zhengzheng Tu, Linbo Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06652">LFSamba: Marry SAM with Mamba for Light Field Salient Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A light field camera can reconstruct 3D scenes using captured multi-focus images that contain rich spatial geometric information, enhancing applications in stereoscopic photography, virtual reality, and robotic vision. In this work, a state-of-the-art salient object detection model for multi-focus light field images, called LFSamba, is introduced to emphasize four main insights: (a) Efficient feature extraction, where SAM is used to extract modality-aware discriminative features; (b) Inter-slice relation modeling, leveraging Mamba to capture long-range dependencies across multiple focal slices, thus extracting implicit depth cues; (c) Inter-modal relation modeling, utilizing Mamba to integrate all-focus and multi-focus images, enabling mutual enhancement; (d) Weakly supervised learning capability, developing a scribble annotation dataset from an existing pixel-level mask dataset, establishing the first scribble-supervised baseline for light field salient object detection.https://github.com/liuzywen/LFScribble
<div id='section'>Paperid: <span id='pid'>118, <a href='https://arxiv.org/pdf/2410.14083.pdf' target='_blank'>https://arxiv.org/pdf/2410.14083.pdf</a></span>   <span><a href='https://github.com/sqhuang0103/SAMReg.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiqi Huang, Tingfa Xu, Ziyi Shen, Shaheer Ullah Saeed, Wen Yan, Dean Barratt, Yipeng Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14083">SAMReg: SAM-enabled Image Registration with ROI-based Correspondence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper describes a new spatial correspondence representation based on paired regions-of-interest (ROIs), for medical image registration. The distinct properties of the proposed ROI-based correspondence are discussed, in the context of potential benefits in clinical applications following image registration, compared with alternative correspondence-representing approaches, such as those based on sampled displacements and spatial transformation functions. These benefits include a clear connection between learning-based image registration and segmentation, which in turn motivates two cases of image registration approaches using (pre-)trained segmentation networks. Based on the segment anything model (SAM), a vision foundation model for segmentation, we develop a new registration algorithm SAMReg, which does not require any training (or training data), gradient-based fine-tuning or prompt engineering. The proposed SAMReg models are evaluated across five real-world applications, including intra-subject registration tasks with cardiac MR and lung CT, challenging inter-subject registration scenarios with prostate MR and retinal imaging, and an additional evaluation with a non-clinical example with aerial image registration. The proposed methods outperform both intensity-based iterative algorithms and DDF-predicting learning-based networks across tested metrics including Dice and target registration errors on anatomical structures, and further demonstrates competitive performance compared to weakly-supervised registration approaches that rely on fully-segmented training data. Open source code and examples are available at: https://github.com/sqhuang0103/SAMReg.git.
<div id='section'>Paperid: <span id='pid'>119, <a href='https://arxiv.org/pdf/2410.13621.pdf' target='_blank'>https://arxiv.org/pdf/2410.13621.pdf</a></span>   <span><a href='https://github.com/QI-NemoSong/EP-SAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Joonhyeon Song, Seohwan Yun, Seongho Yoon, Joohyeok Kim, Sangmin Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13621">EP-SAM: Weakly Supervised Histopathology Segmentation via Enhanced Prompt with Segment Anything</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work proposes a novel approach beyond supervised learning for effective pathological image analysis, addressing the challenge of limited robust labeled data. Pathological diagnosis of diseases like cancer has conventionally relied on the evaluation of morphological features by physicians and pathologists. However, recent advancements in compute-aided diagnosis (CAD) systems are gaining significant attention as diagnostic support tools. Although the advancement of deep learning has improved CAD significantly, segmentation models typically require large pixel-level annotated dataset, and such labeling is expensive. Existing studies not based on supervised approaches still struggle with limited generalization, and no practical approach has emerged yet. To address this issue, we present a weakly supervised semantic segmentation (WSSS) model by combining class activation map and Segment Anything Model (SAM)-based pseudo-labeling. For effective pretraining, we adopt the SAM-a foundation model that is pretrained on large datasets and operates in zero-shot configurations using only coarse prompts. The proposed approach transfer enhanced Attention Dropout Layer's knowledge to SAM, thereby generating pseudo-labels. To demonstrate the superiority of the proposed method, experimental studies are conducted on histopathological breast cancer datasets. The proposed method outperformed other WSSS methods across three datasets, demonstrating its efficiency by achieving this with only 12GB of GPU memory during training. Our code is available at : https://github.com/QI-NemoSong/EP-SAM
<div id='section'>Paperid: <span id='pid'>120, <a href='https://arxiv.org/pdf/2410.08509.pdf' target='_blank'>https://arxiv.org/pdf/2410.08509.pdf</a></span>   <span><a href='https://github.com/MoriLabNU/Bayesian_WSS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhou Zheng, Yuichiro Hayashi, Masahiro Oda, Takayuki Kitasaka, Kensaku Mori
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08509">A Bayesian Approach to Weakly-supervised Laparoscopic Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we study weakly-supervised laparoscopic image segmentation with sparse annotations. We introduce a novel Bayesian deep learning approach designed to enhance both the accuracy and interpretability of the model's segmentation, founded upon a comprehensive Bayesian framework, ensuring a robust and theoretically validated method. Our approach diverges from conventional methods that directly train using observed images and their corresponding weak annotations. Instead, we estimate the joint distribution of both images and labels given the acquired data. This facilitates the sampling of images and their high-quality pseudo-labels, enabling the training of a generalizable segmentation model. Each component of our model is expressed through probabilistic formulations, providing a coherent and interpretable structure. This probabilistic nature benefits accurate and practical learning from sparse annotations and equips our model with the ability to quantify uncertainty. Extensive evaluations with two public laparoscopic datasets demonstrated the efficacy of our method, which consistently outperformed existing methods. Furthermore, our method was adapted for scribble-supervised cardiac multi-structure segmentation, presenting competitive performance compared to previous methods. The code is available at https://github.com/MoriLabNU/Bayesian_WSS.
<div id='section'>Paperid: <span id='pid'>121, <a href='https://arxiv.org/pdf/2410.02212.pdf' target='_blank'>https://arxiv.org/pdf/2410.02212.pdf</a></span>   <span><a href='https://github.com/winston52/HNM-WSI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wentao Huang, Xiaoling Hu, Shahira Abousamra, Prateek Prasanna, Chao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02212">Hard Negative Sample Mining for Whole Slide Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised whole slide image (WSI) classification is challenging due to the lack of patch-level labels and high computational costs. State-of-the-art methods use self-supervised patch-wise feature representations for multiple instance learning (MIL). Recently, methods have been proposed to fine-tune the feature representation on the downstream task using pseudo labeling, but mostly focusing on selecting high-quality positive patches. In this paper, we propose to mine hard negative samples during fine-tuning. This allows us to obtain better feature representations and reduce the training cost. Furthermore, we propose a novel patch-wise ranking loss in MIL to better exploit these hard negative samples. Experiments on two public datasets demonstrate the efficacy of these proposed ideas. Our codes are available at https://github.com/winston52/HNM-WSI
<div id='section'>Paperid: <span id='pid'>122, <a href='https://arxiv.org/pdf/2410.01341.pdf' target='_blank'>https://arxiv.org/pdf/2410.01341.pdf</a></span>   <span><a href='https://github.com/ZhaofengSHI/CTDN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaofeng Shi, Heqian Qiu, Lanxiao Wang, Fanman Meng, Qingbo Wu, Hongliang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01341">Cognition Transferring and Decoupling for Text-supervised Egocentric Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we explore a novel Text-supervised Egocentic Semantic Segmentation (TESS) task that aims to assign pixel-level categories to egocentric images weakly supervised by texts from image-level labels. In this task with prospective potential, the egocentric scenes contain dense wearer-object relations and inter-object interference. However, most recent third-view methods leverage the frozen Contrastive Language-Image Pre-training (CLIP) model, which is pre-trained on the semantic-oriented third-view data and lapses in the egocentric view due to the ``relation insensitive" problem. Hence, we propose a Cognition Transferring and Decoupling Network (CTDN) that first learns the egocentric wearer-object relations via correlating the image and text. Besides, a Cognition Transferring Module (CTM) is developed to distill the cognitive knowledge from the large-scale pre-trained model to our model for recognizing egocentric objects with various semantics. Based on the transferred cognition, the Foreground-background Decoupling Module (FDM) disentangles the visual representations to explicitly discriminate the foreground and background regions to mitigate false activation areas caused by foreground-background interferential objects during egocentric relation learning. Extensive experiments on four TESS benchmarks demonstrate the effectiveness of our approach, which outperforms many recent related methods by a large margin. Code will be available at https://github.com/ZhaofengSHI/CTDN.
<div id='section'>Paperid: <span id='pid'>123, <a href='https://arxiv.org/pdf/2409.19720.pdf' target='_blank'>https://arxiv.org/pdf/2409.19720.pdf</a></span>   <span><a href='https://github.com/fukexue/FAST' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kexue Fu, Xiaoyuan Luo, Linhao Qu, Shuo Wang, Ying Xiong, Ilias Maglogiannis, Longxiang Gao, Manning Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19720">FAST: A Dual-tier Few-Shot Learning Paradigm for Whole Slide Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The expensive fine-grained annotation and data scarcity have become the primary obstacles for the widespread adoption of deep learning-based Whole Slide Images (WSI) classification algorithms in clinical practice. Unlike few-shot learning methods in natural images that can leverage the labels of each image, existing few-shot WSI classification methods only utilize a small number of fine-grained labels or weakly supervised slide labels for training in order to avoid expensive fine-grained annotation. They lack sufficient mining of available WSIs, severely limiting WSI classification performance. To address the above issues, we propose a novel and efficient dual-tier few-shot learning paradigm for WSI classification, named FAST. FAST consists of a dual-level annotation strategy and a dual-branch classification framework. Firstly, to avoid expensive fine-grained annotation, we collect a very small number of WSIs at the slide level, and annotate an extremely small number of patches. Then, to fully mining the available WSIs, we use all the patches and available patch labels to build a cache branch, which utilizes the labeled patches to learn the labels of unlabeled patches and through knowledge retrieval for patch classification. In addition to the cache branch, we also construct a prior branch that includes learnable prompt vectors, using the text encoder of visual-language models for patch classification. Finally, we integrate the results from both branches to achieve WSI classification. Extensive experiments on binary and multi-class datasets demonstrate that our proposed method significantly surpasses existing few-shot classification methods and approaches the accuracy of fully supervised methods with only 0.22$\%$ annotation costs. All codes and models will be publicly available on https://github.com/fukexue/FAST.
<div id='section'>Paperid: <span id='pid'>124, <a href='https://arxiv.org/pdf/2409.19483.pdf' target='_blank'>https://arxiv.org/pdf/2409.19483.pdf</a></span>   <span><a href='https://github.com/HealthX-Lab/MedCLIP-SAMv2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Taha Koleilat, Hojat Asgariandehkordi, Hassan Rivaz, Yiming Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19483">MedCLIP-SAMv2: Towards Universal Text-Driven Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Segmentation of anatomical structures and pathological regions in medical images is essential for modern clinical diagnosis, disease research, and treatment planning. While significant advancements have been made in deep learning-based segmentation techniques, many of these methods still suffer from limitations in data efficiency, generalizability, and interactivity. As a result, developing precise segmentation methods that require fewer labeled datasets remains a critical challenge in medical image analysis. Recently, the introduction of foundation models like CLIP and Segment-Anything-Model (SAM), with robust cross-domain representations, has paved the way for interactive and universal image segmentation. However, further exploration of these models for data-efficient segmentation in medical imaging is still needed and highly relevant. In this paper, we introduce MedCLIP-SAMv2, a novel framework that integrates the CLIP and SAM models to perform segmentation on clinical scans using text prompts, in both zero-shot and weakly supervised settings. Our approach includes fine-tuning the BiomedCLIP model with a new Decoupled Hard Negative Noise Contrastive Estimation (DHN-NCE) loss, and leveraging the Multi-modal Information Bottleneck (M2IB) to create visual prompts for generating segmentation masks from SAM in the zero-shot setting. We also investigate using zero-shot segmentation labels within a weakly supervised paradigm to enhance segmentation quality further. Extensive testing across four diverse segmentation tasks and medical imaging modalities (breast tumor ultrasound, brain tumor MRI, lung X-ray, and lung CT) demonstrates the high accuracy of our proposed framework. Our code is available at https://github.com/HealthX-Lab/MedCLIP-SAMv2.
<div id='section'>Paperid: <span id='pid'>125, <a href='https://arxiv.org/pdf/2409.19370.pdf' target='_blank'>https://arxiv.org/pdf/2409.19370.pdf</a></span>   <span><a href='https://github.com/GtLinyer/MambaEviScrib' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoxiang Han, Xinyu Li, Jiang Shang, Yiman Liu, Keyan Chen, Shugong Xu, Qiaohong Liu, Qi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19370">MambaEviScrib: Mamba and Evidence-Guided Consistency Enhance CNN Robustness for Scribble-Based Weakly Supervised Ultrasound Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Segmenting anatomical structures and lesions from ultrasound images contributes to disease assessment. Weakly supervised learning (WSL) based on sparse annotation has achieved encouraging performance and demonstrated the potential to reduce annotation costs. This study attempts to introduce scribble-based WSL into ultrasound image segmentation tasks. However, ultrasound images often suffer from poor contrast and unclear edges, coupled with insufficient supervison signals for edges, posing challenges to edge prediction. Uncertainty modeling has been proven to facilitate models in dealing with these issues. Nevertheless, existing uncertainty estimation paradigms are not robust enough and often filter out predictions near decision boundaries, resulting in unstable edge predictions. Therefore, we propose leveraging predictions near decision boundaries effectively. Specifically, we introduce Dempster-Shafer Theory (DST) of evidence to design an Evidence-Guided Consistency strategy. This strategy utilizes high-evidence predictions, which are more likely to occur near high-density regions, to guide the optimization of low-evidence predictions that may appear near decision boundaries. Furthermore, the diverse sizes and locations of lesions in ultrasound images pose a challenge for CNNs with local receptive fields, as they struggle to model global information. Therefore, we introduce Visual Mamba based on structured state space sequence models, which achieves long-range dependency with linear computational complexity, and we construct a novel hybrid CNN-Mamba framework. During training, the collaboration between the CNN branch and the Mamba branch in the proposed framework draws inspiration from each other based on the EGC strategy. Experiments demonstrate the competitiveness of the proposed method. Dataset and code will be available on https://github.com/GtLinyer/MambaEviScrib.
<div id='section'>Paperid: <span id='pid'>126, <a href='https://arxiv.org/pdf/2409.16213.pdf' target='_blank'>https://arxiv.org/pdf/2409.16213.pdf</a></span>   <span><a href='https://github.com/Harry-Rogers/PSIE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Harry Rogers, Tahmina Zebin, Grzegorz Cielniak, Beatriz De La Iglesia, Ben Magri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16213">Deep Learning for Precision Agriculture: Post-Spraying Evaluation and Deposition Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Precision spraying evaluation requires automation primarily in post-spraying imagery. In this paper we propose an eXplainable Artificial Intelligence (XAI) computer vision pipeline to evaluate a precision spraying system post-spraying without the need for traditional agricultural methods. The developed system can semantically segment potential targets such as lettuce, chickweed, and meadowgrass and correctly identify if targets have been sprayed. Furthermore, this pipeline evaluates using a domain-specific Weakly Supervised Deposition Estimation task, allowing for class-specific quantification of spray deposit weights in Î¼L. Estimation of coverage rates of spray deposition in a class-wise manner allows for further understanding of effectiveness of precision spraying systems. Our study evaluates different Class Activation Mapping techniques, namely AblationCAM and ScoreCAM, to determine which is more effective and interpretable for these tasks. In the pipeline, inference-only feature fusion is used to allow for further interpretability and to enable the automation of precision spraying evaluation post-spray. Our findings indicate that a Fully Convolutional Network with an EfficientNet-B0 backbone and inference-only feature fusion achieves an average absolute difference in deposition values of 156.8 Î¼L across three classes in our test set. The dataset curated in this paper is publicly available at https://github.com/Harry-Rogers/PSIE
<div id='section'>Paperid: <span id='pid'>127, <a href='https://arxiv.org/pdf/2409.14319.pdf' target='_blank'>https://arxiv.org/pdf/2409.14319.pdf</a></span>   <span><a href='https://github.com/zhousheng97/ViTXT-GQA.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sheng Zhou, Junbin Xiao, Xun Yang, Peipei Song, Dan Guo, Angela Yao, Meng Wang, Tat-Seng Chua
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14319">Scene-Text Grounding for Text-Based Video Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing efforts in text-based video question answering (TextVideoQA) are criticized for their opaque decisionmaking and heavy reliance on scene-text recognition. In this paper, we propose to study Grounded TextVideoQA by forcing models to answer questions and spatio-temporally localize the relevant scene-text regions, thus decoupling QA from scenetext recognition and promoting research towards interpretable QA. The task has three-fold significance. First, it encourages scene-text evidence versus other short-cuts for answer predictions. Second, it directly accepts scene-text regions as visual answers, thus circumventing the problem of ineffective answer evaluation by stringent string matching. Third, it isolates the challenges inherited in VideoQA and scene-text recognition. This enables the diagnosis of the root causes for failure predictions, e.g., wrong QA or wrong scene-text recognition? To achieve Grounded TextVideoQA, we propose the T2S-QA model that highlights a disentangled temporal-to-spatial contrastive learning strategy for weakly-supervised scene-text grounding and grounded TextVideoQA. To facilitate evaluation, we construct a new dataset ViTXT-GQA which features 52K scene-text bounding boxes within 2.2K temporal segments related to 2K questions and 729 videos. With ViTXT-GQA, we perform extensive experiments and demonstrate the severe limitations of existing techniques in Grounded TextVideoQA. While T2S-QA achieves superior results, the large performance gap with human leaves ample space for improvement. Our further analysis of oracle scene-text inputs posits that the major challenge is scene-text recognition. To advance the research of Grounded TextVideoQA, our dataset and code are at https://github.com/zhousheng97/ViTXT-GQA.git
<div id='section'>Paperid: <span id='pid'>128, <a href='https://arxiv.org/pdf/2409.13431.pdf' target='_blank'>https://arxiv.org/pdf/2409.13431.pdf</a></span>   <span><a href='https://github.com/wzx99/TMIM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixiao Wang, Hongtao Xie, YuXin Wang, Yadong Qu, Fengjun Guo, Pengwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.13431">Leveraging Text Localization for Scene Text Removal via Text-aware Masked Image Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing scene text removal (STR) task suffers from insufficient training data due to the expensive pixel-level labeling. In this paper, we aim to address this issue by introducing a Text-aware Masked Image Modeling algorithm (TMIM), which can pretrain STR models with low-cost text detection labels (e.g., text bounding box). Different from previous pretraining methods that use indirect auxiliary tasks only to enhance the implicit feature extraction ability, our TMIM first enables the STR task to be directly trained in a weakly supervised manner, which explores the STR knowledge explicitly and efficiently. In TMIM, first, a Background Modeling stream is built to learn background generation rules by recovering the masked non-text region. Meanwhile, it provides pseudo STR labels on the masked text region. Second, a Text Erasing stream is proposed to learn from the pseudo labels and equip the model with end-to-end STR ability. Benefiting from the two collaborative streams, our STR model can achieve impressive performance only with the public text detection datasets, which greatly alleviates the limitation of the high-cost STR labels. Experiments demonstrate that our method outperforms other pretrain methods and achieves state-of-the-art performance (37.35 PSNR on SCUT-EnsText). Code will be available at https://github.com/wzx99/TMIM.
<div id='section'>Paperid: <span id='pid'>129, <a href='https://arxiv.org/pdf/2409.10291.pdf' target='_blank'>https://arxiv.org/pdf/2409.10291.pdf</a></span>   <span><a href='https://github.com/mishgon/ape' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mikhail Goncharov, Valentin Samokhin, Eugenia Soboleva, Roman Sokolov, Boris Shirokikh, Mikhail Belyaev, Anvar Kurmukov, Ivan Oseledets
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10291">Anatomical Positional Embeddings</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a self-supervised model producing 3D anatomical positional embeddings (APE) of individual medical image voxels. APE encodes voxels' anatomical closeness, i.e., voxels of the same organ or nearby organs always have closer positional embeddings than the voxels of more distant body parts. In contrast to the existing models of anatomical positional embeddings, our method is able to efficiently produce a map of voxel-wise embeddings for a whole volumetric input image, which makes it an optimal choice for different downstream applications. We train our APE model on 8400 publicly available CT images of abdomen and chest regions. We demonstrate its superior performance compared with the existing models on anatomical landmark retrieval and weakly-supervised few-shot localization of 13 abdominal organs. As a practical application, we show how to cheaply train APE to crop raw CT images to different anatomical regions of interest with 0.99 recall, while reducing the image volume by 10-100 times. The code and the pre-trained APE model are available at https://github.com/mishgon/ape .
<div id='section'>Paperid: <span id='pid'>130, <a href='https://arxiv.org/pdf/2409.09369.pdf' target='_blank'>https://arxiv.org/pdf/2409.09369.pdf</a></span>   <span><a href='https://github.com/liupei101/VLSA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pei Liu, Luping Ji, Jiaxiang Gou, Bo Fu, Mao Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09369">Interpretable Vision-Language Survival Analysis with Ordinal Inductive Bias for Computational Pathology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Histopathology Whole-Slide Images (WSIs) provide an important tool to assess cancer prognosis in computational pathology (CPATH). While existing survival analysis (SA) approaches have made exciting progress, they are generally limited to adopting highly-expressive network architectures and only coarse-grained patient-level labels to learn visual prognostic representations from gigapixel WSIs. Such learning paradigm suffers from critical performance bottlenecks, when facing present scarce training data and standard multi-instance learning (MIL) framework in CPATH. To overcome it, this paper, for the first time, proposes a new Vision-Language-based SA (VLSA) paradigm. Concretely, (1) VLSA is driven by pathology VL foundation models. It no longer relies on high-capability networks and shows the advantage of data efficiency. (2) In vision-end, VLSA encodes textual prognostic prior and then employs it as auxiliary signals to guide the aggregating of visual prognostic features at instance level, thereby compensating for the weak supervision in MIL. Moreover, given the characteristics of SA, we propose i) ordinal survival prompt learning to transform continuous survival labels into textual prompts; and ii) ordinal incidence function as prediction target to make SA compatible with VL-based prediction. Notably, VLSA's predictions can be interpreted intuitively by our Shapley values-based method. The extensive experiments on five datasets confirm the effectiveness of our scheme. Our VLSA could pave a new way for SA in CPATH by offering weakly-supervised MIL an effective means to learn valuable prognostic clues from gigapixel WSIs. Our source code is available at https://github.com/liupei101/VLSA.
<div id='section'>Paperid: <span id='pid'>131, <a href='https://arxiv.org/pdf/2409.01691.pdf' target='_blank'>https://arxiv.org/pdf/2409.01691.pdf</a></span>   <span><a href='https://github.com/CUHK-AIM-Group/SAMTooth' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Liu, Wuyang Li, Cheng Wang, Hui Chen, Yixuan Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01691">When 3D Partial Points Meets SAM: Tooth Point Cloud Segmentation with Sparse Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tooth point cloud segmentation is a fundamental task in many orthodontic applications. Current research mainly focuses on fully supervised learning which demands expensive and tedious manual point-wise annotation. Although recent weakly-supervised alternatives are proposed to use weak labels for 3D segmentation and achieve promising results, they tend to fail when the labels are extremely sparse. Inspired by the powerful promptable segmentation capability of the Segment Anything Model (SAM), we propose a framework named SAMTooth that leverages such capacity to complement the extremely sparse supervision. To automatically generate appropriate point prompts for SAM, we propose a novel Confidence-aware Prompt Generation strategy, where coarse category predictions are aggregated with confidence-aware filtering. Furthermore, to fully exploit the structural and shape clues in SAM's outputs for assisting the 3D feature learning, we advance a Mask-guided Representation Learning that re-projects the generated tooth masks of SAM into 3D space and constrains these points of different teeth to possess distinguished representations. To demonstrate the effectiveness of the framework, we conduct experiments on the public dataset and surprisingly find with only 0.1\% annotations (one point per tooth), our method can surpass recent weakly supervised methods by a large margin, and the performance is even comparable to the recent fully-supervised methods, showcasing the significant potential of applying SAM to 3D perception tasks with sparse labels. Code is available at https://github.com/CUHK-AIM-Group/SAMTooth.
<div id='section'>Paperid: <span id='pid'>132, <a href='https://arxiv.org/pdf/2409.01472.pdf' target='_blank'>https://arxiv.org/pdf/2409.01472.pdf</a></span>   <span><a href='https://github.com/xuanrui-work/WSSSByRec' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuanrui Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01472">Semantic Segmentation from Image Labels by Reconstruction from Structured Decomposition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised image segmentation (WSSS) from image tags remains challenging due to its under-constraint nature. Most mainstream work focus on the extraction of class activation map (CAM) and imposing various additional regularization. Contrary to the mainstream, we propose to frame WSSS as a problem of reconstruction from decomposition of the image using its mask, under which most regularization are embedded implicitly within the framework of the new problem. Our approach has demonstrated promising results on initial experiments, and shown robustness against the problem of background ambiguity. Our code is available at \url{https://github.com/xuanrui-work/WSSSByRec}.
<div id='section'>Paperid: <span id='pid'>133, <a href='https://arxiv.org/pdf/2502.12917.pdf' target='_blank'>https://arxiv.org/pdf/2502.12917.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haicheng Wang, Chen Ju, Weixiong Lin, Chaofan Ma, Shuai Xiao, Ya Zhang, Yanfeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.12917">Contrast-Unity for Partially-Supervised Temporal Sentence Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Temporal sentence grounding aims to detect event timestamps described by the natural language query from given untrimmed videos. The existing fully-supervised setting achieves great results but requires expensive annotation costs; while the weakly-supervised setting adopts cheap labels but performs poorly. To pursue high performance with less annotation costs, this paper introduces an intermediate partially-supervised setting, i.e., only short-clip is available during training. To make full use of partial labels, we specially design one contrast-unity framework, with the two-stage goal of implicit-explicit progressive grounding. In the implicit stage, we align event-query representations at fine granularity using comprehensive quadruple contrastive learning: event-query gather, event-background separation, intra-cluster compactness and inter-cluster separability. Then, high-quality representations bring acceptable grounding pseudo-labels. In the explicit stage, to explicitly optimize grounding objectives, we train one fully-supervised model using obtained pseudo-labels for grounding refinement and denoising. Extensive experiments and thoroughly ablations on Charades-STA and ActivityNet Captions demonstrate the significance of partial supervision, as well as our superior performance.
<div id='section'>Paperid: <span id='pid'>134, <a href='https://arxiv.org/pdf/2505.16294.pdf' target='_blank'>https://arxiv.org/pdf/2505.16294.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufei Yin, Lechao Cheng, Wengang Zhou, Jiajun Deng, Zhou Yu, Houqiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16294">Self-Classification Enhancement and Correction for Weakly Supervised Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, weakly supervised object detection (WSOD) has attracted much attention due to its low labeling cost. The success of recent WSOD models is often ascribed to the two-stage multi-class classification (MCC) task, i.e., multiple instance learning and online classification refinement. Despite achieving non-trivial progresses, these methods overlook potential classification ambiguities between these two MCC tasks and fail to leverage their unique strengths. In this work, we introduce a novel WSOD framework to ameliorate these two issues. For one thing, we propose a self-classification enhancement module that integrates intra-class binary classification (ICBC) to bridge the gap between the two distinct MCC tasks. The ICBC task enhances the network's discrimination between positive and mis-located samples in a class-wise manner and forges a mutually reinforcing relationship with the MCC task. For another, we propose a self-classification correction algorithm during inference, which combines the results of both MCC tasks to effectively reduce the mis-classified predictions. Extensive experiments on the prevalent VOC 2007 & 2012 datasets demonstrate the superior performance of our framework.
<div id='section'>Paperid: <span id='pid'>135, <a href='https://arxiv.org/pdf/2505.04905.pdf' target='_blank'>https://arxiv.org/pdf/2505.04905.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xi Yang, Songsong Duan, Nannan Wang, Xinbo Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04905">Pro2SAM: Mask Prompt to SAM with Grid Points for Weakly Supervised Object Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Object Localization (WSOL), which aims to localize objects by only using image-level labels, has attracted much attention because of its low annotation cost in real applications. Current studies focus on the Class Activation Map (CAM) of CNN and the self-attention map of transformer to identify the region of objects. However, both CAM and self-attention maps can not learn pixel-level fine-grained information on the foreground objects, which hinders the further advance of WSOL. To address this problem, we initiatively leverage the capability of zero-shot generalization and fine-grained segmentation in Segment Anything Model (SAM) to boost the activation of integral object regions. Further, to alleviate the semantic ambiguity issue accrued in single point prompt-based SAM, we propose an innovative mask prompt to SAM (Pro2SAM) network with grid points for WSOL task. First, we devise a Global Token Transformer (GTFormer) to generate a coarse-grained foreground map as a flexible mask prompt, where the GTFormer jointly embeds patch tokens and novel global tokens to learn foreground semantics. Secondly, we deliver grid points as dense prompts into SAM to maximize the probability of foreground mask, which avoids the lack of objects caused by a single point/box prompt. Finally, we propose a pixel-level similarity metric to come true the mask matching from mask prompt to SAM, where the mask with the highest score is viewed as the final localization map. Experiments show that the proposed Pro2SAM achieves state-of-the-art performance on both CUB-200-2011 and ILSVRC, with 84.03\% and 66.85\% Top-1 Loc, respectively.
<div id='section'>Paperid: <span id='pid'>136, <a href='https://arxiv.org/pdf/2506.20923.pdf' target='_blank'>https://arxiv.org/pdf/2506.20923.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinping Zhao, Xinshuo Hu, Zifei Shan, Shouzheng Huang, Yao Zhou, Zetian Sun, Zhenyu Liu, Dongfang Li, Xinyuan Wei, Qian Chen, Youcheng Pan, Yang Xiang, Meishan Zhang, Haofen Wang, Jun Yu, Baotian Hu, Min Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20923">KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose KaLM-Embedding-V2, a versatile and compact embedding model, which achieves impressive performance in general-purpose text embedding tasks by leveraging superior training techniques and data. Our key innovations include: (1) To better align the architecture with representation learning, we remove the causal attention mask and adopt a fully bidirectional transformer with simple yet effective mean-pooling to produce fixed-length embeddings; (2) We employ a multi-stage training pipeline: (i) pre-training on large-scale weakly supervised open-source corpora; (ii) fine-tuning on high-quality retrieval and non-retrieval datasets; and (iii) model-soup parameter averaging for robust generalization. Besides, we introduce a focal-style reweighting mechanism that concentrates learning on difficult samples and an online hard-negative mixing strategy to continuously enrich hard negatives without expensive offline mining; (3) We collect over 20 categories of data for pre-training and 100 categories of data for fine-tuning, to boost both the performance and generalization of the embedding model. Extensive evaluations on the Massive Text Embedding Benchmark (MTEB) Chinese and English show that our model significantly outperforms others of comparable size, and competes with 3x, 14x, 18x, and 26x larger embedding models, setting a new standard for a versatile and compact embedding model with less than 1B parameters.
<div id='section'>Paperid: <span id='pid'>137, <a href='https://arxiv.org/pdf/2506.15757.pdf' target='_blank'>https://arxiv.org/pdf/2506.15757.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruoyu Wang, Tong Yu, Junda Wu, Yao Liu, Julian McAuley, Lina Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.15757">Weakly-supervised VLM-guided Partial Contrastive Learning for Visual Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Language Navigation (VLN) is a fundamental task within the field of Embodied AI, focusing on the ability of agents to navigate complex environments based on natural language instructions. Despite the progress made by existing methods, these methods often present some common challenges. First, they rely on pre-trained backbone models for visual perception, which struggle with the dynamic viewpoints in VLN scenarios. Second, the performance is limited when using pre-trained LLMs or VLMs without fine-tuning, due to the absence of VLN domain knowledge. Third, while fine-tuning LLMs and VLMs can improve results, their computational costs are higher than those without fine-tuning. To address these limitations, we propose Weakly-supervised Partial Contrastive Learning (WPCL), a method that enhances an agent's ability to identify objects from dynamic viewpoints in VLN scenarios by effectively integrating pre-trained VLM knowledge into the perception process, without requiring VLM fine-tuning. Our method enhances the agent's ability to interpret and respond to environmental cues while ensuring computational efficiency. Experimental results have shown that our method outperforms the baseline methods on multiple benchmarks, which validate the effectiveness, robustness and generalizability of our method.
<div id='section'>Paperid: <span id='pid'>138, <a href='https://arxiv.org/pdf/2511.12229.pdf' target='_blank'>https://arxiv.org/pdf/2511.12229.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhipeng Xue, Zhipeng Gao, Tongtong Xu, Xing Hu, Xin Xia, Shanping Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.12229">Actionable Warning Is Not Enough: Recommending Valid Actionable Warnings with Weak Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The use of static analysis tools has gained increasing popularity among developers in the last few years. However, the widespread adoption of static analysis tools is hindered by their high false alarm rates. Previous studies have introduced the concept of actionable warnings and built a machine-learning method to distinguish actionable warnings from false alarms. However, according to our empirical observation, the current assumption used for actionable warning(s) collection is rather shaky and inaccurate, leading to a large number of invalid actionable warnings. To address this problem, in this study, we build the first large actionable warning dataset by mining 68,274 reversions from Top-500 GitHub C repositories, we then take one step further by assigning each actionable warning a weak label regarding its likelihood of being a real bug. Following that, we propose a two-stage framework called ACWRecommender to automatically recommend the actionable warnings with high probability to be real bugs (AWHB). Our approach warms up the pre-trained model UniXcoder by identifying actionable warnings task (coarse-grained detection stage) and rerank AWHB to the top by weakly supervised learning (fine-grained reranking stage). Experimental results show that our proposed model outperforms several baselines by a large margin in terms of nDCG and MRR for AWHB recommendation. Moreover, we ran our tool on 6 randomly selected projects and manually checked the top-ranked warnings from 2,197 reported warnings, we reported top-10 recommended warnings to developers, 27 of them were already confirmed by developers as real bugs. Developers can quickly find real bugs among the massive amount of reported warnings, which verifies the practical usage of our tool.
<div id='section'>Paperid: <span id='pid'>139, <a href='https://arxiv.org/pdf/2505.06557.pdf' target='_blank'>https://arxiv.org/pdf/2505.06557.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lu Dong, Haiyu Zhang, Hongjie Zhang, Yifei Huang, Zhen-Hua Ling, Yu Qiao, Limin Wang, Yali Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06557">Weakly Supervised Temporal Sentence Grounding via Positive Sample Mining</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The task of weakly supervised temporal sentence grounding (WSTSG) aims to detect temporal intervals corresponding to a language description from untrimmed videos with only video-level video-language correspondence. For an anchor sample, most existing approaches generate negative samples either from other videos or within the same video for contrastive learning. However, some training samples are highly similar to the anchor sample, directly regarding them as negative samples leads to difficulties for optimization and ignores the correlations between these similar samples and the anchor sample. To address this, we propose Positive Sample Mining (PSM), a novel framework that mines positive samples from the training set to provide more discriminative supervision. Specifically, for a given anchor sample, we partition the remaining training set into semantically similar and dissimilar subsets based on the similarity of their text queries. To effectively leverage these correlations, we introduce a PSM-guided contrastive loss to ensure that the anchor proposal is closer to similar samples and further from dissimilar ones. Additionally, we design a PSM-guided rank loss to ensure that similar samples are closer to the anchor proposal than to the negative intra-video proposal, aiming to distinguish the anchor proposal and the negative intra-video proposal. Experiments on the WSTSG and grounded VideoQA tasks demonstrate the effectiveness and superiority of our method.
<div id='section'>Paperid: <span id='pid'>140, <a href='https://arxiv.org/pdf/2501.12632.pdf' target='_blank'>https://arxiv.org/pdf/2501.12632.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shakeeb Murtaza, Soufiane Belharbi, Marco Pedersoli, Eric Granger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.12632">TeD-Loc: Text Distillation for Weakly Supervised Object Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised object localization (WSOL) using classification models trained with only image-class labels remains an important challenge in computer vision. Given their reliance on classification objectives, traditional WSOL methods like class activation mapping focus on the most discriminative object parts, often missing the full spatial extent. In contrast, recent WSOL methods based on vision-language models like CLIP require ground truth classes or external classifiers to produce a localization map, limiting their deployment in downstream tasks. Moreover, methods like GenPromp attempt to address these issues but introduce considerable complexity due to their reliance on conditional denoising processes and intricate prompt learning. This paper introduces Text Distillation for Localization (TeD-Loc), an approach that directly distills knowledge from CLIP text embeddings into the model backbone and produces patch-level localization. Multiple instance learning of these image patches allows for accurate localization and classification using one model without requiring external classifiers. Such integration of textual and visual modalities addresses the longstanding challenge of achieving accurate localization and classification concurrently, as WSOL methods in the literature typically converge at different epochs. Extensive experiments show that leveraging text embeddings and localization cues provides a cost-effective WSOL model. TeD-Loc improves Top-1 LOC accuracy over state-of-the-art models by about 5% on both CUB and ILSVRC datasets, while significantly reducing computational complexity compared to GenPromp.
<div id='section'>Paperid: <span id='pid'>141, <a href='https://arxiv.org/pdf/2509.26281.pdf' target='_blank'>https://arxiv.org/pdf/2509.26281.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Teng Zhang, Ziqian Fan, Mingxin Liu, Xin Zhang, Xudong Lu, Wentong Li, Yue Zhou, Yi Yu, Xiang Li, Junchi Yan, Xue Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26281">Point2RBox-v3: Self-Bootstrapping from Point Annotations via Integrated Pseudo-Label Refinement and Utilization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Driven by the growing need for Oriented Object Detection (OOD), learning from point annotations under a weakly-supervised framework has emerged as a promising alternative to costly and laborious manual labeling. In this paper, we discuss two deficiencies in existing point-supervised methods: inefficient utilization and poor quality of pseudo labels. Therefore, we present Point2RBox-v3. At the core are two principles: 1) Progressive Label Assignment (PLA). It dynamically estimates instance sizes in a coarse yet intelligent manner at different stages of the training process, enabling the use of label assignment methods. 2) Prior-Guided Dynamic Mask Loss (PGDM-Loss). It is an enhancement of the Voronoi Watershed Loss from Point2RBox-v2, which overcomes the shortcomings of Watershed in its poor performance in sparse scenes and SAM's poor performance in dense scenes. To our knowledge, Point2RBox-v3 is the first model to employ dynamic pseudo labels for label assignment, and it creatively complements the advantages of SAM model with the watershed algorithm, which achieves excellent performance in both sparse and dense scenes. Our solution gives competitive performance, especially in scenarios with large variations in object size or sparse object occurrences: 66.09%/56.86%/41.28%/46.40%/19.60%/45.96% on DOTA-v1.0/DOTA-v1.5/DOTA-v2.0/DIOR/STAR/RSAR.
<div id='section'>Paperid: <span id='pid'>142, <a href='https://arxiv.org/pdf/2410.13786.pdf' target='_blank'>https://arxiv.org/pdf/2410.13786.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fengqi Liu, Hexiang Wang, Jingyu Gong, Ran Yi, Qianyu Zhou, Xuequan Lu, Jiangbo Lu, Lizhuang Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13786">Emphasizing Semantic Consistency of Salient Posture for Speech-Driven Gesture Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech-driven gesture generation aims at synthesizing a gesture sequence synchronized with the input speech signal. Previous methods leverage neural networks to directly map a compact audio representation to the gesture sequence, ignoring the semantic association of different modalities and failing to deal with salient gestures. In this paper, we propose a novel speech-driven gesture generation method by emphasizing the semantic consistency of salient posture. Specifically, we first learn a joint manifold space for the individual representation of audio and body pose to exploit the inherent semantic association between two modalities, and propose to enforce semantic consistency via a consistency loss. Furthermore, we emphasize the semantic consistency of salient postures by introducing a weakly-supervised detector to identify salient postures, and reweighting the consistency loss to focus more on learning the correspondence between salient postures and the high-level semantics of speech content. In addition, we propose to extract audio features dedicated to facial expression and body gesture separately, and design separate branches for face and body gesture synthesis. Extensive experimental results demonstrate the superiority of our method over the state-of-the-art approaches.
<div id='section'>Paperid: <span id='pid'>143, <a href='https://arxiv.org/pdf/2512.02224.pdf' target='_blank'>https://arxiv.org/pdf/2512.02224.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Feng, Tianhao Peng, Fan Zhang, David Bull
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.02224">Towards Unified Video Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent works in video quality assessment (VQA) typically employ monolithic models that typically predict a single quality score for each test video. These approaches cannot provide diagnostic, interpretable feedback, offering little insight into why the video quality is degraded. Most of them are also specialized, format-specific metrics rather than truly ``generic" solutions, as they are designed to learn a compromised representation from disparate perceptual domains. To address these limitations, this paper proposes Unified-VQA, a framework that provides a single, unified quality model applicable to various distortion types within multiple video formats by recasting generic VQA as a Diagnostic Mixture-of-Experts (MoE) problem. Unified-VQA employs multiple ``perceptual experts'' dedicated to distinct perceptual domains. A novel multi-proxy expert training strategy is designed to optimize each expert using a ranking-inspired loss, guided by the most suitable proxy metric for its domain. We also integrated a diagnostic multi-task head into this framework to generate a global quality score and an interpretable multi-dimensional artifact vector, which is optimized using a weakly-supervised learning strategy, leveraging the known properties of the large-scale training database generated for this work. With static model parameters (without retraining or fine-tuning), Unified-VQA demonstrates consistent and superior performance compared to over 18 benchmark methods for both generic VQA and diagnostic artifact detection tasks across 17 databases containing diverse streaming artifacts in HD, UHD, HDR and HFR formats. This work represents an important step towards practical, actionable, and interpretable video quality assessment.
<div id='section'>Paperid: <span id='pid'>144, <a href='https://arxiv.org/pdf/2510.04091.pdf' target='_blank'>https://arxiv.org/pdf/2510.04091.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Wang, Tianhao Ma, Ming-Kun Xie, Gang Niu, Masashi Sugiyama
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04091">Rethinking Consistent Multi-Label Classification under Inexact Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partial multi-label learning and complementary multi-label learning are two popular weakly supervised multi-label classification paradigms that aim to alleviate the high annotation costs of collecting precisely annotated multi-label data. In partial multi-label learning, each instance is annotated with a candidate label set, among which only some labels are relevant; in complementary multi-label learning, each instance is annotated with complementary labels indicating the classes to which the instance does not belong. Existing consistent approaches for the two paradigms either require accurate estimation of the generation process of candidate or complementary labels or assume a uniform distribution to eliminate the estimation problem. However, both conditions are usually difficult to satisfy in real-world scenarios. In this paper, we propose consistent approaches that do not rely on the aforementioned conditions to handle both problems in a unified way. Specifically, we propose two unbiased risk estimators based on first- and second-order strategies. Theoretically, we prove consistency w.r.t. two widely used multi-label classification evaluation metrics and derive convergence rates for the estimation errors of the proposed risk estimators. Empirically, extensive experimental results validate the effectiveness of our proposed approaches against state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>145, <a href='https://arxiv.org/pdf/2509.24228.pdf' target='_blank'>https://arxiv.org/pdf/2509.24228.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Wang, Dong-Dong Wu, Ming Li, Jingxiong Zhang, Gang Niu, Masashi Sugiyama
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24228">Accessible, Realistic, and Fair Evaluation of Positive-Unlabeled Learning Algorithms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Positive-unlabeled (PU) learning is a weakly supervised binary classification problem, in which the goal is to learn a binary classifier from only positive and unlabeled data, without access to negative data. In recent years, many PU learning algorithms have been developed to improve model performance. However, experimental settings are highly inconsistent, making it difficult to identify which algorithm performs better. In this paper, we propose the first PU learning benchmark to systematically compare PU learning algorithms. During our implementation, we identify subtle yet critical factors that affect the realistic and fair evaluation of PU learning algorithms. On the one hand, many PU learning algorithms rely on a validation set that includes negative data for model selection. This is unrealistic in traditional PU learning settings, where no negative data are available. To handle this problem, we systematically investigate model selection criteria for PU learning. On the other hand, the problem settings and solutions of PU learning have different families, i.e., the one-sample and two-sample settings. However, existing evaluation protocols are heavily biased towards the one-sample setting and neglect the significant difference between them. We identify the internal label shift problem of unlabeled training data for the one-sample setting and propose a simple yet effective calibration approach to ensure fair comparisons within and across families. We hope our framework will provide an accessible, realistic, and fair environment for evaluating PU learning algorithms in the future.
<div id='section'>Paperid: <span id='pid'>146, <a href='https://arxiv.org/pdf/2502.10184.pdf' target='_blank'>https://arxiv.org/pdf/2502.10184.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Wang, Dong-Dong Wu, Jindong Wang, Gang Niu, Min-Ling Zhang, Masashi Sugiyama
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10184">Realistic Evaluation of Deep Partial-Label Learning Algorithms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partial-label learning (PLL) is a weakly supervised learning problem in which each example is associated with multiple candidate labels and only one is the true label. In recent years, many deep PLL algorithms have been developed to improve model performance. However, we find that some early developed algorithms are often underestimated and can outperform many later algorithms with complicated designs. In this paper, we delve into the empirical perspective of PLL and identify several critical but previously overlooked issues. First, model selection for PLL is non-trivial, but has never been systematically studied. Second, the experimental settings are highly inconsistent, making it difficult to evaluate the effectiveness of the algorithms. Third, there is a lack of real-world image datasets that can be compatible with modern network architectures. Based on these findings, we propose PLENCH, the first Partial-Label learning bENCHmark to systematically compare state-of-the-art deep PLL algorithms. We investigate the model selection problem for PLL for the first time, and propose novel model selection criteria with theoretical guarantees. We also create Partial-Label CIFAR-10 (PLCIFAR10), an image dataset of human-annotated partial labels collected from Amazon Mechanical Turk, to provide a testbed for evaluating the performance of PLL algorithms in more realistic scenarios. Researchers can quickly and conveniently perform a comprehensive and fair evaluation and verify the effectiveness of newly developed algorithms based on PLENCH. We hope that PLENCH will facilitate standardized, fair, and practical evaluation of PLL algorithms in the future.
<div id='section'>Paperid: <span id='pid'>147, <a href='https://arxiv.org/pdf/2505.04339.pdf' target='_blank'>https://arxiv.org/pdf/2505.04339.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Peng, Xiang Huang, Shuo Sun, Ruitong Zhang, Philip S. Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04339">Adaptive and Robust DBSCAN with Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>DBSCAN, a well-known density-based clustering algorithm, has gained widespread popularity and usage due to its effectiveness in identifying clusters of arbitrary shapes and handling noisy data. However, it encounters challenges in producing satisfactory cluster results when confronted with datasets of varying density scales, a common scenario in real-world applications. In this paper, we propose a novel Adaptive and Robust DBSCAN with Multi-agent Reinforcement Learning cluster framework, namely AR-DBSCAN. First, we model the initial dataset as a two-level encoding tree and categorize the data vertices into distinct density partitions according to the information uncertainty determined in the encoding tree. Each partition is then assigned to an agent to find the best clustering parameters without manual assistance. The allocation is density-adaptive, enabling AR-DBSCAN to effectively handle diverse density distributions within the dataset by utilizing distinct agents for different partitions. Second, a multi-agent deep reinforcement learning guided automatic parameter searching process is designed. The process of adjusting the parameter search direction by perceiving the clustering environment is modeled as a Markov decision process. Using a weakly-supervised reward training policy network, each agent adaptively learns the optimal clustering parameters by interacting with the clusters. Third, a recursive search mechanism adaptable to the data's scale is presented, enabling efficient and controlled exploration of large parameter spaces. Extensive experiments are conducted on nine artificial datasets and a real-world dataset. The results of offline and online tasks show that AR-DBSCAN not only improves clustering accuracy by up to 144.1% and 175.3% in the NMI and ARI metrics, respectively, but also is capable of robustly finding dominant parameters.
<div id='section'>Paperid: <span id='pid'>148, <a href='https://arxiv.org/pdf/2504.00844.pdf' target='_blank'>https://arxiv.org/pdf/2504.00844.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdelrahman Elskhawy, Mengze Li, Nassir Navab, Benjamin Busam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.00844">PRISM-0: A Predicate-Rich Scene Graph Generation Framework for Zero-Shot Open-Vocabulary Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In Scene Graph Generation (SGG), structured representations are extracted from visual inputs as object nodes and connecting predicates, enabling image-based reasoning for diverse downstream tasks. While fully supervised SGG has improved steadily, it suffers from training bias due to limited curated data and long-tail predicate distributions, leading to poor predicate diversity and degraded downstream performance. We present PRISM-0, a zero-shot open-vocabulary SGG framework that leverages foundation models in a bottom-up pipeline to capture a broad spectrum of predicates. Detected object pairs are filtered, described via a Vision-Language Model (VLM), and processed by a Large Language Model (LLM) to generate fine- and coarse-grained predicates, which are then validated by a Visual Question Answering (VQA) model. PRISM-0 modular, dataset-independent design enriches existing SGG datasets such as Visual Genome and produces diverse, unbiased graphs. While operating entirely in a zero-shot setting, PRISM-0 achieves performance on par with state-of-the-art weakly-supervised models on SGG benchmarks and even state-of-the-art supervised methods in tasks such as Sentence-to-Graph Retrieval.
<div id='section'>Paperid: <span id='pid'>149, <a href='https://arxiv.org/pdf/2505.20106.pdf' target='_blank'>https://arxiv.org/pdf/2505.20106.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zuyao Chen, Jinlin Wu, Zhen Lei, Chang Wen Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20106">From Data to Modeling: Fully Open-vocabulary Scene Graph Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present OvSGTR, a novel transformer-based framework for fully open-vocabulary scene graph generation that overcomes the limitations of traditional closed-set models. Conventional methods restrict both object and relationship recognition to a fixed vocabulary, hindering their applicability to real-world scenarios where novel concepts frequently emerge. In contrast, our approach jointly predicts objects (nodes) and their inter-relationships (edges) beyond predefined categories. OvSGTR leverages a DETR-like architecture featuring a frozen image backbone and text encoder to extract high-quality visual and semantic features, which are then fused via a transformer decoder for end-to-end scene graph prediction. To enrich the model's understanding of complex visual relations, we propose a relation-aware pre-training strategy that synthesizes scene graph annotations in a weakly supervised manner. Specifically, we investigate three pipelines--scene parser-based, LLM-based, and multimodal LLM-based--to generate transferable supervision signals with minimal manual annotation. Furthermore, we address the common issue of catastrophic forgetting in open-vocabulary settings by incorporating a visual-concept retention mechanism coupled with a knowledge distillation strategy, ensuring that the model retains rich semantic cues during fine-tuning. Extensive experiments on the VG150 benchmark demonstrate that OvSGTR achieves state-of-the-art performance across multiple settings, including closed-set, open-vocabulary object detection-based, relation-based, and fully open-vocabulary scenarios. Our results highlight the promise of large-scale relation-aware pre-training and transformer architectures for advancing scene graph generation towards more generalized and reliable visual understanding.
<div id='section'>Paperid: <span id='pid'>150, <a href='https://arxiv.org/pdf/2411.16803.pdf' target='_blank'>https://arxiv.org/pdf/2411.16803.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marta Ligero, Tim Lenz, Georg WÃ¶lflein, Omar S. M. El Nahhas, Daniel Truhn, Jakob Nikolas Kather
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16803">Abnormality-Driven Representation Learning for Radiology Imaging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To date, the most common approach for radiology deep learning pipelines is the use of end-to-end 3D networks based on models pre-trained on other tasks, followed by fine-tuning on the task at hand. In contrast, adjacent medical fields such as pathology, which focus on 2D images, have effectively adopted task-agnostic foundational models based on self-supervised learning (SSL), combined with weakly-supervised deep learning (DL). However, the field of radiology still lacks task-agnostic representation models due to the computational and data demands of 3D imaging and the anatomical complexity inherent to radiology scans. To address this gap, we propose CLEAR, a framework for radiology images that uses extracted embeddings from 2D slices along with attention-based aggregation for efficiently predicting clinical endpoints. As part of this framework, we introduce lesion-enhanced contrastive learning (LeCL), a novel approach to obtain visual representations driven by abnormalities in 2D axial slices across different locations of the CT scans. Specifically, we trained single-domain contrastive learning approaches using three different architectures: Vision Transformers, Vision State Space Models and Gated Convolutional Neural Networks. We evaluate our approach across three clinical tasks: tumor lesion location, lung disease detection, and patient staging, benchmarking against four state-of-the-art foundation models, including BiomedCLIP. Our findings demonstrate that CLEAR using representations learned through LeCL, outperforms existing foundation models, while being substantially more compute- and data-efficient.
<div id='section'>Paperid: <span id='pid'>151, <a href='https://arxiv.org/pdf/2503.20685.pdf' target='_blank'>https://arxiv.org/pdf/2503.20685.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhao Huang, Ao Chang, Haoran Dou, Xing Tao, Xinrui Zhou, Yan Cao, Ruobing Huang, Alejandro F Frangi, Lingyun Bao, Xin Yang, Dong Ni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20685">Flip Learning: Weakly Supervised Erase to Segment Nodules in Breast Ultrasound</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate segmentation of nodules in both 2D breast ultrasound (BUS) and 3D automated breast ultrasound (ABUS) is crucial for clinical diagnosis and treatment planning. Therefore, developing an automated system for nodule segmentation can enhance user independence and expedite clinical analysis. Unlike fully-supervised learning, weakly-supervised segmentation (WSS) can streamline the laborious and intricate annotation process. However, current WSS methods face challenges in achieving precise nodule segmentation, as many of them depend on inaccurate activation maps or inefficient pseudo-mask generation algorithms. In this study, we introduce a novel multi-agent reinforcement learning-based WSS framework called Flip Learning, which relies solely on 2D/3D boxes for accurate segmentation. Specifically, multiple agents are employed to erase the target from the box to facilitate classification tag flipping, with the erased region serving as the predicted segmentation mask. The key contributions of this research are as follows: (1) Adoption of a superpixel/supervoxel-based approach to encode the standardized environment, capturing boundary priors and expediting the learning process. (2) Introduction of three meticulously designed rewards, comprising a classification score reward and two intensity distribution rewards, to steer the agents' erasing process precisely, thereby avoiding both under- and over-segmentation. (3) Implementation of a progressive curriculum learning strategy to enable agents to interact with the environment in a progressively challenging manner, thereby enhancing learning efficiency. Extensively validated on the large in-house BUS and ABUS datasets, our Flip Learning method outperforms state-of-the-art WSS methods and foundation models, and achieves comparable performance as fully-supervised learning algorithms.
<div id='section'>Paperid: <span id='pid'>152, <a href='https://arxiv.org/pdf/2601.06222.pdf' target='_blank'>https://arxiv.org/pdf/2601.06222.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinghao Wang, Changtao Miao, Dianmo Sheng, Tao Gong, Qi Chu, Nenghai Yu, Quanchen Zou, Deyue Zhang, Xiangzheng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.06222">SAPL: Semantic-Agnostic Prompt Learning in CLIP for Weakly Supervised Image Manipulation Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Malicious image manipulation threatens public safety and requires efficient localization methods. Existing approaches depend on costly pixel-level annotations which make training expensive. Existing weakly supervised methods rely only on image-level binary labels and focus on global classification, often overlooking local edge cues that are critical for precise localization. We observe that feature variations at manipulated boundaries are substantially larger than in interior regions. To address this gap, we propose Semantic-Agnostic Prompt Learning (SAPL) in CLIP, which learns text prompts that intentionally encode non-semantic, boundary-centric cues so that CLIPs multimodal similarity highlights manipulation edges rather than high-level object semantics. SAPL combines two complementary modules Edge-aware Contextual Prompt Learning (ECPL) and Hierarchical Edge Contrastive Learning (HECL) to exploit edge information in both textual and visual spaces. The proposed ECPL leverages edge-enhanced image features to generate learnable textual prompts via an attention mechanism, embedding semantic-irrelevant information into text features, to guide CLIP focusing on manipulation edges. The proposed HECL extract genuine and manipulated edge patches, and utilize contrastive learning to boost the discrimination between genuine edge patches and manipulated edge patches. Finally, we predict the manipulated regions from the similarity map after processing. Extensive experiments on multiple public benchmarks demonstrate that SAPL significantly outperforms existing approaches, achieving state-of-the-art localization performance.
<div id='section'>Paperid: <span id='pid'>153, <a href='https://arxiv.org/pdf/2510.10111.pdf' target='_blank'>https://arxiv.org/pdf/2510.10111.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Chen, Bin Liu, Changtao Miao, Xinghao Wang, Yi Li, Tao Gong, Qi Chu, Nenghai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10111">Training-Free In-Context Forensic Chain for Image Manipulation Detection and Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advances in image tampering pose serious security threats, underscoring the need for effective image manipulation localization (IML). While supervised IML achieves strong performance, it depends on costly pixel-level annotations. Existing weakly supervised or training-free alternatives often underperform and lack interpretability. We propose the In-Context Forensic Chain (ICFC), a training-free framework that leverages multi-modal large language models (MLLMs) for interpretable IML tasks. ICFC integrates an objectified rule construction with adaptive filtering to build a reliable knowledge base and a multi-step progressive reasoning pipeline that mirrors expert forensic workflows from coarse proposals to fine-grained forensics results. This design enables systematic exploitation of MLLM reasoning for image-level classification, pixel-level localization, and text-level interpretability. Across multiple benchmarks, ICFC not only surpasses state-of-the-art training-free methods but also achieves competitive or superior performance compared to weakly and fully supervised approaches.
<div id='section'>Paperid: <span id='pid'>154, <a href='https://arxiv.org/pdf/2503.20294.pdf' target='_blank'>https://arxiv.org/pdf/2503.20294.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinghao Wang, Tao Gong, Qi Chu, Bin Liu, Nenghai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20294">Context-Aware Weakly Supervised Image Manipulation Localization with SAM Refinement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Malicious image manipulation poses societal risks, increasing the importance of effective image manipulation detection methods. Recent approaches in image manipulation detection have largely been driven by fully supervised approaches, which require labor-intensive pixel-level annotations. Thus, it is essential to explore weakly supervised image manipulation localization methods that only require image-level binary labels for training. However, existing weakly supervised image manipulation methods overlook the importance of edge information for accurate localization, leading to suboptimal localization performance. To address this, we propose a Context-Aware Boundary Localization (CABL) module to aggregate boundary features and learn context-inconsistency for localizing manipulated areas. Furthermore, by leveraging Class Activation Mapping (CAM) and Segment Anything Model (SAM), we introduce the CAM-Guided SAM Refinement (CGSR) module to generate more accurate manipulation localization maps. By integrating two modules, we present a novel weakly supervised framework based on a dual-branch Transformer-CNN architecture. Our method achieves outstanding localization performance across multiple datasets.
<div id='section'>Paperid: <span id='pid'>155, <a href='https://arxiv.org/pdf/2509.26004.pdf' target='_blank'>https://arxiv.org/pdf/2509.26004.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicola Messina, Rosario Leonardi, Luca Ciampi, Fabio Carrara, Giovanni Maria Farinella, Fabrizio Falchi, Antonino Furnari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26004">Learning Egocentric In-Hand Object Segmentation through Weak Supervision from Human Narrations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pixel-level recognition of objects manipulated by the user from egocentric images enables key applications spanning assistive technologies, industrial safety, and activity monitoring. However, progress in this area is currently hindered by the scarcity of annotated datasets, as existing approaches rely on costly manual labels. In this paper, we propose to learn human-object interaction detection leveraging narrations -- natural language descriptions of the actions performed by the camera wearer which contain clues about manipulated objects (e.g., "I am pouring vegetables from the chopping board to the pan"). Narrations provide a form of weak supervision that is cheap to acquire and readily available in state-of-the-art egocentric datasets. We introduce Narration-Supervised in-Hand Object Segmentation (NS-iHOS), a novel task where models have to learn to segment in-hand objects by learning from natural-language narrations. Narrations are then not employed at inference time. We showcase the potential of the task by proposing Weakly-Supervised In-hand Object Segmentation from Human Narrations (WISH), an end-to-end model distilling knowledge from narrations to learn plausible hand-object associations and enable in-hand object segmentation without using narrations at test time. We benchmark WISH against different baselines based on open-vocabulary object detectors and vision-language models, showing the superiority of its design. Experiments on EPIC-Kitchens and Ego4D show that WISH surpasses all baselines, recovering more than 50% of the performance of fully supervised methods, without employing fine-grained pixel-wise annotations.
<div id='section'>Paperid: <span id='pid'>156, <a href='https://arxiv.org/pdf/2409.11223.pdf' target='_blank'>https://arxiv.org/pdf/2409.11223.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuta Kaneko, Abu Saleh Musa Miah, Najmul Hassan, Hyoun-Sup Lee, Si-Woong Jang, Jungpil Shin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11223">Multimodal Attention-Enhanced Feature Fusion-based Weekly Supervised Anomaly Violence Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised video anomaly detection (WS-VAD) is a crucial area in computer vision for developing intelligent surveillance systems. This system uses three feature streams: RGB video, optical flow, and audio signals, where each stream extracts complementary spatial and temporal features using an enhanced attention module to improve detection accuracy and robustness. In the first stream, we employed an attention-based, multi-stage feature enhancement approach to improve spatial and temporal features from the RGB video where the first stage consists of a ViT-based CLIP module, with top-k features concatenated in parallel with I3D and Temporal Contextual Aggregation (TCA) based rich spatiotemporal features. The second stage effectively captures temporal dependencies using the Uncertainty-Regulated Dual Memory Units (UR-DMU) model, which learns representations of normal and abnormal data simultaneously, and the third stage is employed to select the most relevant spatiotemporal features. The second stream extracted enhanced attention-based spatiotemporal features from the flow data modality-based feature by taking advantage of the integration of the deep learning and attention module. The audio stream captures auditory cues using an attention module integrated with the VGGish model, aiming to detect anomalies based on sound patterns. These streams enrich the model by incorporating motion and audio signals often indicative of abnormal events undetectable through visual analysis alone. The concatenation of the multimodal fusion leverages the strengths of each modality, resulting in a comprehensive feature set that significantly improves anomaly detection accuracy and robustness across three datasets. The extensive experiment and high performance with the three benchmark datasets proved the effectiveness of the proposed system over the existing state-of-the-art system.
<div id='section'>Paperid: <span id='pid'>157, <a href='https://arxiv.org/pdf/2502.09967.pdf' target='_blank'>https://arxiv.org/pdf/2502.09967.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuming Wang, Yihao Zheng, Jiarui Li, Yaofei Wu, Yan Huang, Zun Li, Lifang Wu, Liang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09967">VicKAM: Visual Conceptual Knowledge Guided Action Map for Weakly Supervised Group Activity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing weakly supervised group activity recognition methods rely on object detectors or attention mechanisms to capture key areas automatically. However, they overlook the semantic information associated with captured areas, which may adversely affect the recognition performance. In this paper, we propose a novel framework named Visual Conceptual Knowledge Guided Action Map (VicKAM) which effectively captures the locations of individual actions and integrates them with action semantics for weakly supervised group activity recognition.It generates individual action prototypes from training set as visual conceptual knowledge to bridge action semantics and visual representations. Guided by this knowledge, VicKAM produces action maps that indicate the likelihood of each action occurring at various locations, based on image correlation theorem. It further augments individual action maps using group activity related statistical information, representing individual action distribution under different group activities, to establish connections between action maps and specific group activities. The augmented action map is incorporated with action semantic representations for group activity recognition.Extensive experiments on two public benchmarks, the Volleyball and the NBA datasets, demonstrate the effectiveness of our proposed method, even in cases of limited training data. The code will be released later.
<div id='section'>Paperid: <span id='pid'>158, <a href='https://arxiv.org/pdf/2501.13584.pdf' target='_blank'>https://arxiv.org/pdf/2501.13584.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Wang, Mingxuan Xia, Chang Yao, Lei Feng, Junbo Zhao, Gang Chen, Haobo Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13584">Towards Robust Incremental Learning under Ambiguous Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional Incremental Learning (IL) targets to handle sequential fully-supervised learning problems where novel classes emerge from time to time. However, due to inherent annotation uncertainty and ambiguity, collecting high-quality annotated data in a dynamic learning system can be extremely expensive. To mitigate this problem, we propose a novel weakly-supervised learning paradigm called Incremental Partial Label Learning (IPLL), where the sequentially arrived data relate to a set of candidate labels rather than the ground truth. Technically, we develop the Prototype-Guided Disambiguation and Replay Algorithm (PGDR) which leverages the class prototypes as a proxy to mitigate two intertwined challenges in IPLL, i.e., label ambiguity and catastrophic forgetting. To handle the former, PGDR encapsulates a momentum-based pseudo-labeling algorithm along with prototype-guided initialization, resulting in a balanced perception of classes. To alleviate forgetting, we develop a memory replay technique that collects well-disambiguated samples while maintaining representativeness and diversity. By jointly distilling knowledge from curated memory data, our framework exhibits a great disambiguation ability for samples of new tasks and achieves less forgetting of knowledge. Extensive experiments demonstrate that PGDR achieves superior
<div id='section'>Paperid: <span id='pid'>159, <a href='https://arxiv.org/pdf/2510.14532.pdf' target='_blank'>https://arxiv.org/pdf/2510.14532.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinrui Huang, Fan Xiao, Dongming He, Anqi Gao, Dandan Li, Xiaofan Zhang, Shaoting Zhang, Xudong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.14532">Towards Generalist Intelligence in Dentistry: Vision Foundation Models for Oral and Maxillofacial Radiology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Oral and maxillofacial radiology plays a vital role in dental healthcare, but radiographic image interpretation is limited by a shortage of trained professionals. While AI approaches have shown promise, existing dental AI systems are restricted by their single-modality focus, task-specific design, and reliance on costly labeled data, hindering their generalization across diverse clinical scenarios. To address these challenges, we introduce DentVFM, the first family of vision foundation models (VFMs) designed for dentistry. DentVFM generates task-agnostic visual representations for a wide range of dental applications and uses self-supervised learning on DentVista, a large curated dental imaging dataset with approximately 1.6 million multi-modal radiographic images from various medical centers. DentVFM includes 2D and 3D variants based on the Vision Transformer (ViT) architecture. To address gaps in dental intelligence assessment and benchmarks, we introduce DentBench, a comprehensive benchmark covering eight dental subspecialties, more diseases, imaging modalities, and a wide geographical distribution. DentVFM shows impressive generalist intelligence, demonstrating robust generalization to diverse dental tasks, such as disease diagnosis, treatment analysis, biomarker identification, and anatomical landmark detection and segmentation. Experimental results indicate DentVFM significantly outperforms supervised, self-supervised, and weakly supervised baselines, offering superior generalization, label efficiency, and scalability. Additionally, DentVFM enables cross-modality diagnostics, providing more reliable results than experienced dentists in situations where conventional imaging is unavailable. DentVFM sets a new paradigm for dental AI, offering a scalable, adaptable, and label-efficient model to improve intelligent dental healthcare and address critical gaps in global oral healthcare.
<div id='section'>Paperid: <span id='pid'>160, <a href='https://arxiv.org/pdf/2412.19650.pdf' target='_blank'>https://arxiv.org/pdf/2412.19650.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongxing Xu, Feilong Tang, Zhe Chen, Yingxue Su, Zhiyi Zhao, Ge Zhang, Jionglong Su, Zongyuan Ge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19650">Toward Modality Gap: Vision Prototype Learning for Weakly-supervised Semantic Segmentation with CLIP</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The application of Contrastive Language-Image Pre-training (CLIP) in Weakly Supervised Semantic Segmentation (WSSS) research powerful cross-modal semantic understanding capabilities. Existing methods attempt to optimize input text prompts for improved alignment of images and text, by finely adjusting text prototypes to facilitate semantic matching. Nevertheless, given the modality gap between text and vision spaces, the text prototypes employed by these methods have not effectively established a close correspondence with pixel-level vision features. In this work, our theoretical analysis indicates that the inherent modality gap results in misalignment of text and region features, and that this gap cannot be sufficiently reduced by minimizing contrast loss in CLIP. To mitigate the impact of the modality gap, we propose a Vision Prototype Learning (VPL) framework, by introducing more representative vision prototypes. The core of this framework is to learn class-specific vision prototypes in vision space with the help of text prototypes, for capturing high-quality localization maps. Moreover, we propose a regional semantic contrast module that contrasts regions embedding with corresponding prototypes, leading to more comprehensive and robust feature learning. Experimental results show that our proposed framework achieves state-of-the-art performance on two benchmark datasets.
<div id='section'>Paperid: <span id='pid'>161, <a href='https://arxiv.org/pdf/2511.19527.pdf' target='_blank'>https://arxiv.org/pdf/2511.19527.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyu Lyu, Thomas Monninger, Julie Stephany Berrio Perez, Mao Shan, Zhenxing Ming, Stewart Worrall
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19527">MapRF: Weakly Supervised Online HD Map Construction via NeRF-Guided Self-Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous driving systems benefit from high-definition (HD) maps that provide critical information about road infrastructure. The online construction of HD maps offers a scalable approach to generate local maps from on-board sensors. However, existing methods typically rely on costly 3D map annotations for training, which limits their generalization and scalability across diverse driving environments. In this work, we propose MapRF, a weakly supervised framework that learns to construct 3D maps using only 2D image labels. To generate high-quality pseudo labels, we introduce a novel Neural Radiance Fields (NeRF) module conditioned on map predictions, which reconstructs view-consistent 3D geometry and semantics. These pseudo labels are then iteratively used to refine the map network in a self-training manner, enabling progressive improvement without additional supervision. Furthermore, to mitigate error accumulation during self-training, we propose a Map-to-Ray Matching strategy that aligns map predictions with camera rays derived from 2D labels. Extensive experiments on the Argoverse 2 and nuScenes datasets demonstrate that MapRF achieves performance comparable to fully supervised methods, attaining around 75% of the baseline while surpassing several approaches using only 2D labels. This highlights the potential of MapRF to enable scalable and cost-effective online HD map construction for autonomous driving.
<div id='section'>Paperid: <span id='pid'>162, <a href='https://arxiv.org/pdf/2509.13629.pdf' target='_blank'>https://arxiv.org/pdf/2509.13629.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue He, Min Liu, Qinghao Liu, Jiazheng Wang, Yaonan Wang, Hang Zhang, Xiang Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13629">SAMIR, an efficient registration framework via robust feature learning from SAM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image registration is a fundamental task in medical image analysis. Deformations are often closely related to the morphological characteristics of tissues, making accurate feature extraction crucial. Recent weakly supervised methods improve registration by incorporating anatomical priors such as segmentation masks or landmarks, either as inputs or in the loss function. However, such weak labels are often not readily available, limiting their practical use. Motivated by the strong representation learning ability of visual foundation models, this paper introduces SAMIR, an efficient medical image registration framework that utilizes the Segment Anything Model (SAM) to enhance feature extraction. SAM is pretrained on large-scale natural image datasets and can learn robust, general-purpose visual representations. Rather than using raw input images, we design a task-specific adaptation pipeline using SAM's image encoder to extract structure-aware feature embeddings, enabling more accurate modeling of anatomical consistency and deformation patterns. We further design a lightweight 3D head to refine features within the embedding space, adapting to local deformations in medical images. Additionally, we introduce a Hierarchical Feature Consistency Loss to guide coarse-to-fine feature matching and improve anatomical alignment. Extensive experiments demonstrate that SAMIR significantly outperforms state-of-the-art methods on benchmark datasets for both intra-subject cardiac image registration and inter-subject abdomen CT image registration, achieving performance improvements of 2.68% on ACDC and 6.44% on the abdomen dataset. The source code will be publicly available on GitHub following the acceptance of this paper.
<div id='section'>Paperid: <span id='pid'>163, <a href='https://arxiv.org/pdf/2509.11312.pdf' target='_blank'>https://arxiv.org/pdf/2509.11312.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenchao Gu, Yupan Chen, Yanlin Wang, Hongyu Zhang, Cuiyun Gao, Michael R. Lyu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11312">Weakly Supervised Vulnerability Localization via Multiple Instance Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Software vulnerability detection has emerged as a significant concern in the field of software security recently, capturing the attention of numerous researchers and developers. Most previous approaches focus on coarse-grained vulnerability detection, such as at the function or file level. However, the developers would still encounter the challenge of manually inspecting a large volume of code inside the vulnerable function to identify the specific vulnerable statements for modification, indicating the importance of vulnerability localization. Training the model for vulnerability localization usually requires ground-truth labels at the statement-level, and labeling vulnerable statements demands expert knowledge, which incurs high costs. Hence, the demand for an approach that eliminates the need for additional labeling at the statement-level is on the rise. To tackle this problem, we propose a novel approach called WAVES for WeAkly supervised Vulnerability Localization via multiplE inStance learning, which does not need the additional statement-level labels during the training. WAVES has the capability to determine whether a function is vulnerable (i.e., vulnerability detection) and pinpoint the vulnerable statements (i.e., vulnerability localization). Specifically, inspired by the concept of multiple instance learning, WAVES converts the ground-truth label at the function-level into pseudo labels for individual statements, eliminating the need for additional statement-level labeling. These pseudo labels are utilized to train the classifiers for the function-level representation vectors. Extensive experimentation on three popular benchmark datasets demonstrates that, in comparison to previous baselines, our approach achieves comparable performance in vulnerability detection and state-of-the-art performance in statement-level vulnerability localization.
<div id='section'>Paperid: <span id='pid'>164, <a href='https://arxiv.org/pdf/2505.22063.pdf' target='_blank'>https://arxiv.org/pdf/2505.22063.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingchen Shao, Xinfa Zhu, Chengyou Wang, Bingshen Mu, Hai Li, Ying Yan, Junhui Liu, Danming Xie, Lei Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22063">Weakly Supervised Data Refinement and Flexible Sequence Compression for Efficient Thai LLM-based ASR</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite remarkable achievements, automatic speech recognition (ASR) in low-resource scenarios still faces two challenges: high-quality data scarcity and high computational demands. This paper proposes EThai-ASR, the first to apply large language models (LLMs) to Thai ASR and create an efficient LLM-based ASR system. EThai-ASR comprises a speech encoder, a connection module and a Thai LLM decoder. To address the data scarcity and obtain a powerful speech encoder, EThai-ASR introduces a self-evolving data refinement strategy to refine weak labels, yielding an enhanced speech encoder. Moreover, we propose a pluggable sequence compression module used in the connection module with three modes designed to reduce the sequence length, thus decreasing computational demands while maintaining decent performance. Extensive experiments demonstrate that EThai-ASR has achieved state-of-the-art accuracy in multiple datasets. We release our refined text transcripts to promote further research.
<div id='section'>Paperid: <span id='pid'>165, <a href='https://arxiv.org/pdf/2504.04495.pdf' target='_blank'>https://arxiv.org/pdf/2504.04495.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Wu, Wanshun Su, Guansong Pang, Yujia Sun, Qingsen Yan, Peng Wang, Yanning Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04495">AVadCLIP: Audio-Visual Collaboration for Robust Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the increasing adoption of video anomaly detection in intelligent surveillance domains, conventional visual-based detection approaches often struggle with information insufficiency and high false-positive rates in complex environments. To address these limitations, we present a novel weakly supervised framework that leverages audio-visual collaboration for robust video anomaly detection. Capitalizing on the exceptional cross-modal representation learning capabilities of Contrastive Language-Image Pretraining (CLIP) across visual, audio, and textual domains, our framework introduces two major innovations: an efficient audio-visual fusion that enables adaptive cross-modal integration through lightweight parametric adaptation while maintaining the frozen CLIP backbone, and a novel audio-visual prompt that dynamically enhances text embeddings with key multimodal information based on the semantic correlation between audio-visual features and textual labels, significantly improving CLIP's generalization for the video anomaly detection task. Moreover, to enhance robustness against modality deficiency during inference, we further develop an uncertainty-driven feature distillation module that synthesizes audio-visual representations from visual-only inputs. This module employs uncertainty modeling based on the diversity of audio-visual features to dynamically emphasize challenging features during the distillation process. Our framework demonstrates superior performance across multiple benchmarks, with audio integration significantly boosting anomaly detection accuracy in various scenarios. Notably, with unimodal data enhanced by uncertainty-driven distillation, our approach consistently outperforms current unimodal VAD methods.
<div id='section'>Paperid: <span id='pid'>166, <a href='https://arxiv.org/pdf/2409.05383.pdf' target='_blank'>https://arxiv.org/pdf/2409.05383.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Wu, Chengyu Pan, Yuting Yan, Guansong Pang, Peng Wang, Yanning Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.05383">Deep Learning for Video Anomaly Detection: A Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video anomaly detection (VAD) aims to discover behaviors or events deviating from the normality in videos. As a long-standing task in the field of computer vision, VAD has witnessed much good progress. In the era of deep learning, with the explosion of architectures of continuously growing capability and capacity, a great variety of deep learning based methods are constantly emerging for the VAD task, greatly improving the generalization ability of detection algorithms and broadening the application scenarios. Therefore, such a multitude of methods and a large body of literature make a comprehensive survey a pressing necessity. In this paper, we present an extensive and comprehensive research review, covering the spectrum of five different categories, namely, semi-supervised, weakly supervised, fully supervised, unsupervised and open-set supervised VAD, and we also delve into the latest VAD works based on pre-trained large models, remedying the limitations of past reviews in terms of only focusing on semi-supervised VAD and small model based methods. For the VAD task with different levels of supervision, we construct a well-organized taxonomy, profoundly discuss the characteristics of different types of methods, and show their performance comparisons. In addition, this review involves the public datasets, open-source codes, and evaluation metrics covering all the aforementioned VAD tasks. Finally, we provide several important research directions for the VAD community.
<div id='section'>Paperid: <span id='pid'>167, <a href='https://arxiv.org/pdf/2510.14668.pdf' target='_blank'>https://arxiv.org/pdf/2510.14668.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md. Abdur Rahman, Mohaimenul Azam Khan Raiaan, Sami Azam, Asif Karim, Jemima Beissbarth, Amanda Leach
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.14668">WeCKD: Weakly-supervised Chained Distillation Network for Efficient Multimodal Medical Imaging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Knowledge distillation (KD) has traditionally relied on a static teacher-student framework, where a large, well-trained teacher transfers knowledge to a single student model. However, these approaches often suffer from knowledge degradation, inefficient supervision, and reliance on either a very strong teacher model or large labeled datasets, which limits their effectiveness in real-world, limited-data scenarios. To address these, we present the first-ever Weakly-supervised Chain-based KD network (WeCKD) that redefines knowledge transfer through a structured sequence of interconnected models. Unlike conventional KD, it forms a progressive distillation chain, where each model not only learns from its predecessor but also refines the knowledge before passing it forward. This structured knowledge transfer further enhances feature learning, reduces data dependency, and mitigates the limitations of one-step KD. Each model in the distillation chain is trained on only a fraction of the dataset and demonstrates that effective learning can be achieved with minimal supervision. Extensive evaluations across four otoscopic imaging datasets demonstrate that it not only matches but in many cases surpasses the performance of existing supervised methods. Experimental results on two other datasets further underscore its generalization across diverse medical imaging modalities, including microscopic and magnetic resonance imaging. Furthermore, our evaluations resulted in cumulative accuracy gains of up to +23% over a single backbone trained on the same limited data, which highlights its potential for real-world adoption.
<div id='section'>Paperid: <span id='pid'>168, <a href='https://arxiv.org/pdf/2502.08888.pdf' target='_blank'>https://arxiv.org/pdf/2502.08888.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruichao Yang, Jing Ma, Wei Gao, Hongzhan Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.08888">LLM-Enhanced Multiple Instance Learning for Joint Rumor and Stance Detection with Social Context Information</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The proliferation of misinformation, such as rumors on social media, has drawn significant attention, prompting various expressions of stance among users. Although rumor detection and stance detection are distinct tasks, they can complement each other. Rumors can be identified by cross-referencing stances in related posts, and stances are influenced by the nature of the rumor. However, existing stance detection methods often require post-level stance annotations, which are costly to obtain. We propose a novel LLM-enhanced MIL approach to jointly predict post stance and claim class labels, supervised solely by claim labels, using an undirected microblog propagation model. Our weakly supervised approach relies only on bag-level labels of claim veracity, aligning with multi-instance learning (MIL) principles. To achieve this, we transform the multi-class problem into multiple MIL-based binary classification problems. We then employ a discriminative attention layer to aggregate the outputs from these classifiers into finer-grained classes. Experiments conducted on three rumor datasets and two stance datasets demonstrate the effectiveness of our approach, highlighting strong connections between rumor veracity and expressed stances in responding posts. Our method shows promising performance in joint rumor and stance detection compared to the state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>169, <a href='https://arxiv.org/pdf/2412.10410.pdf' target='_blank'>https://arxiv.org/pdf/2412.10410.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaofei Cai, Bowei Zhang, Zihao Wang, Haowei Lin, Xiaojian Ma, Anji Liu, Yitao Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10410">GROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing agents that can follow multimodal instructions remains a fundamental challenge in robotics and AI. Although large-scale pre-training on unlabeled datasets (no language instruction) has enabled agents to learn diverse behaviors, these agents often struggle with following instructions. While augmenting the dataset with instruction labels can mitigate this issue, acquiring such high-quality annotations at scale is impractical. To address this issue, we frame the problem as a semi-supervised learning task and introduce GROOT-2, a multimodal instructable agent trained using a novel approach that combines weak supervision with latent variable models. Our method consists of two key components: constrained self-imitating, which utilizes large amounts of unlabeled demonstrations to enable the policy to learn diverse behaviors, and human intention alignment, which uses a smaller set of labeled demonstrations to ensure the latent space reflects human intentions. GROOT-2's effectiveness is validated across four diverse environments, ranging from video games to robotic manipulation, demonstrating its robust multimodal instruction-following capabilities.
<div id='section'>Paperid: <span id='pid'>170, <a href='https://arxiv.org/pdf/2509.03893.pdf' target='_blank'>https://arxiv.org/pdf/2509.03893.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stefan Stojanov, Linan Zhao, Yunzhi Zhang, Daniel L. K. Yamins, Jiajun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03893">Weakly-Supervised Learning of Dense Functional Correspondences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Establishing dense correspondences across image pairs is essential for tasks such as shape reconstruction and robot manipulation. In the challenging setting of matching across different categories, the function of an object, i.e., the effect that an object can cause on other objects, can guide how correspondences should be established. This is because object parts that enable specific functions often share similarities in shape and appearance. We derive the definition of dense functional correspondence based on this observation and propose a weakly-supervised learning paradigm to tackle the prediction task. The main insight behind our approach is that we can leverage vision-language models to pseudo-label multi-view images to obtain functional parts. We then integrate this with dense contrastive learning from pixel correspondences to distill both functional and spatial knowledge into a new model that can establish dense functional correspondence. Further, we curate synthetic and real evaluation datasets as task benchmarks. Our results demonstrate the advantages of our approach over baseline solutions consisting of off-the-shelf self-supervised image representations and grounded vision language models.
<div id='section'>Paperid: <span id='pid'>171, <a href='https://arxiv.org/pdf/2502.15370.pdf' target='_blank'>https://arxiv.org/pdf/2502.15370.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kibum Kim, Kanghoon Yoon, Yeonjun In, Jaehyeong Jeon, Jinyoung Moon, Donghyun Kim, Chanyoung Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.15370">Weakly Supervised Video Scene Graph Generation via Natural Language Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing Video Scene Graph Generation (VidSGG) studies are trained in a fully supervised manner, which requires all frames in a video to be annotated, thereby incurring high annotation cost compared to Image Scene Graph Generation (ImgSGG). Although the annotation cost of VidSGG can be alleviated by adopting a weakly supervised approach commonly used for ImgSGG (WS-ImgSGG) that uses image captions, there are two key reasons that hinder such a naive adoption: 1) Temporality within video captions, i.e., unlike image captions, video captions include temporal markers (e.g., before, while, then, after) that indicate time related details, and 2) Variability in action duration, i.e., unlike human actions in image captions, human actions in video captions unfold over varying duration. To address these issues, we propose a Natural Language-based Video Scene Graph Generation (NL-VSGG) framework that only utilizes the readily available video captions for training a VidSGG model. NL-VSGG consists of two key modules: Temporality-aware Caption Segmentation (TCS) module and Action Duration Variability-aware caption-frame alignment (ADV) module. Specifically, TCS segments the video captions into multiple sentences in a temporal order based on a Large Language Model (LLM), and ADV aligns each segmented sentence with appropriate frames considering the variability in action duration. Our approach leads to a significant enhancement in performance compared to simply applying the WS-ImgSGG pipeline to VidSGG on the Action Genome dataset. As a further benefit of utilizing the video captions as weak supervision, we show that the VidSGG model trained by NL-VSGG is able to predict a broader range of action classes that are not included in the training data, which makes our framework practical in reality.
<div id='section'>Paperid: <span id='pid'>172, <a href='https://arxiv.org/pdf/2412.19847.pdf' target='_blank'>https://arxiv.org/pdf/2412.19847.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexandr Korchemnyi, Alexey K. Kovalev, Aleksandr I. Panov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19847">Symbolic Disentangled Representations for Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The idea of disentangled representations is to reduce the data to a set of generative factors that produce it. Typically, such representations are vectors in latent space, where each coordinate corresponds to one of the generative factors. The object can then be modified by changing the value of a particular coordinate, but it is necessary to determine which coordinate corresponds to the desired generative factor -- a difficult task if the vector representation has a high dimension. In this article, we propose ArSyD (Architecture for Symbolic Disentanglement), which represents each generative factor as a vector of the same dimension as the resulting representation. In ArSyD, the object representation is obtained as a superposition of the generative factor vector representations. We call such a representation a \textit{symbolic disentangled representation}. We use the principles of Hyperdimensional Computing (also known as Vector Symbolic Architectures), where symbols are represented as hypervectors, allowing vector operations on them. Disentanglement is achieved by construction, no additional assumptions about the underlying distributions are made during training, and the model is only trained to reconstruct images in a weakly supervised manner. We study ArSyD on the dSprites and CLEVR datasets and provide a comprehensive analysis of the learned symbolic disentangled representations. We also propose new disentanglement metrics that allow comparison of methods using latent representations of different dimensions. ArSyD allows to edit the object properties in a controlled and interpretable way, and the dimensionality of the object property representation coincides with the dimensionality of the object representation itself.
<div id='section'>Paperid: <span id='pid'>173, <a href='https://arxiv.org/pdf/2505.18989.pdf' target='_blank'>https://arxiv.org/pdf/2505.18989.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Catalina Tan, Yipeng Hu, Shaheer U. Saeed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18989">SPARS: Self-Play Adversarial Reinforcement Learning for Segmentation of Liver Tumours</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate tumour segmentation is vital for various targeted diagnostic and therapeutic procedures for cancer, e.g., planning biopsies or tumour ablations. Manual delineation is extremely labour-intensive, requiring substantial expert time. Fully-supervised machine learning models aim to automate such localisation tasks, but require a large number of costly and often subjective 3D voxel-level labels for training. The high-variance and subjectivity in such labels impacts model generalisability, even when large datasets are available. Histopathology labels may offer more objective labels but the infeasibility of acquiring pixel-level annotations to develop tumour localisation methods based on histology remains challenging in-vivo. In this work, we propose a novel weakly-supervised semantic segmentation framework called SPARS (Self-Play Adversarial Reinforcement Learning for Segmentation), which utilises an object presence classifier, trained on a small number of image-level binary cancer presence labels, to localise cancerous regions on CT scans. Such binary labels of patient-level cancer presence can be sourced more feasibly from biopsies and histopathology reports, enabling a more objective cancer localisation on medical images. Evaluating with real patient data, we observed that SPARS yielded a mean dice score of $77.3 \pm 9.4$, which outperformed other weakly-supervised methods by large margins. This performance was comparable with recent fully-supervised methods that require voxel-level annotations. Our results demonstrate the potential of using SPARS to reduce the need for extensive human-annotated labels to detect cancer in real-world healthcare settings.
<div id='section'>Paperid: <span id='pid'>174, <a href='https://arxiv.org/pdf/2505.17915.pdf' target='_blank'>https://arxiv.org/pdf/2505.17915.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lynn Karam, Yipei Wang, Veeru Kasivisvanathan, Mirabela Rusu, Yipeng Hu, Shaheer U. Saeed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17915">Promptable cancer segmentation using minimal expert-curated data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automated segmentation of cancer on medical images can aid targeted diagnostic and therapeutic procedures. However, its adoption is limited by the high cost of expert annotations required for training and inter-observer variability in datasets. While weakly-supervised methods mitigate some challenges, using binary histology labels for training as opposed to requiring full segmentation, they require large paired datasets of histology and images, which are difficult to curate. Similarly, promptable segmentation aims to allow segmentation with no re-training for new tasks at inference, however, existing models perform poorly on pathological regions, again necessitating large datasets for training. In this work we propose a novel approach for promptable segmentation requiring only 24 fully-segmented images, supplemented by 8 weakly-labelled images, for training. Curating this minimal data to a high standard is relatively feasible and thus issues with the cost and variability of obtaining labels can be mitigated. By leveraging two classifiers, one weakly-supervised and one fully-supervised, our method refines segmentation through a guided search process initiated by a single-point prompt. Our approach outperforms existing promptable segmentation methods, and performs comparably with fully-supervised methods, for the task of prostate cancer segmentation, while using substantially less annotated data (up to 100X less). This enables promptable segmentation with very minimal labelled data, such that the labels can be curated to a very high standard.
<div id='section'>Paperid: <span id='pid'>175, <a href='https://arxiv.org/pdf/2503.20190.pdf' target='_blank'>https://arxiv.org/pdf/2503.20190.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Chen, Jiawen Li, Jiali Hu, Xitong Ling, Tian Guan, Anjia Han, Yonghong He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20190">Cross-Modal Prototype Allocation: Unsupervised Slide Representation Learning via Patch-Text Contrast in Computational Pathology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancement of pathology foundation models (FMs), the representation learning of whole slide images (WSIs) attracts increasing attention. Existing studies develop high-quality patch feature extractors and employ carefully designed aggregation schemes to derive slide-level representations. However, mainstream weakly supervised slide representation learning methods, primarily based on multiple instance learning (MIL), are tailored to specific downstream tasks, which limits their generalizability. To address this issue, some studies explore unsupervised slide representation learning. However, these approaches focus solely on the visual modality of patches, neglecting the rich semantic information embedded in textual data. In this work, we propose ProAlign, a cross-modal unsupervised slide representation learning framework. Specifically, we leverage a large language model (LLM) to generate descriptive text for the prototype types present in a WSI, introducing patch-text contrast to construct initial prototype embeddings. Furthermore, we propose a parameter-free attention aggregation strategy that utilizes the similarity between patches and these prototypes to form unsupervised slide embeddings applicable to a wide range of downstream tasks. Extensive experiments on four public datasets show that ProAlign outperforms existing unsupervised frameworks and achieves performance comparable to some weakly supervised models.
<div id='section'>Paperid: <span id='pid'>176, <a href='https://arxiv.org/pdf/2503.00915.pdf' target='_blank'>https://arxiv.org/pdf/2503.00915.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xitong Ling, Yifeng Ping, Jiawen Li, Jing Peng, Yuxuan Chen, Minxi Ouyang, Yizhi Wang, Yonghong He, Tian Guan, Xiaoping Liu, Lianghui Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00915">Multimodal Distillation-Driven Ensemble Learning for Long-Tailed Histopathology Whole Slide Images Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple Instance Learning (MIL) plays a significant role in computational pathology, enabling weakly supervised analysis of Whole Slide Image (WSI) datasets. The field of WSI analysis is confronted with a severe long-tailed distribution problem, which significantly impacts the performance of classifiers. Long-tailed distributions lead to class imbalance, where some classes have sparse samples while others are abundant, making it difficult for classifiers to accurately identify minority class samples. To address this issue, we propose an ensemble learning method based on MIL, which employs expert decoders with shared aggregators and consistency constraints to learn diverse distributions and reduce the impact of class imbalance on classifier performance. Moreover, we introduce a multimodal distillation framework that leverages text encoders pre-trained on pathology-text pairs to distill knowledge and guide the MIL aggregator in capturing stronger semantic features relevant to class information. To ensure flexibility, we use learnable prompts to guide the distillation process of the pre-trained text encoder, avoiding limitations imposed by specific prompts. Our method, MDE-MIL, integrates multiple expert branches focusing on specific data distributions to address long-tailed issues. Consistency control ensures generalization across classes. Multimodal distillation enhances feature extraction. Experiments on Camelyon+-LT and PANDA-LT datasets show it outperforms state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>177, <a href='https://arxiv.org/pdf/2502.20823.pdf' target='_blank'>https://arxiv.org/pdf/2502.20823.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawen Li, Jiali Hu, Qiehe Sun, Renao Yan, Minxi Ouyang, Tian Guan, Anjia Han, Chao He, Yonghong He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20823">Can We Simplify Slide-level Fine-tuning of Pathology Foundation Models?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emergence of foundation models in computational pathology has transformed histopathological image analysis, with whole slide imaging (WSI) diagnosis being a core application. Traditionally, weakly supervised fine-tuning via multiple instance learning (MIL) has been the primary method for adapting foundation models to WSIs. However, in this work we present a key experimental finding: a simple nonlinear mapping strategy combining mean pooling and a multilayer perceptron, called SiMLP, can effectively adapt patch-level foundation models to slide-level tasks without complex MIL-based learning. Through extensive experiments across diverse downstream tasks, we demonstrate the superior performance of SiMLP with state-of-the-art methods. For instance, on a large-scale pan-cancer classification task, SiMLP surpasses popular MIL-based methods by 3.52%. Furthermore, SiMLP shows strong learning ability in few-shot classification and remaining highly competitive with slide-level foundation models pretrained on tens of thousands of slides. Finally, SiMLP exhibits remarkable robustness and transferability in lung cancer subtyping. Overall, our findings challenge the conventional MIL-based fine-tuning paradigm, demonstrating that a task-agnostic representation strategy alone can effectively adapt foundation models to WSI analysis. These insights offer a unique and meaningful perspective for future research in digital pathology, paving the way for more efficient and broadly applicable methodologies.
<div id='section'>Paperid: <span id='pid'>178, <a href='https://arxiv.org/pdf/2411.06702.pdf' target='_blank'>https://arxiv.org/pdf/2411.06702.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jia Syuen Lim, Yadan Luo, Zhi Chen, Tianqi Wei, Scott Chapman, Zi Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06702">Track Any Peppers: Weakly Supervised Sweet Pepper Tracking Using VLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the Detection and Multi-Object Tracking of Sweet Peppers Challenge, we present Track Any Peppers (TAP) - a weakly supervised ensemble technique for sweet peppers tracking. TAP leverages the zero-shot detection capabilities of vision-language foundation models like Grounding DINO to automatically generate pseudo-labels for sweet peppers in video sequences with minimal human intervention. These pseudo-labels, refined when necessary, are used to train a YOLOv8 segmentation network. To enhance detection accuracy under challenging conditions, we incorporate pre-processing techniques such as relighting adjustments and apply depth-based filtering during post-inference. For object tracking, we integrate the Matching by Segment Anything (MASA) adapter with the BoT-SORT algorithm. Our approach achieves a HOTA score of 80.4%, MOTA of 66.1%, Recall of 74.0%, and Precision of 90.7%, demonstrating effective tracking of sweet peppers without extensive manual effort. This work highlights the potential of foundation models for efficient and accurate object detection and tracking in agricultural settings.
<div id='section'>Paperid: <span id='pid'>179, <a href='https://arxiv.org/pdf/2409.11664.pdf' target='_blank'>https://arxiv.org/pdf/2409.11664.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xitong Ling, Minxi Ouyang, Yizhi Wang, Xinrui Chen, Renao Yan, Hongbo Chu, Junru Cheng, Tian Guan, Sufang Tian, Xiaoping Liu, Yonghong He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11664">Agent Aggregator with Mask Denoise Mechanism for Histopathology Whole Slide Image Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Histopathology analysis is the gold standard for medical diagnosis. Accurate classification of whole slide images (WSIs) and region-of-interests (ROIs) localization can assist pathologists in diagnosis. The gigapixel resolution of WSI and the absence of fine-grained annotations make direct classification and analysis challenging. In weakly supervised learning, multiple instance learning (MIL) presents a promising approach for WSI classification. The prevailing strategy is to use attention mechanisms to measure instance importance for classification. However, attention mechanisms fail to capture inter-instance information, and self-attention causes quadratic computational complexity. To address these challenges, we propose AMD-MIL, an agent aggregator with a mask denoise mechanism. The agent token acts as an intermediate variable between the query and key for computing instance importance. Mask and denoising matrices, mapped from agents-aggregated value, dynamically mask low-contribution representations and eliminate noise. AMD-MIL achieves better attention allocation by adjusting feature representations, capturing micro-metastases in cancer, and improving interpretability. Extensive experiments on CAMELYON-16, CAMELYON-17, TCGA-KIDNEY, and TCGA-LUNG show AMD-MIL's superiority over state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>180, <a href='https://arxiv.org/pdf/2509.12090.pdf' target='_blank'>https://arxiv.org/pdf/2509.12090.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihong Chen, Jiancheng Yang, Deniz Sayin Mercadier, Hieu Le, Juerg Schwitter, Pascal Fua
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12090">End-to-End 4D Heart Mesh Recovery Across Full-Stack and Sparse Cardiac MRI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing cardiac motion from cine CMR sequences is critical for diagnosis, prediction, and intervention. Existing methods rely on complete CMR stacks to infer full heart motion, limiting their utility in intra-procedural scenarios where only sparse observations are available. We present TetHeart, the first end-to-end framework that unifies full 4D multi-structure heart mesh recovery from both offline full-stack acquisitions and intra-procedural sparse-slice observations. Our method leverages deep deformable tetrahedra, an explicit-implicit hybrid representation, to capture shape and motion in a coherent space shared across cardiac structures. It is initialized from high-quality pre-procedural or offline-acquired full stacks to build detailed, patient-specific heart meshes, which can then be updated using whatever slices are available, from full stacks down to a single slice. We further incorporate several key innovations: (i) an attentive mechanism for slice-adaptive 2D-3D feature assembly that dynamically integrates information from arbitrary numbers of slices at any position, combined with a distillation strategy from full-slice to sparse-slice settings to ensure accurate reconstruction under extreme sparsity; and (ii) a two-stage weakly supervised motion learning scheme requiring only keyframe (e.g., ED and ES) annotations. Trained and validated on three large public datasets and externally evaluated zero-shot on additional private interventional and public CMR datasets, TetHeart achieves state-of-the-art accuracy and strong generalization in both pre- and intra-procedural settings.
<div id='section'>Paperid: <span id='pid'>181, <a href='https://arxiv.org/pdf/2411.10356.pdf' target='_blank'>https://arxiv.org/pdf/2411.10356.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrea Agostini, DaphnÃ© Chopard, Yang Meng, Norbert Fortin, Babak Shahbaba, Stephan Mandt, Thomas M. Sutter, Julia E. Vogt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10356">Weakly-Supervised Multimodal Learning on MIMIC-CXR</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal data integration and label scarcity pose significant challenges for machine learning in medical settings. To address these issues, we conduct an in-depth evaluation of the newly proposed Multimodal Variational Mixture-of-Experts (MMVM) VAE on the challenging MIMIC-CXR dataset. Our analysis demonstrates that the MMVM VAE consistently outperforms other multimodal VAEs and fully supervised approaches, highlighting its strong potential for real-world medical applications.
<div id='section'>Paperid: <span id='pid'>182, <a href='https://arxiv.org/pdf/2510.16536.pdf' target='_blank'>https://arxiv.org/pdf/2510.16536.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Niranjana Arun Menon, Yulong Li, Iqra Farooq, Sara Ahmed, Muhammad Awais, Imran Razzak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.16536">Few-Label Multimodal Modeling of SNP Variants and ECG Phenotypes Using Large Language Models for Cardiovascular Risk Stratification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cardiovascular disease (CVD) risk stratification remains a major challenge due to its multifactorial nature and limited availability of high-quality labeled datasets. While genomic and electrophysiological data such as SNP variants and ECG phenotypes are increasingly accessible, effectively integrating these modalities in low-label settings is non-trivial. This challenge arises from the scarcity of well-annotated multimodal datasets and the high dimensionality of biological signals, which limit the effectiveness of conventional supervised models. To address this, we present a few-label multimodal framework that leverages large language models (LLMs) to combine genetic and electrophysiological information for cardiovascular risk stratification. Our approach incorporates a pseudo-label refinement strategy to adaptively distill high-confidence labels from weakly supervised predictions, enabling robust model fine-tuning with only a small set of ground-truth annotations. To enhance the interpretability, we frame the task as a Chain of Thought (CoT) reasoning problem, prompting the model to produce clinically relevant rationales alongside predictions. Experimental results demonstrate that the integration of multimodal inputs, few-label supervision, and CoT reasoning improves robustness and generalizability across diverse patient profiles. Experimental results using multimodal SNP variants and ECG-derived features demonstrated comparable performance to models trained on the full dataset, underscoring the promise of LLM-based few-label multimodal modeling for advancing personalized cardiovascular care.
<div id='section'>Paperid: <span id='pid'>183, <a href='https://arxiv.org/pdf/2510.11073.pdf' target='_blank'>https://arxiv.org/pdf/2510.11073.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Tian, Min Zhou, Yitong Chen, Fang Li, Lingzi Qi, Shuo Wang, Xieyang Xu, Yu Yu, Shiqiong Xu, Chaoyu Lei, Yankai Jiang, Rongzhao Zhang, Jia Tan, Li Wu, Hong Chen, Xiaowei Liu, Wei Lu, Lin Li, Huifang Zhou, Xuefei Song, Guangtao Zhai, Xianqun Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.11073">ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Patient face images provide a convenient mean for evaluating eye diseases, while also raising privacy concerns. Here, we introduce ROFI, a deep learning-based privacy protection framework for ophthalmology. Using weakly supervised learning and neural identity translation, ROFI anonymizes facial features while retaining disease features (over 98\% accuracy, $κ> 0.90$). It achieves 100\% diagnostic sensitivity and high agreement ($κ> 0.90$) across eleven eye diseases in three cohorts, anonymizing over 95\% of images. ROFI works with AI systems, maintaining original diagnoses ($κ> 0.80$), and supports secure image reversal (over 98\% similarity), enabling audits and long-term care. These results show ROFI's effectiveness of protecting patient privacy in the digital medicine era.
<div id='section'>Paperid: <span id='pid'>184, <a href='https://arxiv.org/pdf/2510.03016.pdf' target='_blank'>https://arxiv.org/pdf/2510.03016.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dong-Dong Wu, Jiacheng Cui, Wei Wang, Zhiqiang She, Masashi Sugiyama
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03016">Learning Robust Diffusion Models from Imprecise Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conditional diffusion models have achieved remarkable success in various generative tasks recently, but their training typically relies on large-scale datasets that inevitably contain imprecise information in conditional inputs. Such supervision, often stemming from noisy, ambiguous, or incomplete labels, will cause condition mismatch and degrade generation quality. To address this challenge, we propose DMIS, a unified framework for training robust Diffusion Models from Imprecise Supervision, which is the first systematic study within diffusion models. Our framework is derived from likelihood maximization and decomposes the objective into generative and classification components: the generative component models imprecise-label distributions, while the classification component leverages a diffusion classifier to infer class-posterior probabilities, with its efficiency further improved by an optimized timestep sampling strategy. Extensive experiments on diverse forms of imprecise supervision, covering tasks of image generation, weakly supervised learning, and noisy dataset condensation demonstrate that DMIS consistently produces high-quality and class-discriminative samples.
<div id='section'>Paperid: <span id='pid'>185, <a href='https://arxiv.org/pdf/2512.15310.pdf' target='_blank'>https://arxiv.org/pdf/2512.15310.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wangyu Wu, Zhenhong Chen, Xiaowei Huang, Fei Ma, Jimin Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.15310">SynthSeg-Agents: Multi-Agent Synthetic Data Generation for Zero-Shot Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Semantic Segmentation (WSSS) with image level labels aims to produce pixel level predictions without requiring dense annotations. While recent approaches have leveraged generative models to augment existing data, they remain dependent on real world training samples. In this paper, we introduce a novel direction, Zero Shot Weakly Supervised Semantic Segmentation (ZSWSSS), and propose SynthSeg Agents, a multi agent framework driven by Large Language Models (LLMs) to generate synthetic training data entirely without real images. SynthSeg Agents comprises two key modules, a Self Refine Prompt Agent and an Image Generation Agent. The Self Refine Prompt Agent autonomously crafts diverse and semantically rich image prompts via iterative refinement, memory mechanisms, and prompt space exploration, guided by CLIP based similarity and nearest neighbor diversity filtering. These prompts are then passed to the Image Generation Agent, which leverages Vision Language Models (VLMs) to synthesize candidate images. A frozen CLIP scoring model is employed to select high quality samples, and a ViT based classifier is further trained to relabel the entire synthetic dataset with improved semantic precision. Our framework produces high quality training data without any real image supervision. Experiments on PASCAL VOC 2012 and COCO 2014 show that SynthSeg Agents achieves competitive performance without using real training images. This highlights the potential of LLM driven agents in enabling cost efficient and scalable semantic segmentation.
<div id='section'>Paperid: <span id='pid'>186, <a href='https://arxiv.org/pdf/2508.17009.pdf' target='_blank'>https://arxiv.org/pdf/2508.17009.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wangyu Wu, Zhenhong Chen, Xiaowen Ma, Wenqiao Zhang, Xianglin Qiu, Siqi Song, Xiaowei Huang, Fei Ma, Jimin Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17009">Contrastive Prompt Clustering for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Semantic Segmentation (WSSS) with image-level labels has gained attention for its cost-effectiveness. Most existing methods emphasize inter-class separation, often neglecting the shared semantics among related categories and lacking fine-grained discrimination. To address this, we propose Contrastive Prompt Clustering (CPC), a novel WSSS framework. CPC exploits Large Language Models (LLMs) to derive category clusters that encode intrinsic inter-class relationships, and further introduces a class-aware patch-level contrastive loss to enforce intra-class consistency and inter-class separation. This hierarchical design leverages clusters as coarse-grained semantic priors while preserving fine-grained boundaries, thereby reducing confusion among visually similar categories. Experiments on PASCAL VOC 2012 and MS COCO 2014 demonstrate that CPC surpasses existing state-of-the-art methods in WSSS.
<div id='section'>Paperid: <span id='pid'>187, <a href='https://arxiv.org/pdf/2412.20439.pdf' target='_blank'>https://arxiv.org/pdf/2412.20439.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wangyu Wu, Xianglin Qiu, Siqi Song, Zhenhong Chen, Xiaowei Huang, Fei Ma, Jimin Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20439">Image Augmentation Agent for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised semantic segmentation (WSSS) has achieved remarkable progress using only image-level labels. However, most existing WSSS methods focus on designing new network structures and loss functions to generate more accurate dense labels, overlooking the limitations imposed by fixed datasets, which can constrain performance improvements. We argue that more diverse trainable images provides WSSS richer information and help model understand more comprehensive semantic pattern. Therefore in this paper, we introduce a novel approach called Image Augmentation Agent (IAA) which shows that it is possible to enhance WSSS from data generation perspective. IAA mainly design an augmentation agent that leverages large language models (LLMs) and diffusion models to automatically generate additional images for WSSS. In practice, to address the instability in prompt generation by LLMs, we develop a prompt self-refinement mechanism. It allow LLMs to re-evaluate the rationality of generated prompts to produce more coherent prompts. Additionally, we insert an online filter into diffusion generation process to dynamically ensure the quality and balance of generated images. Experimental results show that our method significantly surpasses state-of-the-art WSSS approaches on the PASCAL VOC 2012 and MS COCO 2014 datasets.
<div id='section'>Paperid: <span id='pid'>188, <a href='https://arxiv.org/pdf/2412.13823.pdf' target='_blank'>https://arxiv.org/pdf/2412.13823.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wangyu Wu, Xianglin Qiu, Siqi Song, Xiaowei Huang, Fei Ma, Jimin Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13823">Prompt Categories Cluster for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Semantic Segmentation (WSSS), which leverages image-level labels, has garnered significant attention due to its cost-effectiveness. The previous methods mainly strengthen the inter-class differences to avoid class semantic ambiguity which may lead to erroneous activation. However, they overlook the positive function of some shared information between similar classes. Categories within the same cluster share some similar features. Allowing the model to recognize these features can further relieve the semantic ambiguity between these classes. To effectively identify and utilize this shared information, in this paper, we introduce a novel WSSS framework called Prompt Categories Clustering (PCC). Specifically, we explore the ability of Large Language Models (LLMs) to derive category clusters through prompts. These clusters effectively represent the intrinsic relationships between categories. By integrating this relational information into the training network, our model is able to better learn the hidden connections between categories. Experimental results demonstrate the effectiveness of our approach, showing its ability to enhance performance on the PASCAL VOC 2012 dataset and surpass existing state-of-the-art methods in WSSS.
<div id='section'>Paperid: <span id='pid'>189, <a href='https://arxiv.org/pdf/2410.23834.pdf' target='_blank'>https://arxiv.org/pdf/2410.23834.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cosmin I. Bercea, Philippe C. Cattin, Julia A. Schnabel, Julia Wolleb
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.23834">Denoising Diffusion Models for Anomaly Localization in Medical Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This chapter explores anomaly localization in medical images using denoising diffusion models. After providing a brief methodological background of these models, including their application to image reconstruction and their conditioning using guidance mechanisms, we provide an overview of available datasets and evaluation metrics suitable for their application to anomaly localization in medical images. In this context, we discuss supervision schemes ranging from fully supervised segmentation to semi-supervised, weakly supervised, self-supervised, and unsupervised methods, and provide insights into the effectiveness and limitations of these approaches. Furthermore, we highlight open challenges in anomaly localization, including detection bias, domain shift, computational cost, and model interpretability. Our goal is to provide an overview of the current state of the art in the field, outline research gaps, and highlight the potential of diffusion models for robust anomaly localization in medical images.
<div id='section'>Paperid: <span id='pid'>190, <a href='https://arxiv.org/pdf/2509.18711.pdf' target='_blank'>https://arxiv.org/pdf/2509.18711.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ke Li, Di Wang, Ting Wang, Fuyu Dong, Yiming Zhang, Luyao Zhang, Xiangyu Wang, Shaofeng Li, Quan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18711">RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot Open-Vocabulary Visual Grounding in Remote Sensing Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Remote sensing visual grounding (RSVG) aims to localize objects in remote sensing images based on free-form natural language expressions. Existing approaches are typically constrained to closed-set vocabularies, limiting their applicability in open-world scenarios. While recent attempts to leverage generic foundation models for open-vocabulary RSVG, they overly rely on expensive high-quality datasets and time-consuming fine-tuning. To address these limitations, we propose \textbf{RSVG-ZeroOV}, a training-free framework that aims to explore the potential of frozen generic foundation models for zero-shot open-vocabulary RSVG. Specifically, RSVG-ZeroOV comprises three key stages: (i) Overview: We utilize a vision-language model (VLM) to obtain cross-attention\footnote[1]{In this paper, although decoder-only VLMs use self-attention over all tokens, we refer to the image-text interaction part as cross-attention to distinguish it from pure visual self-attention.}maps that capture semantic correlations between text queries and visual regions. (ii) Focus: By leveraging the fine-grained modeling priors of a diffusion model (DM), we fill in gaps in structural and shape information of objects, which are often overlooked by VLM. (iii) Evolve: A simple yet effective attention evolution module is introduced to suppress irrelevant activations, yielding purified segmentation masks over the referred objects. Without cumbersome task-specific training, RSVG-ZeroOV offers an efficient and scalable solution. Extensive experiments demonstrate that the proposed framework consistently outperforms existing weakly-supervised and zero-shot methods.
<div id='section'>Paperid: <span id='pid'>191, <a href='https://arxiv.org/pdf/2509.08126.pdf' target='_blank'>https://arxiv.org/pdf/2509.08126.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Houjian Yu, Zheming Zhou, Min Sun, Omid Ghasemalizadeh, Yuyin Sun, Cheng-Hao Kuo, Arnie Sen, Changhyun Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08126">Attribute-based Object Grounding and Robot Grasp Detection with Spatial Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enabling robots to grasp objects specified through natural language is essential for effective human-robot interaction, yet it remains a significant challenge. Existing approaches often struggle with open-form language expressions and typically assume unambiguous target objects without duplicates. Moreover, they frequently rely on costly, dense pixel-wise annotations for both object grounding and grasp configuration. We present Attribute-based Object Grounding and Robotic Grasping (OGRG), a novel framework that interprets open-form language expressions and performs spatial reasoning to ground target objects and predict planar grasp poses, even in scenes containing duplicated object instances. We investigate OGRG in two settings: (1) Referring Grasp Synthesis (RGS) under pixel-wise full supervision, and (2) Referring Grasp Affordance (RGA) using weakly supervised learning with only single-pixel grasp annotations. Key contributions include a bi-directional vision-language fusion module and the integration of depth information to enhance geometric reasoning, improving both grounding and grasping performance. Experiment results show that OGRG outperforms strong baselines in tabletop scenes with diverse spatial language instructions. In RGS, it operates at 17.59 FPS on a single NVIDIA RTX 2080 Ti GPU, enabling potential use in closed-loop or multi-object sequential grasping, while delivering superior grounding and grasp prediction accuracy compared to all the baselines considered. Under the weakly supervised RGA setting, OGRG also surpasses baseline grasp-success rates in both simulation and real-robot trials, underscoring the effectiveness of its spatial reasoning design. Project page: https://z.umn.edu/ogrg
<div id='section'>Paperid: <span id='pid'>192, <a href='https://arxiv.org/pdf/2508.00235.pdf' target='_blank'>https://arxiv.org/pdf/2508.00235.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Erin Rainville, Amirhossein Rasoulian, Hassan Rivaz, Yiming Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00235">Weakly Supervised Intracranial Aneurysm Detection and Segmentation in MR angiography via Multi-task UNet with Vesselness Prior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intracranial aneurysms (IAs) are abnormal dilations of cerebral blood vessels that, if ruptured, can lead to life-threatening consequences. However, their small size and soft contrast in radiological scans often make it difficult to perform accurate and efficient detection and morphological analyses, which are critical in the clinical care of the disorder. Furthermore, the lack of large public datasets with voxel-wise expert annotations pose challenges for developing deep learning algorithms to address the issues. Therefore, we proposed a novel weakly supervised 3D multi-task UNet that integrates vesselness priors to jointly perform aneurysm detection and segmentation in time-of-flight MR angiography (TOF-MRA). Specifically, to robustly guide IA detection and segmentation, we employ the popular Frangi's vesselness filter to derive soft cerebrovascular priors for both network input and an attention block to conduct segmentation from the decoder and detection from an auxiliary branch. We train our model on the Lausanne dataset with coarse ground truth segmentation, and evaluate it on the test set with refined labels from the same database. To further assess our model's generalizability, we also validate it externally on the ADAM dataset. Our results demonstrate the superior performance of the proposed technique over the SOTA techniques for aneurysm segmentation (Dice = 0.614, 95%HD =1.38mm) and detection (false positive rate = 1.47, sensitivity = 92.9%).
<div id='section'>Paperid: <span id='pid'>193, <a href='https://arxiv.org/pdf/2506.23519.pdf' target='_blank'>https://arxiv.org/pdf/2506.23519.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Qin, Runmin Cong, Gen Zhan, Yiting Liao, Sam Kwong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23519">From Sight to Insight: Unleashing Eye-Tracking in Weakly Supervised Video Salient Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The eye-tracking video saliency prediction (VSP) task and video salient object detection (VSOD) task both focus on the most attractive objects in video and show the result in the form of predictive heatmaps and pixel-level saliency masks, respectively. In practical applications, eye tracker annotations are more readily obtainable and align closely with the authentic visual patterns of human eyes. Therefore, this paper aims to introduce fixation information to assist the detection of video salient objects under weak supervision. On the one hand, we ponder how to better explore and utilize the information provided by fixation, and then propose a Position and Semantic Embedding (PSE) module to provide location and semantic guidance during the feature learning process. On the other hand, we achieve spatiotemporal feature modeling under weak supervision from the aspects of feature selection and feature contrast. A Semantics and Locality Query (SLQ) Competitor with semantic and locality constraints is designed to effectively select the most matching and accurate object query for spatiotemporal modeling. In addition, an Intra-Inter Mixed Contrastive (IIMC) model improves the spatiotemporal modeling capabilities under weak supervision by forming an intra-video and inter-video contrastive learning paradigm. Experimental results on five popular VSOD benchmarks indicate that our model outperforms other competitors on various evaluation metrics.
<div id='section'>Paperid: <span id='pid'>194, <a href='https://arxiv.org/pdf/2411.12615.pdf' target='_blank'>https://arxiv.org/pdf/2411.12615.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Yang, Nitish Mehta, Xiaoling Hu, Chao Chen, Chia-Ling Tsai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.12615">A Multimodal Approach Combining Structural and Cross-domain Textual Guidance for Weakly Supervised OCT Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate segmentation of Optical Coherence Tomography (OCT) images is crucial for diagnosing and monitoring retinal diseases. However, the labor-intensive nature of pixel-level annotation limits the scalability of supervised learning with large datasets. Weakly Supervised Semantic Segmentation (WSSS) provides a promising alternative by leveraging image-level labels. In this study, we propose a novel WSSS approach that integrates structural guidance with text-driven strategies to generate high-quality pseudo labels, significantly improving segmentation performance. In terms of visual information, our method employs two processing modules that exchange raw image features and structural features from OCT images, guiding the model to identify where lesions are likely to occur. In terms of textual information, we utilize large-scale pretrained models from cross-domain sources to implement label-informed textual guidance and synthetic descriptive integration with two textual processing modules that combine local semantic features with consistent synthetic descriptions. By fusing these visual and textual components within a multimodal framework, our approach enhances lesion localization accuracy. Experimental results on three OCT datasets demonstrate that our method achieves state-of-the-art performance, highlighting its potential to improve diagnostic accuracy and efficiency in medical imaging.
<div id='section'>Paperid: <span id='pid'>195, <a href='https://arxiv.org/pdf/2506.18042.pdf' target='_blank'>https://arxiv.org/pdf/2506.18042.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongdong Meng, Sheng Li, Hao Wu, Suqing Tian, Wenjun Ma, Guoping Wang, Xueqing Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18042">CmFNet: Cross-modal Fusion Network for Weakly-supervised Segmentation of Medical Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate automatic medical image segmentation relies on high-quality, dense annotations, which are costly and time-consuming. Weakly supervised learning provides a more efficient alternative by leveraging sparse and coarse annotations instead of dense, precise ones. However, segmentation performance degradation and overfitting caused by sparse annotations remain key challenges. To address these issues, we propose CmFNet, a novel 3D weakly supervised cross-modal medical image segmentation approach. CmFNet consists of three main components: a modality-specific feature learning network, a cross-modal feature learning network, and a hybrid-supervised learning strategy. Specifically, the modality-specific feature learning network and the cross-modal feature learning network effectively integrate complementary information from multi-modal images, enhancing shared features across modalities to improve segmentation performance. Additionally, the hybrid-supervised learning strategy guides segmentation through scribble supervision, intra-modal regularization, and inter-modal consistency, modeling spatial and contextual relationships while promoting feature alignment. Our approach effectively mitigates overfitting, delivering robust segmentation results. It excels in segmenting both challenging small tumor regions and common anatomical structures. Extensive experiments on a clinical cross-modal nasopharyngeal carcinoma (NPC) dataset (including CT and MR imaging) and the publicly available CT Whole Abdominal Organ dataset (WORD) show that our approach outperforms state-of-the-art weakly supervised methods. In addition, our approach also outperforms fully supervised methods when full annotation is used. Our approach can facilitate clinical therapy and benefit various specialists, including physicists, radiologists, pathologists, and oncologists.
<div id='section'>Paperid: <span id='pid'>196, <a href='https://arxiv.org/pdf/2506.14912.pdf' target='_blank'>https://arxiv.org/pdf/2506.14912.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dyah Adila, Shuai Zhang, Boran Han, Bonan Min, Yuyang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14912">CrEst: Credibility Estimation for Contexts in LLMs via Weak Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of contextual information has significantly enhanced the performance of large language models (LLMs) on knowledge-intensive tasks. However, existing methods often overlook a critical challenge: the credibility of context documents can vary widely, potentially leading to the propagation of unreliable information. In this paper, we introduce CrEst, a novel weakly supervised framework for assessing the credibility of context documents during LLM inference--without requiring manual annotations. Our approach is grounded in the insight that credible documents tend to exhibit higher semantic coherence with other credible documents, enabling automated credibility estimation through inter-document agreement. To incorporate credibility into LLM inference, we propose two integration strategies: a black-box approach for models without access to internal weights or activations, and a white-box method that directly modifies attention mechanisms. Extensive experiments across three model architectures and five datasets demonstrate that CrEst consistently outperforms strong baselines, achieving up to a 26.86% improvement in accuracy and a 3.49% increase in F1 score. Further analysis shows that CrEst maintains robust performance even under high-noise conditions.
<div id='section'>Paperid: <span id='pid'>197, <a href='https://arxiv.org/pdf/2505.01809.pdf' target='_blank'>https://arxiv.org/pdf/2505.01809.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoqi Li, Jiaming Liu, Nuowei Han, Liang Heng, Yandong Guo, Hao Dong, Yang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01809">3DWG: 3D Weakly Supervised Visual Grounding via Category and Instance-Level Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The 3D weakly-supervised visual grounding task aims to localize oriented 3D boxes in point clouds based on natural language descriptions without requiring annotations to guide model learning. This setting presents two primary challenges: category-level ambiguity and instance-level complexity. Category-level ambiguity arises from representing objects of fine-grained categories in a highly sparse point cloud format, making category distinction challenging. Instance-level complexity stems from multiple instances of the same category coexisting in a scene, leading to distractions during grounding. To address these challenges, we propose a novel weakly-supervised grounding approach that explicitly differentiates between categories and instances. In the category-level branch, we utilize extensive category knowledge from a pre-trained external detector to align object proposal features with sentence-level category features, thereby enhancing category awareness. In the instance-level branch, we utilize spatial relationship descriptions from language queries to refine object proposal features, ensuring clear differentiation among objects. These designs enable our model to accurately identify target-category objects while distinguishing instances within the same category. Compared to previous methods, our approach achieves state-of-the-art performance on three widely used benchmarks: Nr3D, Sr3D, and ScanRef.
<div id='section'>Paperid: <span id='pid'>198, <a href='https://arxiv.org/pdf/2511.12020.pdf' target='_blank'>https://arxiv.org/pdf/2511.12020.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xianglong Shi, Silin Cheng, Sirui Zhao, Yunhan Jiang, Enhong Chen, Yang Liu, Sebastien Ourselin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.12020">LIHE: Linguistic Instance-Split Hyperbolic-Euclidean Framework for Generalized Weakly-Supervised Referring Expression Comprehension</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing Weakly-Supervised Referring Expression Comprehension (WREC) methods, while effective, are fundamentally limited by a one-to-one mapping assumption, hindering their ability to handle expressions corresponding to zero or multiple targets in realistic scenarios. To bridge this gap, we introduce the Weakly-Supervised Generalized Referring Expression Comprehension task (WGREC), a more practical paradigm that handles expressions with variable numbers of referents. However, extending WREC to WGREC presents two fundamental challenges: supervisory signal ambiguity, where weak image-level supervision is insufficient for training a model to infer the correct number and identity of referents, and semantic representation collapse, where standard Euclidean similarity forces hierarchically-related concepts into non-discriminative clusters, blurring categorical boundaries. To tackle these challenges, we propose a novel WGREC framework named Linguistic Instance-Split Hyperbolic-Euclidean (LIHE), which operates in two stages. The first stage, Referential Decoupling, predicts the number of target objects and decomposes the complex expression into simpler sub-expressions. The second stage, Referent Grounding, then localizes these sub-expressions using HEMix, our innovative hybrid similarity module that synergistically combines the precise alignment capabilities of Euclidean proximity with the hierarchical modeling strengths of hyperbolic geometry. This hybrid approach effectively prevents semantic collapse while preserving fine-grained distinctions between related concepts. Extensive experiments demonstrate LIHE establishes the first effective weakly supervised WGREC baseline on gRefCOCO and Ref-ZOM, while HEMix achieves consistent improvements on standard REC benchmarks, improving IoU@0.5 by up to 2.5\%. The code is available at https://anonymous.4open.science/r/LIHE.
<div id='section'>Paperid: <span id='pid'>199, <a href='https://arxiv.org/pdf/2511.11407.pdf' target='_blank'>https://arxiv.org/pdf/2511.11407.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Manyu Li, Ruian He, Chenxi Ma, Weimin Tan, Bo Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.11407">MicroVQA++: High-Quality Microscopy Reasoning Dataset with Weakly Supervised Graphs for Multimodal Large Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal Large Language Models are increasingly applied to biomedical imaging, yet scientific reasoning for microscopy remains limited by the scarcity of large-scale, high-quality training data. We introduce MicroVQA++, a three-stage, large-scale and high-quality microscopy VQA corpus derived from the BIOMEDICA archive. Stage one bootstraps supervision from expert-validated figure-caption pairs sourced from peer-reviewed articles. Stage two applies HiCQA-Graph, a novel heterogeneous graph over images, captions, and QAs that fuses NLI-based textual entailment, CLIP-based vision-language alignment, and agent signals to identify and filter inconsistent samples. Stage three uses a MultiModal Large Language Model (MLLM) agent to generate multiple-choice questions (MCQ) followed by human screening. The resulting release comprises a large training split and a human-checked test split whose Bloom's level hard-sample distribution exceeds the MicroVQA benchmark. Our work delivers (i) a quality-controlled dataset that couples expert literature with graph-based filtering and human refinement; (ii) HiCQA-Graph, the first graph that jointly models (image, caption, QA) for cross-modal consistency filtering; (iii) evidence that careful data construction enables 4B-scale MLLMs to reach competitive microscopy reasoning performance (e.g., GPT-5) and achieve state-of-the-art performance among open-source MLLMs. Code and dataset will be released after the review process concludes.
<div id='section'>Paperid: <span id='pid'>200, <a href='https://arxiv.org/pdf/2511.04035.pdf' target='_blank'>https://arxiv.org/pdf/2511.04035.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongji Gao, Chenda Liao, Changliang Liu, Matthew Wiesner, Leibny Paola Garcia, Daniel Povey, Sanjeev Khudanpur, Jian Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.04035">WST: Weakly Supervised Transducer for Automatic Speech Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Recurrent Neural Network-Transducer (RNN-T) is widely adopted in end-to-end (E2E) automatic speech recognition (ASR) tasks but depends heavily on large-scale, high-quality annotated data, which are often costly and difficult to obtain. To mitigate this reliance, we propose a Weakly Supervised Transducer (WST), which integrates a flexible training graph designed to robustly handle errors in the transcripts without requiring additional confidence estimation or auxiliary pre-trained models. Empirical evaluations on synthetic and industrial datasets reveal that WST effectively maintains performance even with transcription error rates of up to 70%, consistently outperforming existing Connectionist Temporal Classification (CTC)-based weakly supervised approaches, such as Bypass Temporal Classification (BTC) and Omni-Temporal Classification (OTC). These results demonstrate the practical utility and robustness of WST in realistic ASR settings. The implementation will be publicly available.
<div id='section'>Paperid: <span id='pid'>201, <a href='https://arxiv.org/pdf/2505.22230.pdf' target='_blank'>https://arxiv.org/pdf/2505.22230.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhisong Wang, Yiwen Ye, Ziyang Chen, Yong Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22230">Enjoying Information Dividend: Gaze Track-based Medical Weakly Supervised Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation (WSSS) in medical imaging struggles with effectively using sparse annotations. One promising direction for WSSS leverages gaze annotations, captured via eye trackers that record regions of interest during diagnostic procedures. However, existing gaze-based methods, such as GazeMedSeg, do not fully exploit the rich information embedded in gaze data. In this paper, we propose GradTrack, a framework that utilizes physicians' gaze track, including fixation points, durations, and temporal order, to enhance WSSS performance. GradTrack comprises two key components: Gaze Track Map Generation and Track Attention, which collaboratively enable progressive feature refinement through multi-level gaze supervision during the decoding process. Experiments on the Kvasir-SEG and NCI-ISBI datasets demonstrate that GradTrack consistently outperforms existing gaze-based methods, achieving Dice score improvements of 3.21\% and 2.61\%, respectively. Moreover, GradTrack significantly narrows the performance gap with fully supervised models such as nnUNet.
<div id='section'>Paperid: <span id='pid'>202, <a href='https://arxiv.org/pdf/2510.05196.pdf' target='_blank'>https://arxiv.org/pdf/2510.05196.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daqian Shi, Xiaolei Diao, Jinge Wu, Honghan Wu, Xiongfeng Tang, Felix Naughton, Paulina Bondaronek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05196">Graph-based LLM over Semi-Structured Population Data for Dynamic Policy Response</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Timely and accurate analysis of population-level data is crucial for effective decision-making during public health emergencies such as the COVID-19 pandemic. However, the massive input of semi-structured data, including structured demographic information and unstructured human feedback, poses significant challenges to conventional analysis methods. Manual expert-driven assessments, though accurate, are inefficient, while standard NLP pipelines often require large task-specific labeled datasets and struggle with generalization across diverse domains. To address these challenges, we propose a novel graph-based reasoning framework that integrates large language models with structured demographic attributes and unstructured public feedback in a weakly supervised pipeline. The proposed approach dynamically models evolving citizen needs into a need-aware graph, enabling population-specific analyses based on key features such as age, gender, and the Index of Multiple Deprivation. It generates interpretable insights to inform responsive health policy decision-making. We test our method using a real-world dataset, and preliminary experimental results demonstrate its feasibility. This approach offers a scalable solution for intelligent population health monitoring in resource-constrained clinical and governmental settings.
<div id='section'>Paperid: <span id='pid'>203, <a href='https://arxiv.org/pdf/2509.01878.pdf' target='_blank'>https://arxiv.org/pdf/2509.01878.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Scarlett Raine, Tobias Fischer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01878">AI-Driven Marine Robotics: Emerging Trends in Underwater Perception and Ecosystem Monitoring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Marine ecosystems face increasing pressure due to climate change, driving the need for scalable, AI-powered monitoring solutions. This paper examines the rapid emergence of underwater AI as a major research frontier and analyzes the factors that have transformed marine perception from a niche application into a catalyst for AI innovation. We identify three convergent drivers: environmental necessity for ecosystem-scale monitoring, democratization of underwater datasets through citizen science platforms, and researcher migration from saturated terrestrial computer vision domains. Our analysis reveals how unique underwater challenges - turbidity, cryptic species detection, expert annotation bottlenecks, and cross-ecosystem generalization - are driving fundamental advances in weakly supervised learning, open-set recognition, and robust perception under degraded conditions. We survey emerging trends in datasets, scene understanding and 3D reconstruction, highlighting the paradigm shift from passive observation toward AI-driven, targeted intervention capabilities. The paper demonstrates how underwater constraints are pushing the boundaries of foundation models, self-supervised learning, and perception, with methodological innovations that extend far beyond marine applications to benefit general computer vision, robotics, and environmental monitoring.
<div id='section'>Paperid: <span id='pid'>204, <a href='https://arxiv.org/pdf/2503.12068.pdf' target='_blank'>https://arxiv.org/pdf/2503.12068.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingchen Tang, Lei Fan, Maurice Pagnucco, Yang Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12068">Prototype-Based Image Prompting for Weakly Supervised Histopathological Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised image segmentation with image-level labels has drawn attention due to the high cost of pixel-level annotations. Traditional methods using Class Activation Maps (CAMs) often highlight only the most discriminative regions, leading to incomplete masks. Recent approaches that introduce textual information struggle with histopathological images due to inter-class homogeneity and intra-class heterogeneity. In this paper, we propose a prototype-based image prompting framework for histopathological image segmentation. It constructs an image bank from the training set using clustering, extracting multiple prototype features per class to capture intra-class heterogeneity. By designing a matching loss between input features and class-specific prototypes using contrastive learning, our method addresses inter-class homogeneity and guides the model to generate more accurate CAMs. Experiments on four datasets (LUAD-HistoSeg, BCSS-WSSS, GCSS, and BCSS) show that our method outperforms existing weakly supervised segmentation approaches, setting new benchmarks in histopathological image segmentation.
<div id='section'>Paperid: <span id='pid'>205, <a href='https://arxiv.org/pdf/2501.06038.pdf' target='_blank'>https://arxiv.org/pdf/2501.06038.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tsui Qin Mok, Shuyong Gao, Haozhe Xing, Miaoyang He, Yan Wang, Wenqiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.06038">A Holistically Point-guided Text Framework for Weakly-Supervised Camouflaged Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-Supervised Camouflaged Object Detection (WSCOD) has gained popularity for its promise to train models with weak labels to segment objects that visually blend into their surroundings. Recently, some methods using sparsely-annotated supervision shown promising results through scribbling in WSCOD, while point-text supervision remains underexplored. Hence, this paper introduces a novel holistically point-guided text framework for WSCOD by decomposing into three phases: segment, choose, train. Specifically, we propose Point-guided Candidate Generation (PCG), where the point's foreground serves as a correction for the text path to explicitly correct and rejuvenate the loss detection object during the mask generation process (SEGMENT). We also introduce a Qualified Candidate Discriminator (QCD) to choose the optimal mask from a given text prompt using CLIP (CHOOSE), and employ the chosen pseudo mask for training with a self-supervised Vision Transformer (TRAIN). Additionally, we developed a new point-supervised dataset (P2C-COD) and a text-supervised dataset (T-COD). Comprehensive experiments on four benchmark datasets demonstrate our method outperforms state-of-the-art methods by a large margin, and also outperforms some existing fully-supervised camouflaged object detection methods.
<div id='section'>Paperid: <span id='pid'>206, <a href='https://arxiv.org/pdf/2411.11287.pdf' target='_blank'>https://arxiv.org/pdf/2411.11287.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Scarlett Raine, Frederic Maire, Niko Suenderhauf, Tobias Fischer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11287">Reducing Label Dependency for Underwater Scene Understanding: A Survey of Datasets, Techniques and Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Underwater surveys provide long-term data for informing management strategies, monitoring coral reef health, and estimating blue carbon stocks. Advances in broad-scale survey methods, such as robotic underwater vehicles, have increased the range of marine surveys but generate large volumes of imagery requiring analysis. Computer vision methods such as semantic segmentation aid automated image analysis, but typically rely on fully supervised training with extensive labelled data. While ground truth label masks for tasks like street scene segmentation can be quickly and affordably generated by non-experts through crowdsourcing services like Amazon Mechanical Turk, ecology presents greater challenges. The complexity of underwater images, coupled with the specialist expertise needed to accurately identify species at the pixel level, makes this process costly, time-consuming, and heavily dependent on domain experts. In recent years, some works have performed automated analysis of underwater imagery, and a smaller number of studies have focused on weakly supervised approaches which aim to reduce the expert-provided labelled data required. This survey focuses on approaches which reduce dependency on human expert input, while reviewing the prior and related approaches to position these works in the wider field of underwater perception. Further, we offer an overview of coastal ecosystems and the challenges of underwater imagery. We provide background on weakly and self-supervised deep learning and integrate these elements into a taxonomy that centres on the intersection of underwater monitoring, computer vision, and deep learning, while motivating approaches for weakly supervised deep learning with reduced dependency on domain expert data annotations. Lastly, the survey examines available datasets and platforms, and identifies gaps, barriers, and opportunities for automating underwater surveys.
<div id='section'>Paperid: <span id='pid'>207, <a href='https://arxiv.org/pdf/2601.15786.pdf' target='_blank'>https://arxiv.org/pdf/2601.15786.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenzhong Wang, Yongjie Hou, Chenggong Huang, Yuxuan Du, Dacheng Tao, Min Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.15786">Endowing Molecular Language with Geometry Perception via Modality Compensation for High-Throughput Quantum Hamiltonian Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The quantum Hamiltonian is a fundamental property that governs a molecule's electronic structure and behavior, and its calculation and prediction are paramount in computational chemistry and materials science. Accurate prediction is highly reliant on extensive training data, including precise molecular geometries and the Hamiltonian matrices, which are expensive to acquire via either experimental or computational methods. Towards a fast yet accurate method for Hamiltonian prediction, we first introduce a geometry information-aware molecular language model to bypass the use of expensive molecular geometries by only using the readily available molecular language -- simplified molecular input line entry system (SMILES). Our method employs multimodal alignment to bridge the relationship between SMILES strings and their corresponding molecular geometries. Recognizing that the molecular language inherently lacks explicit geometric information, we propose a geometry modality compensation strategy to imbue molecular language representations with essential geometric features, thereby enabling accurate predictions using SMILES. In addition, given the high cost of acquiring Hamiltonian data, we devise a weakly supervised strategy to fine-tune the molecular language model, thus improving the data efficiency. Theoretically, we prove that the prediction generalization error without explicit molecular geometry can be bounded through our modality compensation scheme. Empirically, our method achieves superior computational efficiency, providing up to 100x speedup over conventional quantum mechanical methods while maintaining comparable prediction accuracy. We further demonstrate the practical case study of our approach in the screening of electrolyte formulations.
<div id='section'>Paperid: <span id='pid'>208, <a href='https://arxiv.org/pdf/2601.01684.pdf' target='_blank'>https://arxiv.org/pdf/2601.01684.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhichao Xu, Shengyao Zhuang, Crystina Zhang, Xueguang Ma, Yijun Tian, Maitrey Mehta, Jimmy Lin, Vivek Srikumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.01684">LACONIC: Dense-Level Effectiveness for Scalable Sparse Retrieval via a Two-Phase Training Curriculum</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While dense retrieval models have become the standard for state-of-the-art information retrieval, their deployment is often constrained by high memory requirements and reliance on GPU accelerators for vector similarity search. Learned sparse retrieval offers a compelling alternative by enabling efficient search via inverted indices, yet it has historically received less attention than dense approaches. In this report, we introduce LACONIC, a family of learned sparse retrievers based on the Llama-3 architecture (1B, 3B, and 8B). We propose a streamlined two-phase training curriculum consisting of (1) weakly supervised pre-finetuning to adapt causal LLMs for bidirectional contextualization and (2) high-signal finetuning using curated hard negatives. Our results demonstrate that LACONIC effectively bridges the performance gap with dense models: the 8B variant achieves a state-of-the-art 60.2 nDCG on the MTEB Retrieval benchmark, ranking 15th on the leaderboard as of January 1, 2026, while utilizing 71\% less index memory than an equivalent dense model. By delivering high retrieval effectiveness on commodity CPU hardware with a fraction of the compute budget required by competing models, LACONIC provides a scalable and efficient solution for real-world search applications.
<div id='section'>Paperid: <span id='pid'>209, <a href='https://arxiv.org/pdf/2511.17735.pdf' target='_blank'>https://arxiv.org/pdf/2511.17735.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Samuel Stevens, Jacob Beattie, Tanya Berger-Wolf, Yu Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17735">Towards Open-Ended Visual Scientific Discovery with Sparse Autoencoders</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scientific archives now contain hundreds of petabytes of data across genomics, ecology, climate, and molecular biology that could reveal undiscovered patterns if systematically analyzed at scale. Large-scale, weakly-supervised datasets in language and vision have driven the development of foundation models whose internal representations encode structure (patterns, co-occurrences and statistical regularities) beyond their training objectives. Most existing methods extract structure only for pre-specified targets; they excel at confirmation but do not support open-ended discovery of unknown patterns. We ask whether sparse autoencoders (SAEs) can enable open-ended feature discovery from foundation model representations. We evaluate this question in controlled rediscovery studies, where the learned SAE features are tested for alignment with semantic concepts on a standard segmentation benchmark and compared against strong label-free alternatives on concept-alignment metrics. Applied to ecological imagery, the same procedure surfaces fine-grained anatomical structure without access to segmentation or part labels, providing a scientific case study with ground-truth validation. While our experiments focus on vision with an ecology case study, the method is domain-agnostic and applicable to models in other sciences (e.g., proteins, genomics, weather). Our results indicate that sparse decomposition provides a practical instrument for exploring what scientific foundation models have learned, an important prerequisite for moving from confirmation to genuine discovery.
<div id='section'>Paperid: <span id='pid'>210, <a href='https://arxiv.org/pdf/2510.00072.pdf' target='_blank'>https://arxiv.org/pdf/2510.00072.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenhui Xu, Fuxun Yu, Michael J. Bianco, Jacob Kovarskiy, Raphael Tang, Qi Zhang, Zirui Xu, Will LeVine, Brandon Dubbs, Heming Liao, Cassandra Burgess, Suvam Bag, Jay Patravali, Rupanjali Kukal, Mikael Figueroa, Rishi Madhok, Nikolaos Karianakis, Jinjun Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00072">Geo-R1: Unlocking VLM Geospatial Reasoning with Cross-View Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Geo-R1, a reasoning-centric post-training framework that unlocks geospatial reasoning in vision-language models by combining thinking scaffolding and elevating. In the scaffolding stage, Geo-R1 instills a ``geospatial thinking paradigm" via supervised fine-tuning on synthetic chain-of-thought exemplars, enabling models to connect visual cues with geographic priors without costly human reasoning annotations. In the elevating stage, it uses GRPO-based reinforcement learning on a weakly-supervised cross-view pairing proxy. This design supplies a verifiable and scalable reward signal: teaching models to capture and reconcile features across modalities, and harnessing reasoning for accurate prediction. Geo-R1 extends geospatial modeling from domain pretraining / supervised finetuning to reasoning-first post-training, and achieves state-of-the-art performance across various geospatial reasoning benchmarks. Our model is available at https://huggingface.co/miniHui/Geo-R1.
<div id='section'>Paperid: <span id='pid'>211, <a href='https://arxiv.org/pdf/2507.11344.pdf' target='_blank'>https://arxiv.org/pdf/2507.11344.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zara Hall, Melanie Subbiah, Thomas P Zollo, Kathleen McKeown, Richard Zemel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11344">Guiding LLM Decision-Making with Fairness Reward Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models are increasingly used to support high-stakes decisions, potentially influencing who is granted bail or receives a loan. Naive chain-of-thought sampling can improve average decision accuracy, but has also been shown to amplify unfair bias. To address this challenge and enable the trustworthy use of reasoning models in high-stakes decision-making, we propose a framework for training a generalizable Fairness Reward Model (FRM). Our model assigns a fairness score to LLM reasoning, enabling the system to down-weight biased trajectories and favor equitable ones when aggregating decisions across reasoning chains. We show that a single Fairness Reward Model, trained on weakly supervised, LLM-annotated examples of biased versus unbiased reasoning, transfers across tasks, domains, and model families without additional fine-tuning. Applied to real-world decision-making tasks including recidivism prediction and social media moderation, we show that our approach consistently improves fairness while matching, or even surpassing, baseline accuracy.
<div id='section'>Paperid: <span id='pid'>212, <a href='https://arxiv.org/pdf/2506.10233.pdf' target='_blank'>https://arxiv.org/pdf/2506.10233.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ana Lawry Aguila, Peirong Liu, Oula Puonti, Juan Eugenio Iglesias
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10233">Conditional diffusion models for guided anomaly detection in brain images using fluid-driven anomaly randomization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Supervised machine learning has enabled accurate pathology detection in brain MRI, but requires training data from diseased subjects that may not be readily available in some scenarios, for example, in the case of rare diseases. Reconstruction-based unsupervised anomaly detection, in particular using diffusion models, has gained popularity in the medical field as it allows for training on healthy images alone, eliminating the need for large disease-specific cohorts. These methods assume that a model trained on normal data cannot accurately represent or reconstruct anomalies. However, this assumption often fails with models failing to reconstruct healthy tissue or accurately reconstruct abnormal regions i.e., failing to remove anomalies. In this work, we introduce a novel conditional diffusion model framework for anomaly detection and healthy image reconstruction in brain MRI. Our weakly supervised approach integrates synthetically generated pseudo-pathology images into the modeling process to better guide the reconstruction of healthy images. To generate these pseudo-pathologies, we apply fluid-driven anomaly randomization to augment real pathology segmentation maps from an auxiliary dataset, ensuring that the synthetic anomalies are both realistic and anatomically coherent. We evaluate our model's ability to detect pathology, using both synthetic anomaly datasets and real pathology from the ATLAS dataset. In our extensive experiments, our model: (i) consistently outperforms variational autoencoders, and conditional and unconditional latent diffusion; and (ii) surpasses on most datasets, the performance of supervised inpainting methods with access to paired diseased/healthy images.
<div id='section'>Paperid: <span id='pid'>213, <a href='https://arxiv.org/pdf/2503.13895.pdf' target='_blank'>https://arxiv.org/pdf/2503.13895.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinliang Zhang, Lei Zhu, Shuang Zeng, Hangzhou He, Ourui Fu, Zhengjian Yao, Zhaoheng Xie, Yanye Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13895">Exploiting Inherent Class Label: Towards Robust Scribble Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scribble-based weakly supervised semantic segmentation leverages only a few annotated pixels as labels to train a segmentation model, presenting significant potential for reducing the human labor involved in the annotation process. This approach faces two primary challenges: first, the sparsity of scribble annotations can lead to inconsistent predictions due to limited supervision; second, the variability in scribble annotations, reflecting differing human annotator preferences, can prevent the model from consistently capturing the discriminative regions of objects, potentially leading to unstable predictions. To address these issues, we propose a holistic framework, the class-driven scribble promotion network, for robust scribble-supervised semantic segmentation. This framework not only utilizes the provided scribble annotations but also leverages their associated class labels to generate reliable pseudo-labels. Within the network, we introduce a localization rectification module to mitigate noisy labels and a distance perception module to identify reliable regions surrounding scribble annotations and pseudo-labels. In addition, we introduce new large-scale benchmarks, ScribbleCOCO and ScribbleCityscapes, accompanied by a scribble simulation algorithm that enables evaluation across varying scribble styles. Our method demonstrates competitive performance in both accuracy and robustness, underscoring its superiority over existing approaches. The datasets and the codes will be made publicly available.
<div id='section'>Paperid: <span id='pid'>214, <a href='https://arxiv.org/pdf/2502.03856.pdf' target='_blank'>https://arxiv.org/pdf/2502.03856.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lin Li, Chuhan Zhang, Dong Zhang, Chong Sun, Chen Li, Long Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03856">Taking A Closer Look at Interacting Objects: Interaction-Aware Open Vocabulary Scene Graph Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Today's open vocabulary scene graph generation (OVSGG) extends traditional SGG by recognizing novel objects and relationships beyond predefined categories, leveraging the knowledge from pre-trained large-scale models. Most existing methods adopt a two-stage pipeline: weakly supervised pre-training with image captions and supervised fine-tuning (SFT) on fully annotated scene graphs. Nonetheless, they omit explicit modeling of interacting objects and treat all objects equally, resulting in mismatched relation pairs. To this end, we propose an interaction-aware OVSGG framework INOVA. During pre-training, INOVA employs an interaction-aware target generation strategy to distinguish interacting objects from non-interacting ones. In SFT, INOVA devises an interaction-guided query selection tactic to prioritize interacting objects during bipartite graph matching. Besides, INOVA is equipped with an interaction-consistent knowledge distillation to enhance the robustness by pushing interacting object pairs away from the background. Extensive experiments on two benchmarks (VG and GQA) show that INOVA achieves state-of-the-art performance, demonstrating the potential of interaction-aware mechanisms for real-world applications.
<div id='section'>Paperid: <span id='pid'>215, <a href='https://arxiv.org/pdf/2412.12791.pdf' target='_blank'>https://arxiv.org/pdf/2412.12791.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiping Ge, Qiang Chen, Zhiwei Jiang, Yafeng Yin, Liu Qin, Ziyao Chen, Qing Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12791">Implicit Location-Caption Alignment via Complementary Masking for Weakly-Supervised Dense Video Captioning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-Supervised Dense Video Captioning (WSDVC) aims to localize and describe all events of interest in a video without requiring annotations of event boundaries. This setting poses a great challenge in accurately locating the temporal location of event, as the relevant supervision is unavailable. Existing methods rely on explicit alignment constraints between event locations and captions, which involve complex event proposal procedures during both training and inference. To tackle this problem, we propose a novel implicit location-caption alignment paradigm by complementary masking, which simplifies the complex event proposal and localization process while maintaining effectiveness. Specifically, our model comprises two components: a dual-mode video captioning module and a mask generation module. The dual-mode video captioning module captures global event information and generates descriptive captions, while the mask generation module generates differentiable positive and negative masks for localizing the events. These masks enable the implicit alignment of event locations and captions by ensuring that captions generated from positively and negatively masked videos are complementary, thereby forming a complete video description. In this way, even under weak supervision, the event location and event caption can be aligned implicitly. Extensive experiments on the public datasets demonstrate that our method outperforms existing weakly-supervised methods and achieves competitive results compared to fully-supervised methods.
<div id='section'>Paperid: <span id='pid'>216, <a href='https://arxiv.org/pdf/2410.20371.pdf' target='_blank'>https://arxiv.org/pdf/2410.20371.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxing Huang, Jingyi Zhang, Kai Jiang, Shijian Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20371">Open-Vocabulary Object Detection via Language Hierarchy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent studies on generalizable object detection have attracted increasing attention with additional weak supervision from large-scale datasets with image-level labels. However, weakly-supervised detection learning often suffers from image-to-box label mismatch, i.e., image-level labels do not convey precise object information. We design Language Hierarchical Self-training (LHST) that introduces language hierarchy into weakly-supervised detector training for learning more generalizable detectors. LHST expands the image-level labels with language hierarchy and enables co-regularization between the expanded labels and self-training. Specifically, the expanded labels regularize self-training by providing richer supervision and mitigating the image-to-box label mismatch, while self-training allows assessing and selecting the expanded labels according to the predicted reliability. In addition, we design language hierarchical prompt generation that introduces language hierarchy into prompt generation which helps bridge the vocabulary gaps between training and testing. Extensive experiments show that the proposed techniques achieve superior generalization performance consistently across 14 widely studied object detection datasets.
<div id='section'>Paperid: <span id='pid'>217, <a href='https://arxiv.org/pdf/2601.21458.pdf' target='_blank'>https://arxiv.org/pdf/2601.21458.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Midou Guo, Qilin Yin, Wei Lu, Xiangyang Luo, Rui Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.21458">Mining Forgery Traces from Reconstruction Error: A Weakly Supervised Framework for Multimodal Deepfake Temporal Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern deepfakes have evolved into localized and intermittent manipulations that require fine-grained temporal localization. The prohibitive cost of frame-level annotation makes weakly supervised methods a practical necessity, which rely only on video-level labels. To this end, we propose Reconstruction-based Temporal Deepfake Localization (RT-DeepLoc), a weakly supervised temporal forgery localization framework that identifies forgeries via reconstruction errors. Our framework uses a Masked Autoencoder (MAE) trained exclusively on authentic data to learn its intrinsic spatiotemporal patterns; this allows the model to produce significant reconstruction discrepancies for forged segments, effectively providing the missing fine-grained cues for localization. To robustly leverage these indicators, we introduce a novel Asymmetric Intra-video Contrastive Loss (AICL). By focusing on the compactness of authentic features guided by these reconstruction cues, AICL establishes a stable decision boundary that enhances local discrimination while preserving generalization to unseen forgeries. Extensive experiments on large-scale datasets, including LAV-DF, demonstrate that RT-DeepLoc achieves state-of-the-art performance in weakly-supervised temporal forgery localization.
<div id='section'>Paperid: <span id='pid'>218, <a href='https://arxiv.org/pdf/2508.05585.pdf' target='_blank'>https://arxiv.org/pdf/2508.05585.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haijing Liu, Tao Pu, Hefeng Wu, Keze Wang, Liang Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05585">DART: Dual Adaptive Refinement Transfer for Open-Vocabulary Multi-Label Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-Vocabulary Multi-Label Recognition (OV-MLR) aims to identify multiple seen and unseen object categories within an image, requiring both precise intra-class localization to pinpoint objects and effective inter-class reasoning to model complex category dependencies. While Vision-Language Pre-training (VLP) models offer a strong open-vocabulary foundation, they often struggle with fine-grained localization under weak supervision and typically fail to explicitly leverage structured relational knowledge beyond basic semantics, limiting performance especially for unseen classes. To overcome these limitations, we propose the Dual Adaptive Refinement Transfer (DART) framework. DART enhances a frozen VLP backbone via two synergistic adaptive modules. For intra-class refinement, an Adaptive Refinement Module (ARM) refines patch features adaptively, coupled with a novel Weakly Supervised Patch Selecting (WPS) loss that enables discriminative localization using only image-level labels. Concurrently, for inter-class transfer, an Adaptive Transfer Module (ATM) leverages a Class Relationship Graph (CRG), constructed using structured knowledge mined from a Large Language Model (LLM), and employs graph attention network to adaptively transfer relational information between class representations. DART is the first framework, to our knowledge, to explicitly integrate external LLM-derived relational knowledge for adaptive inter-class transfer while simultaneously performing adaptive intra-class refinement under weak supervision for OV-MLR. Extensive experiments on challenging benchmarks demonstrate that our DART achieves new state-of-the-art performance, validating its effectiveness.
<div id='section'>Paperid: <span id='pid'>219, <a href='https://arxiv.org/pdf/2508.02179.pdf' target='_blank'>https://arxiv.org/pdf/2508.02179.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenbo Xu, Wei Lu, Xiangyang Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02179">Weakly Supervised Multimodal Temporal Forgery Localization via Multitask Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The spread of Deepfake videos has caused a trust crisis and impaired social stability. Although numerous approaches have been proposed to address the challenges of Deepfake detection and localization, there is still a lack of systematic research on the weakly supervised multimodal fine-grained temporal forgery localization (WS-MTFL). In this paper, we propose a novel weakly supervised multimodal temporal forgery localization via multitask learning (WMMT), which addresses the WS-MTFL under the multitask learning paradigm. WMMT achieves multimodal fine-grained Deepfake detection and temporal partial forgery localization using merely video-level annotations. Specifically, visual and audio modality detection are formulated as two binary classification tasks. The multitask learning paradigm is introduced to integrate these tasks into a multimodal task. Furthermore, WMMT utilizes a Mixture-of-Experts structure to adaptively select appropriate features and localization head, achieving excellent flexibility and localization precision in WS-MTFL. A feature enhancement module with temporal property preserving attention mechanism is proposed to identify the intra- and inter-modality feature deviation and construct comprehensive video features. To further explore the temporal information for weakly supervised learning, an extensible deviation perceiving loss has been proposed, which aims to enlarge the deviation of adjacent segments of the forged samples and reduce the deviation of genuine samples. Extensive experiments demonstrate the effectiveness of multitask learning for WS-MTFL, and the WMMT achieves comparable results to fully supervised approaches in several evaluation metrics.
<div id='section'>Paperid: <span id='pid'>220, <a href='https://arxiv.org/pdf/2507.16596.pdf' target='_blank'>https://arxiv.org/pdf/2507.16596.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenbo Xu, Junyan Wu, Wei Lu, Xiangyang Luo, Qian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16596">A Multimodal Deviation Perceiving Framework for Weakly-Supervised Temporal Forgery Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current researches on Deepfake forensics often treat detection as a classification task or temporal forgery localization problem, which are usually restrictive, time-consuming, and challenging to scale for large datasets. To resolve these issues, we present a multimodal deviation perceiving framework for weakly-supervised temporal forgery localization (MDP), which aims to identify temporal partial forged segments using only video-level annotations. The MDP proposes a novel multimodal interaction mechanism (MI) and an extensible deviation perceiving loss to perceive multimodal deviation, which achieves the refined start and end timestamps localization of forged segments. Specifically, MI introduces a temporal property preserving cross-modal attention to measure the relevance between the visual and audio modalities in the probabilistic embedding space. It could identify the inter-modality deviation and construct comprehensive video features for temporal forgery localization. To explore further temporal deviation for weakly-supervised learning, an extensible deviation perceiving loss has been proposed, aiming at enlarging the deviation of adjacent segments of the forged samples and reducing that of genuine samples. Extensive experiments demonstrate the effectiveness of the proposed framework and achieve comparable results to fully-supervised approaches in several evaluation metrics.
<div id='section'>Paperid: <span id='pid'>221, <a href='https://arxiv.org/pdf/2506.09445.pdf' target='_blank'>https://arxiv.org/pdf/2506.09445.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ayush Gupta, Anirban Roy, Rama Chellappa, Nathaniel D. Bastian, Alvaro Velasquez, Susmit Jha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09445">TOGA: Temporally Grounded Open-Ended Video QA with Weak Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address the problem of video question answering (video QA) with temporal grounding in a weakly supervised setup, without any temporal annotations. Given a video and a question, we generate an open-ended answer grounded with the start and end time. For this task, we propose TOGA: a vision-language model for Temporally Grounded Open-Ended Video QA with Weak Supervision. We instruct-tune TOGA to jointly generate the answer and the temporal grounding. We operate in a weakly supervised setup where the temporal grounding annotations are not available. We generate pseudo labels for temporal grounding and ensure the validity of these labels by imposing a consistency constraint between the question of a grounding response and the response generated by a question referring to the same temporal segment. We notice that jointly generating the answers with the grounding improves performance on question answering as well as grounding. We evaluate TOGA on grounded QA and open-ended QA tasks. For grounded QA, we consider the NExT-GQA benchmark which is designed to evaluate weakly supervised grounded question answering. For open-ended QA, we consider the MSVD-QA and ActivityNet-QA benchmarks. We achieve state-of-the-art performance for both tasks on these benchmarks.
<div id='section'>Paperid: <span id='pid'>222, <a href='https://arxiv.org/pdf/2505.01880.pdf' target='_blank'>https://arxiv.org/pdf/2505.01880.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyan Wu, Wenbo Xu, Wei Lu, Xiangyang Luo, Rui Yang, Shize Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01880">Weakly-supervised Audio Temporal Forgery Localization via Progressive Audio-language Co-learning Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio temporal forgery localization (ATFL) aims to find the precise forgery regions of the partial spoof audio that is purposefully modified. Existing ATFL methods rely on training efficient networks using fine-grained annotations, which are obtained costly and challenging in real-world scenarios. To meet this challenge, in this paper, we propose a progressive audio-language co-learning network (LOCO) that adopts co-learning and self-supervision manners to prompt localization performance under weak supervision scenarios. Specifically, an audio-language co-learning module is first designed to capture forgery consensus features by aligning semantics from temporal and global perspectives. In this module, forgery-aware prompts are constructed by using utterance-level annotations together with learnable prompts, which can incorporate semantic priors into temporal content features dynamically. In addition, a forgery localization module is applied to produce forgery proposals based on fused forgery-class activation sequences. Finally, a progressive refinement strategy is introduced to generate pseudo frame-level labels and leverage supervised semantic contrastive learning to amplify the semantic distinction between real and fake content, thereby continuously optimizing forgery-aware features. Extensive experiments show that the proposed LOCO achieves SOTA performance on three public benchmarks.
<div id='section'>Paperid: <span id='pid'>223, <a href='https://arxiv.org/pdf/2409.04758.pdf' target='_blank'>https://arxiv.org/pdf/2409.04758.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuchang Ye, Mingyuan Meng, Mingjian Li, Dagan Feng, Jinman Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.04758">SGSeg: Enabling Text-free Inference in Language-guided Segmentation of Chest X-rays via Self-guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Segmentation of infected areas in chest X-rays is pivotal for facilitating the accurate delineation of pulmonary structures and pathological anomalies. Recently, multi-modal language-guided image segmentation methods have emerged as a promising solution for chest X-rays where the clinical text reports, depicting the assessment of the images, are used as guidance. Nevertheless, existing language-guided methods require clinical reports alongside the images, and hence, they are not applicable for use in image segmentation in a decision support context, but rather limited to retrospective image analysis after clinical reporting has been completed. In this study, we propose a self-guided segmentation framework (SGSeg) that leverages language guidance for training (multi-modal) while enabling text-free inference (uni-modal), which is the first that enables text-free inference in language-guided segmentation. We exploit the critical location information of both pulmonary and pathological structures depicted in the text reports and introduce a novel localization-enhanced report generation (LERG) module to generate clinical reports for self-guidance. Our LERG integrates an object detector and a location-based attention aggregator, weakly-supervised by a location-aware pseudo-label extraction module. Extensive experiments on a well-benchmarked QaTa-COV19 dataset demonstrate that our SGSeg achieved superior performance than existing uni-modal segmentation methods and closely matched the state-of-the-art performance of multi-modal language-guided segmentation methods.
<div id='section'>Paperid: <span id='pid'>224, <a href='https://arxiv.org/pdf/2510.17384.pdf' target='_blank'>https://arxiv.org/pdf/2510.17384.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiajin Tang, Zhengxuan Wei, Ge Zheng, Sibei Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17384">Closed-Loop Transfer for Weakly-supervised Affordance Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans can perform previously unexperienced interactions with novel objects simply by observing others engage with them. Weakly-supervised affordance grounding mimics this process by learning to locate object regions that enable actions on egocentric images, using exocentric interaction images with image-level annotations. However, extracting affordance knowledge solely from exocentric images and transferring it one-way to egocentric images limits the applicability of previous works in complex interaction scenarios. Instead, this study introduces LoopTrans, a novel closed-loop framework that not only transfers knowledge from exocentric to egocentric but also transfers back to enhance exocentric knowledge extraction. Within LoopTrans, several innovative mechanisms are introduced, including unified cross-modal localization and denoising knowledge distillation, to bridge domain gaps between object-centered egocentric and interaction-centered exocentric images while enhancing knowledge transfer. Experiments show that LoopTrans achieves consistent improvements across all metrics on image and video benchmarks, even handling challenging scenarios where object interaction regions are fully occluded by the human body.
<div id='section'>Paperid: <span id='pid'>225, <a href='https://arxiv.org/pdf/2507.12015.pdf' target='_blank'>https://arxiv.org/pdf/2507.12015.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoxun Li, Leyuan Qu, Jiaxi Hu, Taihao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12015">EME-TTS: Unlocking the Emphasis and Emotion Link in Speech Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, emotional Text-to-Speech (TTS) synthesis and emphasis-controllable speech synthesis have advanced significantly. However, their interaction remains underexplored. We propose Emphasis Meets Emotion TTS (EME-TTS), a novel framework designed to address two key research questions: (1) how to effectively utilize emphasis to enhance the expressiveness of emotional speech, and (2) how to maintain the perceptual clarity and stability of target emphasis across different emotions. EME-TTS employs weakly supervised learning with emphasis pseudo-labels and variance-based emphasis features. Additionally, the proposed Emphasis Perception Enhancement (EPE) block enhances the interaction between emotional signals and emphasis positions. Experimental results show that EME-TTS, when combined with large language models for emphasis position prediction, enables more natural emotional speech synthesis while preserving stable and distinguishable target emphasis across emotions. Synthesized samples are available on-line.
<div id='section'>Paperid: <span id='pid'>226, <a href='https://arxiv.org/pdf/2502.19698.pdf' target='_blank'>https://arxiv.org/pdf/2502.19698.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangfeng Jiang, Jun Liu, Yongxuan Lv, Yuzhi Wu, Xianfei Li, Wenlong Liao, Tao He, Pai Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19698">You Only Click Once: Single Point Weakly Supervised 3D Instance Segmentation for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Outdoor LiDAR point cloud 3D instance segmentation is a crucial task in autonomous driving. However, it requires laborious human efforts to annotate the point cloud for training a segmentation model. To address this challenge, we propose a YoCo framework, which generates 3D pseudo labels using minimal coarse click annotations in the bird's eye view plane. It is a significant challenge to produce high-quality pseudo labels from sparse annotations. Our YoCo framework first leverages vision foundation models combined with geometric constraints from point clouds to enhance pseudo label generation. Second, a temporal and spatial-based label updating module is designed to generate reliable updated labels. It leverages predictions from adjacent frames and utilizes the inherent density variation of point clouds (dense near, sparse far). Finally, to further improve label quality, an IoU-guided enhancement module is proposed, replacing pseudo labels with high-confidence and high-IoU predictions. Experiments on the Waymo dataset demonstrate YoCo's effectiveness and generality, achieving state-of-the-art performance among weakly supervised methods and surpassing fully supervised Cylinder3D. Additionally, the YoCo is suitable for various networks, achieving performance comparable to fully supervised methods with minimal fine-tuning using only 0.8% of the fully labeled data, significantly reducing annotation costs.
<div id='section'>Paperid: <span id='pid'>227, <a href='https://arxiv.org/pdf/2502.01458.pdf' target='_blank'>https://arxiv.org/pdf/2502.01458.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Yao, Wenkai Yang, Gengze Xu, Ziqiao Wang, Yankai Lin, Yong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01458">The Capabilities and Limitations of Weak-to-Strong Generalization: Generalization and Calibration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weak-to-strong generalization, where weakly supervised strong models outperform their weaker teachers, offers a promising approach to aligning superhuman models with human values. To deepen the understanding of this approach, we provide theoretical insights into its capabilities and limitations. First, in the classification setting, we establish upper and lower generalization error bounds for the strong model, identifying the primary limitations as stemming from the weak model's generalization error and the optimization objective itself. Additionally, we derive lower and upper bounds on the calibration error of the strong model. These theoretical bounds reveal two critical insights: (1) the weak model should demonstrate strong generalization performance and maintain well-calibrated predictions, and (2) the strong model's training process must strike a careful balance, as excessive optimization could undermine its generalization capability by over-relying on the weak supervision signals. Finally, in the regression setting, we extend the work of Charikar et al. (2024) to a loss function based on Kullback-Leibler (KL) divergence, offering guarantees that the strong student can outperform its weak teacher by at least the magnitude of their disagreement. We conduct sufficient experiments to validate our theory.
<div id='section'>Paperid: <span id='pid'>228, <a href='https://arxiv.org/pdf/2512.06849.pdf' target='_blank'>https://arxiv.org/pdf/2512.06849.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matan Atad, Alexander W. Marka, Lisa Steinhelfer, Anna Curto-Vilalta, Yannik Leonhardt, Sarah C. Foreman, Anna-Sophia Walburga Dietrich, Robert Graf, Alexandra S. Gersing, Bjoern Menze, Daniel Rueckert, Jan S. Kirschke, Hendrik Möller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.06849">Hide-and-Seek Attribution: Weakly Supervised Segmentation of Vertebral Metastases in CT</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate segmentation of vertebral metastasis in CT is clinically important yet difficult to scale, as voxel-level annotations are scarce and both lytic and blastic lesions often resemble benign degenerative changes. We introduce a weakly supervised method trained solely on vertebra-level healthy/malignant labels, without any lesion masks. The method combines a Diffusion Autoencoder (DAE) that produces a classifier-guided healthy edit of each vertebra with pixel-wise difference maps that propose candidate lesion regions. To determine which regions truly reflect malignancy, we introduce Hide-and-Seek Attribution: each candidate is revealed in turn while all others are hidden, the edited image is projected back to the data manifold by the DAE, and a latent-space classifier quantifies the isolated malignant contribution of that component. High-scoring regions form the final lytic or blastic segmentation. On held-out radiologist annotations, we achieve strong blastic/lytic performance despite no mask supervision (F1: 0.91/0.85; Dice: 0.87/0.78), exceeding baselines (F1: 0.79/0.67; Dice: 0.74/0.55). These results show that vertebra-level labels can be transformed into reliable lesion masks, demonstrating that generative editing combined with selective occlusion supports accurate weakly supervised segmentation in CT.
<div id='section'>Paperid: <span id='pid'>229, <a href='https://arxiv.org/pdf/2511.20359.pdf' target='_blank'>https://arxiv.org/pdf/2511.20359.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiqing Guo, Dongdong Xi, Songlin Li, Gaobo Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.20359">From Passive Perception to Active Memory: A Weakly Supervised Image Manipulation Localization Framework Driven by Coarse-Grained Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image manipulation localization (IML) faces a fundamental trade-off between minimizing annotation cost and achieving fine-grained localization accuracy. Existing fully-supervised IML methods depend heavily on dense pixel-level mask annotations, which limits scalability to large datasets or real-world deployment.In contrast, the majority of existing weakly-supervised IML approaches are based on image-level labels, which greatly reduce annotation effort but typically lack precise spatial localization. To address this dilemma, we propose BoxPromptIML, a novel weakly-supervised IML framework that effectively balances annotation cost and localization performance. Specifically, we propose a coarse region annotation strategy, which can generate relatively accurate manipulation masks at lower cost. To improve model efficiency and facilitate deployment, we further design an efficient lightweight student model, which learns to perform fine-grained localization through knowledge distillation from a fixed teacher model based on the Segment Anything Model (SAM). Moreover, inspired by the human subconscious memory mechanism, our feature fusion module employs a dual-guidance strategy that actively contextualizes recalled prototypical patterns with real-time observational cues derived from the input. Instead of passive feature extraction, this strategy enables a dynamic process of knowledge recollection, where long-term memory is adapted to the specific context of the current image, significantly enhancing localization accuracy and robustness. Extensive experiments across both in-distribution and out-of-distribution datasets show that BoxPromptIML outperforms or rivals fully-supervised models, while maintaining strong generalization, low annotation cost, and efficient deployment characteristics.
<div id='section'>Paperid: <span id='pid'>230, <a href='https://arxiv.org/pdf/2511.14238.pdf' target='_blank'>https://arxiv.org/pdf/2511.14238.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Huang, Yongyi Su, Xin Lin, Le Zhang, Xun Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.14238">Enhancing Generalization of Depth Estimation Foundation Model via Weakly-Supervised Adaptation with Regularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emergence of foundation models has substantially advanced zero-shot generalization in monocular depth estimation (MDE), as exemplified by the Depth Anything series. However, given access to some data from downstream tasks, a natural question arises: can the performance of these models be further improved? To this end, we propose WeSTAR, a parameter-efficient framework that performs Weakly supervised Self-Training Adaptation with Regularization, designed to enhance the robustness of MDE foundation models in unseen and diverse domains. We first adopt a dense self-training objective as the primary source of structural self-supervision. To further improve robustness, we introduce semantically-aware hierarchical normalization, which exploits instance-level segmentation maps to perform more stable and multi-scale structural normalization. Beyond dense supervision, we introduce a cost-efficient weak supervision in the form of pairwise ordinal depth annotations to further guide the adaptation process, which enforces informative ordinal constraints to mitigate local topological errors. Finally, a weight regularization loss is employed to anchor the LoRA updates, ensuring training stability and preserving the model's generalizable knowledge. Extensive experiments on both realistic and corrupted out-of-distribution datasets under diverse and challenging scenarios demonstrate that WeSTAR consistently improves generalization and achieves state-of-the-art performance across a wide range of benchmarks.
<div id='section'>Paperid: <span id='pid'>231, <a href='https://arxiv.org/pdf/2507.13018.pdf' target='_blank'>https://arxiv.org/pdf/2507.13018.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Songlin Li, Guofeng Yu, Zhiqing Guo, Yunfeng Diao, Dan Ma, Gaobo Yang, Liejun Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13018">Beyond Fully Supervised Pixel Annotations: Scribble-Driven Weakly-Supervised Framework for Image Manipulation Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning-based image manipulation localization (IML) methods have achieved remarkable performance in recent years, but typically rely on large-scale pixel-level annotated datasets. To address the challenge of acquiring high-quality annotations, some recent weakly supervised methods utilize image-level labels to segment manipulated regions. However, the performance is still limited due to insufficient supervision signals. In this study, we explore a form of weak supervision that improves the annotation efficiency and detection performance, namely scribble annotation supervision. We re-annotated mainstream IML datasets with scribble labels and propose the first scribble-based IML (Sc-IML) dataset. Additionally, we propose the first scribble-based weakly supervised IML framework. Specifically, we employ self-supervised training with a structural consistency loss to encourage the model to produce consistent predictions under multi-scale and augmented inputs. In addition, we propose a prior-aware feature modulation module (PFMM) that adaptively integrates prior information from both manipulated and authentic regions for dynamic feature adjustment, further enhancing feature discriminability and prediction consistency in complex scenes. We also propose a gated adaptive fusion module (GAFM) that utilizes gating mechanisms to regulate information flow during feature fusion, guiding the model toward emphasizing potential tampered regions. Finally, we propose a confidence-aware entropy minimization loss (${\mathcal{L}}_{ {CEM }}$). This loss dynamically regularizes predictions in weakly annotated or unlabeled regions based on model uncertainty, effectively suppressing unreliable predictions. Experimental results show that our method outperforms existing fully supervised approaches in terms of average performance both in-distribution and out-of-distribution.
<div id='section'>Paperid: <span id='pid'>232, <a href='https://arxiv.org/pdf/2507.12942.pdf' target='_blank'>https://arxiv.org/pdf/2507.12942.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yafei Zhang, Lingqi Kong, Huafeng Li, Jie Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12942">Weakly Supervised Visible-Infrared Person Re-Identification via Heterogeneous Expert Collaborative Consistency Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To reduce the reliance of visible-infrared person re-identification (ReID) models on labeled cross-modal samples, this paper explores a weakly supervised cross-modal person ReID method that uses only single-modal sample identity labels, addressing scenarios where cross-modal identity labels are unavailable. To mitigate the impact of missing cross-modal labels on model performance, we propose a heterogeneous expert collaborative consistency learning framework, designed to establish robust cross-modal identity correspondences in a weakly supervised manner. This framework leverages labeled data from each modality to independently train dedicated classification experts. To associate cross-modal samples, these classification experts act as heterogeneous predictors, predicting the identities of samples from the other modality. To improve prediction accuracy, we design a cross-modal relationship fusion mechanism that effectively integrates predictions from different experts. Under the implicit supervision provided by cross-modal identity correspondences, collaborative and consistent learning among the experts is encouraged, significantly enhancing the model's ability to extract modality-invariant features and improve cross-modal identity recognition. Experimental results on two challenging datasets validate the effectiveness of the proposed method.
<div id='section'>Paperid: <span id='pid'>233, <a href='https://arxiv.org/pdf/2507.06744.pdf' target='_blank'>https://arxiv.org/pdf/2507.06744.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yafei Zhang, Yongle Shang, Huafeng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06744">Dual-Granularity Cross-Modal Identity Association for Weakly-Supervised Text-to-Person Image Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised text-to-person image matching, as a crucial approach to reducing models' reliance on large-scale manually labeled samples, holds significant research value. However, existing methods struggle to predict complex one-to-many identity relationships, severely limiting performance improvements. To address this challenge, we propose a local-and-global dual-granularity identity association mechanism. Specifically, at the local level, we explicitly establish cross-modal identity relationships within a batch, reinforcing identity constraints across different modalities and enabling the model to better capture subtle differences and correlations. At the global level, we construct a dynamic cross-modal identity association network with the visual modality as the anchor and introduce a confidence-based dynamic adjustment mechanism, effectively enhancing the model's ability to identify weakly associated samples while improving overall sensitivity. Additionally, we propose an information-asymmetric sample pair construction method combined with consistency learning to tackle hard sample mining and enhance model robustness. Experimental results demonstrate that the proposed method substantially boosts cross-modal matching accuracy, providing an efficient and practical solution for text-to-person image matching.
<div id='section'>Paperid: <span id='pid'>234, <a href='https://arxiv.org/pdf/2503.24135.pdf' target='_blank'>https://arxiv.org/pdf/2503.24135.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexis Guichemerre, Soufiane Belharbi, Mohammadhadi Shateri, Luke McCaffrey, Eric Granger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.24135">PixelCAM: Pixel Class Activation Mapping for Histology Image Classification and ROI Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised object localization (WSOL) methods allow training models to classify images and localize ROIs. WSOL only requires low-cost image-class annotations yet provides a visually interpretable classifier. Standard WSOL methods rely on class activation mapping (CAM) methods to produce spatial localization maps according to a single- or two-step strategy. While both strategies have made significant progress, they still face several limitations with histology images. Single-step methods can easily result in under- or over-activation due to the limited visual ROI saliency in histology images and scarce localization cues. They also face the well-known issue of asynchronous convergence between classification and localization tasks. The two-step approach is sub-optimal because it is constrained to a frozen classifier, limiting the capacity for localization. Moreover, these methods also struggle when applied to out-of-distribution (OOD) datasets. In this paper, a multi-task approach for WSOL is introduced for simultaneous training of both tasks to address the asynchronous convergence problem. In particular, localization is performed in the pixel-feature space of an image encoder that is shared with classification. This allows learning discriminant features and accurate delineation of foreground/background regions to support ROI localization and image classification. We propose PixelCAM, a cost-effective foreground/background pixel-wise classifier in the pixel-feature space that allows for spatial object localization. Using partial-cross entropy, PixelCAM is trained using pixel pseudo-labels collected from a pretrained WSOL model. Both image and pixel-wise classifiers are trained simultaneously using standard gradient descent. In addition, our pixel classifier can easily be integrated into CNN- and transformer-based architectures without any modifications.
<div id='section'>Paperid: <span id='pid'>235, <a href='https://arxiv.org/pdf/2501.17148.pdf' target='_blank'>https://arxiv.org/pdf/2501.17148.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengxuan Wu, Aryaman Arora, Atticus Geiger, Zheng Wang, Jing Huang, Dan Jurafsky, Christopher D. Manning, Christopher Potts
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.17148">AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse Autoencoders</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fine-grained steering of language model outputs is essential for safety and reliability. Prompting and finetuning are widely used to achieve these goals, but interpretability researchers have proposed a variety of representation-based techniques as well, including sparse autoencoders (SAEs), linear artificial tomography, supervised steering vectors, linear probes, and representation finetuning. At present, there is no benchmark for making direct comparisons between these proposals. Therefore, we introduce AxBench, a large-scale benchmark for steering and concept detection, and report experiments on Gemma-2-2B and 9B. For steering, we find that prompting outperforms all existing methods, followed by finetuning. For concept detection, representation-based methods such as difference-in-means, perform the best. On both evaluations, SAEs are not competitive. We introduce a novel weakly-supervised representational method (Rank-1 Representation Finetuning; ReFT-r1), which is competitive on both tasks while providing the interpretability advantages that prompting lacks. Along with AxBench, we train and publicly release SAE-scale feature dictionaries for ReFT-r1 and DiffMean.
<div id='section'>Paperid: <span id='pid'>236, <a href='https://arxiv.org/pdf/2411.04607.pdf' target='_blank'>https://arxiv.org/pdf/2411.04607.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chong Wang, Fengbei Liu, Yuanhong Chen, Helen Frazer, Gustavo Carneiro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04607">Cross- and Intra-image Prototypical Learning for Multi-label Disease Diagnosis and Interpretation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in prototypical learning have shown remarkable potential to provide useful decision interpretations associating activation maps and predictions with class-specific training prototypes. Such prototypical learning has been well-studied for various single-label diseases, but for quite relevant and more challenging multi-label diagnosis, where multiple diseases are often concurrent within an image, existing prototypical learning models struggle to obtain meaningful activation maps and effective class prototypes due to the entanglement of the multiple diseases. In this paper, we present a novel Cross- and Intra-image Prototypical Learning (CIPL) framework, for accurate multi-label disease diagnosis and interpretation from medical images. CIPL takes advantage of common cross-image semantics to disentangle the multiple diseases when learning the prototypes, allowing a comprehensive understanding of complicated pathological lesions. Furthermore, we propose a new two-level alignment-based regularisation strategy that effectively leverages consistent intra-image information to enhance interpretation robustness and predictive performance. Extensive experiments show that our CIPL attains the state-of-the-art (SOTA) classification accuracy in two public multi-label benchmarks of disease diagnosis: thoracic radiography and fundus images. Quantitative interpretability results show that CIPL also has superiority in weakly-supervised thoracic disease localisation over other leading saliency- and prototype-based explanation methods.
<div id='section'>Paperid: <span id='pid'>237, <a href='https://arxiv.org/pdf/2509.11984.pdf' target='_blank'>https://arxiv.org/pdf/2509.11984.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meng Wei, Zhongnian Li, Peng Ying, Xinzheng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11984">Learning from Uncertain Similarity and Unlabeled Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing similarity-based weakly supervised learning approaches often rely on precise similarity annotations between data pairs, which may inadvertently expose sensitive label information and raise privacy risks. To mitigate this issue, we propose Uncertain Similarity and Unlabeled Learning (USimUL), a novel framework where each similarity pair is embedded with an uncertainty component to reduce label leakage. In this paper, we propose an unbiased risk estimator that learns from uncertain similarity and unlabeled data. Additionally, we theoretically prove that the estimator achieves statistically optimal parametric convergence rates. Extensive experiments on both benchmark and real-world datasets show that our method achieves superior classification performance compared to conventional similarity-based approaches.
<div id='section'>Paperid: <span id='pid'>238, <a href='https://arxiv.org/pdf/2508.04566.pdf' target='_blank'>https://arxiv.org/pdf/2508.04566.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinxing Zhou, Ziheng Zhou, Yanghao Zhou, Yuxin Mao, Zhangling Duan, Dan Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04566">CLASP: Cross-modal Salient Anchor-based Semantic Propagation for Weakly-supervised Dense Audio-Visual Event Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Dense Audio-Visual Event Localization (DAVEL) task aims to temporally localize events in untrimmed videos that occur simultaneously in both the audio and visual modalities. This paper explores DAVEL under a new and more challenging weakly-supervised setting (W-DAVEL task), where only video-level event labels are provided and the temporal boundaries of each event are unknown. We address W-DAVEL by exploiting \textit{cross-modal salient anchors}, which are defined as reliable timestamps that are well predicted under weak supervision and exhibit highly consistent event semantics across audio and visual modalities. Specifically, we propose a \textit{Mutual Event Agreement Evaluation} module, which generates an agreement score by measuring the discrepancy between the predicted audio and visual event classes. Then, the agreement score is utilized in a \textit{Cross-modal Salient Anchor Identification} module, which identifies the audio and visual anchor features through global-video and local temporal window identification mechanisms. The anchor features after multimodal integration are fed into an \textit{Anchor-based Temporal Propagation} module to enhance event semantic encoding in the original temporal audio and visual features, facilitating better temporal localization under weak supervision. We establish benchmarks for W-DAVEL on both the UnAV-100 and ActivityNet1.3 datasets. Extensive experiments demonstrate that our method achieves state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>239, <a href='https://arxiv.org/pdf/2505.15123.pdf' target='_blank'>https://arxiv.org/pdf/2505.15123.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ta Duc Huy, Duy Anh Huynh, Yutong Xie, Yuankai Qi, Qi Chen, Phi Le Nguyen, Sen Kim Tran, Son Lam Phung, Anton van den Hengel, Zhibin Liao, Minh-Son To, Johan W. Verjans, Vu Minh Hieu Phan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.15123">Seeing the Trees for the Forest: Rethinking Weakly-Supervised Medical Visual Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual grounding (VG) is the capability to identify the specific regions in an image associated with a particular text description. In medical imaging, VG enhances interpretability by highlighting relevant pathological features corresponding to textual descriptions, improving model transparency and trustworthiness for wider adoption of deep learning models in clinical practice. Current models struggle to associate textual descriptions with disease regions due to inefficient attention mechanisms and a lack of fine-grained token representations. In this paper, we empirically demonstrate two key observations. First, current VLMs assign high norms to background tokens, diverting the model's attention from regions of disease. Second, the global tokens used for cross-modal learning are not representative of local disease tokens. This hampers identifying correlations between the text and disease tokens. To address this, we introduce simple, yet effective Disease-Aware Prompting (DAP) process, which uses the explainability map of a VLM to identify the appropriate image features. This simple strategy amplifies disease-relevant regions while suppressing background interference. Without any additional pixel-level annotations, DAP improves visual grounding accuracy by 20.74% compared to state-of-the-art methods across three major chest X-ray datasets.
<div id='section'>Paperid: <span id='pid'>240, <a href='https://arxiv.org/pdf/2409.01030.pdf' target='_blank'>https://arxiv.org/pdf/2409.01030.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahe Tian, Peng Chen, Cai Yu, Xiaomeng Fu, Xi Wang, Jiao Dai, Jizhong Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01030">Learning to Discover Forgery Cues for Face Forgery Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Locating manipulation maps, i.e., pixel-level annotation of forgery cues, is crucial for providing interpretable detection results in face forgery detection. Related learning objects have also been widely adopted as auxiliary tasks to improve the classification performance of detectors whereas they require comparisons between paired real and forged faces to obtain manipulation maps as supervision. This requirement restricts their applicability to unpaired faces and contradicts real-world scenarios. Moreover, the used comparison methods annotate all changed pixels, including noise introduced by compression and upsampling. Using such maps as supervision hinders the learning of exploitable cues and makes models prone to overfitting. To address these issues, we introduce a weakly supervised model in this paper, named Forgery Cue Discovery (FoCus), to locate forgery cues in unpaired faces. Unlike some detectors that claim to locate forged regions in attention maps, FoCus is designed to sidestep their shortcomings of capturing partial and inaccurate forgery cues. Specifically, we propose a classification attentive regions proposal module to locate forgery cues during classification and a complementary learning module to facilitate the learning of richer cues. The produced manipulation maps can serve as better supervision to enhance face forgery detectors. Visualization of the manipulation maps of the proposed FoCus exhibits superior interpretability and robustness compared to existing methods. Experiments on five datasets and four multi-task models demonstrate the effectiveness of FoCus in both in-dataset and cross-dataset evaluations.
<div id='section'>Paperid: <span id='pid'>241, <a href='https://arxiv.org/pdf/2511.22823.pdf' target='_blank'>https://arxiv.org/pdf/2511.22823.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Miao Zhang, Junpeng Li, Changchun Hua, Yana Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.22823">A Unified and Stable Risk Minimization Framework for Weakly Supervised Learning with Theoretical Guarantees</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised learning has emerged as a practical alternative to fully supervised learning when complete and accurate labels are costly or infeasible to acquire. However, many existing methods are tailored to specific supervision patterns -- such as positive-unlabeled (PU), unlabeled-unlabeled (UU), complementary-label (CLL), partial-label (PLL), or similarity-unlabeled annotations -- and rely on post-hoc corrections to mitigate instability induced by indirect supervision. We propose a principled, unified framework that bypasses such post-hoc adjustments by directly formulating a stable surrogate risk grounded in the structure of weakly supervised data. The formulation naturally subsumes diverse settings -- including PU, UU, CLL, PLL, multi-class unlabeled, and tuple-based learning -- under a single optimization objective. We further establish a non-asymptotic generalization bound via Rademacher complexity that clarifies how supervision structure, model capacity, and sample size jointly govern performance. Beyond this, we analyze the effect of class-prior misspecification on the bound, deriving explicit terms that quantify its impact, and we study identifiability, giving sufficient conditions -- most notably via supervision stratification across groups -- under which the target risk is recoverable. Extensive experiments show consistent gains across class priors, dataset scales, and class counts -- without heuristic stabilization -- while exhibiting robustness to overfitting.
<div id='section'>Paperid: <span id='pid'>242, <a href='https://arxiv.org/pdf/2510.25413.pdf' target='_blank'>https://arxiv.org/pdf/2510.25413.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shakib Yazdani, Yasser Hamidullah, Cristina España-Bonet, Josef van Genabith
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.25413">Seeing, Signing, and Saying: A Vision-Language Model-Assisted Pipeline for Sign Language Data Acquisition and Curation from Social Media</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most existing sign language translation (SLT) datasets are limited in scale, lack multilingual coverage, and are costly to curate due to their reliance on expert annotation and controlled recording setup. Recently, Vision Language Models (VLMs) have demonstrated strong capabilities as evaluators and real-time assistants. Despite these advancements, their potential remains untapped in the context of sign language dataset acquisition. To bridge this gap, we introduce the first automated annotation and filtering framework that utilizes VLMs to reduce reliance on manual effort while preserving data quality. Our method is applied to TikTok videos across eight sign languages and to the already curated YouTube-SL-25 dataset in German Sign Language for the purpose of additional evaluation. Our VLM-based pipeline includes a face visibility detection, a sign activity recognition, a text extraction from video content, and a judgment step to validate alignment between video and text, implementing generic filtering, annotation and validation steps. Using the resulting corpus, TikTok-SL-8, we assess the performance of two off-the-shelf SLT models on our filtered dataset for German and American Sign Languages, with the goal of establishing baselines and evaluating the robustness of recent models on automatically extracted, slightly noisy data. Our work enables scalable, weakly supervised pretraining for SLT and facilitates data acquisition from social media.
<div id='section'>Paperid: <span id='pid'>243, <a href='https://arxiv.org/pdf/2510.18406.pdf' target='_blank'>https://arxiv.org/pdf/2510.18406.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Miao Zhang, Junpeng Li, ChangChun HUa, Yana Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.18406">Learning from N-Tuple Data with M Positive Instances: Unbiased Risk Estimation and Theoretical Guarantees</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised learning often operates with coarse aggregate signals rather than instance labels. We study a setting where each training example is an $n$-tuple containing exactly m positives, while only the count m per tuple is observed. This NTMP (N-tuple with M positives) supervision arises in, e.g., image classification with region proposals and multi-instance measurements. We show that tuple counts admit a trainable unbiased risk estimator (URE) by linking the tuple-generation process to latent instance marginals. Starting from fixed (n,m), we derive a closed-form URE and extend it to variable tuple sizes, variable counts, and their combination. Identification holds whenever the effective mixing rate is separated from the class prior. We establish generalization bounds via Rademacher complexity and prove statistical consistency with standard rates under mild regularity assumptions. To improve finite-sample stability, we introduce simple ReLU corrections to the URE that preserve asymptotic correctness. Across benchmarks converted to NTMP tasks, the approach consistently outperforms representative weak-supervision baselines and yields favorable precision-recall and F1 trade-offs. It remains robust under class-prior imbalance and across diverse tuple configurations, demonstrating that count-only supervision can be exploited effectively through a theoretically grounded and practically stable objective.
<div id='section'>Paperid: <span id='pid'>244, <a href='https://arxiv.org/pdf/2509.14097.pdf' target='_blank'>https://arxiv.org/pdf/2509.14097.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaru Chen, Ruohao Guo, Liting Gao, Yang Xiang, Qingyu Luo, Zhenbo Li, Wenwu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14097">Teacher-Guided Pseudo Supervision and Cross-Modal Alignment for Audio-Visual Video Parsing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised audio-visual video parsing (AVVP) seeks to detect audible, visible, and audio-visual events without temporal annotations. Previous work has emphasized refining global predictions through contrastive or collaborative learning, but neglected stable segment-level supervision and class-aware cross-modal alignment. To address this, we propose two strategies: (1) an exponential moving average (EMA)-guided pseudo supervision framework that generates reliable segment-level masks via adaptive thresholds or top-k selection, offering stable temporal guidance beyond video-level labels; and (2) a class-aware cross-modal agreement (CMA) loss that aligns audio and visual embeddings at reliable segment-class pairs, ensuring consistency across modalities while preserving temporal structure. Evaluations on LLP and UnAV-100 datasets shows that our method achieves state-of-the-art (SOTA) performance across multiple metrics.
<div id='section'>Paperid: <span id='pid'>245, <a href='https://arxiv.org/pdf/2509.04086.pdf' target='_blank'>https://arxiv.org/pdf/2509.04086.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaru Chen, Faegheh Sardari, Peiliang Zhang, Ruohao Guo, Yang Xiang, Zhenbo Li, Wenwu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04086">TEn-CATS: Text-Enriched Audio-Visual Video Parsing with Multi-Scale Category-Aware Temporal Graph</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-Visual Video Parsing (AVVP) task aims to identify event categories and their occurrence times in a given video with weakly supervised labels. Existing methods typically fall into two categories: (i) designing enhanced architectures based on attention mechanism for better temporal modeling, and (ii) generating richer pseudo-labels to compensate for the absence of frame-level annotations. However, the first type methods treat noisy segment-level pseudo labels as reliable supervision and the second type methods let indiscriminate attention spread them across all frames, the initial errors are repeatedly amplified during training. To address this issue, we propose a method that combines the Bi-Directional Text Fusion (BiT) module and Category-Aware Temporal Graph (CATS) module. Specifically, we integrate the strengths and complementarity of the two previous research directions. We first perform semantic injection and dynamic calibration on audio and visual modality features through the BiT module, to locate and purify cleaner and richer semantic cues. Then, we leverage the CATS module for semantic propagation and connection to enable precise semantic information dissemination across time. Experimental results demonstrate that our proposed method achieves state-of-the-art (SOTA) performance in multiple key indicators on two benchmark datasets, LLP and UnAV-100.
<div id='section'>Paperid: <span id='pid'>246, <a href='https://arxiv.org/pdf/2507.19592.pdf' target='_blank'>https://arxiv.org/pdf/2507.19592.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meng Wei, Charlie Budd, Oluwatosin Alabi, Miaojing Shi, Tom Vercauteren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19592">SurgPIS: Surgical-instrument-level Instances and Part-level Semantics for Weakly-supervised Part-aware Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Consistent surgical instrument segmentation is critical for automation in robot-assisted surgery. Yet, existing methods only treat instrument-level instance segmentation (IIS) or part-level semantic segmentation (PSS) separately, without interaction between these tasks. In this work, we formulate a surgical tool segmentation as a unified part-aware instance segmentation (PIS) problem and introduce SurgPIS, the first PIS model for surgical instruments. Our method adopts a transformer-based mask classification approach and introduces part-specific queries derived from instrument-level object queries, explicitly linking parts to their parent instrument instances. In order to address the lack of large-scale datasets with both instance- and part-level labels, we propose a weakly-supervised learning strategy for SurgPIS to learn from disjoint datasets labelled for either IIS or PSS purposes. During training, we aggregate our PIS predictions into IIS or PSS masks, thereby allowing us to compute a loss against partially labelled datasets. A student-teacher approach is developed to maintain prediction consistency for missing PIS information in the partially labelled data, e.g., parts of the IIS labelled data. Extensive experiments across multiple datasets validate the effectiveness of SurgPIS, achieving state-of-the-art performance in PIS as well as IIS, PSS, and instrument-level semantic segmentation.
<div id='section'>Paperid: <span id='pid'>247, <a href='https://arxiv.org/pdf/2507.07771.pdf' target='_blank'>https://arxiv.org/pdf/2507.07771.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuying Huang, Junpeng Li, Changchun Hua, Yana Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07771">A Unified Empirical Risk Minimization Framework for Flexible N-Tuples Weak Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To alleviate the annotation burden in supervised learning, N-tuples learning has recently emerged as a powerful weakly-supervised method. While existing N-tuples learning approaches extend pairwise learning to higher-order comparisons and accommodate various real-world scenarios, they often rely on task-specific designs and lack a unified theoretical foundation. In this paper, we propose a general N-tuples learning framework based on empirical risk minimization, which systematically integrates pointwise unlabeled data to enhance learning performance. This paper first unifies the data generation processes of N-tuples and pointwise unlabeled data under a shared probabilistic formulation. Based on this unified view, we derive an unbiased empirical risk estimator that generalizes a broad class of existing N-tuples models. We further establish a generalization error bound for theoretical support. To demonstrate the flexibility of the framework, we instantiate it in four representative weakly supervised scenarios, each recoverable as a special case of our general model. Additionally, to address overfitting issues arising from negative risk terms, we adopt correction functions to adjust the empirical risk. Extensive experiments on benchmark datasets validate the effectiveness of the proposed framework and demonstrate that leveraging pointwise unlabeled data consistently improves generalization across various N-tuples learning tasks.
<div id='section'>Paperid: <span id='pid'>248, <a href='https://arxiv.org/pdf/2506.22301.pdf' target='_blank'>https://arxiv.org/pdf/2506.22301.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Takumi Okuo, Shinnosuke Matsuo, Shota Harada, Kiyohito Tanaka, Ryoma Bise
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22301">Weakly-Supervised Domain Adaptation with Proportion-Constrained Pseudo-Labeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain shift is a significant challenge in machine learning, particularly in medical applications where data distributions differ across institutions due to variations in data collection practices, equipment, and procedures. This can degrade performance when models trained on source domain data are applied to the target domain. Domain adaptation methods have been widely studied to address this issue, but most struggle when class proportions between the source and target domains differ. In this paper, we propose a weakly-supervised domain adaptation method that leverages class proportion information from the target domain, which is often accessible in medical datasets through prior knowledge or statistical reports. Our method assigns pseudo-labels to the unlabeled target data based on class proportion (called proportion-constrained pseudo-labeling), improving performance without the need for additional annotations. Experiments on two endoscopic datasets demonstrate that our method outperforms semi-supervised domain adaptation techniques, even when 5% of the target domain is labeled. Additionally, the experimental results with noisy proportion labels highlight the robustness of our method, further demonstrating its effectiveness in real-world application scenarios.
<div id='section'>Paperid: <span id='pid'>249, <a href='https://arxiv.org/pdf/2505.22490.pdf' target='_blank'>https://arxiv.org/pdf/2505.22490.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ke Zhang, Tianyu Ding, Jiachen Jiang, Tianyi Chen, Ilya Zharkov, Vishal M. Patel, Luming Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22490">ProCrop: Learning Aesthetic Image Cropping from Professional Compositions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image cropping is crucial for enhancing the visual appeal and narrative impact of photographs, yet existing rule-based and data-driven approaches often lack diversity or require annotated training data. We introduce ProCrop, a retrieval-based method that leverages professional photography to guide cropping decisions. By fusing features from professional photographs with those of the query image, ProCrop learns from professional compositions, significantly boosting performance. Additionally, we present a large-scale dataset of 242K weakly-annotated images, generated by out-painting professional images and iteratively refining diverse crop proposals. This composition-aware dataset generation offers diverse high-quality crop proposals guided by aesthetic principles and becomes the largest publicly available dataset for image cropping. Extensive experiments show that ProCrop significantly outperforms existing methods in both supervised and weakly-supervised settings. Notably, when trained on the new dataset, our ProCrop surpasses previous weakly-supervised methods and even matches fully supervised approaches. Both the code and dataset will be made publicly available to advance research in image aesthetics and composition analysis.
<div id='section'>Paperid: <span id='pid'>250, <a href='https://arxiv.org/pdf/2412.04383.pdf' target='_blank'>https://arxiv.org/pdf/2412.04383.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rong Li, Shijie Li, Lingdong Kong, Xulei Yang, Junwei Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04383">SeeGround: See and Ground for Zero-Shot Open-Vocabulary 3D Visual Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Visual Grounding (3DVG) aims to locate objects in 3D scenes based on textual descriptions, essential for applications like augmented reality and robotics. Traditional 3DVG approaches rely on annotated 3D datasets and predefined object categories, limiting scalability and adaptability. To overcome these limitations, we introduce SeeGround, a zero-shot 3DVG framework leveraging 2D Vision-Language Models (VLMs) trained on large-scale 2D data. SeeGround represents 3D scenes as a hybrid of query-aligned rendered images and spatially enriched text descriptions, bridging the gap between 3D data and 2D-VLMs input formats. We propose two modules: the Perspective Adaptation Module, which dynamically selects viewpoints for query-relevant image rendering, and the Fusion Alignment Module, which integrates 2D images with 3D spatial descriptions to enhance object localization. Extensive experiments on ScanRefer and Nr3D demonstrate that our approach outperforms existing zero-shot methods by large margins. Notably, we exceed weakly supervised methods and rival some fully supervised ones, outperforming previous SOTA by 7.7% on ScanRefer and 7.1% on Nr3D, showcasing its effectiveness in complex 3DVG tasks.
<div id='section'>Paperid: <span id='pid'>251, <a href='https://arxiv.org/pdf/2411.18225.pdf' target='_blank'>https://arxiv.org/pdf/2411.18225.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zak Buzzard, Konstantin Hemker, Nikola Simidjievski, Mateja Jamnik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18225">PATHS: A Hierarchical Transformer for Efficient Whole Slide Image Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computational analysis of whole slide images (WSIs) has seen significant research progress in recent years, with applications ranging across important diagnostic and prognostic tasks such as survival or cancer subtype prediction. Many state-of-the-art models process the entire slide - which may be as large as $150,000 \times 150,000$ pixels - as a bag of many patches, the size of which necessitates computationally cheap feature aggregation methods. However, a large proportion of these patches are uninformative, such as those containing only healthy or adipose tissue, adding significant noise and size to the bag. We propose Pathology Transformer with Hierarchical Selection (PATHS), a novel top-down method for hierarchical weakly supervised representation learning on slide-level tasks in computational pathology. PATHS is inspired by the cross-magnification manner in which a human pathologist examines a slide, recursively filtering patches at each magnification level to a small subset relevant to the diagnosis. Our method overcomes the complications of processing the entire slide, enabling quadratic self-attention and providing a simple interpretable measure of region importance. We apply PATHS to five datasets of The Cancer Genome Atlas (TCGA), and achieve superior performance on slide-level prediction tasks when compared to previous methods, despite processing only a small proportion of the slide.
<div id='section'>Paperid: <span id='pid'>252, <a href='https://arxiv.org/pdf/2410.01544.pdf' target='_blank'>https://arxiv.org/pdf/2410.01544.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zaiquan Yang, Yuhao Liu, Jiaying Lin, Gerhard Hancke, Rynson W. H. Lau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01544">Boosting Weakly-Supervised Referring Image Segmentation via Progressive Comprehension</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper explores the weakly-supervised referring image segmentation (WRIS) problem, and focuses on a challenging setup where target localization is learned directly from image-text pairs. We note that the input text description typically already contains detailed information on how to localize the target object, and we also observe that humans often follow a step-by-step comprehension process (\ie, progressively utilizing target-related attributes and relations as cues) to identify the target object. Hence, we propose a novel Progressive Comprehension Network (PCNet) to leverage target-related textual cues from the input description for progressively localizing the target object. Specifically, we first use a Large Language Model (LLM) to decompose the input text description into short phrases. These short phrases are taken as target-related cues and fed into a Conditional Referring Module (CRM) in multiple stages, to allow updating the referring text embedding and enhance the response map for target localization in a multi-stage manner. Based on the CRM, we then propose a Region-aware Shrinking (RaS) loss to constrain the visual localization to be conducted progressively in a coarse-to-fine manner across different stages. Finally, we introduce an Instance-aware Disambiguation (IaD) loss to suppress instance localization ambiguity by differentiating overlapping response maps generated by different referring texts on the same image. Extensive experiments show that our method outperforms SOTA methods on three common benchmarks.
<div id='section'>Paperid: <span id='pid'>253, <a href='https://arxiv.org/pdf/2511.18136.pdf' target='_blank'>https://arxiv.org/pdf/2511.18136.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunming He, Rihan Zhang, Longxiang Tang, Ziyun Yang, Kai Li, Deng-Ping Fan, Sina Farsiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.18136">SCALER: SAM-Enhanced Collaborative Learning for Label-Deficient Concealed Object Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing methods for label-deficient concealed object segmentation (LDCOS) either rely on consistency constraints or Segment Anything Model (SAM)-based pseudo-labeling. However, their performance remains limited due to the intrinsic concealment of targets and the scarcity of annotations. This study investigates two key questions: (1) Can consistency constraints and SAM-based supervision be jointly integrated to better exploit complementary information and enhance the segmenter? and (2) beyond that, can the segmenter in turn guide SAM through reciprocal supervision, enabling mutual improvement? To answer these questions, we present SCALER, a unified collaborative framework toward LDCOS that jointly optimizes a mean-teacher segmenter and a learnable SAM. SCALER operates in two alternating phases. In \textbf{Phase \uppercase\expandafter{\romannumeral1}}, the segmenter is optimized under fixed SAM supervision using entropy-based image-level and uncertainty-based pixel-level weighting to select reliable pseudo-label regions and emphasize harder examples. In \textbf{Phase \uppercase\expandafter{\romannumeral2}}, SAM is updated via augmentation invariance and noise resistance losses, leveraging its inherent robustness to perturbations. Experiments demonstrate that SCALER yields consistent performance gains across eight semi- and weakly-supervised COS tasks. The results further suggest that SCALER can serve as a general training paradigm to enhance both lightweight segmenters and large foundation models under label-scarce conditions. Code will be released.
<div id='section'>Paperid: <span id='pid'>254, <a href='https://arxiv.org/pdf/2511.10241.pdf' target='_blank'>https://arxiv.org/pdf/2511.10241.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinxuan Li, Yi Zhang, Jian-Fang Hu, Chaolei Tan, Tianming Liang, Beihao Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.10241">TubeRMC: Tube-conditioned Reconstruction with Mutual Constraints for Weakly-supervised Spatio-Temporal Video Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spatio-Temporal Video Grounding (STVG) aims to localize a spatio-temporal tube that corresponds to a given language query in an untrimmed video. This is a challenging task since it involves complex vision-language understanding and spatiotemporal reasoning. Recent works have explored weakly-supervised setting in STVG to eliminate reliance on fine-grained annotations like bounding boxes or temporal stamps. However, they typically follow a simple late-fusion manner, which generates tubes independent of the text description, often resulting in failed target identification and inconsistent target tracking. To address this limitation, we propose a Tube-conditioned Reconstruction with Mutual Constraints (\textbf{TubeRMC}) framework that generates text-conditioned candidate tubes with pre-trained visual grounding models and further refine them via tube-conditioned reconstruction with spatio-temporal constraints. Specifically, we design three reconstruction strategies from temporal, spatial, and spatio-temporal perspectives to comprehensively capture rich tube-text correspondences. Each strategy is equipped with a Tube-conditioned Reconstructor, utilizing spatio-temporal tubes as condition to reconstruct the key clues in the query. We further introduce mutual constraints between spatial and temporal proposals to enhance their quality for reconstruction. TubeRMC outperforms existing methods on two public benchmarks VidSTG and HCSTVG. Further visualization shows that TubeRMC effectively mitigates both target identification errors and inconsistent tracking.
<div id='section'>Paperid: <span id='pid'>255, <a href='https://arxiv.org/pdf/2510.23217.pdf' target='_blank'>https://arxiv.org/pdf/2510.23217.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alois Thomas, Maya Varma, Jean-Benoit Delbrouck, Curtis P. Langlotz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.23217">Process Reward Models for Sentence-Level Verification of LVLM Radiology Reports</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automating radiology report generation with Large Vision-Language Models (LVLMs) holds great potential, yet these models often produce clinically critical hallucinations, posing serious risks. Existing hallucination detection methods frequently lack the necessary sentence-level granularity or robust generalization across different LVLM generators. We introduce a novel approach: a sentence-level Process Reward Model (PRM) adapted for this vision-language task. Our PRM predicts the factual correctness of each generated sentence, conditioned on clinical context and preceding text. When fine-tuned on MIMIC-CXR with weakly-supervised labels, a lightweight 0.5B-parameter PRM outperforms existing verification techniques, demonstrating, for instance, relative improvements of 7.5% in Matthews Correlation Coefficient and 1.8% in AUROC over strong white-box baselines on outputs from one LVLM. Unlike methods reliant on internal model states, our PRM demonstrates strong generalization to an unseen LVLM. We further show its practical utility: PRM scores effectively filter low-quality reports, improving F1-CheXbert scores by 4.5% (when discarding the worst 10% of reports). Moreover, when guiding a novel weighted best-of-N selection process on the MIMIC-CXR test set, our PRM show relative improvements in clinical metrics of 7.4% for F1-CheXbert and 0.6% for BERTScore. These results demonstrate that a lightweight, context-aware PRM provides a model-agnostic safety layer for clinical LVLMs without access to internal activations
<div id='section'>Paperid: <span id='pid'>256, <a href='https://arxiv.org/pdf/2509.04737.pdf' target='_blank'>https://arxiv.org/pdf/2509.04737.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryoga Oishi, Sho Sakaino, Toshiaki Tsuji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04737">Imitation Learning Based on Disentangled Representation Learning of Behavioral Characteristics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of robot learning, coordinating robot actions through language instructions is becoming increasingly feasible. However, adapting actions to human instructions remains challenging, as such instructions are often qualitative and require exploring behaviors that satisfy varying conditions. This paper proposes a motion generation model that adapts robot actions in response to modifier directives human instructions imposing behavioral conditions during task execution. The proposed method learns a mapping from modifier directives to actions by segmenting demonstrations into short sequences, assigning weakly supervised labels corresponding to specific modifier types. We evaluated our method in wiping and pick and place tasks. Results show that it can adjust motions online in response to modifier directives, unlike conventional batch-based methods that cannot adapt during execution.
<div id='section'>Paperid: <span id='pid'>257, <a href='https://arxiv.org/pdf/2507.10300.pdf' target='_blank'>https://arxiv.org/pdf/2507.10300.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hatef Otroshi Shahreza, SÃ©bastien Marcel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10300">FaceLLM: A Multimodal Large Language Model for Face Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal large language models (MLLMs) have shown remarkable performance in vision-language tasks. However, existing MLLMs are primarily trained on generic datasets, limiting their ability to reason on domain-specific visual cues such as those in facial images. In particular, tasks that require detailed understanding of facial structure, expression, emotion, and demographic features remain underexplored by MLLMs due to the lack of large-scale annotated face image-text datasets. In this work, we introduce FaceLLM, a multimodal large language model trained specifically for facial image understanding. To construct the training data, we propose a novel weakly supervised pipeline that uses ChatGPT with attribute-aware prompts to generate high-quality question-answer pairs based on images from the FairFace dataset. The resulting corpus, called FairFaceGPT, covers a diverse set of attributes including expression, pose, skin texture, and forensic information. Our experiments demonstrate that FaceLLM improves the performance of MLLMs on various face-centric tasks and achieves state-of-the-art performance. This work highlights the potential of synthetic supervision via language models for building domain-specialized MLLMs, and sets a precedent for trustworthy, human-centric multimodal AI systems. FairFaceGPT dataset and pretrained FaceLLM models are publicly available in the project page.
<div id='section'>Paperid: <span id='pid'>258, <a href='https://arxiv.org/pdf/2505.24656.pdf' target='_blank'>https://arxiv.org/pdf/2505.24656.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dimitrios Damianos, Georgios Paraskevopoulos, Alexandros Potamianos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24656">MSDA: Combining Pseudo-labeling and Self-Supervision for Unsupervised Domain Adaptation in ASR</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we investigate the Meta PL unsupervised domain adaptation framework for Automatic Speech Recognition (ASR). We introduce a Multi-Stage Domain Adaptation pipeline (MSDA), a sample-efficient, two-stage adaptation approach that integrates self-supervised learning with semi-supervised techniques. MSDA is designed to enhance the robustness and generalization of ASR models, making them more adaptable to diverse conditions. It is particularly effective for low-resource languages like Greek and in weakly supervised scenarios where labeled data is scarce or noisy. Through extensive experiments, we demonstrate that Meta PL can be applied effectively to ASR tasks, achieving state-of-the-art results, significantly outperforming state-of-the-art methods, and providing more robust solutions for unsupervised domain adaptation in ASR. Our ablations highlight the necessity of utilizing a cascading approach when combining self-supervision with self-training.
<div id='section'>Paperid: <span id='pid'>259, <a href='https://arxiv.org/pdf/2503.13821.pdf' target='_blank'>https://arxiv.org/pdf/2503.13821.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chi Hsuan Wu, Kumar Ashutosh, Kristen Grauman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13821">Stitch-a-Recipe: Video Demonstration from Multistep Descriptions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>When obtaining visual illustrations from text descriptions, today's methods take a description with-a single text context caption, or an action description-and retrieve or generate the matching visual context. However, prior work does not permit visual illustration of multistep descriptions, e.g. a cooking recipe composed of multiple steps. Furthermore, simply handling each step description in isolation would result in an incoherent demonstration. We propose Stitch-a-Recipe, a novel retrieval-based method to assemble a video demonstration from a multistep description. The resulting video contains clips, possibly from different sources, that accurately reflect all the step descriptions, while being visually coherent. We formulate a training pipeline that creates large-scale weakly supervised data containing diverse and novel recipes and injects hard negatives that promote both correctness and coherence. Validated on in-the-wild instructional videos, Stitch-a-Recipe achieves state-of-the-art performance, with quantitative gains up to 24% as well as dramatic wins in a human preference study.
<div id='section'>Paperid: <span id='pid'>260, <a href='https://arxiv.org/pdf/2503.12905.pdf' target='_blank'>https://arxiv.org/pdf/2503.12905.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanbin Qian, Shuhan Ye, Chong Wang, Xiaojie Cai, Jiangbo Qian, Jiafei Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12905">UCF-Crime-DVS: A Novel Event-Based Dataset for Video Anomaly Detection with Spiking Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video anomaly detection plays a significant role in intelligent surveillance systems. To enhance model's anomaly recognition ability, previous works have typically involved RGB, optical flow, and text features. Recently, dynamic vision sensors (DVS) have emerged as a promising technology, which capture visual information as discrete events with a very high dynamic range and temporal resolution. It reduces data redundancy and enhances the capture capacity of moving objects compared to conventional camera. To introduce this rich dynamic information into the surveillance field, we created the first DVS video anomaly detection benchmark, namely UCF-Crime-DVS. To fully utilize this new data modality, a multi-scale spiking fusion network (MSF) is designed based on spiking neural networks (SNNs). This work explores the potential application of dynamic information from event data in video anomaly detection. Our experiments demonstrate the effectiveness of our framework on UCF-Crime-DVS and its superior performance compared to other models, establishing a new baseline for SNN-based weakly supervised video anomaly detection.
<div id='section'>Paperid: <span id='pid'>261, <a href='https://arxiv.org/pdf/2503.04154.pdf' target='_blank'>https://arxiv.org/pdf/2503.04154.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chupeng Liu, Runkai Zhao, Weidong Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04154">CA-W3D: Leveraging Context-Aware Knowledge for Weakly Supervised Monocular 3D Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised monocular 3D detection, while less annotation-intensive, often struggles to capture the global context required for reliable 3D reasoning. Conventional label-efficient methods focus on object-centric features, neglecting contextual semantic relationships that are critical in complex scenes. In this work, we propose a Context-Aware Weak Supervision for Monocular 3D object detection, namely CA-W3D, to address this limitation in a two-stage training paradigm. Specifically, we first introduce a pre-training stage employing Region-wise Object Contrastive Matching (ROCM), which aligns regional object embeddings derived from a trainable monocular 3D encoder and a frozen open-vocabulary 2D visual grounding model. This alignment encourages the monocular encoder to discriminate scene-specific attributes and acquire richer contextual knowledge. In the second stage, we incorporate a pseudo-label training process with a Dual-to-One Distillation (D2OD) mechanism, which effectively transfers contextual priors into the monocular encoder while preserving spatial fidelity and maintaining computational efficiency during inference. Extensive experiments conducted on the public KITTI benchmark demonstrate the effectiveness of our approach, surpassing the SoTA method over all metrics, highlighting the importance of contextual-aware knowledge in weakly-supervised monocular 3D detection.
<div id='section'>Paperid: <span id='pid'>262, <a href='https://arxiv.org/pdf/2412.14579.pdf' target='_blank'>https://arxiv.org/pdf/2412.14579.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qianpu Sun, Changyong Shu, Sifan Zhou, Zichen Yu, Yan Chen, Dawei Yang, Yuan Chun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14579">GSRender: Deduplicated Occupancy Prediction via Weakly Supervised 3D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D occupancy perception is gaining increasing attention due to its capability to offer detailed and precise environment representations. Previous weakly-supervised NeRF methods balance efficiency and accuracy, with mIoU varying by 5-10 points due to sampling count along camera rays. Recently, real-time Gaussian splatting has gained widespread popularity in 3D reconstruction, and the occupancy prediction task can also be viewed as a reconstruction task. Consequently, we propose GSRender, which naturally employs 3D Gaussian Splatting for occupancy prediction, simplifying the sampling process. In addition, the limitations of 2D supervision result in duplicate predictions along the same camera ray. We implemented the Ray Compensation (RC) module, which mitigates this issue by compensating for features from adjacent frames. Finally, we redesigned the loss to eliminate the impact of dynamic objects from adjacent frames. Extensive experiments demonstrate that our approach achieves SOTA (state-of-the-art) results in RayIoU (+6.0), while narrowing the gap with 3D supervision methods. Our code will be released soon.
<div id='section'>Paperid: <span id='pid'>263, <a href='https://arxiv.org/pdf/2411.08530.pdf' target='_blank'>https://arxiv.org/pdf/2411.08530.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ravi Kant Gupta, Dadi Dharani, Shambhavi Shanker, Amit Sethi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08530">Efficient Whole Slide Image Classification through Fisher Vector Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advancement of digital pathology, particularly through computational analysis of whole slide images (WSI), is poised to significantly enhance diagnostic precision and efficiency. However, the large size and complexity of WSIs make it difficult to analyze and classify them using computers. This study introduces a novel method for WSI classification by automating the identification and examination of the most informative patches, thus eliminating the need to process the entire slide. Our method involves two-stages: firstly, it extracts only a few patches from the WSIs based on their pathological significance; and secondly, it employs Fisher vectors (FVs) for representing features extracted from these patches, which is known for its robustness in capturing fine-grained details. This approach not only accentuates key pathological features within the WSI representation but also significantly reduces computational overhead, thus making the process more efficient and scalable. We have rigorously evaluated the proposed method across multiple datasets to benchmark its performance against comprehensive WSI analysis and contemporary weakly-supervised learning methodologies. The empirical results indicate that our focused analysis of select patches, combined with Fisher vector representation, not only aligns with, but at times surpasses, the classification accuracy of standard practices. Moreover, this strategy notably diminishes computational load and resource expenditure, thereby establishing an efficient and precise framework for WSI analysis in the realm of digital pathology.
<div id='section'>Paperid: <span id='pid'>264, <a href='https://arxiv.org/pdf/2411.03551.pdf' target='_blank'>https://arxiv.org/pdf/2411.03551.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiling Yue, Yingying Fang, Liutao Yang, Nikhil Baid, Simon Walsh, Guang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03551">Enhancing Weakly Supervised Semantic Segmentation for Fibrosis via Controllable Image Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fibrotic Lung Disease (FLD) is a severe condition marked by lung stiffening and scarring, leading to respiratory decline. High-resolution computed tomography (HRCT) is critical for diagnosing and monitoring FLD; however, fibrosis appears as irregular, diffuse patterns with unclear boundaries, leading to high inter-observer variability and time-intensive manual annotation. To tackle this challenge, we propose DiffSeg, a novel weakly supervised semantic segmentation (WSSS) method that uses image-level annotations to generate pixel-level fibrosis segmentation, reducing the need for fine-grained manual labeling. Additionally, our DiffSeg incorporates a diffusion-based generative model to synthesize HRCT images with different levels of fibrosis from healthy slices, enabling the generation of the fibrosis-injected slices and their paired fibrosis location. Experiments indicate that our method significantly improves the accuracy of pseudo masks generated by existing WSSS methods, greatly reducing the complexity of manual labeling and enhancing the consistency of the generated masks.
<div id='section'>Paperid: <span id='pid'>265, <a href='https://arxiv.org/pdf/2601.04405.pdf' target='_blank'>https://arxiv.org/pdf/2601.04405.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yike Zhang, Eduardo Davalos, Dingjie Su, Ange Lou, Jack Noble
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.04405">From Preoperative CT to Postmastoidectomy Mesh Construction: Mastoidectomy Shape Prediction for Cochlear Implant Surgery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cochlear Implant (CI) surgery treats severe hearing loss by inserting an electrode array into the cochlea to stimulate the auditory nerve. An important step in this procedure is mastoidectomy, which removes part of the mastoid region of the temporal bone to provide surgical access. Accurate mastoidectomy shape prediction from preoperative imaging improves pre-surgical planning, reduces risks, and enhances surgical outcomes. Despite its importance, there are limited deep-learning-based studies regarding this topic due to the challenges of acquiring ground-truth labels. We address this gap by investigating self-supervised and weakly-supervised learning models to predict the mastoidectomy region without human annotations. We propose a hybrid self-supervised and weakly-supervised learning framework to predict the mastoidectomy region directly from preoperative CT scans, where the mastoid remains intact. Our hybrid method achieves a mean Dice score of 0.72 when predicting the complex and boundary-less mastoidectomy shape, surpassing state-of-the-art approaches and demonstrating strong performance. The method provides groundwork for constructing 3D postmastoidectomy surfaces directly from the corresponding preoperative CT scans. To our knowledge, this is the first work that integrating self-supervised and weakly-supervised learning for mastoidectomy shape prediction, offering a robust and efficient solution for CI surgical planning while leveraging 3D T-distribution loss in weakly-supervised medical imaging.
<div id='section'>Paperid: <span id='pid'>266, <a href='https://arxiv.org/pdf/2508.10104.pdf' target='_blank'>https://arxiv.org/pdf/2508.10104.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Oriane SimÃ©oni, Huy V. Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, MichaÃ«l Ramamonjisoa, Francisco Massa, Daniel Haziza, Luca Wehrstedt, Jianyuan Wang, TimothÃ©e Darcet, ThÃ©o Moutakanni, Leonel Sentana, Claire Roberts, Andrea Vedaldi, Jamie Tolan, John Brandt, Camille Couprie, Julien Mairal, HervÃ© JÃ©gou, Patrick Labatut, Piotr Bojanowski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10104">DINOv3</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-supervised learning holds the promise of eliminating the need for manual data annotation, enabling models to scale effortlessly to massive datasets and larger architectures. By not being tailored to specific tasks or domains, this training paradigm has the potential to learn visual representations from diverse sources, ranging from natural to aerial images -- using a single algorithm. This technical report introduces DINOv3, a major milestone toward realizing this vision by leveraging simple yet effective strategies. First, we leverage the benefit of scaling both dataset and model size by careful data preparation, design, and optimization. Second, we introduce a new method called Gram anchoring, which effectively addresses the known yet unsolved issue of dense feature maps degrading during long training schedules. Finally, we apply post-hoc strategies that further enhance our models' flexibility with respect to resolution, model size, and alignment with text. As a result, we present a versatile vision foundation model that outperforms the specialized state of the art across a broad range of settings, without fine-tuning. DINOv3 produces high-quality dense features that achieve outstanding performance on various vision tasks, significantly surpassing previous self- and weakly-supervised foundation models. We also share the DINOv3 suite of vision models, designed to advance the state of the art on a wide spectrum of tasks and data by providing scalable solutions for diverse resource constraints and deployment scenarios.
<div id='section'>Paperid: <span id='pid'>267, <a href='https://arxiv.org/pdf/2508.03745.pdf' target='_blank'>https://arxiv.org/pdf/2508.03745.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenwen Li, Chia-Yu Hsu, Maosheng Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03745">Tobler's First Law in GeoAI: A Spatially Explicit Deep Learning Model for Terrain Feature Detection Under Weak Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent interest in geospatial artificial intelligence (GeoAI) has fostered a wide range of applications using artificial intelligence (AI), especially deep learning, for geospatial problem solving. However, major challenges such as a lack of training data and the neglect of spatial principles and spatial effects in AI model design remain, significantly hindering the in-depth integration of AI with geospatial research. This paper reports our work in developing a deep learning model that enables object detection, particularly of natural features, in a weakly supervised manner. Our work makes three contributions: First, we present a method of object detection using only weak labels. This is achieved by developing a spatially explicit model based on Tobler's first law of geography. Second, we incorporate attention maps into the object detection pipeline and develop a multistage training strategy to improve performance. Third, we apply this model to detect impact craters on Mars, a task that previously required extensive manual effort. The model generalizes to both natural and human-made features on the surfaces of Earth and other planets. This research advances the theoretical and methodological foundations of GeoAI.
<div id='section'>Paperid: <span id='pid'>268, <a href='https://arxiv.org/pdf/2507.01792.pdf' target='_blank'>https://arxiv.org/pdf/2507.01792.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Zheng, Ye Wang, Rui Ma, Zuxuan Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01792">FreeLoRA: Enabling Training-Free LoRA Fusion for Autoregressive Multi-Subject Personalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Subject-driven image generation plays a crucial role in applications such as virtual try-on and poster design. Existing approaches typically fine-tune pretrained generative models or apply LoRA-based adaptations for individual subjects. However, these methods struggle with multi-subject personalization, as combining independently adapted modules often requires complex re-tuning or joint optimization. We present FreeLoRA, a simple and generalizable framework that enables training-free fusion of subject-specific LoRA modules for multi-subject personalization. Each LoRA module is adapted on a few images of a specific subject using a Full Token Tuning strategy, where it is applied across all tokens in the prompt to encourage weakly supervised token-content alignment. At inference, we adopt Subject-Aware Inference, activating each module only on its corresponding subject tokens. This enables training-free fusion of multiple personalized subjects within a single image, while mitigating overfitting and mutual interference between subjects. Extensive experiments show that FreeLoRA achieves strong performance in both subject fidelity and prompt consistency.
<div id='section'>Paperid: <span id='pid'>269, <a href='https://arxiv.org/pdf/2505.17905.pdf' target='_blank'>https://arxiv.org/pdf/2505.17905.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xie Ting, Ye Huang, Zhilin Liu, Lixin Duan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17905">Semantic segmentation with reward</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In real-world scenarios, pixel-level labeling is not always available. Sometimes, we need a semantic segmentation network, and even a visual encoder can have a high compatibility, and can be trained using various types of feedback beyond traditional labels, such as feedback that indicates the quality of the parsing results. To tackle this issue, we proposed RSS (Reward in Semantic Segmentation), the first practical application of reward-based reinforcement learning on pure semantic segmentation offered in two granular levels (pixel-level and image-level). RSS incorporates various novel technologies, such as progressive scale rewards (PSR) and pair-wise spatial difference (PSD), to ensure that the reward facilitates the convergence of the semantic segmentation network, especially under image-level rewards. Experiments and visualizations on benchmark datasets demonstrate that the proposed RSS can successfully ensure the convergence of the semantic segmentation network on two levels of rewards. Additionally, the RSS, which utilizes an image-level reward, outperforms existing weakly supervised methods that also rely solely on image-level signals during training.
<div id='section'>Paperid: <span id='pid'>270, <a href='https://arxiv.org/pdf/2504.01919.pdf' target='_blank'>https://arxiv.org/pdf/2504.01919.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Baban Gain, Dibyanayan Bandyopadhyay, Asif Ekbal, Trilok Nath Singh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01919">Bridging the Linguistic Divide: A Survey on Leveraging Large Language Models for Machine Translation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) are rapidly reshaping machine translation (MT), particularly by introducing instruction-following, in-context learning, and preference-based alignment into what has traditionally been a supervised encoder-decoder paradigm. This survey provides a comprehensive and up-to-date overview of how LLMs are being leveraged for MT across data regimes, languages, and application settings. We systematically analyze prompting-based methods, parameter-efficient and full fine-tuning strategies, synthetic data generation, preference-based optimization, and reinforcement learning with human and weakly supervised feedback. Special attention is given to low-resource translation, where we examine the roles of synthetic data quality, diversity, and preference signals, as well as the limitations of current RLHF pipelines. We further review recent advances in Mixture-of-Experts models, MT-focused LLMs, and multilingual alignment, highlighting trade-offs between scalability, specialization, and accessibility. Beyond sentence-level translation, we survey emerging document-level and discourse-aware MT methods with LLMs, showing that most approaches extend sentence-level pipelines through structured context selection, post-editing, or reranking rather than requiring fundamentally new data regimes or architectures. Finally, we discuss LLM-based evaluation, its strengths and biases, and its role alongside learned metrics. Overall, this survey positions LLM-based MT as an evolution of traditional MT systems, where gains increasingly depend on data quality, preference alignment, and context utilization rather than scale alone, and outlines open challenges for building robust, inclusive, and controllable translation systems.
<div id='section'>Paperid: <span id='pid'>271, <a href='https://arxiv.org/pdf/2502.01455.pdf' target='_blank'>https://arxiv.org/pdf/2502.01455.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrea Marelli, Luca Magri, Federica Arrigoni, Giacomo Boracchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01455">Temporal-consistent CAMs for Weakly Supervised Video Segmentation in Waste Sorting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In industrial settings, weakly supervised (WS) methods are usually preferred over their fully supervised (FS) counterparts as they do not require costly manual annotations. Unfortunately, the segmentation masks obtained in the WS regime are typically poor in terms of accuracy. In this work, we present a WS method capable of producing accurate masks for semantic segmentation in the case of video streams. More specifically, we build saliency maps that exploit the temporal coherence between consecutive frames in a video, promoting consistency when objects appear in different frames. We apply our method in a waste-sorting scenario, where we perform weakly supervised video segmentation (WSVS) by training an auxiliary classifier that distinguishes between videos recorded before and after a human operator, who manually removes specific wastes from a conveyor belt. The saliency maps of this classifier identify materials to be removed, and we modify the classifier training to minimize differences between the saliency map of a central frame and those in adjacent frames, after having compensated object displacement. Experiments on a real-world dataset demonstrate the benefits of integrating temporal coherence directly during the training phase of the classifier. Code and dataset are available upon request.
<div id='section'>Paperid: <span id='pid'>272, <a href='https://arxiv.org/pdf/2501.14148.pdf' target='_blank'>https://arxiv.org/pdf/2501.14148.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuvendu Roy, Ali Etemad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.14148">SelfPrompt: Confidence-Aware Semi-Supervised Tuning for Robust Vision-Language Model Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present SelfPrompt, a novel prompt-tuning approach for vision-language models (VLMs) in a semi-supervised learning setup. Existing methods for tuning VLMs in semi-supervised setups struggle with the negative impact of the miscalibrated VLMs on pseudo-labelling, and the accumulation of noisy pseudo-labels. SelfPrompt addresses these challenges by introducing a cluster-guided pseudo-labelling method that improves pseudo-label accuracy, and a confidence-aware semi-supervised learning module that maximizes the utilization of unlabelled data by combining supervised learning and weakly-supervised learning. Additionally, we investigate our method in an active semi-supervised learning setup, where the labelled set is strategically selected to ensure the best utilization of a limited labelling budget. To this end, we propose a weakly-supervised sampling technique that selects a diverse and representative labelled set, which can be seamlessly integrated into existing methods to enhance their performance. We conduct extensive evaluations across 13 datasets, significantly surpassing state-of-the-art performances with average improvements of 6.23% in standard semi-supervised learning, 6.25% in active semi-supervised learning, and 4.9% in base-to-novel generalization, using a 2-shot setup. Furthermore, SelfPrompt shows excellent generalization in single-shot settings, achieving an average improvement of 11.78%.
<div id='section'>Paperid: <span id='pid'>273, <a href='https://arxiv.org/pdf/2501.13497.pdf' target='_blank'>https://arxiv.org/pdf/2501.13497.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qijie Shao, Linhao Dong, Kun Wei, Sining Sun, Lei Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13497">DQ-Data2vec: Decoupling Quantization for Multilingual Speech Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data2vec is a self-supervised learning (SSL) approach that employs a teacher-student architecture for contextual representation learning via masked prediction, demonstrating remarkable performance in monolingual ASR. Previous studies have revealed that data2vec's shallow layers capture speaker and language information, middle layers encode phoneme and word features, while deep layers are responsible for reconstruction. Language and phoneme features are crucial for multilingual ASR. However, data2vec's masked representation generation relies on multi-layer averaging, inevitably coupling these features. To address this limitation, we propose a decoupling quantization based data2vec (DQ-Data2vec) for multilingual ASR, which includes a data2vec backbone and two improved online K-means quantizers. Our core idea is using the K-means quantizer with specified cluster numbers to decouple language and phoneme information for masked prediction. Specifically, in the language quantization, considering that the number of languages is significantly different from other irrelevant features (e.g., speakers), we assign the cluster number to match the number of languages, explicitly decoupling shallow layers' language-related information from irrelevant features. This strategy is also applied to decoupling middle layers' phoneme and word features. In a self-supervised scenario, experiments on the CommonVoice dataset demonstrate that DQ-Data2vec achieves a relative reduction of 9.51% in phoneme error rate (PER) and 11.58% in word error rate (WER) compared to data2vec and UniData2vec. Moreover, in a weakly-supervised scenario incorporating language labels and high-resource language text labels, the relative reduction is 18.09% and 1.55%, respectively.
<div id='section'>Paperid: <span id='pid'>274, <a href='https://arxiv.org/pdf/2601.20776.pdf' target='_blank'>https://arxiv.org/pdf/2601.20776.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huanyu Tian, Martin Huber, Lingyun Zeng, Zhe Han, Wayne Bennett, Giuseppe Silvestri, Gerardo Mendizabal-Ruiz, Tom Vercauteren, Alejandro Chavez-Badiola, Christos Bergeles
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.20776">Learning From a Steady Hand: A Weakly Supervised Agent for Robot Assistance under Microscopy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper rethinks steady-hand robotic manipulation by using a weakly supervised framework that fuses calibration-aware perception with admittance control. Unlike conventional automation that relies on labor-intensive 2D labeling, our framework leverages reusable warm-up trajectories to extract implicit spatial information, thereby achieving calibration-aware, depth-resolved perception without the need for external fiducials or manual depth annotation. By explicitly characterizing residuals from observation and calibration models, the system establishes a task-space error budget from recorded warm-ups. The uncertainty budget yields a lateral closed-loop accuracy of approx. 49 micrometers at 95% confidence (worst-case testing subset) and a depth accuracy of <= 291 micrometers at 95% confidence bound during large in-plane moves. In a within-subject user study (N=8), the learned agent reduces overall NASA-TLX workload by 77.1% relative to the simple steady-hand assistance baseline. These results demonstrate that the weakly supervised agent improves the reliability of microscope-guided biomedical micromanipulation without introducing complex setup requirements, offering a practical framework for microscope-guided intervention.
<div id='section'>Paperid: <span id='pid'>275, <a href='https://arxiv.org/pdf/2512.20260.pdf' target='_blank'>https://arxiv.org/pdf/2512.20260.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei Ge, Jiuxin Cao, Xinyi Li, Xuelin Zhu, Chang Liu, Bo Liu, Chen Feng, Ioannis Patras
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.20260">${D}^{3}${ETOR}: ${D}$ebate-Enhanced Pseudo Labeling and Frequency-Aware Progressive ${D}$ebiasing for Weakly-Supervised Camouflaged Object ${D}$etection with Scribble Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-Supervised Camouflaged Object Detection (WSCOD) aims to locate and segment objects that are visually concealed within their surrounding scenes, relying solely on sparse supervision such as scribble annotations. Despite recent progress, existing WSCOD methods still lag far behind fully supervised ones due to two major limitations: (1) the pseudo masks generated by general-purpose segmentation models (e.g., SAM) and filtered via rules are often unreliable, as these models lack the task-specific semantic understanding required for effective pseudo labeling in COD; and (2) the neglect of inherent annotation bias in scribbles, which hinders the model from capturing the global structure of camouflaged objects. To overcome these challenges, we propose ${D}^{3}$ETOR, a two-stage WSCOD framework consisting of Debate-Enhanced Pseudo Labeling and Frequency-Aware Progressive Debiasing. In the first stage, we introduce an adaptive entropy-driven point sampling method and a multi-agent debate mechanism to enhance the capability of SAM for COD, improving the interpretability and precision of pseudo masks. In the second stage, we design FADeNet, which progressively fuses multi-level frequency-aware features to balance global semantic understanding with local detail modeling, while dynamically reweighting supervision strength across regions to alleviate scribble bias. By jointly exploiting the supervision signals from both the pseudo masks and scribble semantics, ${D}^{3}$ETOR significantly narrows the gap between weakly and fully supervised COD, achieving state-of-the-art performance on multiple benchmarks.
<div id='section'>Paperid: <span id='pid'>276, <a href='https://arxiv.org/pdf/2512.13806.pdf' target='_blank'>https://arxiv.org/pdf/2512.13806.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siegfried Ludwig, Stylianos Bakas, Konstantinos Barmpas, Georgios Zoumpourlis, Dimitrios A. Adamos, Nikolaos Laskaris, Yannis Panagakis, Stefanos Zafeiriou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.13806">EEG-D3: A Solution to the Hidden Overfitting Problem of Deep Learning Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning for decoding EEG signals has gained traction, with many claims to state-of-the-art accuracy. However, despite the convincing benchmark performance, successful translation to real applications is limited. The frequent disconnect between performance on controlled BCI benchmarks and its lack of generalisation to practical settings indicates hidden overfitting problems. We introduce Disentangled Decoding Decomposition (D3), a weakly supervised method for training deep learning models across EEG datasets. By predicting the place in the respective trial sequence from which the input window was sampled, EEG-D3 separates latent components of brain activity, akin to non-linear ICA. We utilise a novel model architecture with fully independent sub-networks for strict interpretability. We outline a feature interpretation paradigm to contrast the component activation profiles on different datasets and inspect the associated temporal and spatial filters. The proposed method reliably separates latent components of brain activity on motor imagery data. Training downstream classifiers on an appropriate subset of these components prevents hidden overfitting caused by task-correlated artefacts, which severely affects end-to-end classifiers. We further exploit the linearly separable latent space for effective few-shot learning on sleep stage classification. The ability to distinguish genuine components of brain activity from spurious features results in models that avoid the hidden overfitting problem and generalise well to real-world applications, while requiring only minimal labelled data. With interest to the neuroscience community, the proposed method gives researchers a tool to separate individual brain processes and potentially even uncover heretofore unknown dynamics.
<div id='section'>Paperid: <span id='pid'>277, <a href='https://arxiv.org/pdf/2511.18319.pdf' target='_blank'>https://arxiv.org/pdf/2511.18319.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xian Yeow Lee, Lasitha Vidyaratne, Gregory Sin, Ahmed Farahat, Chetan Gupta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.18319">Weakly-supervised Latent Models for Task-specific Visual-Language Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous inspection in hazardous environments requires AI agents that can interpret high-level goals and execute precise control. A key capability for such agents is spatial grounding, for example when a drone must center a detected object in its camera view to enable reliable inspection. While large language models provide a natural interface for specifying goals, using them directly for visual control achieves only 58\% success in this task. We envision that equipping agents with a world model as a tool would allow them to roll out candidate actions and perform better in spatially grounded settings, but conventional world models are data and compute intensive. To address this, we propose a task-specific latent dynamics model that learns state-specific action-induced shifts in a shared latent space using only goal-state supervision. The model leverages global action embeddings and complementary training losses to stabilize learning. In experiments, our approach achieves 71\% success and generalizes to unseen images and instructions, highlighting the potential of compact, domain-specific latent dynamics models for spatial alignment in autonomous inspection.
<div id='section'>Paperid: <span id='pid'>278, <a href='https://arxiv.org/pdf/2509.01899.pdf' target='_blank'>https://arxiv.org/pdf/2509.01899.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhimeng Luo, Zhendong Wang, Rui Meng, Diyang Xue, Adam Frisch, Daqing He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01899">Weakly Supervised Medical Entity Extraction and Linking for Chief Complaints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A Chief complaint (CC) is the reason for the medical visit as stated in the patient's own words. It helps medical professionals to quickly understand a patient's situation, and also serves as a short summary for medical text mining. However, chief complaint records often take a variety of entering methods, resulting in a wide variation of medical notations, which makes it difficult to standardize across different medical institutions for record keeping or text mining. In this study, we propose a weakly supervised method to automatically extract and link entities in chief complaints in the absence of human annotation. We first adopt a split-and-match algorithm to produce weak annotations, including entity mention spans and class labels, on 1.2 million real-world de-identified and IRB approved chief complaint records. Then we train a BERT-based model with generated weak labels to locate entity mentions in chief complaint text and link them to a pre-defined ontology. We conducted extensive experiments, and the results showed that our Weakly Supervised Entity Extraction and Linking (\ours) method produced superior performance over previous methods without any human annotation.
<div id='section'>Paperid: <span id='pid'>279, <a href='https://arxiv.org/pdf/2508.07877.pdf' target='_blank'>https://arxiv.org/pdf/2508.07877.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>WonJun Moon, Hyun Seok Seong, Jae-Pil Heo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07877">Selective Contrastive Learning for Weakly Supervised Affordance Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Facilitating an entity's interaction with objects requires accurately identifying parts that afford specific actions. Weakly supervised affordance grounding (WSAG) seeks to imitate human learning from third-person demonstrations, where humans intuitively grasp functional parts without needing pixel-level annotations. To achieve this, grounding is typically learned using a shared classifier across images from different perspectives, along with distillation strategies incorporating part discovery process. However, since affordance-relevant parts are not always easily distinguishable, models primarily rely on classification, often focusing on common class-specific patterns that are unrelated to affordance. To address this limitation, we move beyond isolated part-level learning by introducing selective prototypical and pixel contrastive objectives that adaptively learn affordance-relevant cues at both the part and object levels, depending on the granularity of the available information. Initially, we find the action-associated objects in both egocentric (object-focused) and exocentric (third-person example) images by leveraging CLIP. Then, by cross-referencing the discovered objects of complementary views, we excavate the precise part-level affordance clues in each perspective. By consistently learning to distinguish affordance-relevant regions from affordance-irrelevant background context, our approach effectively shifts activation from irrelevant areas toward meaningful affordance cues. Experimental results demonstrate the effectiveness of our method. Codes are available at github.com/hynnsk/SelectiveCL.
<div id='section'>Paperid: <span id='pid'>280, <a href='https://arxiv.org/pdf/2505.17088.pdf' target='_blank'>https://arxiv.org/pdf/2505.17088.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmed Adel Attia, Dorottya Demszky, Jing Liu, Carol Espy-Wilson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17088">From Weak Labels to Strong Results: Utilizing 5,000 Hours of Noisy Classroom Transcripts with Minimal Accurate Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent progress in speech recognition has relied on models trained on vast amounts of labeled data. However, classroom Automatic Speech Recognition (ASR) faces the real-world challenge of abundant weak transcripts paired with only a small amount of accurate, gold-standard data. In such low-resource settings, high transcription costs make re-transcription impractical. To address this, we ask: what is the best approach when abundant inexpensive weak transcripts coexist with limited gold-standard data, as is the case for classroom speech data? We propose Weakly Supervised Pretraining (WSP), a two-step process where models are first pretrained on weak transcripts in a supervised manner, and then fine-tuned on accurate data. Our results, based on both synthetic and real weak transcripts, show that WSP outperforms alternative methods, establishing it as an effective training methodology for low-resource ASR in real-world scenarios.
<div id='section'>Paperid: <span id='pid'>281, <a href='https://arxiv.org/pdf/2504.01452.pdf' target='_blank'>https://arxiv.org/pdf/2504.01452.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Encheng Su, Hu Cao, Alois Knoll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01452">BiSeg-SAM: Weakly-Supervised Post-Processing Framework for Boosting Binary Segmentation in Segment Anything Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate segmentation of polyps and skin lesions is essential for diagnosing colorectal and skin cancers. While various segmentation methods for polyps and skin lesions using fully supervised deep learning techniques have been developed, the pixel-level annotation of medical images by doctors is both time-consuming and costly. Foundational vision models like the Segment Anything Model (SAM) have demonstrated superior performance; however, directly applying SAM to medical segmentation may not yield satisfactory results due to the lack of domain-specific medical knowledge. In this paper, we propose BiSeg-SAM, a SAM-guided weakly supervised prompting and boundary refinement network for the segmentation of polyps and skin lesions. Specifically, we fine-tune SAM combined with a CNN module to learn local features. We introduce a WeakBox with two functions: automatically generating box prompts for the SAM model and using our proposed Multi-choice Mask-to-Box (MM2B) transformation for rough mask-to-box conversion, addressing the mismatch between coarse labels and precise predictions. Additionally, we apply scale consistency (SC) loss for prediction scale alignment. Our DetailRefine module enhances boundary precision and segmentation accuracy by refining coarse predictions using a limited amount of ground truth labels. This comprehensive approach enables BiSeg-SAM to achieve excellent multi-task segmentation performance. Our method demonstrates significant superiority over state-of-the-art (SOTA) methods when tested on five polyp datasets and one skin cancer dataset.
<div id='section'>Paperid: <span id='pid'>282, <a href='https://arxiv.org/pdf/2412.12870.pdf' target='_blank'>https://arxiv.org/pdf/2412.12870.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenjiang Mao, Ivan Ruchkin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12870">Towards Physically Interpretable World Models: Meaningful Weakly Supervised Representations for Visual Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning models are increasingly employed for perception, prediction, and control in robotic systems. For for achieving realistic and consistent outputs, it is crucial to embed physical knowledge into their learned representations. However, doing so is difficult due to high-dimensional observation data, such as images, particularly under conditions of incomplete system knowledge and imprecise state sensing. To address this, we propose Physically Interpretable World Models, a novel architecture that aligns learned latent representations with real-world physical quantities. To this end, our architecture combines three key elements: (1) a vector-quantized image autoencoder, (2) a transformer-based physically interpretable autoencoder, and (3) a partially known dynamical model. The training incorporates weak interval-based supervision to eliminate the impractical reliance on ground-truth physical knowledge. Three case studies demonstrate that our approach achieves physical interpretability and accurate state predictions, thus advancing representation learning for robotics.
<div id='section'>Paperid: <span id='pid'>283, <a href='https://arxiv.org/pdf/2409.15801.pdf' target='_blank'>https://arxiv.org/pdf/2409.15801.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soojin Jang, Jungmin Yun, Junehyoung Kwon, Eunju Lee, Youngbin Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.15801">DIAL: Dense Image-text ALignment for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation (WSSS) approaches typically rely on class activation maps (CAMs) for initial seed generation, which often fail to capture global context due to limited supervision from image-level labels. To address this issue, we introduce DALNet, Dense Alignment Learning Network that leverages text embeddings to enhance the comprehensive understanding and precise localization of objects across different levels of granularity. Our key insight is to employ a dual-level alignment strategy: (1) Global Implicit Alignment (GIA) to capture global semantics by maximizing the similarity between the class token and the corresponding text embeddings while minimizing the similarity with background embeddings, and (2) Local Explicit Alignment (LEA) to improve object localization by utilizing spatial information from patch tokens. Moreover, we propose a cross-contrastive learning approach that aligns foreground features between image and text modalities while separating them from the background, encouraging activation in missing regions and suppressing distractions. Through extensive experiments on the PASCAL VOC and MS COCO datasets, we demonstrate that DALNet significantly outperforms state-of-the-art WSSS methods. Our approach, in particular, allows for more efficient end-to-end process as a single-stage method.
<div id='section'>Paperid: <span id='pid'>284, <a href='https://arxiv.org/pdf/2409.04011.pdf' target='_blank'>https://arxiv.org/pdf/2409.04011.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weijie He, Mushui Liu, Yunlong Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.04011">Hybrid Mask Generation for Infrared Small Target Detection with Single-Point Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single-frame infrared small target (SIRST) detection poses a significant challenge due to the requirement to discern minute targets amidst complex infrared background clutter. In this paper, we focus on a weakly-supervised paradigm to obtain high-quality pseudo masks from the point-level annotation by integrating a novel learning-free method with the hybrid of the learning-based method. The learning-free method adheres to a sequential process, progressing from a point annotation to the bounding box that encompasses the target, and subsequently to detailed pseudo masks, while the hybrid is achieved through filtering out false alarms and retrieving missed detections in the network's prediction to provide a reliable supplement for learning-free masks. The experimental results show that our learning-free method generates pseudo masks with an average Intersection over Union (IoU) that is 4.3% higher than the second-best learning-free competitor across three datasets, while the hybrid learning-based method further enhances the quality of pseudo masks, achieving an additional average IoU increase of 3.4%.
<div id='section'>Paperid: <span id='pid'>285, <a href='https://arxiv.org/pdf/2511.17619.pdf' target='_blank'>https://arxiv.org/pdf/2511.17619.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qinghao Meng, Junbo Yin, Jianbing Shen, Yunde Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17619">Rethinking the Encoding and Annotating of 3D Bounding Box: Corner-Aware 3D Object Detection from Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Center-aligned regression remains dominant in LiDAR-based 3D object detection, yet it suffers from fundamental instability: object centers often fall in sparse or empty regions of the bird's-eye-view (BEV) due to the front-surface-biased nature of LiDAR point clouds, leading to noisy and inaccurate bounding box predictions. To circumvent this limitation, we revisit bounding box representation and propose corner-aligned regression, which shifts the prediction target from unstable centers to geometrically informative corners that reside in dense, observable regions. Leveraging the inherent geometric constraints among corners and image 2D boxes, partial parameters of 3D bounding boxes can be recovered from corner annotations, enabling a weakly supervised paradigm without requiring complete 3D labels. We design a simple yet effective corner-aware detection head that can be plugged into existing detectors. Experiments on KITTI show our method improves performance by 3.5% AP over center-based baseline, and achieves 83% of fully supervised accuracy using only BEV corner clicks, demonstrating the effectiveness of our corner-aware regression strategy.
<div id='section'>Paperid: <span id='pid'>286, <a href='https://arxiv.org/pdf/2511.14832.pdf' target='_blank'>https://arxiv.org/pdf/2511.14832.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marie Hein, Gregor Kasieczka, Michael Krämer, Louis Moureaux, Alexander Mück, David Shih
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.14832">How to pick the best anomaly detector?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anomaly detection has the potential to discover new physics in unexplored regions of the data. However, choosing the best anomaly detector for a given data set in a model-agnostic way is an important challenge which has hitherto largely been neglected. In this paper, we introduce the data-driven ARGOS metric, which has a sound theoretical foundation and is empirically shown to robustly select the most sensitive anomaly detection model given the data. Focusing on weakly-supervised, classifier-based anomaly detection methods, we show that the ARGOS metric outperforms other model selection metrics previously used in the literature, in particular the binary cross-entropy loss. We explore several realistic applications, including hyperparameter tuning as well as architecture and feature selection, and in all cases we demonstrate that ARGOS is robust to the noisy conditions of anomaly detection.
<div id='section'>Paperid: <span id='pid'>287, <a href='https://arxiv.org/pdf/2510.22209.pdf' target='_blank'>https://arxiv.org/pdf/2510.22209.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sofoklis Kitharidis, Cor J. Veenman, Thomas Bäck, Niki van Stein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.22209">Visual Model Selection using Feature Importance Clusters in Fairness-Performance Similarity Optimized Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the context of algorithmic decision-making, fair machine learning methods often yield multiple models that balance predictive fairness and performance in varying degrees. This diversity introduces a challenge for stakeholders who must select a model that aligns with their specific requirements and values. To address this, we propose an interactive framework that assists in navigating and interpreting the trade-offs across a portfolio of models. Our approach leverages weakly supervised metric learning to learn a Mahalanobis distance that reflects similarity in fairness and performance outcomes, effectively structuring the feature importance space of the models according to stakeholder-relevant criteria. We then apply clustering technique (k-means) to group models based on their transformed representations of feature importances, allowing users to explore clusters of models with similar predictive behaviors and fairness characteristics. This facilitates informed decision-making by helping users understand how models differ not only in their fairness-performance balance but also in the features that drive their predictions.
<div id='section'>Paperid: <span id='pid'>288, <a href='https://arxiv.org/pdf/2509.13116.pdf' target='_blank'>https://arxiv.org/pdf/2509.13116.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruibo Li, Hanyu Shi, Zhe Wang, Guosheng Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13116">Weakly and Self-Supervised Class-Agnostic Motion Prediction for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding motion in dynamic environments is critical for autonomous driving, thereby motivating research on class-agnostic motion prediction. In this work, we investigate weakly and self-supervised class-agnostic motion prediction from LiDAR point clouds. Outdoor scenes typically consist of mobile foregrounds and static backgrounds, allowing motion understanding to be associated with scene parsing. Based on this observation, we propose a novel weakly supervised paradigm that replaces motion annotations with fully or partially annotated (1%, 0.1%) foreground/background masks for supervision. To this end, we develop a weakly supervised approach utilizing foreground/background cues to guide the self-supervised learning of motion prediction models. Since foreground motion generally occurs in non-ground regions, non-ground/ground masks can serve as an alternative to foreground/background masks, further reducing annotation effort. Leveraging non-ground/ground cues, we propose two additional approaches: a weakly supervised method requiring fewer (0.01%) foreground/background annotations, and a self-supervised method without annotations. Furthermore, we design a Robust Consistency-aware Chamfer Distance loss that incorporates multi-frame information and robust penalty functions to suppress outliers in self-supervised learning. Experiments show that our weakly and self-supervised models outperform existing self-supervised counterparts, and our weakly supervised models even rival some supervised ones. This demonstrates that our approaches effectively balance annotation effort and performance.
<div id='section'>Paperid: <span id='pid'>289, <a href='https://arxiv.org/pdf/2506.18261.pdf' target='_blank'>https://arxiv.org/pdf/2506.18261.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Su, Dong Xu, Luping Zhou, Wanli Ouyang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18261">Improving Weakly Supervised Temporal Action Localization by Exploiting Multi-resolution Information in Temporal Domain</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised temporal action localization is a challenging task as only the video-level annotation is available during the training process. To address this problem, we propose a two-stage approach to fully exploit multi-resolution information in the temporal domain and generate high quality frame-level pseudo labels based on both appearance and motion streams. Specifically, in the first stage, we generate reliable initial frame-level pseudo labels, and in the second stage, we iteratively refine the pseudo labels and use a set of selected frames with highly confident pseudo labels to train neural networks and better predict action class scores at each frame. We fully exploit temporal information at multiple scales to improve temporal action localization performance. Specifically, in order to obtain reliable initial frame-level pseudo labels, in the first stage, we propose an Initial Label Generation (ILG) module, which leverages temporal multi-resolution consistency to generate high quality class activation sequences (CASs), which consist of a number of sequences with each sequence measuring how likely each video frame belongs to one specific action class. In the second stage, we propose a Progressive Temporal Label Refinement (PTLR) framework. In our PTLR framework, two networks called Network-OTS and Network-RTS, which are respectively used to generate CASs for the original temporal scale and the reduced temporal scales, are used as two streams (i.e., the OTS stream and the RTS stream) to refine the pseudo labels in turn. By this way, the multi-resolution information in the temporal domain is exchanged at the pseudo label level, and our work can help improve each stream (i.e., the OTS/RTS stream) by exploiting the refined pseudo labels from another stream (i.e., the RTS/OTS stream).
<div id='section'>Paperid: <span id='pid'>290, <a href='https://arxiv.org/pdf/2504.18866.pdf' target='_blank'>https://arxiv.org/pdf/2504.18866.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxu Leng, Zhanjie Wu, Mingpi Tan, Mengjingcheng Mo, Jiankang Zheng, Qingqing Li, Ji Gan, Xinbo Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.18866">PiercingEye: Dual-Space Video Violence Detection with Hyperbolic Vision-Language Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing weakly supervised video violence detection (VVD) methods primarily rely on Euclidean representation learning, which often struggles to distinguish visually similar yet semantically distinct events due to limited hierarchical modeling and insufficient ambiguous training samples. To address this challenge, we propose PiercingEye, a novel dual-space learning framework that synergizes Euclidean and hyperbolic geometries to enhance discriminative feature representation. Specifically, PiercingEye introduces a layer-sensitive hyperbolic aggregation strategy with hyperbolic Dirichlet energy constraints to progressively model event hierarchies, and a cross-space attention mechanism to facilitate complementary feature interactions between Euclidean and hyperbolic spaces. Furthermore, to mitigate the scarcity of ambiguous samples, we leverage large language models to generate logic-guided ambiguous event descriptions, enabling explicit supervision through a hyperbolic vision-language contrastive loss that prioritizes high-confusion samples via dynamic similarity-aware weighting. Extensive experiments on XD-Violence and UCF-Crime benchmarks demonstrate that PiercingEye achieves state-of-the-art performance, with particularly strong results on a newly curated ambiguous event subset, validating its superior capability in fine-grained violence detection.
<div id='section'>Paperid: <span id='pid'>291, <a href='https://arxiv.org/pdf/2504.13405.pdf' target='_blank'>https://arxiv.org/pdf/2504.13405.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengqin Jiang, Linfei Li, Haokui Zhang, Qingshan Liu, Amin Beheshti, Jian Yang, Anton van den Hengel, Quan Z. Sheng, Yuankai Qi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13405">ProgRoCC: A Progressive Approach to Rough Crowd Counting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As the number of individuals in a crowd grows, enumeration-based techniques become increasingly infeasible and their estimates increasingly unreliable. We propose instead an estimation-based version of the problem: we label Rough Crowd Counting that delivers better accuracy on the basis of training data that is easier to acquire. Rough crowd counting requires only rough annotations of the number of targets in an image, instead of the more traditional, and far more expensive, per-target annotations. We propose an approach to the rough crowd counting problem based on CLIP, termed ProgRoCC. Specifically, we introduce a progressive estimation learning strategy that determines the object count through a coarse-to-fine approach. This approach delivers answers quickly, outperforms the state-of-the-art in semi- and weakly-supervised crowd counting. In addition, we design a vision-language matching adapter that optimizes key-value pairs by mining effective matches of two modalities to refine the visual features, thereby improving the final performance. Extensive experimental results on three widely adopted crowd counting datasets demonstrate the effectiveness of our method.
<div id='section'>Paperid: <span id='pid'>292, <a href='https://arxiv.org/pdf/2502.03382.pdf' target='_blank'>https://arxiv.org/pdf/2502.03382.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tom Labiausse, Laurent MazarÃ©, Edouard Grave, Patrick PÃ©rez, Alexandre DÃ©fossez, Neil Zeghidour
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03382">High-Fidelity Simultaneous Speech-To-Speech Translation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Hibiki, a decoder-only model for simultaneous speech translation. Hibiki leverages a multistream language model to synchronously process source and target speech, and jointly produces text and audio tokens to perform speech-to-text and speech-to-speech translation. We furthermore address the fundamental challenge of simultaneous interpretation, which unlike its consecutive counterpart, where one waits for the end of the source utterance to start translating, adapts its flow to accumulate just enough context to produce a correct translation in real-time, chunk by chunk. To do so, we introduce a weakly-supervised method that leverages the perplexity of an off-the-shelf text translation system to identify optimal delays on a per-word basis and create aligned synthetic data. After supervised training, Hibiki performs adaptive, simultaneous speech translation with vanilla temperature sampling. On a French-English simultaneous speech translation task, Hibiki demonstrates state-of-the-art performance in translation quality, speaker fidelity and naturalness. Moreover, the simplicity of its inference process makes it compatible with batched translation and even real-time on-device deployment. We provide examples as well as models and inference code.
<div id='section'>Paperid: <span id='pid'>293, <a href='https://arxiv.org/pdf/2412.18842.pdf' target='_blank'>https://arxiv.org/pdf/2412.18842.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Heng-Bo Fan, Ming-Kun Xie, Jia-Hao Xiao, Sheng-Jun Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.18842">Context-Based Semantic-Aware Alignment for Semi-Supervised Multi-Label Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the lack of extensive precisely-annotated multi-label data in real word, semi-supervised multi-label learning (SSMLL) has gradually gained attention. Abundant knowledge embedded in vision-language models (VLMs) pre-trained on large-scale image-text pairs could alleviate the challenge of limited labeled data under SSMLL setting.Despite existing methods based on fine-tuning VLMs have achieved advances in weakly-supervised multi-label learning, they failed to fully leverage the information from labeled data to enhance the learning of unlabeled data. In this paper, we propose a context-based semantic-aware alignment method to solve the SSMLL problem by leveraging the knowledge of VLMs. To address the challenge of handling multiple semantics within an image, we introduce a novel framework design to extract label-specific image features. This design allows us to achieve a more compact alignment between text features and label-specific image features, leading the model to generate high-quality pseudo-labels. To incorporate the model with comprehensive understanding of image, we design a semi-supervised context identification auxiliary task to enhance the feature representation by capturing co-occurrence information. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our proposed method.
<div id='section'>Paperid: <span id='pid'>294, <a href='https://arxiv.org/pdf/2410.14790.pdf' target='_blank'>https://arxiv.org/pdf/2410.14790.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianchao Ci, Eldert J. van Henten, Xin Wang, Akshay K. Burusa, Gert Kootstra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14790">SSL-NBV: A Self-Supervised-Learning-Based Next-Best-View algorithm for Efficient 3D Plant Reconstruction by a Robot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The 3D reconstruction of plants is challenging due to their complex shape causing many occlusions. Next-Best-View (NBV) methods address this by iteratively selecting new viewpoints to maximize information gain (IG). Deep-learning-based NBV (DL-NBV) methods demonstrate higher computational efficiency over classic voxel-based NBV approaches but current methods require extensive training using ground-truth plant models, making them impractical for real-world plants. These methods, moreover, rely on offline training with pre-collected data, limiting adaptability in changing agricultural environments. This paper proposes a self-supervised learning-based NBV method (SSL-NBV) that uses a deep neural network to predict the IG for candidate viewpoints. The method allows the robot to gather its own training data during task execution by comparing new 3D sensor data to the earlier gathered data and by employing weakly-supervised learning and experience replay for efficient online learning. Comprehensive evaluations were conducted in simulation and real-world environments using cross-validation. The results showed that SSL-NBV required fewer views for plant reconstruction than non-NBV methods and was over 800 times faster than a voxel-based method. SSL-NBV reduced training annotations by over 90% compared to a baseline DL-NBV. Furthermore, SSL-NBV could adapt to novel scenarios through online fine-tuning. Also using real plants, the results showed that the proposed method can learn to effectively plan new viewpoints for 3D plant reconstruction. Most importantly, SSL-NBV automated the entire network training and uses continuous online learning, allowing it to operate in changing agricultural environments.
<div id='section'>Paperid: <span id='pid'>295, <a href='https://arxiv.org/pdf/2409.19252.pdf' target='_blank'>https://arxiv.org/pdf/2409.19252.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxu Leng, Zhanjie Wu, Mingpi Tan, Yiran Liu, Ji Gan, Haosheng Chen, Xinbo Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19252">Beyond Euclidean: Dual-Space Representation Learning for Weakly Supervised Video Violence Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While numerous Video Violence Detection (VVD) methods have focused on representation learning in Euclidean space, they struggle to learn sufficiently discriminative features, leading to weaknesses in recognizing normal events that are visually similar to violent events (\emph{i.e.}, ambiguous violence). In contrast, hyperbolic representation learning, renowned for its ability to model hierarchical and complex relationships between events, has the potential to amplify the discrimination between visually similar events. Inspired by these, we develop a novel Dual-Space Representation Learning (DSRL) method for weakly supervised VVD to utilize the strength of both Euclidean and hyperbolic geometries, capturing the visual features of events while also exploring the intrinsic relations between events, thereby enhancing the discriminative capacity of the features. DSRL employs a novel information aggregation strategy to progressively learn event context in hyperbolic spaces, which selects aggregation nodes through layer-sensitive hyperbolic association degrees constrained by hyperbolic Dirichlet energy. Furthermore, DSRL attempts to break the cyber-balkanization of different spaces, utilizing cross-space attention to facilitate information interactions between Euclidean and hyperbolic space to capture better discriminative features for final violence detection. Comprehensive experiments demonstrate the effectiveness of our proposed DSRL.
<div id='section'>Paperid: <span id='pid'>296, <a href='https://arxiv.org/pdf/2409.06210.pdf' target='_blank'>https://arxiv.org/pdf/2409.06210.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ji Ha Jang, Hoigi Seo, Se Young Chun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.06210">INTRA: Interaction Relationship-aware Weakly Supervised Affordance Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Affordance denotes the potential interactions inherent in objects. The perception of affordance can enable intelligent agents to navigate and interact with new environments efficiently. Weakly supervised affordance grounding teaches agents the concept of affordance without costly pixel-level annotations, but with exocentric images. Although recent advances in weakly supervised affordance grounding yielded promising results, there remain challenges including the requirement for paired exocentric and egocentric image dataset, and the complexity in grounding diverse affordances for a single object. To address them, we propose INTeraction Relationship-aware weakly supervised Affordance grounding (INTRA). Unlike prior arts, INTRA recasts this problem as representation learning to identify unique features of interactions through contrastive learning with exocentric images only, eliminating the need for paired datasets. Moreover, we leverage vision-language model embeddings for performing affordance grounding flexibly with any text, designing text-conditioned affordance map generation to reflect interaction relationship for contrastive learning and enhancing robustness with our text synonym augmentation. Our method outperformed prior arts on diverse datasets such as AGD20K, IIT-AFF, CAD and UMD. Additionally, experimental results demonstrate that our method has remarkable domain scalability for synthesized images / illustrations and is capable of performing affordance grounding for novel interactions and objects.
<div id='section'>Paperid: <span id='pid'>297, <a href='https://arxiv.org/pdf/2512.16243.pdf' target='_blank'>https://arxiv.org/pdf/2512.16243.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Zhang, Yunfei Gong, Zhidan Xie, Zhizi Wang, Antoni B. Chan, Hui Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.16243">Semi-Supervised Multi-View Crowd Counting by Ranking Multi-View Fusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-view crowd counting has been proposed to deal with the severe occlusion issue of crowd counting in large and wide scenes. However, due to the difficulty of collecting and annotating multi-view images, the datasets for multi-view counting have a limited number of multi-view frames and scenes. To solve the problem of limited data, one approach is to collect synthetic data to bypass the annotating step, while another is to propose semi- or weakly-supervised or unsupervised methods that demand less multi-view data. In this paper, we propose two semi-supervised multi-view crowd counting frameworks by ranking the multi-view fusion models of different numbers of input views, in terms of the model predictions or the model uncertainties. Specifically, for the first method (vanilla model), we rank the multi-view fusion models' prediction results of different numbers of camera-view inputs, namely, the model's predictions with fewer camera views shall not be larger than the predictions with more camera views. For the second method, we rank the estimated model uncertainties of the multi-view fusion models with a variable number of view inputs, guided by the multi-view fusion models' prediction errors, namely, the model uncertainties with more camera views shall not be larger than those with fewer camera views. These constraints are introduced into the model training in a semi-supervised fashion for multi-view counting with limited labeled data. The experiments demonstrate the advantages of the proposed multi-view model ranking methods compared with other semi-supervised counting methods.
<div id='section'>Paperid: <span id='pid'>298, <a href='https://arxiv.org/pdf/2512.10316.pdf' target='_blank'>https://arxiv.org/pdf/2512.10316.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Khang Le, Ha Thach, Anh M. Vu, Trang T. K. Vo, Han H. Huynh, David Yang, Minh H. N. Le, Thanh-Huy Nguyen, Akash Awasthi, Chandra Mohan, Zhu Han, Hien Van Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.10316">ConStruct: Structural Distillation of Foundation Models for Prototype-Based Weakly Supervised Histopathology Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation (WSSS) in histopathology relies heavily on classification backbones, yet these models often localize only the most discriminative regions and struggle to capture the full spatial extent of tissue structures. Vision-language models such as CONCH offer rich semantic alignment and morphology-aware representations, while modern segmentation backbones like SegFormer preserve fine-grained spatial cues. However, combining these complementary strengths remains challenging, especially under weak supervision and without dense annotations. We propose a prototype learning framework for WSSS in histopathological images that integrates morphology-aware representations from CONCH, multi-scale structural cues from SegFormer, and text-guided semantic alignment to produce prototypes that are simultaneously semantically discriminative and spatially coherent. To effectively leverage these heterogeneous sources, we introduce text-guided prototype initialization that incorporates pathology descriptions to generate more complete and semantically accurate pseudo-masks. A structural distillation mechanism transfers spatial knowledge from SegFormer to preserve fine-grained morphological patterns and local tissue boundaries during prototype learning. Our approach produces high-quality pseudo masks without pixel-level annotations, improves localization completeness, and enhances semantic consistency across tissue types. Experiments on BCSS-WSSS datasets demonstrate that our prototype learning framework outperforms existing WSSS methods while remaining computationally efficient through frozen foundation model backbones and lightweight trainable adapters.
<div id='section'>Paperid: <span id='pid'>299, <a href='https://arxiv.org/pdf/2512.10314.pdf' target='_blank'>https://arxiv.org/pdf/2512.10314.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anh M. Vu, Khang P. Le, Trang T. K. Vo, Ha Thach, Huy Hung Nguyen, David Yang, Han H. Huynh, Quynh Nguyen, Tuan M. Pham, Tuan-Anh Le, Minh H. N. Le, Thanh-Huy Nguyen, Akash Awasthi, Chandra Mohan, Zhu Han, Hien Van Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.10314">DualProtoSeg: Simple and Efficient Design with Text- and Image-Guided Prototype Learning for Weakly Supervised Histopathology Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation (WSSS) in histopathology seeks to reduce annotation cost by learning from image-level labels, yet it remains limited by inter-class homogeneity, intra-class heterogeneity, and the region-shrinkage effect of CAM-based supervision. We propose a simple and effective prototype-driven framework that leverages vision-language alignment to improve region discovery under weak supervision. Our method integrates CoOp-style learnable prompt tuning to generate text-based prototypes and combines them with learnable image prototypes, forming a dual-modal prototype bank that captures both semantic and appearance cues. To address oversmoothing in ViT representations, we incorporate a multi-scale pyramid module that enhances spatial precision and improves localization quality. Experiments on the BCSS-WSSS benchmark show that our approach surpasses existing state-of-the-art methods, and detailed analyses demonstrate the benefits of text description diversity, context length, and the complementary behavior of text and image prototypes. These results highlight the effectiveness of jointly leveraging textual semantics and visual prototype learning for WSSS in digital pathology.
<div id='section'>Paperid: <span id='pid'>300, <a href='https://arxiv.org/pdf/2510.17484.pdf' target='_blank'>https://arxiv.org/pdf/2510.17484.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Umer Ramzan, Ali Zia, Abdelwahed Khamis, Noman Ali, Usman Ali, Wei Xiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17484">Split-Fuse-Transport: Annotation-Free Saliency via Dual Clustering and Optimal Transport Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Salient object detection (SOD) aims to segment visually prominent regions in images and serves as a foundational task for various computer vision applications. We posit that SOD can now reach near-supervised accuracy without a single pixel-level label, but only when reliable pseudo-masks are available. We revisit the prototype-based line of work and make two key observations. First, boundary pixels and interior pixels obey markedly different geometry; second, the global consistency enforced by optimal transport (OT) is underutilized if prototype quality is weak. To address this, we introduce POTNet, an adaptation of Prototypical Optimal Transport that replaces POT's single k-means step with an entropy-guided dual-clustering head: high-entropy pixels are organized by spectral clustering, low-entropy pixels by k-means, and the two prototype sets are subsequently aligned by OT. This split-fuse-transport design yields sharper, part-aware pseudo-masks in a single forward pass, without handcrafted priors. Those masks supervise a standard MaskFormer-style encoder-decoder, giving rise to AutoSOD, an end-to-end unsupervised SOD pipeline that eliminates SelfMask's offline voting yet improves both accuracy and training efficiency. Extensive experiments on five benchmarks show that AutoSOD outperforms unsupervised methods by up to 26% and weakly supervised methods by up to 36% in F-measure, further narrowing the gap to fully supervised models.
<div id='section'>Paperid: <span id='pid'>301, <a href='https://arxiv.org/pdf/2509.22132.pdf' target='_blank'>https://arxiv.org/pdf/2509.22132.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingjing Lu, Huilong Pi, Yunchuan Qin, Zhuo Tang, Ruihui Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22132">Self-Supervised Point Cloud Completion based on Multi-View Augmentations of Single Partial Point Cloud</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Point cloud completion aims to reconstruct complete shapes from partial observations. Although current methods have achieved remarkable performance, they still have some limitations: Supervised methods heavily rely on ground truth, which limits their generalization to real-world datasets due to the synthetic-to-real domain gap. Unsupervised methods require complete point clouds to compose unpaired training data, and weakly-supervised methods need multi-view observations of the object. Existing self-supervised methods frequently produce unsatisfactory predictions due to the limited capabilities of their self-supervised signals. To overcome these challenges, we propose a novel self-supervised point cloud completion method. We design a set of novel self-supervised signals based on multi-view augmentations of the single partial point cloud. Additionally, to enhance the model's learning ability, we first incorporate Mamba into self-supervised point cloud completion task, encouraging the model to generate point clouds with better quality. Experiments on synthetic and real-world datasets demonstrate that our method achieves state-of-the-art results.
<div id='section'>Paperid: <span id='pid'>302, <a href='https://arxiv.org/pdf/2509.08129.pdf' target='_blank'>https://arxiv.org/pdf/2509.08129.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Francisco M. Castro-MacÃ­as, Francisco J. SÃ¡ez-Maldonado, Pablo Morales-Ãlvarez, Rafael Molina
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08129">torchmil: A PyTorch-based library for deep Multiple Instance Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple Instance Learning (MIL) is a powerful framework for weakly supervised learning, particularly useful when fine-grained annotations are unavailable. Despite growing interest in deep MIL methods, the field lacks standardized tools for model development, evaluation, and comparison, which hinders reproducibility and accessibility. To address this, we present torchmil, an open-source Python library built on PyTorch. torchmil offers a unified, modular, and extensible framework, featuring basic building blocks for MIL models, a standardized data format, and a curated collection of benchmark datasets and models. The library includes comprehensive documentation and tutorials to support both practitioners and researchers. torchmil aims to accelerate progress in MIL and lower the entry barrier for new users. Available at https://torchmil.readthedocs.io.
<div id='section'>Paperid: <span id='pid'>303, <a href='https://arxiv.org/pdf/2508.12322.pdf' target='_blank'>https://arxiv.org/pdf/2508.12322.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Deutges, Chen Yang, Raheleh Salehi, Nassir Navab, Carsten Marr, Ario Sadafi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12322">Neural Cellular Automata for Weakly Supervised Segmentation of White Blood Cells</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The detection and segmentation of white blood cells in blood smear images is a key step in medical diagnostics, supporting various downstream tasks such as automated blood cell counting, morphological analysis, cell classification, and disease diagnosis and monitoring. Training robust and accurate models requires large amounts of labeled data, which is both time-consuming and expensive to acquire. In this work, we propose a novel approach for weakly supervised segmentation using neural cellular automata (NCA-WSS). By leveraging the feature maps generated by NCA during classification, we can extract segmentation masks without the need for retraining with segmentation labels. We evaluate our method on three white blood cell microscopy datasets and demonstrate that NCA-WSS significantly outperforms existing weakly supervised approaches. Our work illustrates the potential of NCA for both classification and segmentation in a weakly supervised framework, providing a scalable and efficient solution for medical image analysis.
<div id='section'>Paperid: <span id='pid'>304, <a href='https://arxiv.org/pdf/2508.06318.pdf' target='_blank'>https://arxiv.org/pdf/2508.06318.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giacomo D'Amicantonio, Snehashis Majhi, Quan Kong, Lorenzo Garattoni, Gianpiero Francesca, FranÃ§ois Bremond, Egor Bondarev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06318">Mixture of Experts Guided by Gaussian Splatters Matters: A new Approach to Weakly-Supervised Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Anomaly Detection (VAD) is a challenging task due to the variability of anomalous events and the limited availability of labeled data. Under the Weakly-Supervised VAD (WSVAD) paradigm, only video-level labels are provided during training, while predictions are made at the frame level. Although state-of-the-art models perform well on simple anomalies (e.g., explosions), they struggle with complex real-world events (e.g., shoplifting). This difficulty stems from two key issues: (1) the inability of current models to address the diversity of anomaly types, as they process all categories with a shared model, overlooking category-specific features; and (2) the weak supervision signal, which lacks precise temporal information, limiting the ability to capture nuanced anomalous patterns blended with normal events. To address these challenges, we propose Gaussian Splatting-guided Mixture of Experts (GS-MoE), a novel framework that employs a set of expert models, each specialized in capturing specific anomaly types. These experts are guided by a temporal Gaussian splatting loss, enabling the model to leverage temporal consistency and enhance weak supervision. The Gaussian splatting approach encourages a more precise and comprehensive representation of anomalies by focusing on temporal segments most likely to contain abnormal events. The predictions from these specialized experts are integrated through a mixture-of-experts mechanism to model complex relationships across diverse anomaly patterns. Our approach achieves state-of-the-art performance, with a 91.58% AUC on the UCF-Crime dataset, and demonstrates superior results on XD-Violence and MSAD datasets. By leveraging category-specific expertise and temporal guidance, GS-MoE sets a new benchmark for VAD under weak supervision.
<div id='section'>Paperid: <span id='pid'>305, <a href='https://arxiv.org/pdf/2508.03997.pdf' target='_blank'>https://arxiv.org/pdf/2508.03997.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Zhang, Tianzhuzi Tan, Guanchun Yin, Bo Zhang, Xiuzhuang Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03997">JanusNet: Hierarchical Slice-Block Shuffle and Displacement for Semi-Supervised 3D Multi-Organ Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Limited by the scarcity of training samples and annotations, weakly supervised medical image segmentation often employs data augmentation to increase data diversity, while randomly mixing volumetric blocks has demonstrated strong performance. However, this approach disrupts the inherent anatomical continuity of 3D medical images along orthogonal axes, leading to severe structural inconsistencies and insufficient training in challenging regions, such as small-sized organs, etc. To better comply with and utilize human anatomical information, we propose JanusNet}, a data augmentation framework for 3D medical data that globally models anatomical continuity while locally focusing on hard-to-segment regions. Specifically, our Slice-Block Shuffle step performs aligned shuffling of same-index slice blocks across volumes along a random axis, while preserving the anatomical context on planes perpendicular to the perturbation axis. Concurrently, the Confidence-Guided Displacement step uses prediction reliability to replace blocks within each slice, amplifying signals from difficult areas. This dual-stage, axis-aligned framework is plug-and-play, requiring minimal code changes for most teacher-student schemes. Extensive experiments on the Synapse and AMOS datasets demonstrate that JanusNet significantly surpasses state-of-the-art methods, achieving, for instance, a 4% DSC gain on the Synapse dataset with only 20% labeled data.
<div id='section'>Paperid: <span id='pid'>306, <a href='https://arxiv.org/pdf/2508.01697.pdf' target='_blank'>https://arxiv.org/pdf/2508.01697.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiqi Huang, Tingfa Xu, Wen Yan, Dean Barratt, Yipeng Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01697">Register Anything: Estimating "Corresponding Prompts" for Segment Anything Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Establishing pixel/voxel-level or region-level correspondences is the core challenge in image registration. The latter, also known as region-based correspondence representation, leverages paired regions of interest (ROIs) to enable regional matching while preserving fine-grained capability at pixel/voxel level. Traditionally, this representation is implemented via two steps: segmenting ROIs in each image then matching them between the two images. In this paper, we simplify this into one step by directly "searching for corresponding prompts", using extensively pre-trained segmentation models (e.g., SAM) for a training-free registration approach, PromptReg. Firstly, we introduce the "corresponding prompt problem", which aims to identify a corresponding Prompt Y in Image Y for any given visual Prompt X in Image X, such that the two respectively prompt-conditioned segmentations are a pair of corresponding ROIs from the two images. Secondly, we present an "inverse prompt" solution that generates primary and optionally auxiliary prompts, inverting Prompt X into the prompt space of Image Y. Thirdly, we propose a novel registration algorithm that identifies multiple paired corresponding ROIs by marginalizing the inverted Prompt X across both prompt and spatial dimensions. Comprehensive experiments are conducted on five applications of registering 3D prostate MR, 3D abdomen MR, 3D lung CT, 2D histopathology and, as a non-medical example, 2D aerial images. Based on metrics including Dice and target registration errors on anatomical structures, the proposed registration outperforms both intensity-based iterative algorithms and learning-based DDF-predicting networks, even yielding competitive performance with weakly-supervised approaches that require fully-segmented training data.
<div id='section'>Paperid: <span id='pid'>307, <a href='https://arxiv.org/pdf/2506.23460.pdf' target='_blank'>https://arxiv.org/pdf/2506.23460.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dewen Zeng, Xinrong Hu, Yu-Jen Chen, Yawen Wu, Xiaowei Xu, Yiyu Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23460">Contrastive Learning with Diffusion Features for Weakly Supervised Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation (WSSS) methods using class labels often rely on class activation maps (CAMs) to localize objects. However, traditional CAM-based methods struggle with partial activations and imprecise object boundaries due to optimization discrepancies between classification and segmentation. Recently, the conditional diffusion model (CDM) has been used as an alternative for generating segmentation masks in WSSS, leveraging its strong image generation capabilities tailored to specific class distributions. By modifying or perturbing the condition during diffusion sampling, the related objects can be highlighted in the generated images. Yet, the saliency maps generated by CDMs are prone to noise from background alterations during reverse diffusion. To alleviate the problem, we introduce Contrastive Learning with Diffusion Features (CLDF), a novel method that uses contrastive learning to train a pixel decoder to map the diffusion features from a frozen CDM to a low-dimensional embedding space for segmentation. Specifically, we integrate gradient maps generated from CDM external classifier with CAMs to identify foreground and background pixels with fewer false positives/negatives for contrastive learning, enabling robust pixel embedding learning. Experimental results on four segmentation tasks from two public medical datasets demonstrate that our method significantly outperforms existing baselines.
<div id='section'>Paperid: <span id='pid'>308, <a href='https://arxiv.org/pdf/2506.06006.pdf' target='_blank'>https://arxiv.org/pdf/2506.06006.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifu Qiu, Yftah Ziser, Anna Korhonen, Shay B. Cohen, Edoardo M. Ponti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06006">Bootstrapping World Models from Dynamics Models in Multimodal Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To what extent do vision-and-language foundation models possess a realistic world model (observation $\times$ action $\rightarrow$ observation) and a dynamics model (observation $\times$ observation $\rightarrow$ action), when actions are expressed through language? While open-source foundation models struggle with both, we find that fine-tuning them to acquire a dynamics model through supervision is significantly easier than acquiring a world model. In turn, dynamics models can be used to bootstrap world models through two main strategies: 1) weakly supervised learning from synthetic data and 2) inference time verification. Firstly, the dynamics model can annotate actions for unlabelled pairs of video frame observations to expand the training data. We further propose a new objective, where image tokens in observation pairs are weighted by their importance, as predicted by a recognition model. Secondly, the dynamics models can assign rewards to multiple samples of the world model to score them, effectively guiding search at inference time. We evaluate the world models resulting from both strategies through the task of action-centric image editing on Aurora-Bench. Our best model achieves a performance competitive with state-of-the-art image editing models, improving on them by a margin of $15\%$ on real-world subsets according to GPT4o-as-judge, and achieving the best average human evaluation across all subsets of Aurora-Bench.
<div id='section'>Paperid: <span id='pid'>309, <a href='https://arxiv.org/pdf/2505.13123.pdf' target='_blank'>https://arxiv.org/pdf/2505.13123.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Snehashis Majhi, Giacomo D'Amicantonio, Antitza Dantcheva, Quan Kong, Lorenzo Garattoni, Gianpiero Francesca, Egor Bondarev, Francois Bremond
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13123">Just Dance with $Ï$! A Poly-modal Inductor for Weakly-supervised Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised methods for video anomaly detection (VAD) are conventionally based merely on RGB spatio-temporal features, which continues to limit their reliability in real-world scenarios. This is due to the fact that RGB-features are not sufficiently distinctive in setting apart categories such as shoplifting from visually similar events. Therefore, towards robust complex real-world VAD, it is essential to augment RGB spatio-temporal features by additional modalities. Motivated by this, we introduce the Poly-modal Induced framework for VAD: "PI-VAD", a novel approach that augments RGB representations by five additional modalities. Specifically, the modalities include sensitivity to fine-grained motion (Pose), three dimensional scene and entity representation (Depth), surrounding objects (Panoptic masks), global motion (optical flow), as well as language cues (VLM). Each modality represents an axis of a polygon, streamlined to add salient cues to RGB. PI-VAD includes two plug-in modules, namely Pseudo-modality Generation module and Cross Modal Induction module, which generate modality-specific prototypical representation and, thereby, induce multi-modal information into RGB cues. These modules operate by performing anomaly-aware auxiliary tasks and necessitate five modality backbones -- only during training. Notably, PI-VAD achieves state-of-the-art accuracy on three prominent VAD datasets encompassing real-world scenarios, without requiring the computational overhead of five modality backbones at inference.
<div id='section'>Paperid: <span id='pid'>310, <a href='https://arxiv.org/pdf/2503.20826.pdf' target='_blank'>https://arxiv.org/pdf/2503.20826.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiwei Yang, Yucong Meng, Kexue Fu, Feilong Tang, Shuo Wang, Zhijian Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20826">Exploring CLIP's Dense Knowledge for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Semantic Segmentation (WSSS) with image-level labels aims to achieve pixel-level predictions using Class Activation Maps (CAMs). Recently, Contrastive Language-Image Pre-training (CLIP) has been introduced in WSSS. However, recent methods primarily focus on image-text alignment for CAM generation, while CLIP's potential in patch-text alignment remains unexplored. In this work, we propose ExCEL to explore CLIP's dense knowledge via a novel patch-text alignment paradigm for WSSS. Specifically, we propose Text Semantic Enrichment (TSE) and Visual Calibration (VC) modules to improve the dense alignment across both text and vision modalities. To make text embeddings semantically informative, our TSE module applies Large Language Models (LLMs) to build a dataset-wide knowledge base and enriches the text representations with an implicit attribute-hunting process. To mine fine-grained knowledge from visual features, our VC module first proposes Static Visual Calibration (SVC) to propagate fine-grained knowledge in a non-parametric manner. Then Learnable Visual Calibration (LVC) is further proposed to dynamically shift the frozen features towards distributions with diverse semantics. With these enhancements, ExCEL not only retains CLIP's training-free advantages but also significantly outperforms other state-of-the-art methods with much less training cost on PASCAL VOC and MS COCO.
<div id='section'>Paperid: <span id='pid'>311, <a href='https://arxiv.org/pdf/2411.08753.pdf' target='_blank'>https://arxiv.org/pdf/2411.08753.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sagnik Majumder, Tushar Nagarajan, Ziad Al-Halah, Reina Pradhan, Kristen Grauman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08753">Which Viewpoint Shows it Best? Language for Weakly Supervising View Selection in Multi-view Instructional Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Given a multi-view video, which viewpoint is most informative for a human observer? Existing methods rely on heuristics or expensive "best-view" supervision to answer this question, limiting their applicability. We propose a weakly supervised approach that leverages language accompanying an instructional multi-view video as a means to recover its most informative viewpoint(s). Our key hypothesis is that the more accurately an individual view can predict a view-agnostic text summary, the more informative it is. To put this into action, we propose LangView, a framework that uses the relative accuracy of view-dependent caption predictions as a proxy for best view pseudo-labels. Then, those pseudo-labels are used to train a view selector, together with an auxiliary camera pose predictor that enhances view-sensitivity. During inference, our model takes as input only a multi-view video--no language or camera poses--and returns the best viewpoint to watch at each timestep. On two challenging datasets comprised of diverse multi-camera setups and how-to activities, our model consistently outperforms state-of-the-art baselines, both with quantitative metrics and human evaluation. Project page: https://vision.cs.utexas.edu/projects/which-view-shows-it-best.
<div id='section'>Paperid: <span id='pid'>312, <a href='https://arxiv.org/pdf/2410.08091.pdf' target='_blank'>https://arxiv.org/pdf/2410.08091.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyi Pan, Wei Gao, Shan Liu, Ge Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08091">Distribution Guidance Network for Weakly Supervised Point Cloud Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite alleviating the dependence on dense annotations inherent to fully supervised methods, weakly supervised point cloud semantic segmentation suffers from inadequate supervision signals. In response to this challenge, we introduce a novel perspective that imparts auxiliary constraints by regulating the feature space under weak supervision. Our initial investigation identifies which distributions accurately characterize the feature space, subsequently leveraging this priori to guide the alignment of the weakly supervised embeddings. Specifically, we analyze the superiority of the mixture of von Mises-Fisher distributions (moVMF) among several common distribution candidates. Accordingly, we develop a Distribution Guidance Network (DGNet), which comprises a weakly supervised learning branch and a distribution alignment branch. Leveraging reliable clustering initialization derived from the weakly supervised learning branch, the distribution alignment branch alternately updates the parameters of the moVMF and the network, ensuring alignment with the moVMF-defined latent space. Extensive experiments validate the rationality and effectiveness of our distribution choice and network design. Consequently, DGNet achieves state-of-the-art performance under multiple datasets and various weakly supervised settings.
<div id='section'>Paperid: <span id='pid'>313, <a href='https://arxiv.org/pdf/2511.01131.pdf' target='_blank'>https://arxiv.org/pdf/2511.01131.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Nahiduzzaman, Steven Korevaar, Alireza Bab-Hadiashar, Ruwan Tennakoon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.01131">Weakly Supervised Concept Learning with Class-Level Priors for Interpretable Medical Diagnosis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-interpretable predictions are essential for deploying AI in medical imaging, yet most interpretable-by-design (IBD) frameworks require concept annotations for training data, which are costly and impractical to obtain in clinical contexts. Recent attempts to bypass annotation, such as zero-shot vision-language models or concept-generation frameworks, struggle to capture domain-specific medical features, leading to poor reliability. In this paper, we propose a novel Prior-guided Concept Predictor (PCP), a weakly supervised framework that enables concept answer prediction without explicit supervision or reliance on language models. PCP leverages class-level concept priors as weak supervision and incorporates a refinement mechanism with KL divergence and entropy regularization to align predictions with clinical reasoning. Experiments on PH2 (dermoscopy) and WBCatt (hematology) show that PCP improves concept-level F1-score by over 33% compared to zero-shot baselines, while delivering competitive classification performance on four medical datasets (PH2, WBCatt, HAM10000, and CXR4) relative to fully supervised concept bottleneck models (CBMs) and V-IP.
<div id='section'>Paperid: <span id='pid'>314, <a href='https://arxiv.org/pdf/2509.14573.pdf' target='_blank'>https://arxiv.org/pdf/2509.14573.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Takamasa Yamaguchi, Brian Kenji Iwana, Ryoma Bise, Shota Harada, Takumi Okuo, Kiyohito Tanaka, Kaito Shiku
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14573">Domain Adaptation for Ulcerative Colitis Severity Estimation Using Patient-Level Diagnoses</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of methods to estimate the severity of Ulcerative Colitis (UC) is of significant importance. However, these methods often suffer from domain shifts caused by differences in imaging devices and clinical settings across hospitals. Although several domain adaptation methods have been proposed to address domain shift, they still struggle with the lack of supervision in the target domain or the high cost of annotation. To overcome these challenges, we propose a novel Weakly Supervised Domain Adaptation method that leverages patient-level diagnostic results, which are routinely recorded in UC diagnosis, as weak supervision in the target domain. The proposed method aligns class-wise distributions across domains using Shared Aggregation Tokens and a Max-Severity Triplet Loss, which leverages the characteristic that patient-level diagnoses are determined by the most severe region within each patient. Experimental results demonstrate that our method outperforms comparative DA approaches, improving UC severity estimation in a domain-shifted setting.
<div id='section'>Paperid: <span id='pid'>315, <a href='https://arxiv.org/pdf/2508.11472.pdf' target='_blank'>https://arxiv.org/pdf/2508.11472.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Wang, Yaxin Zhao, Xinyu Jiao, Sihan Xu, Xiangrui Cai, Ying Zhang, Xiaojie Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11472">RMSL: Weakly-Supervised Insider Threat Detection with Robust Multi-sphere Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Insider threat detection aims to identify malicious user behavior by analyzing logs that record user interactions. Due to the lack of fine-grained behavior-level annotations, detecting specific behavior-level anomalies within user behavior sequences is challenging. Unsupervised methods face high false positive rates and miss rates due to the inherent ambiguity between normal and anomalous behaviors. In this work, we instead introduce weak labels of behavior sequences, which have lower annotation costs, i.e., the training labels (anomalous or normal) are at sequence-level instead of behavior-level, to enhance the detection capability for behavior-level anomalies by learning discriminative features. To achieve this, we propose a novel framework called Robust Multi-sphere Learning (RMSL). RMSL uses multiple hyper-spheres to represent the normal patterns of behaviors. Initially, a one-class classifier is constructed as a good anomaly-supervision-free starting point. Building on this, using multiple instance learning and adaptive behavior-level self-training debiasing based on model prediction confidence, the framework further refines hyper-spheres and feature representations using weak sequence-level labels. This approach enhances the model's ability to distinguish between normal and anomalous behaviors. Extensive experiments demonstrate that RMSL significantly improves the performance of behavior-level insider threat detection.
<div id='section'>Paperid: <span id='pid'>316, <a href='https://arxiv.org/pdf/2507.15203.pdf' target='_blank'>https://arxiv.org/pdf/2507.15203.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyue Liu, Xicheng Sheng, Xiahai Zhuang, Vicente Grau, Mark YY Chan, Ching-Hui Sia, Lei Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15203">Personalized 4D Whole Heart Geometry Reconstruction from Cine MRI for Cardiac Digital Twins</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cardiac digital twins (CDTs) provide personalized in-silico cardiac representations and hold great potential for precision medicine in cardiology. However, whole-heart CDT models that simulate the full organ-scale electromechanics of all four heart chambers remain limited. In this work, we propose a weakly supervised learning model to reconstruct 4D (3D+t) heart mesh directly from multi-view 2D cardiac cine MRIs. This is achieved by learning a self-supervised mapping between cine MRIs and 4D cardiac meshes, enabling the generation of personalized heart models that closely correspond to input cine MRIs. The resulting 4D heart meshes can facilitate the automatic extraction of key cardiac variables, including ejection fraction and dynamic chamber volume changes with high temporal resolution. It demonstrates the feasibility of inferring personalized 4D heart models from cardiac MRIs, paving the way for an efficient CDT platform for precision medicine. The code will be publicly released once the manuscript is accepted.
<div id='section'>Paperid: <span id='pid'>317, <a href='https://arxiv.org/pdf/2505.08687.pdf' target='_blank'>https://arxiv.org/pdf/2505.08687.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hangwei Zhang, Zhimu Huang, Yan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08687">AC-PKAN: Attention-Enhanced and Chebyshev Polynomial-Based Physics-Informed Kolmogorov-Arnold Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Kolmogorov-Arnold Networks (KANs) have recently shown promise for solving partial differential equations (PDEs). Yet their original formulation is computationally and memory intensive, motivating the introduction of Chebyshev Type-I-based KANs (Chebyshev1KANs). Although Chebyshev1KANs have outperformed the vanilla KANs architecture, our rigorous theoretical analysis reveals that they still suffer from rank collapse, ultimately limiting their expressive capacity. To overcome these limitations, we enhance Chebyshev1KANs by integrating wavelet-activated MLPs with learnable parameters and an internal attention mechanism. We prove that this design preserves a full-rank Jacobian and is capable of approximating solutions to PDEs of arbitrary order. Furthermore, to alleviate the loss instability and imbalance introduced by the Chebyshev polynomial basis, we externally incorporate a Residual Gradient Attention (RGA) mechanism that dynamically re-weights individual loss terms according to their gradient norms and residual magnitudes. By jointly leveraging internal and external attention, we present AC-PKAN, a novel architecture that constitutes an enhancement to weakly supervised Physics-Informed Neural Networks (PINNs) and extends the expressive power of KANs. Experimental results from nine benchmark tasks across three domains show that AC-PKAN consistently outperforms or matches state-of-the-art models such as PINNsFormer, establishing it as a highly effective tool for solving complex real-world engineering problems in zero-data or data-sparse regimes. The code will be made publicly available upon acceptance.
<div id='section'>Paperid: <span id='pid'>318, <a href='https://arxiv.org/pdf/2503.11376.pdf' target='_blank'>https://arxiv.org/pdf/2503.11376.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Panggih Kusuma Ningrum, Philipp Mayr, Nina Smirnova, Iana Atanassova
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11376">Annotating Scientific Uncertainty: A comprehensive model using linguistic patterns and comparison with existing approaches</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>UnScientify, a system designed to detect scientific uncertainty in scholarly full text. The system utilizes a weakly supervised technique to identify verbally expressed uncertainty in scientific texts and their authorial references. The core methodology of UnScientify is based on a multi-faceted pipeline that integrates span pattern matching, complex sentence analysis and author reference checking. This approach streamlines the labeling and annotation processes essential for identifying scientific uncertainty, covering a variety of uncertainty expression types to support diverse applications including information retrieval, text mining and scientific document processing. The evaluation results highlight the trade-offs between modern large language models (LLMs) and the UnScientify system. UnScientify, which employs more traditional techniques, achieved superior performance in the scientific uncertainty detection task, attaining an accuracy score of 0.808. This finding underscores the continued relevance and efficiency of UnScientify's simple rule-based and pattern matching strategy for this specific application. The results demonstrate that in scenarios where resource efficiency, interpretability, and domain-specific adaptability are critical, traditional methods can still offer significant advantages.
<div id='section'>Paperid: <span id='pid'>319, <a href='https://arxiv.org/pdf/2411.10497.pdf' target='_blank'>https://arxiv.org/pdf/2411.10497.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xavier Bou, Gabriele Facciolo, Rafael Grompone von Gioi, Jean-Michel Morel, Thibaud Ehret
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10497">Structure Tensor Representation for Robust Oriented Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Oriented object detection predicts orientation in addition to object location and bounding box. Precisely predicting orientation remains challenging due to angular periodicity, which introduces boundary discontinuity issues and symmetry ambiguities. Inspired by classical works on edge and corner detection, this paper proposes to represent orientation in oriented bounding boxes as a structure tensor. This representation combines the strengths of Gaussian-based methods and angle-coder solutions, providing a simple yet efficient approach that is robust to angular periodicity issues without additional hyperparameters. Extensive evaluations across five datasets demonstrate that the proposed structure tensor representation outperforms previous methods in both fully-supervised and weakly supervised tasks, achieving high precision in angular prediction with minimal computational overhead. Thus, this work establishes structure tensors as a robust and modular alternative for encoding orientation in oriented object detection. We make our code publicly available, allowing for seamless integration into existing object detectors.
<div id='section'>Paperid: <span id='pid'>320, <a href='https://arxiv.org/pdf/2410.19176.pdf' target='_blank'>https://arxiv.org/pdf/2410.19176.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dachun Sun, Ruijie Wang, Jinning Li, Ruipeng Han, Xinyi Liu, You Lyu, Tarek Abdelzaher
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.19176">Perturbation-based Graph Active Learning for Weakly-Supervised Belief Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the problem of optimizing the allocation of labeling resources for semi-supervised belief representation learning in social networks. The objective is to strategically identify valuable messages on social media graphs that are worth labeling within a constrained budget, ultimately maximizing the task's performance. Despite the progress in unsupervised or semi-supervised methods in advancing belief and ideology representation learning on social networks and the remarkable efficacy of graph learning techniques, the availability of high-quality curated labeled social data can greatly benefit and further improve performances. Consequently, allocating labeling efforts is a critical research problem in scenarios where labeling resources are limited. This paper proposes a graph data augmentation-inspired perturbation-based active learning strategy (PerbALGraph) that progressively selects messages for labeling according to an automatic estimator, obviating human guidance. This estimator is based on the principle that messages in the network that exhibit heightened sensitivity to structural features of the observational data indicate landmark quality that significantly influences semi-supervision processes. We design the estimator to be the prediction variance under a set of designed graph perturbations, which is model-agnostic and application-independent. Extensive experiment results demonstrate the effectiveness of the proposed strategy for belief representation learning tasks.
<div id='section'>Paperid: <span id='pid'>321, <a href='https://arxiv.org/pdf/2409.16566.pdf' target='_blank'>https://arxiv.org/pdf/2409.16566.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kartikeya Singh, Yash Turkar, Christo Aluckal, Charuvarahan Adhivarahan, Karthik Dantu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16566">PANOS: Payload-Aware Navigation in Offroad Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nature has evolved humans to walk on different terrains by developing a detailed understanding of their physical characteristics. Similarly, legged robots need to develop their capability to walk on complex terrains with a variety of task-dependent payloads to achieve their goals. However, conventional terrain adaptation methods are susceptible to failure with varying payloads. In this work, we introduce PANOS, a weakly supervised approach that integrates proprioception and exteroception from onboard sensing to achieve a stable gait while walking by a legged robot over various terrains. Our work also provides evidence of its adaptability over varying payloads. We evaluate our method on multiple terrains and payloads using a legged robot. PANOS improves the stability up to 44% without any payload and 53% with 15 lbs payload. We also notice a reduction in the vibration cost of 20% with the payload for various terrain types when compared to state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>322, <a href='https://arxiv.org/pdf/2512.17788.pdf' target='_blank'>https://arxiv.org/pdf/2512.17788.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Tang, Yin-Fang Yang, Weijia Zhang, Min-Ling Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.17788">Calibratable Disambiguation Loss for Multi-Instance Partial-Label Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-instance partial-label learning (MIPL) is a weakly supervised framework that extends the principles of multi-instance learning (MIL) and partial-label learning (PLL) to address the challenges of inexact supervision in both instance and label spaces. However, existing MIPL approaches often suffer from poor calibration, undermining classifier reliability. In this work, we propose a plug-and-play calibratable disambiguation loss (CDL) that simultaneously improves classification accuracy and calibration performance. The loss has two instantiations: the first one calibrates predictions based on probabilities from the candidate label set, while the second one integrates probabilities from both candidate and non-candidate label sets. The proposed CDL can be seamlessly incorporated into existing MIPL and PLL frameworks. We provide a theoretical analysis that establishes the lower bound and regularization properties of CDL, demonstrating its superiority over conventional disambiguation losses. Experimental results on benchmark and real-world datasets confirm that our CDL significantly enhances both classification and calibration performance.
<div id='section'>Paperid: <span id='pid'>323, <a href='https://arxiv.org/pdf/2512.10408.pdf' target='_blank'>https://arxiv.org/pdf/2512.10408.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiyue Sun, Tailin Chen, Yinghui Zhang, Yuchen Zhang, Jiangbei Yue, Jianbo Jiao, Zeyu Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.10408">MultiHateLoc: Towards Temporal Localisation of Multimodal Hate Content in Online Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid growth of video content on platforms such as TikTok and YouTube has intensified the spread of multimodal hate speech, where harmful cues emerge subtly and asynchronously across visual, acoustic, and textual streams. Existing research primarily focuses on video-level classification, leaving the practically crucial task of temporal localisation, identifying when hateful segments occur, largely unaddressed. This challenge is even more noticeable under weak supervision, where only video-level labels are available, and static fusion or classification-based architectures struggle to capture cross-modal and temporal dynamics. To address these challenges, we propose MultiHateLoc, the first framework designed for weakly-supervised multimodal hate localisation. MultiHateLoc incorporates (1) modality-aware temporal encoders to model heterogeneous sequential patterns, including a tailored text-based preprocessing module for feature enhancement; (2) dynamic cross-modal fusion to adaptively emphasise the most informative modality at each moment and a cross-modal contrastive alignment strategy to enhance multimodal feature consistency; (3) a modality-aware MIL objective to identify discriminative segments under video-level supervision. Despite relying solely on coarse labels, MultiHateLoc produces fine-grained, interpretable frame-level predictions. Experiments on HateMM and MultiHateClip show that our method achieves state-of-the-art performance in the localisation task.
<div id='section'>Paperid: <span id='pid'>324, <a href='https://arxiv.org/pdf/2512.01701.pdf' target='_blank'>https://arxiv.org/pdf/2512.01701.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiuli Bi, Die Xiao, Junchao Fan, Bin Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.01701">SSR: Semantic and Spatial Rectification for CLIP-based Weakly Supervised Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, Contrastive Language-Image Pretraining (CLIP) has been widely applied to Weakly Supervised Semantic Segmentation (WSSS) tasks due to its powerful cross-modal semantic understanding capabilities. This paper proposes a novel Semantic and Spatial Rectification (SSR) method to address the limitations of existing CLIP-based weakly supervised semantic segmentation approaches: over-activation in non-target foreground regions and background areas. Specifically, at the semantic level, the Cross-Modal Prototype Alignment (CMPA) establishes a contrastive learning mechanism to enforce feature space alignment across modalities, reducing inter-class overlap while enhancing semantic correlations, to rectify over-activation in non-target foreground regions effectively; at the spatial level, the Superpixel-Guided Correction (SGC) leverages superpixel-based spatial priors to precisely filter out interference from non-target regions during affinity propagation, significantly rectifying background over-activation. Extensive experiments on the PASCAL VOC and MS COCO datasets demonstrate that our method outperforms all single-stage approaches, as well as more complex multi-stage approaches, achieving mIoU scores of 79.5% and 50.6%, respectively.
<div id='section'>Paperid: <span id='pid'>325, <a href='https://arxiv.org/pdf/2510.17875.pdf' target='_blank'>https://arxiv.org/pdf/2510.17875.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoxu Xu, Xuexun Liu, Jinlong Li, Yitian Yuan, Qiudan Zhang, Lin Ma, Nicu Sebe, Xu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17875">3D Weakly Supervised Semantic Segmentation via Class-Aware and Geometry-Guided Pseudo-Label Refinement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D weakly supervised semantic segmentation (3D WSSS) aims to achieve semantic segmentation by leveraging sparse or low-cost annotated data, significantly reducing reliance on dense point-wise annotations. Previous works mainly employ class activation maps or pre-trained vision-language models to address this challenge. However, the low quality of pseudo-labels and the insufficient exploitation of 3D geometric priors jointly create significant technical bottlenecks in developing high-performance 3D WSSS models. In this paper, we propose a simple yet effective 3D weakly supervised semantic segmentation method that integrates 3D geometric priors into a class-aware guidance mechanism to generate high-fidelity pseudo labels. Concretely, our designed methodology first employs Class-Aware Label Refinement module to generate more balanced and accurate pseudo labels for semantic categrories. This initial refinement stage focuses on enhancing label quality through category-specific optimization. Subsequently, the Geometry-Aware Label Refinement component is developed, which strategically integrates implicit 3D geometric constraints to effectively filter out low-confidence pseudo labels that fail to comply with geometric plausibility. Moreover, to address the challenge of extensive unlabeled regions, we propose a Label Update strategy that integrates Self-Training to propagate labels into these areas. This iterative process continuously enhances pseudo-label quality while expanding label coverage, ultimately fostering the development of high-performance 3D WSSS models. Comprehensive experimental validation reveals that our proposed methodology achieves state-of-the-art performance on both ScanNet and S3DIS benchmarks while demonstrating remarkable generalization capability in unsupervised settings, maintaining competitive accuracy through its robust design.
<div id='section'>Paperid: <span id='pid'>326, <a href='https://arxiv.org/pdf/2510.17566.pdf' target='_blank'>https://arxiv.org/pdf/2510.17566.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nachuan Ma, Zhengfei Song, Qiang Hu, Xiaoyu Tang, Chengxi Zhang, Rui Fan, Lihua Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17566">WP-CrackNet: A Collaborative Adversarial Learning Framework for End-to-End Weakly-Supervised Road Crack Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Road crack detection is essential for intelligent infrastructure maintenance in smart cities. To reduce reliance on costly pixel-level annotations, we propose WP-CrackNet, an end-to-end weakly-supervised method that trains with only image-level labels for pixel-wise crack detection. WP-CrackNet integrates three components: a classifier generating class activation maps (CAMs), a reconstructor measuring feature inferability, and a detector producing pixel-wise road crack detection results. During training, the classifier and reconstructor alternate in adversarial learning to encourage crack CAMs to cover complete crack regions, while the detector learns from pseudo labels derived from post-processed crack CAMs. This mutual feedback among the three components improves learning stability and detection accuracy. To further boost detection performance, we design a path-aware attention module (PAAM) that fuses high-level semantics from the classifier with low-level structural cues from the reconstructor by modeling spatial and channel-wise dependencies. Additionally, a center-enhanced CAM consistency module (CECCM) is proposed to refine crack CAMs using center Gaussian weighting and consistency constraints, enabling better pseudo-label generation. We create three image-level datasets and extensive experiments show that WP-CrackNet achieves comparable results to supervised methods and outperforms existing weakly-supervised methods, significantly advancing scalable road inspection. The source code package and datasets are available at https://mias.group/WP-CrackNet/.
<div id='section'>Paperid: <span id='pid'>327, <a href='https://arxiv.org/pdf/2506.19222.pdf' target='_blank'>https://arxiv.org/pdf/2506.19222.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinke Ma, Yongsheng Pan, Qingjie Zeng, Mengkang Lu, Bolysbek Murat Yerzhanuly, Bazargul Matkerim, Yong Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19222">Deformable Medical Image Registration with Effective Anatomical Structure Representation and Divide-and-Conquer Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective representation of Regions of Interest (ROI) and independent alignment of these ROIs can significantly enhance the performance of deformable medical image registration (DMIR). However, current learning-based DMIR methods have limitations. Unsupervised techniques disregard ROI representation and proceed directly with aligning pairs of images, while weakly-supervised methods heavily depend on label constraints to facilitate registration. To address these issues, we introduce a novel ROI-based registration approach named EASR-DCN. Our method represents medical images through effective ROIs and achieves independent alignment of these ROIs without requiring labels. Specifically, we first used a Gaussian mixture model for intensity analysis to represent images using multiple effective ROIs with distinct intensities. Furthermore, we propose a novel Divide-and-Conquer Network (DCN) to process these ROIs through separate channels to learn feature alignments for each ROI. The resultant correspondences are seamlessly integrated to generate a comprehensive displacement vector field. Extensive experiments were performed on three MRI and one CT datasets to showcase the superior accuracy and deformation reduction efficacy of our EASR-DCN. Compared to VoxelMorph, our EASR-DCN achieved improvements of 10.31\% in the Dice score for brain MRI, 13.01\% for cardiac MRI, and 5.75\% for hippocampus MRI, highlighting its promising potential for clinical applications. The code for this work will be released upon acceptance of the paper.
<div id='section'>Paperid: <span id='pid'>328, <a href='https://arxiv.org/pdf/2506.05912.pdf' target='_blank'>https://arxiv.org/pdf/2506.05912.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adrien Petralia, Paul Boniol, Philippe Charpentier, Themis Palpanas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05912">DeviceScope: An Interactive App to Detect and Localize Appliance Patterns in Electricity Consumption Time Series</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, electricity suppliers have installed millions of smart meters worldwide to improve the management of the smart grid system. These meters collect a large amount of electrical consumption data to produce valuable information to help consumers reduce their electricity footprint. However, having non-expert users (e.g., consumers or sales advisors) understand these data and derive usage patterns for different appliances has become a significant challenge for electricity suppliers because these data record the aggregated behavior of all appliances. At the same time, ground-truth labels (which could train appliance detection and localization models) are expensive to collect and extremely scarce in practice. This paper introduces DeviceScope, an interactive tool designed to facilitate understanding smart meter data by detecting and localizing individual appliance patterns within a given time period. Our system is based on CamAL (Class Activation Map-based Appliance Localization), a novel weakly supervised approach for appliance localization that only requires the knowledge of the existence of an appliance in a household to be trained. This paper appeared in ICDE 2025.
<div id='section'>Paperid: <span id='pid'>329, <a href='https://arxiv.org/pdf/2505.01064.pdf' target='_blank'>https://arxiv.org/pdf/2505.01064.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hari Chandana Kuchibhotla, Sai Srinivas Kancheti, Abbavaram Gowtham Reddy, Vineeth N Balasubramanian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01064">Efficient Vocabulary-Free Fine-Grained Visual Recognition in the Age of Multimodal LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fine-grained Visual Recognition (FGVR) involves distinguishing between visually similar categories, which is inherently challenging due to subtle inter-class differences and the need for large, expert-annotated datasets. In domains like medical imaging, such curated datasets are unavailable due to issues like privacy concerns and high annotation costs. In such scenarios lacking labeled data, an FGVR model cannot rely on a predefined set of training labels, and hence has an unconstrained output space for predictions. We refer to this task as Vocabulary-Free FGVR (VF-FGVR), where a model must predict labels from an unconstrained output space without prior label information. While recent Multimodal Large Language Models (MLLMs) show potential for VF-FGVR, querying these models for each test input is impractical because of high costs and prohibitive inference times. To address these limitations, we introduce \textbf{Nea}rest-Neighbor Label \textbf{R}efinement (NeaR), a novel approach that fine-tunes a downstream CLIP model using labels generated by an MLLM. Our approach constructs a weakly supervised dataset from a small, unlabeled training set, leveraging MLLMs for label generation. NeaR is designed to handle the noise, stochasticity, and open-endedness inherent in labels generated by MLLMs, and establishes a new benchmark for efficient VF-FGVR.
<div id='section'>Paperid: <span id='pid'>330, <a href='https://arxiv.org/pdf/2411.02466.pdf' target='_blank'>https://arxiv.org/pdf/2411.02466.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Robin Trombetta, Olivier RouviÃ¨re, Carole Lartizien
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.02466">Weakly supervised deep learning model with size constraint for prostate cancer detection in multiparametric MRI and generalization to unseen domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fully supervised deep models have shown promising performance for many medical segmentation tasks. Still, the deployment of these tools in clinics is limited by the very timeconsuming collection of manually expert-annotated data. Moreover, most of the state-ofthe-art models have been trained and validated on moderately homogeneous datasets. It is known that deep learning methods are often greatly degraded by domain or label shifts and are yet to be built in such a way as to be robust to unseen data or label distributions. In the clinical setting, this problematic is particularly relevant as the deployment institutions may have different scanners or acquisition protocols than those from which the data has been collected to train the model. In this work, we propose to address these two challenges on the detection of clinically significant prostate cancer (csPCa) from bi-parametric MRI. We evaluate the method proposed by (Kervadec et al., 2018), which introduces a size constaint loss to produce fine semantic cancer lesions segmentations from weak circle scribbles annotations. Performance of the model is based on two public (PI-CAI and Prostate158) and one private databases. First, we show that the model achieves on-par performance with strong fully supervised baseline models, both on in-distribution validation data and unseen test images. Second, we observe a performance decrease for both fully supervised and weakly supervised models when tested on unseen data domains. This confirms the crucial need for efficient domain adaptation methods if deep learning models are aimed to be deployed in a clinical environment. Finally, we show that ensemble predictions from multiple trainings increase generalization performance.
<div id='section'>Paperid: <span id='pid'>331, <a href='https://arxiv.org/pdf/2410.19836.pdf' target='_blank'>https://arxiv.org/pdf/2410.19836.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ronan Docherty, Antonis Vamvakeros, Samuel J. Cooper
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.19836">Upsampling DINOv2 features for unsupervised vision tasks and weakly supervised materials segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The features of self-supervised vision transformers (ViTs) contain strong semantic and positional information relevant to downstream tasks like object localization and segmentation. Recent works combine these features with traditional methods like clustering, graph partitioning or region correlations to achieve impressive baselines without finetuning or training additional networks. We leverage upsampled features from ViT networks (e.g DINOv2) in two workflows: in a clustering based approach for object localization and segmentation, and paired with standard classifiers in weakly supervised materials segmentation. Both show strong performance on benchmarks, especially in weakly supervised segmentation where the ViT features capture complex relationships inaccessible to classical approaches. We expect the flexibility and generalizability of these features will both speed up and strengthen materials characterization, from segmentation to property-prediction.
<div id='section'>Paperid: <span id='pid'>332, <a href='https://arxiv.org/pdf/2410.15657.pdf' target='_blank'>https://arxiv.org/pdf/2410.15657.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianjun Gao, Chen Cai, Ruoyu Wang, Wenyang Liu, Kim-Hui Yap, Kratika Garg, Boon-Siew Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15657">CL-HOI: Cross-Level Human-Object Interaction Distillation from Vision Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-object interaction (HOI) detection has seen advancements with Vision Language Models (VLMs), but these methods often depend on extensive manual annotations. Vision Large Language Models (VLLMs) can inherently recognize and reason about interactions at the image level but are computationally heavy and not designed for instance-level HOI detection. To overcome these limitations, we propose a Cross-Level HOI distillation (CL-HOI) framework, which distills instance-level HOIs from VLLMs image-level understanding without the need for manual annotations. Our approach involves two stages: context distillation, where a Visual Linguistic Translator (VLT) converts visual information into linguistic form, and interaction distillation, where an Interaction Cognition Network (ICN) reasons about spatial, visual, and context relations. We design contrastive distillation losses to transfer image-level context and interaction knowledge from the teacher to the student model, enabling instance-level HOI detection. Evaluations on HICO-DET and V-COCO datasets demonstrate that our CL-HOI surpasses existing weakly supervised methods and VLLM supervised methods, showing its efficacy in detecting HOIs without manual labels.
<div id='section'>Paperid: <span id='pid'>333, <a href='https://arxiv.org/pdf/2601.05852.pdf' target='_blank'>https://arxiv.org/pdf/2601.05852.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jen Dusseljee, Sarah de Boer, Alessa Hering
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.05852">Kidney Cancer Detection Using 3D-Based Latent Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we present a novel latent diffusion-based pipeline for 3D kidney anomaly detection on contrast-enhanced abdominal CT. The method combines Denoising Diffusion Probabilistic Models (DDPMs), Denoising Diffusion Implicit Models (DDIMs), and Vector-Quantized Generative Adversarial Networks (VQ-GANs). Unlike prior slice-wise approaches, our method operates directly on an image volume and leverages weak supervision with only case-level pseudo-labels. We benchmark our approach against state-of-the-art supervised segmentation and detection models. This study demonstrates the feasibility and promise of 3D latent diffusion for weakly supervised anomaly detection. While the current results do not yet match supervised baselines, they reveal key directions for improving reconstruction fidelity and lesion localization. Our findings provide an important step toward annotation-efficient, generative modeling of complex abdominal anatomy.
<div id='section'>Paperid: <span id='pid'>334, <a href='https://arxiv.org/pdf/2512.06845.pdf' target='_blank'>https://arxiv.org/pdf/2512.06845.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Satoshi Hashimoto, Hitoshi Nishimura, Yanan Wang, Mori Kurokawa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.06845">Pseudo Anomalies Are All You Need: Diffusion-Based Generation for Weakly-Supervised Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deploying video anomaly detection in practice is hampered by the scarcity and collection cost of real abnormal footage. We address this by training without any real abnormal videos while evaluating under the standard weakly supervised split, and we introduce PA-VAD, a generation-driven approach that learns a detector from synthesized pseudo-abnormal videos paired with real normal videos, using only a small set of real normal images to drive synthesis. For synthesis, we select class-relevant initial images with CLIP and refine textual prompts with a vision-language model to improve fidelity and scene consistency before invoking a video diffusion model. For training, we mitigate excessive spatiotemporal magnitude in synthesized anomalies by an domain-aligned regularized module that combines domain alignment and memory usage-aware updates. Extensive experiments show that our approach reaches 98.2% on ShanghaiTech and 82.5% on UCF-Crime, surpassing the strongest real-abnormal method on ShanghaiTech by +0.6% and outperforming the UVAD state-of-the-art on UCF-Crime by +1.9%. The results demonstrate that high-accuracy anomaly detection can be obtained without collecting real anomalies, providing a practical path toward scalable deployment.
<div id='section'>Paperid: <span id='pid'>335, <a href='https://arxiv.org/pdf/2510.16450.pdf' target='_blank'>https://arxiv.org/pdf/2510.16450.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shan Xiong, Jiabao Chen, Ye Wang, Jialin Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.16450">Instance-Aware Pseudo-Labeling and Class-Focused Contrastive Learning for Weakly Supervised Domain Adaptive Segmentation of Electron Microscopy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Annotation-efficient segmentation of the numerous mitochondria instances from various electron microscopy (EM) images is highly valuable for biological and neuroscience research. Although unsupervised domain adaptation (UDA) methods can help mitigate domain shifts and reduce the high costs of annotating each domain, they typically have relatively low performance in practical applications. Thus, we investigate weakly supervised domain adaptation (WDA) that utilizes additional sparse point labels on the target domain, which require minimal annotation effort and minimal expert knowledge. To take full use of the incomplete and imprecise point annotations, we introduce a multitask learning framework that jointly conducts segmentation and center detection with a novel cross-teaching mechanism and class-focused cross-domain contrastive learning. While leveraging unlabeled image regions is essential, we introduce segmentation self-training with a novel instance-aware pseudo-label (IPL) selection strategy. Unlike existing methods that typically rely on pixel-wise pseudo-label filtering, the IPL semantically selects reliable and diverse pseudo-labels with the help of the detection task. Comprehensive validations and comparisons on challenging datasets demonstrate that our method outperforms existing UDA and WDA methods, significantly narrowing the performance gap with the supervised upper bound. Furthermore, under the UDA setting, our method also achieves substantial improvements over other UDA techniques.
<div id='section'>Paperid: <span id='pid'>336, <a href='https://arxiv.org/pdf/2509.18973.pdf' target='_blank'>https://arxiv.org/pdf/2509.18973.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiabao Chen, Shan Xiong, Jialin Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18973">Prompt-DAS: Annotation-Efficient Prompt Learning for Domain Adaptive Semantic Segmentation of Electron Microscopy Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain adaptive segmentation (DAS) of numerous organelle instances from large-scale electron microscopy (EM) is a promising way to enable annotation-efficient learning. Inspired by SAM, we propose a promptable multitask framework, namely Prompt-DAS, which is flexible enough to utilize any number of point prompts during the adaptation training stage and testing stage. Thus, with varying prompt configurations, Prompt-DAS can perform unsupervised domain adaptation (UDA) and weakly supervised domain adaptation (WDA), as well as interactive segmentation during testing. Unlike the foundation model SAM, which necessitates a prompt for each individual object instance, Prompt-DAS is only trained on a small dataset and can utilize full points on all instances, sparse points on partial instances, or even no points at all, facilitated by the incorporation of an auxiliary center-point detection task. Moreover, a novel prompt-guided contrastive learning is proposed to enhance discriminative feature learning. Comprehensive experiments conducted on challenging benchmarks demonstrate the effectiveness of the proposed approach over existing UDA, WDA, and SAM-based approaches.
<div id='section'>Paperid: <span id='pid'>337, <a href='https://arxiv.org/pdf/2509.01209.pdf' target='_blank'>https://arxiv.org/pdf/2509.01209.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>MaÃ«lic Neau, Zoe Falomir, CÃ©dric Buche, Akihiro Sugimoto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01209">Measuring Image-Relation Alignment: Reference-Free Evaluation of VLMs and Synthetic Pre-training for Open-Vocabulary Scene Graph Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene Graph Generation (SGG) encodes visual relationships between objects in images as graph structures. Thanks to the advances of Vision-Language Models (VLMs), the task of Open-Vocabulary SGG has been recently proposed where models are evaluated on their functionality to learn a wide and diverse range of relations. Current benchmarks in SGG, however, possess a very limited vocabulary, making the evaluation of open-source models inefficient. In this paper, we propose a new reference-free metric to fairly evaluate the open-vocabulary capabilities of VLMs for relation prediction. Another limitation of Open-Vocabulary SGG is the reliance on weakly supervised data of poor quality for pre-training. We also propose a new solution for quickly generating high-quality synthetic data through region-specific prompt tuning of VLMs. Experimental results show that pre-training with this new data split can benefit the generalization capabilities of Open-Voc SGG models.
<div id='section'>Paperid: <span id='pid'>338, <a href='https://arxiv.org/pdf/2508.19909.pdf' target='_blank'>https://arxiv.org/pdf/2508.19909.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lechun You, Zhonghua Wu, Weide Liu, Xulei Yang, Jun Cheng, Wei Zhou, Bharadwaj Veeravalli, Guosheng Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19909">Integrating SAM Supervision for 3D Weakly Supervised Point Cloud Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current methods for 3D semantic segmentation propose training models with limited annotations to address the difficulty of annotating large, irregular, and unordered 3D point cloud data. They usually focus on the 3D domain only, without leveraging the complementary nature of 2D and 3D data. Besides, some methods extend original labels or generate pseudo labels to guide the training, but they often fail to fully use these labels or address the noise within them. Meanwhile, the emergence of comprehensive and adaptable foundation models has offered effective solutions for segmenting 2D data. Leveraging this advancement, we present a novel approach that maximizes the utility of sparsely available 3D annotations by incorporating segmentation masks generated by 2D foundation models. We further propagate the 2D segmentation masks into the 3D space by establishing geometric correspondences between 3D scenes and 2D views. We extend the highly sparse annotations to encompass the areas delineated by 3D masks, thereby substantially augmenting the pool of available labels. Furthermore, we apply confidence- and uncertainty-based consistency regularization on augmentations of the 3D point cloud and select the reliable pseudo labels, which are further spread on the 3D masks to generate more labels. This innovative strategy bridges the gap between limited 3D annotations and the powerful capabilities of 2D foundation models, ultimately improving the performance of 3D weakly supervised segmentation.
<div id='section'>Paperid: <span id='pid'>339, <a href='https://arxiv.org/pdf/2508.18790.pdf' target='_blank'>https://arxiv.org/pdf/2508.18790.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhui Tao, Yizhe Zhang, Qiang Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18790">A Closer Look at Edema Area Segmentation in SD-OCT Images Using Adversarial Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of artificial intelligence models for macular edema (ME) analy-sis always relies on expert-annotated pixel-level image datasets which are expen-sive to collect prospectively. While anomaly-detection-based weakly-supervised methods have shown promise in edema area (EA) segmentation task, their per-formance still lags behind fully-supervised approaches. In this paper, we leverage the strong correlation between EA and retinal layers in spectral-domain optical coherence tomography (SD-OCT) images, along with the update characteristics of weakly-supervised learning, to enhance an off-the-shelf adversarial framework for EA segmentation with a novel layer-structure-guided post-processing step and a test-time-adaptation (TTA) strategy. By incorporating additional retinal lay-er information, our framework reframes the dense EA prediction task as one of confirming intersection points between the EA contour and retinal layers, result-ing in predictions that better align with the shape prior of EA. Besides, the TTA framework further helps address discrepancies in the manifestations and presen-tations of EA between training and test sets. Extensive experiments on two pub-licly available datasets demonstrate that these two proposed ingredients can im-prove the accuracy and robustness of EA segmentation, bridging the gap between weakly-supervised and fully-supervised models.
<div id='section'>Paperid: <span id='pid'>340, <a href='https://arxiv.org/pdf/2507.02399.pdf' target='_blank'>https://arxiv.org/pdf/2507.02399.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peilin Zhang, Shaouxan Wua, Jun Feng, Zhuo Jin, Zhizezhang Gao, Jingkun Chen, Yaqiong Xing, Xiao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02399">TABNet: A Triplet Augmentation Self-Recovery Framework with Boundary-Aware Pseudo-Labels for Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Background and objective: Medical image segmentation is a core task in various clinical applications. However, acquiring large-scale, fully annotated medical image datasets is both time-consuming and costly. Scribble annotations, as a form of sparse labeling, provide an efficient and cost-effective alternative for medical image segmentation. However, the sparsity of scribble annotations limits the feature learning of the target region and lacks sufficient boundary supervision, which poses significant challenges for training segmentation networks. Methods: We propose TAB Net, a novel weakly-supervised medical image segmentation framework, consisting of two key components: the triplet augmentation self-recovery (TAS) module and the boundary-aware pseudo-label supervision (BAP) module. The TAS module enhances feature learning through three complementary augmentation strategies: intensity transformation improves the model's sensitivity to texture and contrast variations, cutout forces the network to capture local anatomical structures by masking key regions, and jigsaw augmentation strengthens the modeling of global anatomical layout by disrupting spatial continuity. By guiding the network to recover complete masks from diverse augmented inputs, TAS promotes a deeper semantic understanding of medical images under sparse supervision. The BAP module enhances pseudo-supervision accuracy and boundary modeling by fusing dual-branch predictions into a loss-weighted pseudo-label and introducing a boundary-aware loss for fine-grained contour refinement. Results: Experimental evaluations on two public datasets, ACDC and MSCMR seg, demonstrate that TAB Net significantly outperforms state-of-the-art methods for scribble-based weakly supervised segmentation. Moreover, it achieves performance comparable to that of fully supervised methods.
<div id='section'>Paperid: <span id='pid'>341, <a href='https://arxiv.org/pdf/2506.10633.pdf' target='_blank'>https://arxiv.org/pdf/2506.10633.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Konstantinos Vilouras, Ilias Stogiannidis, Junyu Yan, Alison Q. O'Neil, Sotirios A. Tsaftaris
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10633">Anatomy-Grounded Weakly Supervised Prompt Tuning for Chest X-ray Latent Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Latent Diffusion Models have shown remarkable results in text-guided image synthesis in recent years. In the domain of natural (RGB) images, recent works have shown that such models can be adapted to various vision-language downstream tasks with little to no supervision involved. On the contrary, text-to-image Latent Diffusion Models remain relatively underexplored in the field of medical imaging, primarily due to limited data availability (e.g., due to privacy concerns). In this work, focusing on the chest X-ray modality, we first demonstrate that a standard text-conditioned Latent Diffusion Model has not learned to align clinically relevant information in free-text radiology reports with the corresponding areas of the given scan. Then, to alleviate this issue, we propose a fine-tuning framework to improve multi-modal alignment in a pre-trained model such that it can be efficiently repurposed for downstream tasks such as phrase grounding. Our method sets a new state-of-the-art on a standard benchmark dataset (MS-CXR), while also exhibiting robust performance on out-of-distribution data (VinDr-CXR). Our code will be made publicly available.
<div id='section'>Paperid: <span id='pid'>342, <a href='https://arxiv.org/pdf/2506.04351.pdf' target='_blank'>https://arxiv.org/pdf/2506.04351.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maksym Ivashechkin, Oscar Mendez, Richard Bowden
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.04351">HuGeDiff: 3D Human Generation via Diffusion with Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D human generation is an important problem with a wide range of applications in computer vision and graphics. Despite recent progress in generative AI such as diffusion models or rendering methods like Neural Radiance Fields or Gaussian Splatting, controlling the generation of accurate 3D humans from text prompts remains an open challenge. Current methods struggle with fine detail, accurate rendering of hands and faces, human realism, and controlability over appearance. The lack of diversity, realism, and annotation in human image data also remains a challenge, hindering the development of a foundational 3D human model. We present a weakly supervised pipeline that tries to address these challenges. In the first step, we generate a photorealistic human image dataset with controllable attributes such as appearance, race, gender, etc using a state-of-the-art image diffusion model. Next, we propose an efficient mapping approach from image features to 3D point clouds using a transformer-based architecture. Finally, we close the loop by training a point-cloud diffusion model that is conditioned on the same text prompts used to generate the original samples. We demonstrate orders-of-magnitude speed-ups in 3D human generation compared to the state-of-the-art approaches, along with significantly improved text-prompt alignment, realism, and rendering quality. We will make the code and dataset available.
<div id='section'>Paperid: <span id='pid'>343, <a href='https://arxiv.org/pdf/2506.03229.pdf' target='_blank'>https://arxiv.org/pdf/2506.03229.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qian-Wei Wang, Yuqiu Xie, Letian Zhang, Zimo Liu, Shu-Tao Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03229">Pre-trained Vision-Language Models Assisted Noisy Partial Label Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the context of noisy partial label learning (NPLL), each training sample is associated with a set of candidate labels annotated by multiple noisy annotators. With the emergence of high-performance pre-trained vision-language models (VLMs) such as CLIP, LLaVa and GPT-4V, the direction of using these models to replace time-consuming manual annotation workflows and achieve "manual-annotation-free" training for downstream tasks has become a highly promising research avenue. This paper focuses on learning from noisy partial labels annotated by pre-trained VLMs and proposes an innovative collaborative consistency regularization (Co-Reg) method. Unlike the symmetric noise primarily addressed in traditional noisy label learning, the noise generated by pre-trained models is instance-dependent, embodying the underlying patterns of the pre-trained models themselves, which significantly increases the learning difficulty for the model. To address this, we simultaneously train two neural networks that implement collaborative purification of training labels through a "Co-Pseudo-Labeling" mechanism, while enforcing consistency regularization constraints in both the label space and feature representation space. Our method can also leverage few-shot manually annotated valid labels to further enhance its performances. Comparative experiments with different denoising and disambiguation algorithms, annotation manners, and pre-trained model application schemes fully validate the effectiveness of the proposed method, while revealing the broad prospects of integrating weakly-supervised learning techniques into the knowledge distillation process of pre-trained models.
<div id='section'>Paperid: <span id='pid'>344, <a href='https://arxiv.org/pdf/2503.18082.pdf' target='_blank'>https://arxiv.org/pdf/2503.18082.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nachuan Ma, Zhengfei Song, Qiang Hu, Chuang-Wei Liu, Yu Han, Yanting Zhang, Rui Fan, Lihua Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18082">Vehicular Road Crack Detection with Deep Learning: A New Online Benchmark for Comprehensive Evaluation of Existing Algorithms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the emerging field of urban digital twins (UDTs), advancing intelligent road inspection (IRI) vehicles with automatic road crack detection systems is essential for maintaining civil infrastructure. Over the past decade, deep learning-based road crack detection methods have been developed to detect cracks more efficiently, accurately, and objectively, with the goal of replacing manual visual inspection. Nonetheless, there is a lack of systematic reviews on state-of-the-art (SoTA) deep learning techniques, especially data-fusion and label-efficient algorithms for this task. This paper thoroughly reviews the SoTA deep learning-based algorithms, including (1) supervised, (2) unsupervised, (3) semi-supervised, and (4) weakly-supervised methods developed for road crack detection. Also, we create a dataset called UDTIRI-Crack, comprising $2,500$ high-quality images from seven public annotated sources, as the first extensive online benchmark in this field. Comprehensive experiments are conducted to compare the detection performance, computational efficiency, and generalizability of public SoTA deep learning-based algorithms for road crack detection. In addition, the feasibility of foundation models and large language models (LLMs) for road crack detection is explored. Afterwards, the existing challenges and future development trends of deep learning-based road crack detection algorithms are discussed. We believe this review can serve as practical guidance for developing intelligent road detection vehicles with the next-generation road condition assessment systems. The released benchmark UDTIRI-Crack is available at https://udtiri.com/submission/.
<div id='section'>Paperid: <span id='pid'>345, <a href='https://arxiv.org/pdf/2503.10287.pdf' target='_blank'>https://arxiv.org/pdf/2503.10287.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Zhou, Xiaobao Guo, Yuzhe Zhu, Adams Wai-Kin Kong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10287">MACS: Multi-source Audio-to-image Generation with Contextual Significance and Semantic Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Propelled by the breakthrough in deep generative models, audio-to-image generation has emerged as a pivotal cross-model task that converts complex auditory signals into rich visual representations. However, previous works only focus on single-source audio inputs for image generation, ignoring the multi-source characteristic in natural auditory scenes, thus limiting the performance in generating comprehensive visual content. To bridge this gap, a method called MACS is proposed to conduct multi-source audio-to-image generation. This is the first work that explicitly separates multi-source audio to capture the rich audio components before image generation. MACS is a two-stage method. In the first stage, multi-source audio inputs are separated by a weakly supervised method, where the audio and text labels are semantically aligned by casting into a common space using the large pre-trained CLAP model. We introduce a ranking loss to consider the contextual significance of the separated audio signals. In the second stage, efficient image generation is achieved by mapping the separated audio signals to the generation condition using only a trainable adapter and a MLP layer. We preprocess the LLP dataset as the first full multi-source audio-to-image generation benchmark. The experiments are conducted on multi-source, mixed-source, and single-source audio-to-image generation tasks. The proposed MACS outperforms the current state-of-the-art methods in 17 of the 21 evaluation indexes on all tasks and delivers superior visual quality. The code will be publicly available.
<div id='section'>Paperid: <span id='pid'>346, <a href='https://arxiv.org/pdf/2502.06591.pdf' target='_blank'>https://arxiv.org/pdf/2502.06591.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ron Shapira Weber, Oren Freifeld
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.06591">Diffeomorphic Temporal Alignment Nets for Time-series Joint Alignment and Averaging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In time-series analysis, nonlinear temporal misalignment remains a pivotal challenge that forestalls even simple averaging. Since its introduction, the Diffeomorphic Temporal Alignment Net (DTAN), which we first introduced (Weber et al., 2019) and further developed in (Weber & Freifeld, 2023), has proven itself as an effective solution for this problem (these conference papers are earlier partial versions of the current manuscript). DTAN predicts and applies diffeomorphic transformations in an input-dependent manner, thus facilitating the joint alignment (JA) and averaging of time-series ensembles in an unsupervised or a weakly-supervised manner. The inherent challenges of the weakly/unsupervised setting, particularly the risk of trivial solutions through excessive signal distortion, are mitigated using either one of two distinct strategies: 1) a regularization term for warps; 2) using the Inverse Consistency Averaging Error (ICAE). The latter is a novel, regularization-free approach which also facilitates the JA of variable-length signals. We also further extend our framework to incorporate multi-task learning (MT-DTAN), enabling simultaneous time-series alignment and classification. Additionally, we conduct a comprehensive evaluation of different backbone architectures, demonstrating their efficacy in time-series alignment tasks. Finally, we showcase the utility of our approach in enabling Principal Component Analysis (PCA) for misaligned time-series data. Extensive experiments across 128 UCR datasets validate the superiority of our approach over contemporary averaging methods, including both traditional and learning-based approaches, marking a significant advancement in the field of time-series analysis.
<div id='section'>Paperid: <span id='pid'>347, <a href='https://arxiv.org/pdf/2501.19345.pdf' target='_blank'>https://arxiv.org/pdf/2501.19345.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Masahiro Kato, Fumiaki Kozai, Ryo Inokuchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.19345">PUATE: Efficient Average Treatment Effect Estimation from Treated (Positive) and Unlabeled Units</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The estimation of average treatment effects (ATEs), defined as the difference in expected outcomes between treatment and control groups, is a central topic in causal inference. This study develops semiparametric efficient estimators for ATE in a setting where only a treatment group and an unlabeled group, consisting of units whose treatment status is unknown, are observed. This scenario constitutes a variant of learning from positive and unlabeled data (PU learning) and can be viewed as a special case of ATE estimation with missing data. For this setting, we derive the semiparametric efficiency bounds, which characterize the lowest achievable asymptotic variance for regular estimators. We then construct semiparametric efficient ATE estimators that attain these bounds. Our results contribute to the literature on causal inference with missing data and weakly supervised learning.
<div id='section'>Paperid: <span id='pid'>348, <a href='https://arxiv.org/pdf/2501.01845.pdf' target='_blank'>https://arxiv.org/pdf/2501.01845.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunshuang Yuan, Frank Thiemann, Monika Sester
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01845">Semantic Segmentation for Sequential Historical Maps by Learning from Only One Map</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Historical maps are valuable resources that capture detailed geographical information from the past. However, these maps are typically available in printed formats, which are not conducive to modern computer-based analyses. Digitizing these maps into a machine-readable format enables efficient computational analysis. In this paper, we propose an automated approach to digitization using deep-learning-based semantic segmentation, which assigns a semantic label to each pixel in scanned historical maps. A key challenge in this process is the lack of ground-truth annotations required for training deep neural networks, as manual labeling is time-consuming and labor-intensive. To address this issue, we introduce a weakly-supervised age-tracing strategy for model fine-tuning. This approach exploits the similarity in appearance and land-use patterns between historical maps from neighboring time periods to guide the training process. Specifically, model predictions for one map are utilized as pseudo-labels for training on maps from adjacent time periods. Experiments conducted on our newly curated \textit{Hameln} dataset demonstrate that the proposed age-tracing strategy significantly enhances segmentation performance compared to baseline models. In the best-case scenario, the mean Intersection over Union (mIoU) achieved 77.3\%, reflecting an improvement of approximately 20\% over baseline methods. Additionally, the fine-tuned model achieved an average overall accuracy of 97\%, highlighting the effectiveness of our approach for digitizing historical maps.
<div id='section'>Paperid: <span id='pid'>349, <a href='https://arxiv.org/pdf/2410.05900.pdf' target='_blank'>https://arxiv.org/pdf/2410.05900.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiling Zhang, Erkut Akdag, Egor Bondarev, Peter H. N. De With
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05900">MTFL: Multi-Timescale Feature Learning for Weakly-Supervised Anomaly Detection in Surveillance Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detection of anomaly events is relevant for public safety and requires a combination of fine-grained motion information and contextual events at variable time-scales. To this end, we propose a Multi-Timescale Feature Learning (MTFL) method to enhance the representation of anomaly features. Short, medium, and long temporal tubelets are employed to extract spatio-temporal video features using a Video Swin Transformer. Experimental results demonstrate that MTFL outperforms state-of-the-art methods on the UCF-Crime dataset, achieving an anomaly detection performance 89.78% AUC. Moreover, it performs complementary to SotA with 95.32% AUC on the ShanghaiTech and 84.57% AP on the XD-Violence dataset. Furthermore, we generate an extended dataset of the UCF-Crime for development and evaluation on a wider range of anomalies, namely Video Anomaly Detection Dataset (VADD), involving 2,591 videos in 18 classes with extensive coverage of realistic anomalies.
<div id='section'>Paperid: <span id='pid'>350, <a href='https://arxiv.org/pdf/2409.20253.pdf' target='_blank'>https://arxiv.org/pdf/2409.20253.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Iira HÃ¤kkinen, Iaroslav Melekhov, Erik Englesson, Hossein Azizpour, Juho Kannala
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.20253">Medical Image Segmentation with SAM-generated Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The field of medical image segmentation is hindered by the scarcity of large, publicly available annotated datasets. Not all datasets are made public for privacy reasons, and creating annotations for a large dataset is time-consuming and expensive, as it requires specialized expertise to accurately identify regions of interest (ROIs) within the images. To address these challenges, we evaluate the performance of the Segment Anything Model (SAM) as an annotation tool for medical data by using it to produce so-called "pseudo labels" on the Medical Segmentation Decathlon (MSD) computed tomography (CT) tasks. The pseudo labels are then used in place of ground truth labels to train a UNet model in a weakly-supervised manner. We experiment with different prompt types on SAM and find that the bounding box prompt is a simple yet effective method for generating pseudo labels. This method allows us to develop a weakly-supervised model that performs comparably to a fully supervised model.
<div id='section'>Paperid: <span id='pid'>351, <a href='https://arxiv.org/pdf/2409.19587.pdf' target='_blank'>https://arxiv.org/pdf/2409.19587.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abhijeet Patil, Harsh Diwakar, Jay Sawant, Nikhil Cherian Kurian, Subhash Yadav, Swapnil Rane, Tripti Bameta, Amit Sethi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19587">Efficient Quality Control of Whole Slide Pathology Images with Human-in-the-loop Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Histopathology whole slide images (WSIs) are being widely used to develop deep learning-based diagnostic solutions, especially for precision oncology. Most of these diagnostic softwares are vulnerable to biases and impurities in the training and test data which can lead to inaccurate diagnoses. For instance, WSIs contain multiple types of tissue regions, at least some of which might not be relevant to the diagnosis. We introduce HistoROI, a robust yet lightweight deep learning-based classifier to segregate WSI into six broad tissue regions -- epithelium, stroma, lymphocytes, adipose, artifacts, and miscellaneous. HistoROI is trained using a novel human-in-the-loop and active learning paradigm that ensures variations in training data for labeling-efficient generalization. HistoROI consistently performs well across multiple organs, despite being trained on only a single dataset, demonstrating strong generalization. Further, we have examined the utility of HistoROI in improving the performance of downstream deep learning-based tasks using the CAMELYON breast cancer lymph node and TCGA lung cancer datasets. For the former dataset, the area under the receiver operating characteristic curve (AUC) for metastasis versus normal tissue of a neural network trained using weakly supervised learning increased from 0.88 to 0.92 by filtering the data using HistoROI. Similarly, the AUC increased from 0.88 to 0.93 for the classification between adenocarcinoma and squamous cell carcinoma on the lung cancer dataset. We also found that the performance of the HistoROI improves upon HistoQC for artifact detection on a test dataset of 93 annotated WSIs. The limitations of the proposed model are analyzed, and potential extensions are also discussed.
<div id='section'>Paperid: <span id='pid'>352, <a href='https://arxiv.org/pdf/2409.02145.pdf' target='_blank'>https://arxiv.org/pdf/2409.02145.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zekang Yang, Hong Liu, Xiangdong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.02145">A Multimodal Object-level Contrast Learning Method for Cancer Survival Risk Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computer-aided cancer survival risk prediction plays an important role in the timely treatment of patients. This is a challenging weakly supervised ordinal regression task associated with multiple clinical factors involved such as pathological images, genomic data and etc. In this paper, we propose a new training method, multimodal object-level contrast learning, for cancer survival risk prediction. First, we construct contrast learning pairs based on the survival risk relationship among the samples in the training sample set. Then we introduce the object-level contrast learning method to train the survival risk predictor. We further extend it to the multimodal scenario by applying cross-modal constrast. Considering the heterogeneity of pathological images and genomics data, we construct a multimodal survival risk predictor employing attention-based and self-normalizing based nerural network respectively. Finally, the survival risk predictor trained by our proposed method outperforms state-of-the-art methods on two public multimodal cancer datasets for survival risk prediction.
<div id='section'>Paperid: <span id='pid'>353, <a href='https://arxiv.org/pdf/2601.17747.pdf' target='_blank'>https://arxiv.org/pdf/2601.17747.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaixuan Jiang, Chen Wu, Zhenghui Zhao, Chengxi Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.17747">Bridging Supervision Gaps: A Unified Framework for Remote Sensing Change Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Change detection (CD) aims to identify surface changes from multi-temporal remote sensing imagery. In real-world scenarios, Pixel-level change labels are expensive to acquire, and existing models struggle to adapt to scenarios with diverse annotation availability. To tackle this challenge, we propose a unified change detection framework (UniCD), which collaboratively handles supervised, weakly-supervised, and unsupervised tasks through a coupled architecture. UniCD eliminates architectural barriers through a shared encoder and multi-branch collaborative learning mechanism, achieving deep coupling of heterogeneous supervision signals. Specifically, UniCD consists of three supervision-specific branches. In the supervision branch, UniCD introduces the spatial-temporal awareness module (STAM), achieving efficient synergistic fusion of bi-temporal features. In the weakly-supervised branch, we construct change representation regularization (CRR), which steers model convergence from coarse-grained activations toward coherent and separable change modeling. In the unsupervised branch, we propose semantic prior-driven change inference (SPCI), which transforms unsupervised tasks into controlled weakly-supervised path optimization. Experiments on mainstream datasets demonstrate that UniCD achieves optimal performance across three tasks. It exhibits significant accuracy improvements in weakly and unsupervised scenarios, surpassing current state-of-the-art by 12.72% and 12.37% on LEVIR-CD, respectively.
<div id='section'>Paperid: <span id='pid'>354, <a href='https://arxiv.org/pdf/2601.12919.pdf' target='_blank'>https://arxiv.org/pdf/2601.12919.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun Wan, Yuanzhi Yao, Zhihui Lai, Jie Zhou, Xianxu Hou, Wenwen Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.12919">Supervision-by-Hallucination-and-Transfer: A Weakly-Supervised Approach for Robust and Precise Facial Landmark Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-precision facial landmark detection (FLD) relies on high-resolution deep feature representations. However, low-resolution face images or the compression (via pooling or strided convolution) of originally high-resolution images hinder the learning of such features, thereby reducing FLD accuracy. Moreover, insufficient training data and imprecise annotations further degrade performance. To address these challenges, we propose a weakly-supervised framework called Supervision-by-Hallucination-and-Transfer (SHT) for more robust and precise FLD. SHT contains two novel mutually enhanced modules: Dual Hallucination Learning Network (DHLN) and Facial Pose Transfer Network (FPTN). By incorporating FLD and face hallucination tasks, DHLN is able to learn high-resolution representations with low-resolution inputs for recovering both facial structures and local details and generating more effective landmark heatmaps. Then, by transforming faces from one pose to another, FPTN can further improve landmark heatmaps and faces hallucinated by DHLN for detecting more accurate landmarks. To the best of our knowledge, this is the first study to explore weakly-supervised FLD by integrating face hallucination and facial pose transfer tasks. Experimental results of both face hallucination and FLD demonstrate that our method surpasses state-of-the-art techniques.
<div id='section'>Paperid: <span id='pid'>355, <a href='https://arxiv.org/pdf/2510.15666.pdf' target='_blank'>https://arxiv.org/pdf/2510.15666.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Shi, Gang Li, Junxing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.15666">Uncertainty-Aware Extreme Point Tracing for Weakly Supervised Ultrasound Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic medical image segmentation is a fundamental step in computer-aided diagnosis, yet fully supervised approaches demand extensive pixel-level annotations that are costly and time-consuming. To alleviate this burden, we propose a weakly supervised segmentation framework that leverages only four extreme points as annotation. Specifically, bounding boxes derived from the extreme points are used as prompts for the Segment Anything Model 2 (SAM2) to generate reliable initial pseudo labels. These pseudo labels are progressively refined by an enhanced Feature-Guided Extreme Point Masking (FGEPM) algorithm, which incorporates Monte Carlo dropout-based uncertainty estimation to construct a unified gradient uncertainty cost map for boundary tracing. Furthermore, a dual-branch Uncertainty-aware Scale Consistency (USC) loss and a box alignment loss are introduced to ensure spatial consistency and precise boundary alignment during training. Extensive experiments on two public ultrasound datasets, BUSI and UNS, demonstrate that our method achieves performance comparable to, and even surpassing fully supervised counterparts while significantly reducing annotation cost. These results validate the effectiveness and practicality of the proposed weakly supervised framework for ultrasound image segmentation.
<div id='section'>Paperid: <span id='pid'>356, <a href='https://arxiv.org/pdf/2509.23376.pdf' target='_blank'>https://arxiv.org/pdf/2509.23376.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinghong Zheng, Changlong Jiang, Jiaqi Li, Haohong Kuang, Hang Xu, Tingbing Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23376">UniPose: Unified Cross-modality Pose Prior Propagation towards RGB-D data for Weakly Supervised 3D Human Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present UniPose, a unified cross-modality pose prior propagation method for weakly supervised 3D human pose estimation (HPE) using unannotated single-view RGB-D sequences (RGB, depth, and point cloud data). UniPose transfers 2D HPE annotations from large-scale RGB datasets (e.g., MS COCO) to the 3D domain via self-supervised learning on easily acquired RGB-D sequences, eliminating the need for labor-intensive 3D keypoint annotations. This approach bridges the gap between 2D and 3D domains without suffering from issues related to multi-view camera calibration or synthetic-to-real data shifts. During training, UniPose leverages off-the-shelf 2D pose estimations as weak supervision for point cloud networks, incorporating spatial-temporal constraints like body symmetry and joint motion. The 2D-to-3D back-projection loss and cross-modality interaction further enhance this process. By treating the point cloud network's 3D HPE results as pseudo ground truth, our anchor-to-joint prediction method performs 3D lifting on RGB and depth networks, making it more robust against inaccuracies in 2D HPE results compared to state-of-the-art methods. Experiments on CMU Panoptic and ITOP datasets show that UniPose achieves comparable performance to fully supervised methods. Incorporating large-scale unlabeled data (e.g., NTU RGB+D 60) enhances its performance under challenging conditions, demonstrating its potential for practical applications. Our proposed 3D lifting method also achieves state-of-the-art results.
<div id='section'>Paperid: <span id='pid'>357, <a href='https://arxiv.org/pdf/2509.19028.pdf' target='_blank'>https://arxiv.org/pdf/2509.19028.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ioannis Sarafis, Alexandros Papadopoulos, Anastasios Delopoulos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19028">Weakly Supervised Food Image Segmentation using Vision Transformers and Segment Anything Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a weakly supervised semantic segmentation approach for food images which takes advantage of the zero-shot capabilities and promptability of the Segment Anything Model (SAM) along with the attention mechanisms of Vision Transformers (ViTs). Specifically, we use class activation maps (CAMs) from ViTs to generate prompts for SAM, resulting in masks suitable for food image segmentation. The ViT model, a Swin Transformer, is trained exclusively using image-level annotations, eliminating the need for pixel-level annotations during training. Additionally, to enhance the quality of the SAM-generated masks, we examine the use of image preprocessing techniques in combination with single-mask and multi-mask SAM generation strategies. The methodology is evaluated on the FoodSeg103 dataset, generating an average of 2.4 masks per image (excluding background), and achieving an mIoU of 0.54 for the multi-mask scenario. We envision the proposed approach as a tool to accelerate food image annotation tasks or as an integrated component in food and nutrition tracking applications.
<div id='section'>Paperid: <span id='pid'>358, <a href='https://arxiv.org/pdf/2509.17971.pdf' target='_blank'>https://arxiv.org/pdf/2509.17971.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tan-Ha Mai, Hsuan-Tien Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17971">Intra-Cluster Mixup: An Effective Data Augmentation Technique for Complementary-Label Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we investigate the challenges of complementary-label learning (CLL), a specialized form of weakly-supervised learning (WSL) where models are trained with labels indicating classes to which instances do not belong, rather than standard ordinary labels. This alternative supervision is appealing because collecting complementary labels is generally cheaper and less labor-intensive. Although most existing research in CLL emphasizes the development of novel loss functions, the potential of data augmentation in this domain remains largely underexplored. In this work, we uncover that the widely-used Mixup data augmentation technique is ineffective when directly applied to CLL. Through in-depth analysis, we identify that the complementary-label noise generated by Mixup negatively impacts the performance of CLL models. We then propose an improved technique called Intra-Cluster Mixup (ICM), which only synthesizes augmented data from nearby examples, to mitigate the noise effect. ICM carries the benefits of encouraging complementary label sharing of nearby examples, and leads to substantial performance improvements across synthetic and real-world labeled datasets. In particular, our wide spectrum of experimental results on both balanced and imbalanced CLL settings justifies the potential of ICM in allying with state-of-the-art CLL algorithms, achieving significant accuracy increases of 30% and 10% on MNIST and CIFAR datasets, respectively.
<div id='section'>Paperid: <span id='pid'>359, <a href='https://arxiv.org/pdf/2508.06115.pdf' target='_blank'>https://arxiv.org/pdf/2508.06115.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weichen Zhang, Kebin Liu, Fan Dang, Zhui Zhu, Xikai Sun, Yunhao Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06115">SynSeg: Feature Synergy for Multi-Category Contrastive Learning in Open-Vocabulary Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semantic segmentation in open-vocabulary scenarios presents significant challenges due to the wide range and granularity of semantic categories. Existing weakly-supervised methods often rely on category-specific supervision and ill-suited feature construction methods for contrastive learning, leading to semantic misalignment and poor performance. In this work, we propose a novel weakly-supervised approach, SynSeg, to address the challenges. SynSeg performs Multi-Category Contrastive Learning (MCCL) as a stronger training signal with a new feature reconstruction framework named Feature Synergy Structure (FSS). Specifically, MCCL strategy robustly combines both intra- and inter-category alignment and separation in order to make the model learn the knowledge of correlations from different categories within the same image. Moreover, FSS reconstructs discriminative features for contrastive learning through prior fusion and semantic-activation-map enhancement, effectively avoiding the foreground bias introduced by the visual encoder. In general, SynSeg effectively improves the abilities in semantic localization and discrimination under weak supervision. Extensive experiments on benchmarks demonstrate that our method outperforms state-of-the-art (SOTA) performance. For instance, SynSeg achieves higher accuracy than SOTA baselines by 4.5\% on VOC, 8.9\% on Context, 2.6\% on Object and 2.0\% on City.
<div id='section'>Paperid: <span id='pid'>360, <a href='https://arxiv.org/pdf/2508.00563.pdf' target='_blank'>https://arxiv.org/pdf/2508.00563.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hannah Kniesel, Leon Sick, Tristan Payer, Tim Bergner, Kavitha Shaga Devan, Clarissa Read, Paul Walther, Timo Ropinski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00563">Weakly Supervised Virus Capsid Detection with Image-Level Annotations in Electron Microscopy Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current state-of-the-art methods for object detection rely on annotated bounding boxes of large data sets for training. However, obtaining such annotations is expensive and can require up to hundreds of hours of manual labor. This poses a challenge, especially since such annotations can only be provided by experts, as they require knowledge about the scientific domain. To tackle this challenge, we propose a domain-specific weakly supervised object detection algorithm that only relies on image-level annotations, which are significantly easier to acquire. Our method distills the knowledge of a pre-trained model, on the task of predicting the presence or absence of a virus in an image, to obtain a set of pseudo-labels that can be used to later train a state-of-the-art object detection model. To do so, we use an optimization approach with a shrinking receptive field to extract virus particles directly without specific network architectures. Through a set of extensive studies, we show how the proposed pseudo-labels are easier to obtain, and, more importantly, are able to outperform other existing weak labeling methods, and even ground truth labels, in cases where the time to obtain the annotation is limited.
<div id='section'>Paperid: <span id='pid'>361, <a href='https://arxiv.org/pdf/2507.14237.pdf' target='_blank'>https://arxiv.org/pdf/2507.14237.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Louis Bahrman, Mathieu Fontaine, GaÃ«l Richard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14237">U-DREAM: Unsupervised Dereverberation guided by a Reverberation Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper explores the outcome of training state-ofthe-art dereverberation models with supervision settings ranging from weakly-supervised to fully unsupervised, relying solely on reverberant signals and an acoustic model for training. Most of the existing deep learning approaches typically require paired dry and reverberant data, which are difficult to obtain in practice. We develop instead a sequential learning strategy motivated by a bayesian formulation of the dereverberation problem, wherein acoustic parameters and dry signals are estimated from reverberant inputs using deep neural networks, guided by a reverberation matching loss. Our most data-efficient variant requires only 100 reverberation-parameter-labelled samples to outperform an unsupervised baseline, demonstrating the effectiveness and practicality of the proposed method in low-resource scenarios.
<div id='section'>Paperid: <span id='pid'>362, <a href='https://arxiv.org/pdf/2507.02454.pdf' target='_blank'>https://arxiv.org/pdf/2507.02454.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiwei Duan, Luping Ji, Shengjia Chen, Sicheng Zhu, Jianghong Huang, Mao Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02454">Weakly-supervised Contrastive Learning with Quantity Prompts for Moving Infrared Small Target Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Different from general object detection, moving infrared small target detection faces huge challenges due to tiny target size and weak background contrast.Currently, most existing methods are fully-supervised, heavily relying on a large number of manual target-wise annotations. However, manually annotating video sequences is often expensive and time-consuming, especially for low-quality infrared frame images. Inspired by general object detection, non-fully supervised strategies ($e.g.$, weakly supervised) are believed to be potential in reducing annotation requirements. To break through traditional fully-supervised frameworks, as the first exploration work, this paper proposes a new weakly-supervised contrastive learning (WeCoL) scheme, only requires simple target quantity prompts during model training.Specifically, in our scheme, based on the pretrained segment anything model (SAM), a potential target mining strategy is designed to integrate target activation maps and multi-frame energy accumulation.Besides, contrastive learning is adopted to further improve the reliability of pseudo-labels, by calculating the similarity between positive and negative samples in feature subspace.Moreover, we propose a long-short term motion-aware learning scheme to simultaneously model the local motion patterns and global motion trajectory of small targets.The extensive experiments on two public datasets (DAUB and ITSDT-15K) verify that our weakly-supervised scheme could often outperform early fully-supervised methods. Even, its performance could reach over 90\% of state-of-the-art (SOTA) fully-supervised ones.
<div id='section'>Paperid: <span id='pid'>363, <a href='https://arxiv.org/pdf/2507.02393.pdf' target='_blank'>https://arxiv.org/pdf/2507.02393.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seokyeong Lee, Sithu Aung, Junyong Choi, Seungryong Kim, Ig-Jae Kim, Junghyun Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02393">PLOT: Pseudo-Labeling via Video Object Tracking for Scalable Monocular 3D Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monocular 3D object detection (M3OD) has long faced challenges due to data scarcity caused by high annotation costs and inherent 2D-to-3D ambiguity. Although various weakly supervised methods and pseudo-labeling methods have been proposed to address these issues, they are mostly limited by domain-specific learning or rely solely on shape information from a single observation. In this paper, we propose a novel pseudo-labeling framework that uses only video data and is more robust to occlusion, without requiring a multi-view setup, additional sensors, camera poses, or domain-specific training. Specifically, we explore a technique for aggregating the pseudo-LiDARs of both static and dynamic objects across temporally adjacent frames using object point tracking, enabling 3D attribute extraction in scenarios where 3D data acquisition is infeasible. Extensive experiments demonstrate that our method ensures reliable accuracy and strong scalability, making it a practical and effective solution for M3OD.
<div id='section'>Paperid: <span id='pid'>364, <a href='https://arxiv.org/pdf/2506.21883.pdf' target='_blank'>https://arxiv.org/pdf/2506.21883.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Basudha Pal, Sharif Amit Kamran, Brendon Lutnick, Molly Lucas, Chaitanya Parmar, Asha Patel Shah, David Apfel, Steven Fakharzadeh, Lloyd Miller, Gabriela Cula, Kristopher Standish
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21883">GRASP-PsONet: Gradient-based Removal of Spurious Patterns for PsOriasis Severity Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Psoriasis (PsO) severity scoring is important for clinical trials but is hindered by inter-rater variability and the burden of in person clinical evaluation. Remote imaging using patient captured mobile photos offers scalability but introduces challenges, such as variation in lighting, background, and device quality that are often imperceptible to humans but can impact model performance. These factors, along with inconsistencies in dermatologist annotations, reduce the reliability of automated severity scoring. We propose a framework to automatically flag problematic training images that introduce spurious correlations which degrade model generalization, using a gradient based interpretability approach. By tracing the gradients of misclassified validation images, we detect training samples where model errors align with inconsistently rated examples or are affected by subtle, nonclinical artifacts. We apply this method to a ConvNeXT based weakly supervised model designed to classify PsO severity from phone images. Removing 8.2% of flagged images improves model AUC-ROC by 5% (85% to 90%) on a held out test set. Commonly, multiple annotators and an adjudication process ensure annotation accuracy, which is expensive and time consuming. Our method detects training images with annotation inconsistencies, potentially removing the need for manual review. When applied to a subset of training data rated by two dermatologists, the method identifies over 90% of cases with inter-rater disagreement by reviewing only the top 30% of samples. This improves automated scoring for remote assessments, ensuring robustness despite data collection variability.
<div id='section'>Paperid: <span id='pid'>365, <a href='https://arxiv.org/pdf/2506.07652.pdf' target='_blank'>https://arxiv.org/pdf/2506.07652.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hangbei Cheng, Xiaorong Dong, Xueyu Liu, Jianan Zhang, Xuetao Ma, Mingqiang Wei, Liansheng Wang, Junxin Chen, Yongfei Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07652">FMaMIL: Frequency-Driven Mamba Multi-Instance Learning for Weakly Supervised Lesion Segmentation in Medical Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate lesion segmentation in histopathology images is essential for diagnostic interpretation and quantitative analysis, yet it remains challenging due to the limited availability of costly pixel-level annotations. To address this, we propose FMaMIL, a novel two-stage framework for weakly supervised lesion segmentation based solely on image-level labels. In the first stage, a lightweight Mamba-based encoder is introduced to capture long-range dependencies across image patches under the MIL paradigm. To enhance spatial sensitivity and structural awareness, we design a learnable frequency-domain encoding module that supplements spatial-domain features with spectrum-based information. CAMs generated in this stage are used to guide segmentation training. In the second stage, we refine the initial pseudo labels via a CAM-guided soft-label supervision and a self-correction mechanism, enabling robust training even under label noise. Extensive experiments on both public and private histopathology datasets demonstrate that FMaMIL outperforms state-of-the-art weakly supervised methods without relying on pixel-level annotations, validating its effectiveness and potential for digital pathology applications.
<div id='section'>Paperid: <span id='pid'>366, <a href='https://arxiv.org/pdf/2505.23341.pdf' target='_blank'>https://arxiv.org/pdf/2505.23341.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daoxi Cao, Hangbei Cheng, Yijin Li, Ruolin Zhou, Xuehan Zhang, Xinyi Li, Binwei Li, Xuancheng Gu, Jianan Zhang, Xueyu Liu, Yongfei Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23341">DSAGL: Dual-Stream Attention-Guided Learning for Weakly Supervised Whole Slide Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Whole-slide images (WSIs) are critical for cancer diagnosis due to their ultra-high resolution and rich semantic content. However, their massive size and the limited availability of fine-grained annotations pose substantial challenges for conventional supervised learning. We propose DSAGL (Dual-Stream Attention-Guided Learning), a novel weakly supervised classification framework that combines a teacher-student architecture with a dual-stream design. DSAGL explicitly addresses instance-level ambiguity and bag-level semantic consistency by generating multi-scale attention-based pseudo labels and guiding instance-level learning. A shared lightweight encoder (VSSMamba) enables efficient long-range dependency modeling, while a fusion-attentive module (FASA) enhances focus on sparse but diagnostically relevant regions. We further introduce a hybrid loss to enforce mutual consistency between the two streams. Experiments on CIFAR-10, NCT-CRC, and TCGA-Lung datasets demonstrate that DSAGL consistently outperforms state-of-the-art MIL baselines, achieving superior discriminative performance and robustness under weak supervision.
<div id='section'>Paperid: <span id='pid'>367, <a href='https://arxiv.org/pdf/2505.06710.pdf' target='_blank'>https://arxiv.org/pdf/2505.06710.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yicheng Song, Tiancheng Lin, Die Peng, Su Yang, Yi Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06710">SimMIL: A Universal Weakly Supervised Pre-Training Framework for Multi-Instance Learning in Whole Slide Pathology Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Various multi-instance learning (MIL) based approaches have been developed and successfully applied to whole-slide pathological images (WSI). Existing MIL methods emphasize the importance of feature aggregators, but largely neglect the instance-level representation learning. They assume that the availability of a pre-trained feature extractor can be directly utilized or fine-tuned, which is not always the case. This paper proposes to pre-train feature extractor for MIL via a weakly-supervised scheme, i.e., propagating the weak bag-level labels to the corresponding instances for supervised learning. To learn effective features for MIL, we further delve into several key components, including strong data augmentation, a non-linear prediction head and the robust loss function. We conduct experiments on common large-scale WSI datasets and find it achieves better performance than other pre-training schemes (e.g., ImageNet pre-training and self-supervised learning) in different downstream tasks. We further show the compatibility and scalability of the proposed scheme by deploying it in fine-tuning the pathological-specific models and pre-training on merged multiple datasets. To our knowledge, this is the first work focusing on the representation learning for MIL.
<div id='section'>Paperid: <span id='pid'>368, <a href='https://arxiv.org/pdf/2504.05403.pdf' target='_blank'>https://arxiv.org/pdf/2504.05403.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Manahil Raza, Muhammad Dawood, Talha Qaiser, Nasir M. Rajpoot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.05403">A Novel Approach to Linking Histology Images with DNA Methylation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>DNA methylation is an epigenetic mechanism that regulates gene expression by adding methyl groups to DNA. Abnormal methylation patterns can disrupt gene expression and have been linked to cancer development. To quantify DNA methylation, specialized assays are typically used. However, these assays are often costly and have lengthy processing times, which limits their widespread availability in routine clinical practice. In contrast, whole slide images (WSIs) for the majority of cancer patients can be more readily available. As such, given the ready availability of WSIs, there is a compelling need to explore the potential relationship between WSIs and DNA methylation patterns. To address this, we propose an end-to-end graph neural network based weakly supervised learning framework to predict the methylation state of gene groups exhibiting coherent patterns across samples. Using data from three cohorts from The Cancer Genome Atlas (TCGA) - TCGA-LGG (Brain Lower Grade Glioma), TCGA-GBM (Glioblastoma Multiforme) ($n$=729) and TCGA-KIRC (Kidney Renal Clear Cell Carcinoma) ($n$=511) - we demonstrate that the proposed approach achieves significantly higher AUROC scores than the state-of-the-art (SOTA) methods, by more than $20\%$. We conduct gene set enrichment analyses on the gene groups and show that majority of the gene groups are significantly enriched in important hallmarks and pathways. We also generate spatially enriched heatmaps to further investigate links between histological patterns and DNA methylation states. To the best of our knowledge, this is the first study that explores association of spatially resolved histological patterns with gene group methylation states across multiple cancer types using weakly supervised deep learning.
<div id='section'>Paperid: <span id='pid'>369, <a href='https://arxiv.org/pdf/2503.15260.pdf' target='_blank'>https://arxiv.org/pdf/2503.15260.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Shi, Xi Fang, Naiyu Wang, Junxing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15260">DEPT: Deep Extreme Point Tracing for Ultrasound Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic medical image segmentation plays a crucial role in computer aided diagnosis. However, fully supervised learning approaches often require extensive and labor-intensive annotation efforts. To address this challenge, weakly supervised learning methods, particularly those using extreme points as supervisory signals, have the potential to offer an effective solution. In this paper, we introduce Deep Extreme Point Tracing (DEPT) integrated with Feature-Guided Extreme Point Masking (FGEPM) algorithm for ultrasound image segmentation. Notably, our method generates pseudo labels by identifying the lowest-cost path that connects all extreme points on the feature map-based cost matrix. Additionally, an iterative training strategy is proposed to refine pseudo labels progressively, enabling continuous network improvement. Experimental results on two public datasets demonstrate the effectiveness of our proposed method. The performance of our method approaches that of the fully supervised method and outperforms several existing weakly supervised methods.
<div id='section'>Paperid: <span id='pid'>370, <a href='https://arxiv.org/pdf/2502.20249.pdf' target='_blank'>https://arxiv.org/pdf/2502.20249.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pierre Vuillecard, Jean-Marc Odobez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20249">Enhancing 3D Gaze Estimation in the Wild using Weak Supervision with Gaze Following Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate 3D gaze estimation in unconstrained real-world environments remains a significant challenge due to variations in appearance, head pose, occlusion, and the limited availability of in-the-wild 3D gaze datasets. To address these challenges, we introduce a novel Self-Training Weakly-Supervised Gaze Estimation framework (ST-WSGE). This two-stage learning framework leverages diverse 2D gaze datasets, such as gaze-following data, which offer rich variations in appearances, natural scenes, and gaze distributions, and proposes an approach to generate 3D pseudo-labels and enhance model generalization. Furthermore, traditional modality-specific models, designed separately for images or videos, limit the effective use of available training data. To overcome this, we propose the Gaze Transformer (GaT), a modality-agnostic architecture capable of simultaneously learning static and dynamic gaze information from both image and video datasets. By combining 3D video datasets with 2D gaze target labels from gaze following tasks, our approach achieves the following key contributions: (i) Significant state-of-the-art improvements in within-domain and cross-domain generalization on unconstrained benchmarks like Gaze360 and GFIE, with notable cross-modal gains in video gaze estimation; (ii) Superior cross-domain performance on datasets such as MPIIFaceGaze and Gaze360 compared to frontal face methods. Code and pre-trained models will be released to the community.
<div id='section'>Paperid: <span id='pid'>371, <a href='https://arxiv.org/pdf/2502.11743.pdf' target='_blank'>https://arxiv.org/pdf/2502.11743.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tobias Fuchs, Florian Kalinke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.11743">Robust Partial-Label Learning by Leveraging Class Activation Values</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-world training data is often noisy; for example, human annotators assign conflicting class labels to the same instances. Partial-label learning (PLL) is a weakly supervised learning paradigm that allows training classifiers in this context without manual data cleaning. While state-of-the-art methods have good predictive performance, their predictions are sensitive to high noise levels, out-of-distribution data, and adversarial perturbations. We propose a novel PLL method based on subjective logic, which explicitly represents uncertainty by leveraging the magnitudes of the underlying neural network's class activation values. Thereby, we effectively incorporate prior knowledge about the class labels by using a novel label weight re-distribution strategy that we prove to be optimal. We empirically show that our method yields more robust predictions in terms of predictive performance under high PLL noise levels, handling out-of-distribution examples, and handling adversarial perturbations on the test instances.
<div id='section'>Paperid: <span id='pid'>372, <a href='https://arxiv.org/pdf/2502.06839.pdf' target='_blank'>https://arxiv.org/pdf/2502.06839.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Louis Bahrman, Mathieu Fontaine, Gael Richard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.06839">A Hybrid Model for Weakly-Supervised Speech Dereverberation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a new training strategy to improve speech dereverberation systems using minimal acoustic information and reverberant (wet) speech. Most existing algorithms rely on paired dry/wet data, which is difficult to obtain, or on target metrics that may not adequately capture reverberation characteristics and can lead to poor results on non-target metrics. Our approach uses limited acoustic information, like the reverberation time (RT60), to train a dereverberation system. The system's output is resynthesized using a generated room impulse response and compared with the original reverberant speech, providing a novel reverberation matching loss replacing the standard target metrics. During inference, only the trained dereverberation model is used. Experimental results demonstrate that our method achieves more consistent performance across various objective metrics used in speech dereverberation than the state-of-the-art.
<div id='section'>Paperid: <span id='pid'>373, <a href='https://arxiv.org/pdf/2412.20455.pdf' target='_blank'>https://arxiv.org/pdf/2412.20455.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ayush Ghadiya, Purbayan Kar, Vishal Chudasama, Pankaj Wasnik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20455">Cross-Modal Fusion and Attention Mechanism for Weakly Supervised Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, weakly supervised video anomaly detection (WS-VAD) has emerged as a contemporary research direction to identify anomaly events like violence and nudity in videos using only video-level labels. However, this task has substantial challenges, including addressing imbalanced modality information and consistently distinguishing between normal and abnormal features. In this paper, we address these challenges and propose a multi-modal WS-VAD framework to accurately detect anomalies such as violence and nudity. Within the proposed framework, we introduce a new fusion mechanism known as the Cross-modal Fusion Adapter (CFA), which dynamically selects and enhances highly relevant audio-visual features in relation to the visual modality. Additionally, we introduce a Hyperbolic Lorentzian Graph Attention (HLGAtt) to effectively capture the hierarchical relationships between normal and abnormal representations, thereby enhancing feature separation accuracy. Through extensive experiments, we demonstrate that the proposed model achieves state-of-the-art results on benchmark datasets of violence and nudity detection.
<div id='section'>Paperid: <span id='pid'>374, <a href='https://arxiv.org/pdf/2412.20201.pdf' target='_blank'>https://arxiv.org/pdf/2412.20201.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wen-Dong Jiang, Chih-Yung Chang, Hsiang-Chuan Chang, Ji-Yuan Chen, Diptendu Sinha Roy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20201">Injecting Explainability and Lightweight Design into Weakly Supervised Video Anomaly Detection Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Monitoring Anomaly Detection (WSMAD) utilizes weak supervision learning to identify anomalies, a critical task for smart city monitoring. However, existing multimodal approaches often fail to meet the real-time and interpretability requirements of edge devices due to their complexity. This paper presents TCVADS (Two-stage Cross-modal Video Anomaly Detection System), which leverages knowledge distillation and cross-modal contrastive learning to enable efficient, accurate, and interpretable anomaly detection on edge devices.TCVADS operates in two stages: coarse-grained rapid classification and fine-grained detailed analysis. In the first stage, TCVADS extracts features from video frames and inputs them into a time series analysis module, which acts as the teacher model. Insights are then transferred via knowledge distillation to a simplified convolutional network (student model) for binary classification. Upon detecting an anomaly, the second stage is triggered, employing a fine-grained multi-class classification model. This stage uses CLIP for cross-modal contrastive learning with text and images, enhancing interpretability and achieving refined classification through specially designed triplet textual relationships. Experimental results demonstrate that TCVADS significantly outperforms existing methods in model performance, detection efficiency, and interpretability, offering valuable contributions to smart city monitoring applications.
<div id='section'>Paperid: <span id='pid'>375, <a href='https://arxiv.org/pdf/2412.19504.pdf' target='_blank'>https://arxiv.org/pdf/2412.19504.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Li, Bo Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19504">Hear the Scene: Audio-Enhanced Text Spotting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in scene text spotting have focused on end-to-end methodologies that heavily rely on precise location annotations, which are often costly and labor-intensive to procure. In this study, we introduce an innovative approach that leverages only transcription annotations for training text spotting models, substantially reducing the dependency on elaborate annotation processes. Our methodology employs a query-based paradigm that facilitates the learning of implicit location features through the interaction between text queries and image embeddings. These features are later refined during the text recognition phase using an attention activation map. Addressing the challenges associated with training a weakly-supervised model from scratch, we implement a circular curriculum learning strategy to enhance model convergence. Additionally, we introduce a coarse-to-fine cross-attention localization mechanism for more accurate text instance localization. Notably, our framework supports audio-based annotation, which significantly diminishes annotation time and provides an inclusive alternative for individuals with disabilities. Our approach achieves competitive performance against existing benchmarks, demonstrating that high accuracy in text spotting can be attained without extensive location annotations.
<div id='section'>Paperid: <span id='pid'>376, <a href='https://arxiv.org/pdf/2411.12276.pdf' target='_blank'>https://arxiv.org/pdf/2411.12276.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nai-Xuan Ye, Tan-Ha Mai, Hsiu-Hsuan Wang, Wei-I Lin, Hsuan-Tien Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.12276">libcll: an Extendable Python Toolkit for Complementary-Label Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Complementary-label learning (CLL) is a weakly supervised learning paradigm for multiclass classification, where only complementary labels -- indicating classes an instance does not belong to -- are provided to the learning algorithm. Despite CLL's increasing popularity, previous studies highlight two main challenges: (1) inconsistent results arising from varied assumptions on complementary label generation, and (2) high barriers to entry due to the lack of a standardized evaluation platform across datasets and algorithms. To address these challenges, we introduce \texttt{libcll}, an extensible Python toolkit for CLL research. \texttt{libcll} provides a universal interface that supports a wide range of generation assumptions, both synthetic and real-world datasets, and key CLL algorithms. The toolkit is designed to mitigate inconsistencies and streamline the research process, with easy installation, comprehensive usage guides, and quickstart tutorials that facilitate efficient adoption and implementation of CLL techniques. Extensive ablation studies conducted with \texttt{libcll} demonstrate its utility in generating valuable insights to advance future CLL research.
<div id='section'>Paperid: <span id='pid'>377, <a href='https://arxiv.org/pdf/2410.21991.pdf' target='_blank'>https://arxiv.org/pdf/2410.21991.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wen-Dong Jiang, Chih-Yung Chang, Ssu-Chi Kuai, Diptendu Sinha Roy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.21991">A Lightweight Dual-Branch System for Weakly-Supervised Video Anomaly Detection on Consumer Edge Devices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing demand for intelligent security in consumer electronics, such as smart home cameras and personal monitoring systems, is often hindered by the high computational cost and large model sizes of advanced AI. These limitations prevent the effective deployment of real-time Video Anomaly Detection (VAD) on resource-constrained edge devices. To bridge this gap, this paper introduces Rule-based Video Anomaly Detection (RuleVAD), a novel, lightweight system engineered for high-efficiency and low-complexity threat detection directly on consumer hardware. RuleVAD features an innovative decoupled dual-branch architecture to minimize computational load. An implicit branch uses visual features for rapid, coarse-grained binary classification, efficiently filtering out normal activity to avoid unnecessary processing. For potentially anomalous or complex events, a multimodal explicit branch takes over. This branch leverages YOLO-World to detect objects and applies data mining to generate interpretable, text-based association rules from the scene. By aligning these rules with visual data, RuleVAD achieves a more nuanced, fine-grained classification, significantly reducing the false alarms common in vision-only systems. Extensive experiments on the XD-Violence and UCF-Crime benchmark datasets show that RuleVAD achieves superior performance, surpassing existing state-of-the-art methods in both accuracy and speed. Crucially, the entire system is optimized for low-power operation and is fully deployable on an NVIDIA Jetson Nano board, demonstrating its practical feasibility for bringing advanced, real-time security monitoring to everyday consumer electronic devices.
<div id='section'>Paperid: <span id='pid'>378, <a href='https://arxiv.org/pdf/2410.15051.pdf' target='_blank'>https://arxiv.org/pdf/2410.15051.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vittorio Torri, Elisa Barbieri, Anna Cantarutti, Carlo Giaquinto, Francesca Ieva
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15051">Weakly-supervised diagnosis identification from Italian discharge letters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Objective: Recognizing diseases from discharge letters is crucial for cohort selection and epidemiological analyses, as this is the only type of data consistently produced across hospitals. This is a classic document classification problem, typically requiring supervised learning. However, manual annotation of large datasets of discharge letters is uncommon since it is extremely time-consuming. We propose a novel weakly-supervised pipeline to recognize diseases from Italian discharge letters. Methods: Our Natural Language Processing pipeline is based on a fine-tuned version of the Italian Umberto model. The pipeline extracts diagnosis-related sentences from a subset of letters and applies a two-level clustering using the embeddings generated by the fine-tuned Umberto model. These clusters are summarized and those mapped to the diseases of interest are selected as weak labels. Finally, the same BERT-based model is trained using these weak labels to detect the targeted diseases. Results: A case study related to the identification of bronchiolitis with 33'176 Italian discharge letters from 44 hospitals in the Veneto Region shows the potential of our method, with an AUC of 77.7 % and an F1-Score of 75.1 % on manually annotated labels, improving compared to other non-supervised methods and with a limited loss compared to fully supervised methods. Results are robust to the cluster selection and the identified clusters highlight the potential to recognize a variety of diseases. Conclusions: This study demonstrates the feasibility of diagnosis identification from Italian discharge letters in the absence of labelled data. Our pipeline showed strong performance and robustness, and its flexibility allows for easy adaptation to various diseases. This approach offers a scalable solution for clinical text classification, reducing the need for manual annotation while maintaining good accuracy.
<div id='section'>Paperid: <span id='pid'>379, <a href='https://arxiv.org/pdf/2410.00536.pdf' target='_blank'>https://arxiv.org/pdf/2410.00536.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Krishna Chaitanya, Pablo F. Damasceno, Shreyas Fadnavis, Pooya Mobadersany, Chaitanya Parmar, Emily Scherer, Natalia Zemlianskaia, Lindsey Surace, Louis R. Ghanem, Oana Gabriela Cula, Tommaso Mansi, Kristopher Standish
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.00536">Arges: Spatio-Temporal Transformer for Ulcerative Colitis Severity Assessment in Endoscopy Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate assessment of disease severity from endoscopy videos in ulcerative colitis (UC) is crucial for evaluating drug efficacy in clinical trials. Severity is often measured by the Mayo Endoscopic Subscore (MES) and Ulcerative Colitis Endoscopic Index of Severity (UCEIS) score. However, expert MES/UCEIS annotation is time-consuming and susceptible to inter-rater variability, factors addressable by automation. Automation attempts with frame-level labels face challenges in fully-supervised solutions due to the prevalence of video-level labels in clinical trials. CNN-based weakly-supervised models (WSL) with end-to-end (e2e) training lack generalization to new disease scores and ignore spatio-temporal information crucial for accurate scoring. To address these limitations, we propose "Arges", a deep learning framework that utilizes a transformer with positional encoding to incorporate spatio-temporal information from frame features to estimate disease severity scores in endoscopy video. Extracted features are derived from a foundation model (ArgesFM), pre-trained on a large diverse dataset from multiple clinical trials (61M frames, 3927 videos). We evaluate four UC disease severity scores, including MES and three UCEIS component scores. Test set evaluation indicates significant improvements, with F1 scores increasing by 4.1% for MES and 18.8%, 6.6%, 3.8% for the three UCEIS component scores compared to state-of-the-art methods. Prospective validation on previously unseen clinical trial data further demonstrates the model's successful generalization.
<div id='section'>Paperid: <span id='pid'>380, <a href='https://arxiv.org/pdf/2601.02927.pdf' target='_blank'>https://arxiv.org/pdf/2601.02927.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Iñaki Erregue, Kamal Nasrollahi, Sergio Escalera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.02927">PrismVAU: Prompt-Refined Inference System for Multimodal Video Anomaly Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Anomaly Understanding (VAU) extends traditional Video Anomaly Detection (VAD) by not only localizing anomalies but also describing and reasoning about their context. Existing VAU approaches often rely on fine-tuned multimodal large language models (MLLMs) or external modules such as video captioners, which introduce costly annotations, complex training pipelines, and high inference overhead. In this work, we introduce PrismVAU, a lightweight yet effective system for real-time VAU that leverages a single off-the-shelf MLLM for anomaly scoring, explanation, and prompt optimization. PrismVAU operates in two complementary stages: (1) a coarse anomaly scoring module that computes frame-level anomaly scores via similarity to textual anchors, and (2) an MLLM-based refinement module that contextualizes anomalies through system and user prompts. Both textual anchors and prompts are optimized with a weakly supervised Automatic Prompt Engineering (APE) framework. Extensive experiments on standard VAD benchmarks demonstrate that PrismVAU delivers competitive detection performance and interpretable anomaly explanations -- without relying on instruction tuning, frame-level annotations, and external modules or dense processing -- making it an efficient and practical solution for real-world applications.
<div id='section'>Paperid: <span id='pid'>381, <a href='https://arxiv.org/pdf/2512.13008.pdf' target='_blank'>https://arxiv.org/pdf/2512.13008.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xi Luo, Shixin Xu, Ying Xie, JianZhong Hu, Yuwei He, Yuhui Deng, Huaxiong Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.13008">TWLR: Text-Guided Weakly-Supervised Lesion Localization and Severity Regression for Explainable Diabetic Retinopathy Grading</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate medical image analysis can greatly assist clinical diagnosis, but its effectiveness relies on high-quality expert annotations Obtaining pixel-level labels for medical images, particularly fundus images, remains costly and time-consuming. Meanwhile, despite the success of deep learning in medical imaging, the lack of interpretability limits its clinical adoption. To address these challenges, we propose TWLR, a two-stage framework for interpretable diabetic retinopathy (DR) assessment. In the first stage, a vision-language model integrates domain-specific ophthalmological knowledge into text embeddings to jointly perform DR grading and lesion classification, effectively linking semantic medical concepts with visual features. The second stage introduces an iterative severity regression framework based on weakly-supervised semantic segmentation. Lesion saliency maps generated through iterative refinement direct a progressive inpainting mechanism that systematically eliminates pathological features, effectively downgrading disease severity toward healthier fundus appearances. Critically, this severity regression approach achieves dual benefits: accurate lesion localization without pixel-level supervision and providing an interpretable visualization of disease-to-healthy transformations. Experimental results on the FGADR, DDR, and a private dataset demonstrate that TWLR achieves competitive performance in both DR classification and lesion segmentation, offering a more explainable and annotation-efficient solution for automated retinal image analysis.
<div id='section'>Paperid: <span id='pid'>382, <a href='https://arxiv.org/pdf/2512.06840.pdf' target='_blank'>https://arxiv.org/pdf/2512.06840.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Satoshi Hashimoto, Tatsuya Konishi, Tomoya Kaichi, Kazunori Matsumoto, Mori Kurokawa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.06840">CADE: Continual Weakly-supervised Video Anomaly Detection with Ensembles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video anomaly detection (VAD) has long been studied as a crucial problem in public security and crime prevention. In recent years, weakly-supervised VAD (WVAD) have attracted considerable attention due to their easy annotation process and promising research results. While existing WVAD methods tackle mainly on static datasets, the possibility that the domain of data can vary has been neglected. To adapt such domain-shift, the continual learning (CL) perspective is required because otherwise additional training only with new coming data could easily cause performance degradation for previous data, i.e., forgetting. Therefore, we propose a brand-new approach, called Continual Anomaly Detection with Ensembles (CADE) that is the first work combining CL and WVAD viewpoints. Specifically, CADE uses the Dual-Generator(DG) to address data imbalance and label uncertainty in WVAD. We also found that forgetting exacerbates the "incompleteness'' where the model becomes biased towards certain anomaly modes, leading to missed detections of various anomalies. To address this, we propose to ensemble Multi-Discriminator (MD) that capture missed anomalies in past scenes due to forgetting, using multiple models. Extensive experiments show that CADE significantly outperforms existing VAD methods on the common multi-scene VAD datasets, such as ShanghaiTech and Charlotte Anomaly datasets.
<div id='section'>Paperid: <span id='pid'>383, <a href='https://arxiv.org/pdf/2511.19765.pdf' target='_blank'>https://arxiv.org/pdf/2511.19765.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Torabi, Sanjog Gaihre, Yaqoob Majeed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19765">Lightweight Transformer Framework for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation (WSSS) must learn dense masks from noisy, under-specified cues. We revisit the SegFormer decoder and show that three small, synergistic changes make weak supervision markedly more effective-without altering the MiT backbone or relying on heavy post-processing. Our method, CrispFormer, augments the decoder with: (1) a boundary branch that supervises thin object contours using a lightweight edge head and a boundary-aware loss; (2) an uncertainty-guided refiner that predicts per-pixel aleatoric uncertainty and uses it to weight losses and gate a residual correction of the segmentation logits; and (3) a dynamic multi-scale fusion layer that replaces static concatenation with spatial softmax gating over multi-resolution features, optionally modulated by uncertainty. The result is a single-pass model that preserves crisp boundaries, selects appropriate scales per location, and resists label noise from weak cues. Integrated into a standard WSSS pipeline (seed, student, and EMA relabeling), CrispFormer consistently improves boundary F-score, small-object recall, and mIoU over SegFormer baselines trained on the same seeds, while adding minimal compute. Our decoder-centric formulation is simple to implement, broadly compatible with existing SegFormer variants, and offers a reproducible path to higher-fidelity masks from image-level supervision.
<div id='section'>Paperid: <span id='pid'>384, <a href='https://arxiv.org/pdf/2511.16268.pdf' target='_blank'>https://arxiv.org/pdf/2511.16268.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Erwan Dereure, Robin Louiset, Laura Parkkinen, David A Menassa, David Holcman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.16268">Weakly Supervised Segmentation and Classification of Alpha-Synuclein Aggregates in Brightfield Midbrain Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Parkinson's disease (PD) is a neurodegenerative disorder associated with the accumulation of misfolded alpha-synuclein aggregates, forming Lewy bodies and neuritic shape used for pathology diagnostics. Automatic analysis of immunohistochemistry histopathological images with Deep Learning provides a promising tool for better understanding the spatial organization of these aggregates. In this study, we develop an automated image processing pipeline to segment and classify these aggregates in whole-slide images (WSIs) of midbrain tissue from PD and incidental Lewy Body Disease (iLBD) cases based on weakly supervised segmentation, robust to immunohistochemical labelling variability, with a ResNet50 classifier. Our approach allows to differentiate between major aggregate morphologies, including Lewy bodies and neurites with a balanced accuracy of $80\%$. This framework paves the way for large-scale characterization of the spatial distribution and heterogeneity of alpha-synuclein aggregates in brightfield immunohistochemical tissue, and for investigating their poorly understood relationships with surrounding cells such as microglia and astrocytes.
<div id='section'>Paperid: <span id='pid'>385, <a href='https://arxiv.org/pdf/2511.13891.pdf' target='_blank'>https://arxiv.org/pdf/2511.13891.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seyed Mohamad Ali Tousi, John A. Lory, G. N. DeSouza
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13891">Weakly Supervised Ephemeral Gully Detection In Remote Sensing Images Using Vision Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Among soil erosion problems, Ephemeral Gullies are one of the most concerning phenomena occurring in agricultural fields. Their short temporal cycles increase the difficulty in automatically detecting them using classical computer vision approaches and remote sensing. Also, due to scarcity of and the difficulty in producing accurate labeled data, automatic detection of ephemeral gullies using Machine Learning is limited to zero-shot approaches which are hard to implement. To overcome these challenges, we present the first weakly supervised pipeline for detection of ephemeral gullies. Our method relies on remote sensing and uses Vision Language Models (VLMs) to drastically reduce the labor-intensive task of manual labeling. In order to achieve that, the method exploits: 1) the knowledge embedded in the VLM's pretraining; 2) a teacher-student model where the teacher learns from noisy labels coming from the VLMs, and the student learns by weak supervision using teacher-generate labels and a noise-aware loss function. We also make available the first-of-its-kind dataset for semi-supervised detection of ephemeral gully from remote-sensed images. The dataset consists of a number of locations labeled by a group of soil and plant scientists, as well as a large number of unlabeled locations. The dataset represent more than 18,000 high-resolution remote-sensing images obtained over the course of 13 years. Our experimental results demonstrate the validity of our approach by showing superior performances compared to VLMs and the label model itself when using weak supervision to train an student model. The code and dataset for this work are made publicly available.
<div id='section'>Paperid: <span id='pid'>386, <a href='https://arxiv.org/pdf/2511.10958.pdf' target='_blank'>https://arxiv.org/pdf/2511.10958.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gunho Jung, Heejo Kong, Seong-Whan Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.10958">Text-guided Weakly Supervised Framework for Dynamic Facial Expression Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic facial expression recognition (DFER) aims to identify emotional states by modeling the temporal changes in facial movements across video sequences. A key challenge in DFER is the many-to-one labeling problem, where a video composed of numerous frames is assigned a single emotion label. A common strategy to mitigate this issue is to formulate DFER as a Multiple Instance Learning (MIL) problem. However, MIL-based approaches inherently suffer from the visual diversity of emotional expressions and the complexity of temporal dynamics. To address this challenge, we propose TG-DFER, a text-guided weakly supervised framework that enhances MIL-based DFER by incorporating semantic guidance and coherent temporal modeling. We incorporate a vision-language pre-trained (VLP) model is integrated to provide semantic guidance through fine-grained textual descriptions of emotional context. Furthermore, we introduce visual prompts, which align enriched textual emotion labels with visual instance features, enabling fine-grained reasoning and frame-level relevance estimation. In addition, a multi-grained temporal network is designed to jointly capture short-term facial dynamics and long-range emotional flow, ensuring coherent affective understanding across time. Extensive results demonstrate that TG-DFER achieves improved generalization, interpretability, and temporal sensitivity under weak supervision.
<div id='section'>Paperid: <span id='pid'>387, <a href='https://arxiv.org/pdf/2510.11047.pdf' target='_blank'>https://arxiv.org/pdf/2510.11047.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nivea Roy, Son Tran, Atul Sajjanhar, K. Devaraja, Prakashini Koteshwara, Yong Xiang, Divya Rao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.11047">Benchmarking Deep Learning Models for Laryngeal Cancer Staging Using the LaryngealCT Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Laryngeal cancer imaging research lacks standardised datasets to enable reproducible deep learning (DL) model development. We present LaryngealCT, a curated benchmark of 1,029 computed tomography (CT) scans aggregated from six collections from The Cancer Imaging Archive (TCIA). Uniform 1 mm isotropic volumes of interest encompassing the larynx were extracted using a weakly supervised parameter search framework validated by clinical experts. 3D DL architectures (3D CNN, ResNet18,50,101, DenseNet121) were benchmarked on (i) early (Tis,T1,T2) vs. advanced (T3,T4) and (ii) T4 vs. non-T4 classification tasks. 3D CNN (AUC-0.881, F1-macro-0.821) and ResNet18 (AUC-0.892, F1-macro-0.646) respectively outperformed the other models in the two tasks. Model explainability assessed using 3D GradCAMs with thyroid cartilage overlays revealed greater peri-cartilage attention in non-T4 cases and focal activations in T4 predictions. Through open-source data, pretrained models, and integrated explainability tools, LaryngealCT offers a reproducible foundation for AI-driven research to support clinical decisions in laryngeal oncology.
<div id='section'>Paperid: <span id='pid'>388, <a href='https://arxiv.org/pdf/2509.17702.pdf' target='_blank'>https://arxiv.org/pdf/2509.17702.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Patrick Schmidt, Vasileios Belagiannis, Lazaros Nalpantidis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17702">Depth Edge Alignment Loss: DEALing with Depth in Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous robotic systems applied to new domains require an abundance of expensive, pixel-level dense labels to train robust semantic segmentation models under full supervision. This study proposes a model-agnostic Depth Edge Alignment Loss to improve Weakly Supervised Semantic Segmentation models across different datasets. The methodology generates pixel-level semantic labels from image-level supervision, avoiding expensive annotation processes. While weak supervision is widely explored in traditional computer vision, our approach adds supervision with pixel-level depth information, a modality commonly available in robotic systems. We demonstrate how our approach improves segmentation performance across datasets and models, but can also be combined with other losses for even better performance, with improvements up to +5.439, +1.274 and +16.416 points in mean Intersection over Union on the PASCAL VOC / MS COCO validation, and the HOPE static onboarding split, respectively. Our code will be made publicly available.
<div id='section'>Paperid: <span id='pid'>389, <a href='https://arxiv.org/pdf/2509.12496.pdf' target='_blank'>https://arxiv.org/pdf/2509.12496.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Torabi, Sanjog Gaihre, MD Mahbubur Rahman, Yaqoob Majeed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12496">Instance-Guided Class Activation Mapping for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Semantic Segmentation (WSSS) addresses the challenge of training segmentation models using only image-level annotations, eliminating the need for expensive pixel-level labeling. While existing methods struggle with precise object boundary localization and often focus only on the most discriminative regions, we propose IG-CAM (Instance-Guided Class Activation Mapping), a novel approach that leverages instance-level cues and influence functions to generate high-quality, boundary-aware localization maps. Our method introduces three key innovations: (1) Instance-Guided Refinement that uses ground truth segmentation masks to guide CAM generation, ensuring complete object coverage rather than just discriminative parts; (2) Influence Function Integration that captures the relationship between training samples and model predictions, leading to more robust feature representations; and (3) Multi-Scale Boundary Enhancement that employs progressive refinement strategies to achieve sharp, precise object boundaries. IG-CAM achieves state-of-the-art performance on the PASCAL VOC 2012 dataset with an mIoU of 82.3% before post-processing, which further improves to 86.6% after applying Conditional Random Field (CRF) refinement, significantly outperforming previous WSSS methods. Our approach demonstrates superior localization accuracy, with complete object coverage and precise boundary delineation, while maintaining computational efficiency. Extensive ablation studies validate the contribution of each component, and qualitative comparisons across 600 diverse images showcase the method's robustness and generalization capability. The results establish IG-CAM as a new benchmark for weakly supervised semantic segmentation, offering a practical solution for scenarios where pixel-level annotations are unavailable or prohibitively expensive.
<div id='section'>Paperid: <span id='pid'>390, <a href='https://arxiv.org/pdf/2509.10641.pdf' target='_blank'>https://arxiv.org/pdf/2509.10641.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikita Rajaneesh, Thomas Zollo, Richard Zemel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10641">Test-Time Warmup for Multimodal Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal Large Language Models (MLLMs) hold great promise for advanced reasoning at the intersection of text and images, yet they have not fully realized this potential. MLLMs typically integrate an LLM, a vision encoder, and a connector that maps the vision encoder's embeddings into the LLM's text embedding space. Although each component is pretrained on massive datasets with billions of samples, the entire multimodal model is typically trained on only thousands (or a few million) samples, which can result in weak performance on complex reasoning tasks. To address these shortcomings, instead of relying on extensive labeled datasets for fine-tuning, we propose a Test-Time Warmup method that adapts the MLLM per test instance by leveraging data from weakly supervised auxiliary tasks. With our approach, we observe a relative performance improvement of 4.03% on MMMU, 5.28% on VQA-Rad, and 1.63% on GQA on the Llama-Vision-Instruct model. Our method demonstrates that 'warming up' before inference can enhance MLLMs' robustness across diverse reasoning tasks.
<div id='section'>Paperid: <span id='pid'>391, <a href='https://arxiv.org/pdf/2508.19647.pdf' target='_blank'>https://arxiv.org/pdf/2508.19647.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bikash Kumar Badatya, Vipul Baghel, Ravi Hegde
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19647">UTAL-GNN: Unsupervised Temporal Action Localization using Graph Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fine-grained action localization in untrimmed sports videos presents a significant challenge due to rapid and subtle motion transitions over short durations. Existing supervised and weakly supervised solutions often rely on extensive annotated datasets and high-capacity models, making them computationally intensive and less adaptable to real-world scenarios. In this work, we introduce a lightweight and unsupervised skeleton-based action localization pipeline that leverages spatio-temporal graph neural representations. Our approach pre-trains an Attention-based Spatio-Temporal Graph Convolutional Network (ASTGCN) on a pose-sequence denoising task with blockwise partitions, enabling it to learn intrinsic motion dynamics without any manual labeling. At inference, we define a novel Action Dynamics Metric (ADM), computed directly from low-dimensional ASTGCN embeddings, which detects motion boundaries by identifying inflection points in its curvature profile. Our method achieves a mean Average Precision (mAP) of 82.66% and average localization latency of 29.09 ms on the DSV Diving dataset, matching state-of-the-art supervised performance while maintaining computational efficiency. Furthermore, it generalizes robustly to unseen, in-the-wild diving footage without retraining, demonstrating its practical applicability for lightweight, real-time action analysis systems in embedded or dynamic environments.
<div id='section'>Paperid: <span id='pid'>392, <a href='https://arxiv.org/pdf/2508.08876.pdf' target='_blank'>https://arxiv.org/pdf/2508.08876.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaiyu Wang, Lin Mu, Zhiyao Yang, Ximing Li, Xiaotang Zhou Wanfu Gao, Huimao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08876">Weakly Supervised Fine-grained Span-Level Framework for Chinese Radiology Report Quality Assurance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quality Assurance (QA) for radiology reports refers to judging whether the junior reports (written by junior doctors) are qualified. The QA scores of one junior report are given by the senior doctor(s) after reviewing the image and junior report. This process requires intensive labor costs for senior doctors. Additionally, the QA scores may be inaccurate for reasons like diagnosis bias, the ability of senior doctors, and so on. To address this issue, we propose a Span-level Quality Assurance EvaluaTOR (Sqator) to mark QA scores automatically. Unlike the common document-level semantic comparison method, we try to analyze the semantic difference by exploring more fine-grained text spans. Specifically, Sqator measures QA scores by measuring the importance of revised spans between junior and senior reports, and outputs the final QA scores by merging all revised span scores. We evaluate Sqator using a collection of 12,013 radiology reports. Experimental results show that Sqator can achieve competitive QA scores. Moreover, the importance scores of revised spans can be also consistent with the judgments of senior doctors.
<div id='section'>Paperid: <span id='pid'>393, <a href='https://arxiv.org/pdf/2508.05108.pdf' target='_blank'>https://arxiv.org/pdf/2508.05108.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tomoya Tate, Kosuke Sugiyama, Masato Uchida
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05108">Learning from Similarity-Confidence and Confidence-Difference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In practical machine learning applications, it is often challenging to assign accurate labels to data, and increasing the number of labeled instances is often limited. In such cases, Weakly Supervised Learning (WSL), which enables training with incomplete or imprecise supervision, provides a practical and effective solution. However, most existing WSL methods focus on leveraging a single type of weak supervision. In this paper, we propose a novel WSL framework that leverages complementary weak supervision signals from multiple relational perspectives, which can be especially valuable when labeled data is limited. Specifically, we introduce SconfConfDiff Classification, a method that integrates two distinct forms of weaklabels: similarity-confidence and confidence-difference, which are assigned to unlabeled data pairs. To implement this method, we derive two types of unbiased risk estimators for classification: one based on a convex combination of existing estimators, and another newly designed by modeling the interaction between two weak labels. We prove that both estimators achieve optimal convergence rates with respect to estimation error bounds. Furthermore, we introduce a risk correction approach to mitigate overfitting caused by negative empirical risk, and provide theoretical analysis on the robustness of the proposed method against inaccurate class prior probability and label noise. Experimental results demonstrate that the proposed method consistently outperforms existing baselines across a variety of settings.
<div id='section'>Paperid: <span id='pid'>394, <a href='https://arxiv.org/pdf/2508.01338.pdf' target='_blank'>https://arxiv.org/pdf/2508.01338.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqi Sheng, Junyan Wu, Wei Lu, Jiantao Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01338">Weakly-Supervised Image Forgery Localization via Vision-Language Collaborative Reasoning Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image forgery localization aims to precisely identify tampered regions within images, but it commonly depends on costly pixel-level annotations. To alleviate this annotation burden, weakly supervised image forgery localization (WSIFL) has emerged, yet existing methods still achieve limited localization performance as they mainly exploit intra-image consistency clues and lack external semantic guidance to compensate for weak supervision. In this paper, we propose ViLaCo, a vision-language collaborative reasoning framework that introduces auxiliary semantic supervision distilled from pre-trained vision-language models (VLMs), enabling accurate pixel-level localization using only image-level labels. Specifically, ViLaCo first incorporates semantic knowledge through a vision-language feature modeling network, which jointly extracts textual and visual priors using pre-trained VLMs. Next, an adaptive vision-language reasoning network aligns textual semantics and visual features through mutual interactions, producing semantically aligned representations. Subsequently, these representations are passed into dual prediction heads, where the coarse head performs image-level classification and the fine head generates pixel-level localization masks, thereby bridging the gap between weak supervision and fine-grained localization. Moreover, a contrastive patch consistency module is introduced to cluster tampered features while separating authentic ones, facilitating more reliable forgery discrimination. Extensive experiments on multiple public datasets demonstrate that ViLaCo substantially outperforms existing WSIFL methods, achieving state-of-the-art performance in both detection and localization accuracy.
<div id='section'>Paperid: <span id='pid'>395, <a href='https://arxiv.org/pdf/2507.02998.pdf' target='_blank'>https://arxiv.org/pdf/2507.02998.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kimberly F. Greco, Zongxin Yang, Mengyan Li, Han Tong, Sara Morini Sweet, Alon Geva, Kenneth D. Mandl, Benjamin A. Raby, Tianxi Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02998">A Weakly Supervised Transformer to Support Rare Disease Diagnosis from Electronic Health Records: Methods and Applications in Rare Pulmonary Disease</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rare diseases affect an estimated 300-400 million people worldwide, yet individual conditions often remain poorly characterized and difficult to diagnose due to their low prevalence and limited clinician familiarity. While computational phenotyping algorithms show promise for automating rare disease detection, their development is hindered by the scarcity of labeled data and biases in existing label sources. Gold-standard labels from registries and expert chart reviews are highly accurate but constrained by selection bias and the cost of manual review. In contrast, labels derived from electronic health records (EHRs) cover a broader range of patients but can introduce substantial noise. To address these challenges, we propose a weakly supervised, transformer-based framework that combines a small set of gold-standard labels with a large volume of iteratively updated silver-standard labels derived from EHR data. This hybrid approach enables the training of a highly accurate and generalizable phenotyping model that scales rare disease detection beyond the scope of individual clinical expertise. Our method is initialized by learning embeddings of medical concepts based on their semantic meaning or co-occurrence patterns in EHRs, which are then refined and aggregated into patient-level representations via a multi-layer transformer architecture. Using two rare pulmonary diseases as a case study, we validate our model on EHR data from Boston Children's Hospital. Our framework demonstrates notable improvements in phenotype classification, identification of clinically meaningful subphenotypes through patient clustering, and prediction of disease progression compared to baseline methods. These results highlight the potential of our approach to enable scalable identification and stratification of rare disease patients for clinical care and research applications.
<div id='section'>Paperid: <span id='pid'>396, <a href='https://arxiv.org/pdf/2504.09582.pdf' target='_blank'>https://arxiv.org/pdf/2504.09582.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christos Theodoropoulos, Andrei Catalin Coman, James Henderson, Marie-Francine Moens
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09582">Reduction of Supervision for Biomedical Knowledge Discovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Knowledge discovery is hindered by the increasing volume of publications and the scarcity of extensive annotated data. To tackle the challenge of information overload, it is essential to employ automated methods for knowledge extraction and processing. Finding the right balance between the level of supervision and the effectiveness of models poses a significant challenge. While supervised techniques generally result in better performance, they have the major drawback of demanding labeled data. This requirement is labor-intensive and time-consuming and hinders scalability when exploring new domains. In this context, our study addresses the challenge of identifying semantic relationships between biomedical entities (e.g., diseases, proteins) in unstructured text while minimizing dependency on supervision. We introduce a suite of unsupervised algorithms based on dependency trees and attention mechanisms and employ a range of pointwise binary classification methods. Transitioning from weakly supervised to fully unsupervised settings, we assess the methods' ability to learn from data with noisy labels. The evaluation on biomedical benchmark datasets explores the effectiveness of the methods. Our approach tackles a central issue in knowledge discovery: balancing performance with minimal supervision. By gradually decreasing supervision, we assess the robustness of pointwise binary classification techniques in handling noisy labels, revealing their capability to shift from weakly supervised to entirely unsupervised scenarios. Comprehensive benchmarking offers insights into the effectiveness of these techniques, suggesting an encouraging direction toward adaptable knowledge discovery systems, representing progress in creating data-efficient methodologies for extracting useful insights when annotated data is limited.
<div id='section'>Paperid: <span id='pid'>397, <a href='https://arxiv.org/pdf/2503.18725.pdf' target='_blank'>https://arxiv.org/pdf/2503.18725.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zimin Xia, Alexandre Alahi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18725">FG$^2$: Fine-Grained Cross-View Localization by Fine-Grained Feature Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel fine-grained cross-view localization method that estimates the 3 Degrees of Freedom pose of a ground-level image in an aerial image of the surroundings by matching fine-grained features between the two images. The pose is estimated by aligning a point plane generated from the ground image with a point plane sampled from the aerial image. To generate the ground points, we first map ground image features to a 3D point cloud. Our method then learns to select features along the height dimension to pool the 3D points to a Bird's-Eye-View (BEV) plane. This selection enables us to trace which feature in the ground image contributes to the BEV representation. Next, we sample a set of sparse matches from computed point correspondences between the two point planes and compute their relative pose using Procrustes alignment. Compared to the previous state-of-the-art, our method reduces the mean localization error by 28% on the VIGOR cross-area test set. Qualitative results show that our method learns semantically consistent matches across ground and aerial views through weakly supervised learning from the camera pose.
<div id='section'>Paperid: <span id='pid'>398, <a href='https://arxiv.org/pdf/2503.03042.pdf' target='_blank'>https://arxiv.org/pdf/2503.03042.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Han, Soumava Kumar Roy, Mehrtash Harandi, Lars Petersson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.03042">Learning from Noisy Labels with Contrastive Co-Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning with noisy labels is an interesting challenge in weakly supervised learning. Despite their significant learning capacity, CNNs have a tendency to overfit in the presence of samples with noisy labels. Alleviating this issue, the well known Co-Training framework is used as a fundamental basis for our work. In this paper, we introduce a Contrastive Co-Transformer framework, which is simple and fast, yet able to improve the performance by a large margin compared to the state-of-the-art approaches. We argue the robustness of transformers when dealing with label noise. Our Contrastive Co-Transformer approach is able to utilize all samples in the dataset, irrespective of whether they are clean or noisy. Transformers are trained by a combination of contrastive loss and classification loss. Extensive experimental results on corrupted data from six standard benchmark datasets including Clothing1M, demonstrate that our Contrastive Co-Transformer is superior to existing state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>399, <a href='https://arxiv.org/pdf/2502.20678.pdf' target='_blank'>https://arxiv.org/pdf/2502.20678.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aaryan Garg, Akash Kumar, Yogesh S Rawat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20678">STPro: Spatial and Temporal Progressive Learning for Weakly Supervised Spatio-Temporal Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work we study Weakly Supervised Spatio-Temporal Video Grounding (WSTVG), a challenging task of localizing subjects spatio-temporally in videos using only textual queries and no bounding box supervision. Inspired by recent advances in vision-language foundation models, we investigate their utility for WSTVG, leveraging their zero-shot grounding capabilities. However, we find that a simple adaptation lacks essential spatio-temporal grounding abilities. To bridge this gap, we introduce Tubelet Referral Grounding (TRG), which connects textual queries to tubelets to enable spatio-temporal predictions. Despite its promise, TRG struggles with compositional action understanding and dense scene scenarios. To address these limitations, we propose STPro, a novel progressive learning framework with two key modules: (1) Sub-Action Temporal Curriculum Learning (SA-TCL), which incrementally builds compositional action understanding, and (2) Congestion-Guided Spatial Curriculum Learning (CG-SCL), which adapts the model to complex scenes by spatially increasing task difficulty. STPro achieves state-of-the-art results on three benchmark datasets, with improvements of 1.0% on VidSTG-Declarative and 3.0% on HCSTVG-v1.
<div id='section'>Paperid: <span id='pid'>400, <a href='https://arxiv.org/pdf/2409.06471.pdf' target='_blank'>https://arxiv.org/pdf/2409.06471.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujiao Shi, Hongdong Li, Akhil Perincherry, Ankit Vora
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.06471">Weakly-supervised Camera Localization by Ground-to-satellite Image Registration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ground-to-satellite image matching/retrieval was initially proposed for city-scale ground camera localization. This work addresses the problem of improving camera pose accuracy by ground-to-satellite image matching after a coarse location and orientation have been obtained, either from the city-scale retrieval or from consumer-level GPS and compass sensors. Existing learning-based methods for solving this task require accurate GPS labels of ground images for network training. However, obtaining such accurate GPS labels is difficult, often requiring an expensive {\color{black}Real Time Kinematics (RTK)} setup and suffering from signal occlusion, multi-path signal disruptions, \etc. To alleviate this issue, this paper proposes a weakly supervised learning strategy for ground-to-satellite image registration when only noisy pose labels for ground images are available for network training. It derives positive and negative satellite images for each ground image and leverages contrastive learning to learn feature representations for ground and satellite images useful for translation estimation. We also propose a self-supervision strategy for cross-view image relative rotation estimation, which trains the network by creating pseudo query and reference image pairs. Experimental results show that our weakly supervised learning strategy achieves the best performance on cross-area evaluation compared to recent state-of-the-art methods that are reliant on accurate pose labels for supervision.
<div id='section'>Paperid: <span id='pid'>401, <a href='https://arxiv.org/pdf/2512.19713.pdf' target='_blank'>https://arxiv.org/pdf/2512.19713.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taoran Sheng, Manfred Huber
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.19713">Reducing Label Dependency in Human Activity Recognition with Wearables: From Supervised Learning to Novel Weakly Self-Supervised Approaches</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human activity recognition (HAR) using wearable sensors has advanced through various machine learning paradigms, each with inherent trade-offs between performance and labeling requirements. While fully supervised techniques achieve high accuracy, they demand extensive labeled datasets that are costly to obtain. Conversely, unsupervised methods eliminate labeling needs but often deliver suboptimal performance. This paper presents a comprehensive investigation across the supervision spectrum for wearable-based HAR, with particular focus on novel approaches that minimize labeling requirements while maintaining competitive accuracy. We develop and empirically compare: (1) traditional fully supervised learning, (2) basic unsupervised learning, (3) a weakly supervised learning approach with constraints, (4) a multi-task learning approach with knowledge sharing, (5) a self-supervised approach based on domain expertise, and (6) a novel weakly self-supervised learning framework that leverages domain knowledge and minimal labeled data. Experiments across benchmark datasets demonstrate that: (i) our weakly supervised methods achieve performance comparable to fully supervised approaches while significantly reducing supervision requirements; (ii) the proposed multi-task framework enhances performance through knowledge sharing between related tasks; (iii) our weakly self-supervised approach demonstrates remarkable efficiency with just 10\% of labeled data. These results not only highlight the complementary strengths of different learning paradigms, offering insights into tailoring HAR solutions based on the availability of labeled data, but also establish that our novel weakly self-supervised framework offers a promising solution for practical HAR applications where labeled data are limited.
<div id='section'>Paperid: <span id='pid'>402, <a href='https://arxiv.org/pdf/2512.01294.pdf' target='_blank'>https://arxiv.org/pdf/2512.01294.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Song Zhang, Ruohan Guo, Xiaohua Ge, Perter Mahon, Weixiang Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.01294">Experimental Methods, Health Indicators, and Diagnostic Strategies for Retired Lithium-ion Batteries: A Comprehensive Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reliable health assessment of retired lithium-ion batteries is essential for safe and economically viable second-life deployment, yet remains difficult due to sparse measurements, incomplete historical records, heterogeneous chemistries, and limited or noisy battery health labels. Conventional laboratory diagnostics, such as full charge-discharge cycling, pulse tests, Electrochemical Impedance Spectroscopy (EIS) measurements, and thermal characterization, provide accurate degradation information but are too time-consuming, equipment-intensive, or condition-sensitive to be applied at scale during retirement-stage sorting, leaving real-world datasets fragmented and inconsistent. This review synthesizes recent advances that address these constraints through physical health indicators, experiment testing methods, data-generation and augmentation techniques, and a spectrum of learning-based modeling routes spanning supervised, semi-supervised, weakly supervised, and unsupervised paradigms. We highlight how minimal-test features, synthetic data, domain-invariant representations, and uncertainty-aware prediction enable robust inference under limited or approximate labels and across mixed chemistries and operating histories. A comparative evaluation further reveals trade-offs in accuracy, interpretability, scalability, and computational burden. Looking forward, progress toward physically constrained generative models, cross-chemistry generalization, calibrated uncertainty estimation, and standardized benchmarks will be crucial for building reliable, scalable, and deployment-ready health prediction tools tailored to the realities of retired-battery applications.
<div id='section'>Paperid: <span id='pid'>403, <a href='https://arxiv.org/pdf/2511.17346.pdf' target='_blank'>https://arxiv.org/pdf/2511.17346.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marius Rodrigues, Louis Bahrman, Roland Badeau, Gaël Richard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17346">Is Phase Really Needed for Weakly-Supervised Dereverberation ?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In unsupervised or weakly-supervised approaches for speech dereverberation, the target clean (dry) signals are considered to be unknown during training. In that context, evaluating to what extent information can be retrieved from the sole knowledge of reverberant (wet) speech becomes critical. This work investigates the role of the reverberant (wet) phase in the time-frequency domain. Based on Statistical Wave Field Theory, we show that late reverberation perturbs phase components with white, uniformly distributed noise, except at low frequencies. Consequently, the wet phase carries limited useful information and is not essential for weakly supervised dereverberation. To validate this finding, we train dereverberation models under a recent weak supervision framework and demonstrate that performance can be significantly improved by excluding the reverberant phase from the loss function.
<div id='section'>Paperid: <span id='pid'>404, <a href='https://arxiv.org/pdf/2510.09306.pdf' target='_blank'>https://arxiv.org/pdf/2510.09306.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alemu Sisay Nigru, Michele Svanera, Austin Dibble, Connor Dalby, Mattia Savardi, Sergio Benini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09306">Rewiring Development in Brain Segmentation: Leveraging Adult Brain Priors for Enhancing Infant MRI Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate segmentation of infant brain MRI is critical for studying early neurodevelopment and diagnosing neurological disorders. Yet, it remains a fundamental challenge due to continuously evolving anatomy of the subjects, motion artifacts, and the scarcity of high-quality labeled data. In this work, we present LODi, a novel framework that utilizes prior knowledge from an adult brain MRI segmentation model to enhance the segmentation performance of infant scans. Given the abundance of publicly available adult brain MRI data, we pre-train a segmentation model on a large adult dataset as a starting point. Through transfer learning and domain adaptation strategies, we progressively adapt the model to the 0-2 year-old population, enabling it to account for the anatomical and imaging variability typical of infant scans. The adaptation of the adult model is carried out using weakly supervised learning on infant brain scans, leveraging silver-standard ground truth labels obtained with FreeSurfer. By introducing a novel training strategy that integrates hierarchical feature refinement and multi-level consistency constraints, our method enables fast, accurate, age-adaptive segmentation, while mitigating scanner and site-specific biases. Extensive experiments on both internal and external datasets demonstrate the superiority of our approach over traditional supervised learning and domain-specific models. Our findings highlight the advantage of leveraging adult brain priors as a foundation for age-flexible neuroimaging analysis, paving the way for more reliable and generalizable brain MRI segmentation across the lifespan.
<div id='section'>Paperid: <span id='pid'>405, <a href='https://arxiv.org/pdf/2510.00654.pdf' target='_blank'>https://arxiv.org/pdf/2510.00654.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaocong Zhu, Zhiwei Li, Xinghua Li, Huanfeng Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00654">Weakly Supervised Cloud Detection Combining Spectral Features and Multi-Scale Deep Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Clouds significantly affect the quality of optical satellite images, which seriously limits their precise application. Recently, deep learning has been widely applied to cloud detection and has achieved satisfactory results. However, the lack of distinctive features in thin clouds and the low quality of training samples limit the cloud detection accuracy of deep learning methods, leaving space for further improvements. In this paper, we propose a weakly supervised cloud detection method that combines spectral features and multi-scale scene-level deep network (SpecMCD) to obtain highly accurate pixel-level cloud masks. The method first utilizes a progressive training framework with a multi-scale scene-level dataset to train the multi-scale scene-level cloud detection network. Pixel-level cloud probability maps are then obtained by combining the multi-scale probability maps and cloud thickness map based on the characteristics of clouds in dense cloud coverage and large cloud-area coverage images. Finally, adaptive thresholds are generated based on the differentiated regions of the scene-level cloud masks at different scales and combined with distance-weighted optimization to obtain binary cloud masks. Two datasets, WDCD and GF1MS-WHU, comprising a total of 60 Gaofen-1 multispectral (GF1-MS) images, were used to verify the effectiveness of the proposed method. Compared to the other weakly supervised cloud detection methods such as WDCD and WSFNet, the F1-score of the proposed SpecMCD method shows an improvement of over 7.82%, highlighting the superiority and potential of the SpecMCD method for cloud detection under different cloud coverage conditions.
<div id='section'>Paperid: <span id='pid'>406, <a href='https://arxiv.org/pdf/2509.06485.pdf' target='_blank'>https://arxiv.org/pdf/2509.06485.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrea Marelli, Alberto Foresti, Leonardo Pesce, Giacomo Boracchi, Mario Grosso
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06485">WS$^2$: Weakly Supervised Segmentation using Before-After Supervision in Waste Sorting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In industrial quality control, to visually recognize unwanted items within a moving heterogeneous stream, human operators are often still indispensable. Waste-sorting stands as a significant example, where operators on multiple conveyor belts manually remove unwanted objects to select specific materials. To automate this recognition problem, computer vision systems offer great potential in accurately identifying and segmenting unwanted items in such settings. Unfortunately, considering the multitude and the variety of sorting tasks, fully supervised approaches are not a viable option to address this challange, as they require extensive labeling efforts. Surprisingly, weakly supervised alternatives that leverage the implicit supervision naturally provided by the operator in his removal action are relatively unexplored. In this paper, we define the concept of Before-After Supervision, illustrating how to train a segmentation network by leveraging only the visual differences between images acquired \textit{before} and \textit{after} the operator. To promote research in this direction, we introduce WS$^2$ (Weakly Supervised segmentation for Waste-Sorting), the first multiview dataset consisting of more than 11 000 high-resolution video frames captured on top of a conveyor belt, including "before" and "after" images. We also present a robust end-to-end pipeline, used to benchmark several state-of-the-art weakly supervised segmentation methods on WS$^2$.
<div id='section'>Paperid: <span id='pid'>407, <a href='https://arxiv.org/pdf/2507.10594.pdf' target='_blank'>https://arxiv.org/pdf/2507.10594.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengda Zhuo, Di Wu, Yi He, Shuqiang Huang, Xindong Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10594">Extension OL-MDISF: Online Learning from Mix-Typed, Drifted, and Incomplete Streaming Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Online learning, where feature spaces can change over time, offers a flexible learning paradigm that has attracted considerable attention. However, it still faces three significant challenges. First, the heterogeneity of real-world data streams with mixed feature types presents challenges for traditional parametric modeling. Second, data stream distributions can shift over time, causing an abrupt and substantial decline in model performance. Additionally, the time and cost constraints make it infeasible to label every data instance in a supervised setting. To overcome these challenges, we propose a new algorithm Online Learning from Mix-typed, Drifted, and Incomplete Streaming Features (OL-MDISF), which aims to relax restrictions on both feature types, data distribution, and supervision information. Our approach involves utilizing copula models to create a comprehensive latent space, employing an adaptive sliding window for detecting drift points to ensure model stability, and establishing label proximity information based on geometric structural relationships. To demonstrate the model's efficiency and effectiveness, we provide theoretical analysis and comprehensive experimental results.
  This extension serves as a standalone technical reference to the original OL-MDISF method. It provides (i) a contextual analysis of OL-MDISF within the broader landscape of online learning, covering recent advances in mixed-type feature modeling, concept drift adaptation, and weak supervision, and (ii) a comprehensive set of experiments across 14 real-world datasets under two types of drift scenarios. These include full CER trends, ablation studies, sensitivity analyses, and temporal ensemble dynamics. We hope this document can serve as a reproducible benchmark and technical resource for researchers working on nonstationary, heterogeneous, and weakly supervised data streams.
<div id='section'>Paperid: <span id='pid'>408, <a href='https://arxiv.org/pdf/2507.07297.pdf' target='_blank'>https://arxiv.org/pdf/2507.07297.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengfei Wu, Ronald Seoh, Bingxuan Li, Liqiang Zhang, Fengrong Han, Dan Goldwasser
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07297">MagiC: Evaluating Multimodal Cognition Toward Grounded Visual Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in large vision-language models have led to impressive performance in visual question answering and multimodal reasoning. However, it remains unclear whether these models genuinely perform grounded visual reasoning or rely on superficial patterns and dataset biases. In this work, we introduce MagiC, a comprehensive benchmark designed to evaluate grounded multimodal cognition, assessing not only answer accuracy but also the quality of step-by-step reasoning and its alignment with relevant visual evidence. Our benchmark includes approximately 5,500 weakly supervised QA examples generated from strong model outputs and 900 human-curated examples with fine-grained annotations, including answers, rationales, and bounding box groundings. We evaluate 15 vision-language models ranging from 7B to 70B parameters across four dimensions: final answer correctness, reasoning validity, grounding fidelity, and self-correction ability. MagiC further includes diagnostic settings to probe model robustness under adversarial visual cues and assess their capacity for introspective error correction. We introduce new metrics such as MagiScore and StepSense, and provide comprehensive analyses that reveal key limitations and opportunities in current approaches to grounded visual reasoning.
<div id='section'>Paperid: <span id='pid'>409, <a href='https://arxiv.org/pdf/2506.22866.pdf' target='_blank'>https://arxiv.org/pdf/2506.22866.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hang-Cheng Dong, Lu Zou, Bingguo Liu, Dong Ye, Guodong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22866">Region-Aware CAM: High-Resolution Weakly-Supervised Defect Segmentation via Salient Region Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Surface defect detection plays a critical role in industrial quality inspection. Recent advances in artificial intelligence have significantly enhanced the automation level of detection processes. However, conventional semantic segmentation and object detection models heavily rely on large-scale annotated datasets, which conflicts with the practical requirements of defect detection tasks. This paper proposes a novel weakly supervised semantic segmentation framework comprising two key components: a region-aware class activation map (CAM) and pseudo-label training. To address the limitations of existing CAM methods, especially low-resolution thermal maps, and insufficient detail preservation, we introduce filtering-guided backpropagation (FGBP), which refines target regions by filtering gradient magnitudes to identify areas with higher relevance to defects. Building upon this, we further develop a region-aware weighted module to enhance spatial precision. Finally, pseudo-label segmentation is implemented to refine the model's performance iteratively. Comprehensive experiments on industrial defect datasets demonstrate the superiority of our method. The proposed framework effectively bridges the gap between weakly supervised learning and high-precision defect segmentation, offering a practical solution for resource-constrained industrial scenarios.
<div id='section'>Paperid: <span id='pid'>410, <a href='https://arxiv.org/pdf/2505.23524.pdf' target='_blank'>https://arxiv.org/pdf/2505.23524.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Xia, Dan Jiang, Quan Zhang, Ke Zhang, Chun Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23524">CLIP-AE: CLIP-assisted Cross-view Audio-Visual Enhancement for Unsupervised Temporal Action Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Temporal Action Localization (TAL) has garnered significant attention in information retrieval. Existing supervised or weakly supervised methods heavily rely on labeled temporal boundaries and action categories, which are labor-intensive and time-consuming. Consequently, unsupervised temporal action localization (UTAL) has gained popularity. However, current methods face two main challenges: 1) Classification pre-trained features overly focus on highly discriminative regions; 2) Solely relying on visual modality information makes it difficult to determine contextual boundaries. To address these issues, we propose a CLIP-assisted cross-view audiovisual enhanced UTAL method. Specifically, we introduce visual language pre-training (VLP) and classification pre-training-based collaborative enhancement to avoid excessive focus on highly discriminative regions; we also incorporate audio perception to provide richer contextual boundary information. Finally, we introduce a self-supervised cross-view learning paradigm to achieve multi-view perceptual enhancement without additional annotations. Extensive experiments on two public datasets demonstrate our model's superiority over several state-of-the-art competitors.
<div id='section'>Paperid: <span id='pid'>411, <a href='https://arxiv.org/pdf/2503.20722.pdf' target='_blank'>https://arxiv.org/pdf/2503.20722.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>A. Candito, A. Dragan, R. Holbrey, A. Ribeiro, R. Donners, C. Messiou, N. Tunariu, D. -M. Koh, M. D. Blackledge, The Institute of Cancer Research, London, United Kingdom, The Royal Marsden NHS Foundation Trust, London, United Kingdom, University Hospital Basel, Basel, Switzerland
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20722">A weakly-supervised deep learning model for fast localisation and delineation of the skeleton, internal organs, and spinal canal on Whole-Body Diffusion-Weighted MRI (WB-DWI)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Background: Apparent Diffusion Coefficient (ADC) values and Total Diffusion Volume (TDV) from Whole-body diffusion-weighted MRI (WB-DWI) are recognized cancer imaging biomarkers. However, manual disease delineation for ADC and TDV measurements is unfeasible in clinical practice, demanding automation. As a first step, we propose an algorithm to generate fast and reproducible probability maps of the skeleton, adjacent internal organs (liver, spleen, urinary bladder, and kidneys), and spinal canal. Methods: We developed an automated deep-learning pipeline based on a 3D patch-based Residual U-Net architecture that localizes and delineates these anatomical structures on WB-DWI. The algorithm was trained using "soft-labels" (non-binary segmentations) derived from a computationally intensive atlas-based approach. For training and validation, we employed a multi-center WB-DWI dataset comprising 532 scans from patients with Advanced Prostate Cancer (APC) or Multiple Myeloma (MM), with testing on 45 patients. Results: Our weakly-supervised deep learning model achieved an average dice score/precision/recall of 0.66/0.6/0.73 for skeletal delineations, 0.8/0.79/0.81 for internal organs, and 0.85/0.79/0.94 for spinal canal, with surface distances consistently below 3 mm. Relative median ADC and log-transformed volume differences between automated and manual expert-defined full-body delineations were below 10% and 4%, respectively. The computational time for generating probability maps was 12x faster than the atlas-based registration algorithm (25 s vs. 5 min). An experienced radiologist rated the model's accuracy "good" or "excellent" on test datasets. Conclusion: Our model offers fast and reproducible probability maps for localizing and delineating body regions on WB-DWI, enabling ADC and TDV quantification, potentially supporting clinicians in disease staging and treatment response assessment.
<div id='section'>Paperid: <span id='pid'>412, <a href='https://arxiv.org/pdf/2503.18509.pdf' target='_blank'>https://arxiv.org/pdf/2503.18509.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nijesh Upreti, Vaishak Belle
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18509">Neuro-symbolic Weak Supervision: Theory and Semantics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weak supervision allows machine learning models to learn from limited or noisy labels, but it introduces challenges in interpretability and reliability - particularly in multi-instance partial label learning (MI-PLL), where models must resolve both ambiguous labels and uncertain instance-label mappings. We propose a semantics for neuro-symbolic framework that integrates Inductive Logic Programming (ILP) to improve MI-PLL by providing structured relational constraints that guide learning. Within our semantic characterization, ILP defines a logical hypothesis space for label transitions, clarifies classifier semantics, and establishes interpretable performance standards. This hybrid approach improves robustness, transparency, and accountability in weakly supervised settings, ensuring neural predictions align with domain knowledge. By embedding weak supervision into a logical framework, we enhance both interpretability and learning, making weak supervision more suitable for real-world, high-stakes applications.
<div id='section'>Paperid: <span id='pid'>413, <a href='https://arxiv.org/pdf/2503.18384.pdf' target='_blank'>https://arxiv.org/pdf/2503.18384.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Gao, Shaobo Xia, Pu Wang, Xiaohuan Xi, Sheng Nie, Cheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18384">LiDAR Remote Sensing Meets Weak Supervision: Concepts, Methods, and Perspectives</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LiDAR (Light Detection and Ranging) enables rapid and accurate acquisition of three-dimensional spatial data, widely applied in remote sensing areas such as surface mapping, environmental monitoring, urban modeling, and forestry inventory. LiDAR remote sensing primarily includes data interpretation and LiDAR-based inversion. However, LiDAR interpretation typically relies on dense and precise annotations, which are costly and time-consuming. Similarly, LiDAR inversion depends on scarce supervisory signals and expensive field surveys for annotations. To address this challenge, weakly supervised learning has gained significant attention in recent years, with many methods emerging to tackle LiDAR remote sensing tasks using incomplete, inaccurate, and inexact annotations, as well as annotations from other domains. Existing review articles treat LiDAR interpretation and inversion as separate tasks. This review, for the first time, adopts a unified weakly supervised learning perspective to systematically examine research on both LiDAR interpretation and inversion. We summarize the latest advancements, provide a comprehensive review of the development and application of weakly supervised techniques in LiDAR remote sensing, and discuss potential future research directions in this field.
<div id='section'>Paperid: <span id='pid'>414, <a href='https://arxiv.org/pdf/2503.16546.pdf' target='_blank'>https://arxiv.org/pdf/2503.16546.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Saddam Hussain Khan, Rashid Iqbal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16546">A Comprehensive Survey on Architectural Advances in Deep CNNs: Challenges, Applications, and Emerging Research Directions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep Convolutional Neural Networks (CNNs) have significantly advanced deep learning, driving breakthroughs in computer vision, natural language processing, medical diagnosis, object detection, and speech recognition. Architectural innovations including 1D, 2D, and 3D convolutional models, dilated and grouped convolutions, depthwise separable convolutions, and attention mechanisms address domain-specific challenges and enhance feature representation and computational efficiency. Structural refinements such as spatial-channel exploitation, multi-path design, and feature-map enhancement contribute to robust hierarchical feature extraction and improved generalization, particularly through transfer learning. Efficient preprocessing strategies, including Fourier transforms, structured transforms, low-precision computation, and weight compression, optimize inference speed and facilitate deployment in resource-constrained environments. This survey presents a unified taxonomy that classifies CNN architectures based on spatial exploitation, multi-path structures, depth, width, dimensionality expansion, channel boosting, and attention mechanisms. It systematically reviews CNN applications in face recognition, pose estimation, action recognition, text classification, statistical language modeling, disease diagnosis, radiological analysis, cryptocurrency sentiment prediction, 1D data processing, video analysis, and speech recognition. In addition to consolidating architectural advancements, the review highlights emerging learning paradigms such as few-shot, zero-shot, weakly supervised, federated learning frameworks and future research directions include hybrid CNN-transformer models, vision-language integration, generative learning, etc. This review provides a comprehensive perspective on CNN's evolution from 2015 to 2025, outlining key innovations, challenges, and opportunities.
<div id='section'>Paperid: <span id='pid'>415, <a href='https://arxiv.org/pdf/2503.04165.pdf' target='_blank'>https://arxiv.org/pdf/2503.04165.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bodong Zhang, Hamid Manoochehri, Xiwen Li, Beatrice S. Knudsen, Tolga Tasdizen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04165">WeakSupCon: Weakly Supervised Contrastive Learning for Encoder Pre-training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised multiple instance learning (MIL) is a challenging task given that only bag-level labels are provided, while each bag typically contains multiple instances. This topic has been extensively studied in histopathological image analysis, where labels are usually available only at the whole slide image (WSI) level, while each WSI could be divided into thousands of small image patches for training. The dominant MIL approaches focus on feature aggregation and take fixed patch features as inputs. However, weakly supervised feature representation learning in MIL settings is always neglected. Those features used to be generated by self-supervised learning methods that do not utilize weak labels, or by foundation encoders pre-trained on other large datasets. In this paper, we propose a novel weakly supervised feature representation learning method called Weakly Supervised Contrastive Learning (WeakSupCon) that utilizes bag-level labels. In our method, we employ multi-task learning and define distinct contrastive losses for samples with different bag labels. Our experiments demonstrate that the features generated using WeakSupCon with limited computing resources significantly enhance MIL classification performance compared to self-supervised approaches across three datasets. Our WeakSupCon code is available at github.com/BzhangURU/Paper_WeakSupCon
<div id='section'>Paperid: <span id='pid'>416, <a href='https://arxiv.org/pdf/2502.09080.pdf' target='_blank'>https://arxiv.org/pdf/2502.09080.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiwei Wang, Shaoxun Wu, Yujiao Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09080">BevSplat: Resolving Height Ambiguity via Feature-Based Gaussian Primitives for Weakly-Supervised Cross-View Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the problem of weakly supervised cross-view localization, where the goal is to estimate the pose of a ground camera relative to a satellite image with noisy ground truth annotations. A common approach to bridge the cross-view domain gap for pose estimation is Bird's-Eye View (BEV) synthesis. However, existing methods struggle with height ambiguity due to the lack of depth information in ground images and satellite height maps. Previous solutions either assume a flat ground plane or rely on complex models, such as cross-view transformers. We propose BevSplat, a novel method that resolves height ambiguity by using feature-based Gaussian primitives. Each pixel in the ground image is represented by a 3D Gaussian with semantic and spatial features, which are synthesized into a BEV feature map for relative pose estimation. Additionally, to address challenges with panoramic query images, we introduce an icosphere-based supervision strategy for the Gaussian primitives. We validate our method on the widely used KITTI and VIGOR datasets, which include both pinhole and panoramic query images. Experimental results show that BevSplat significantly improves localization accuracy over prior approaches.
<div id='section'>Paperid: <span id='pid'>417, <a href='https://arxiv.org/pdf/2501.19048.pdf' target='_blank'>https://arxiv.org/pdf/2501.19048.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rita Pereira, M. Rita Verdelho, Catarina Barata, Carlos Santiago
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.19048">The Role of Graph-based MIL and Interventional Training in the Generalization of WSI Classifiers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Whole Slide Imaging (WSI), which involves high-resolution digital scans of pathology slides, has become the gold standard for cancer diagnosis, but its gigapixel resolution and the scarcity of annotated datasets present challenges for deep learning models. Multiple Instance Learning (MIL), a widely-used weakly supervised approach, bypasses the need for patch-level annotations. However, conventional MIL methods overlook the spatial relationships between patches, which are crucial for tasks such as cancer grading and diagnosis. To address this, graph-based approaches have gained prominence by incorporating spatial information through node connections. Despite their potential, both MIL and graph-based models are vulnerable to learning spurious associations, like color variations in WSIs, affecting their robustness. In this dissertation, we conduct an extensive comparison of multiple graph construction techniques, MIL models, graph-MIL approaches, and interventional training, introducing a new framework, Graph-based Multiple Instance Learning with Interventional Training (GMIL-IT), for WSI classification. We evaluate their impact on model generalization through domain shift analysis and demonstrate that graph-based models alone achieve the generalization initially anticipated from interventional training. Our code is available here: github.com/ritamartinspereira/GMIL-IT
<div id='section'>Paperid: <span id='pid'>418, <a href='https://arxiv.org/pdf/2501.11124.pdf' target='_blank'>https://arxiv.org/pdf/2501.11124.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Quan Zhang, Yuxin Qi, Xi Tang, Rui Yuan, Xi Lin, Ke Zhang, Chun Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.11124">Rethinking Pseudo-Label Guided Learning for Weakly Supervised Temporal Action Localization from the Perspective of Noise Correction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pseudo-label learning methods have been widely applied in weakly-supervised temporal action localization. Existing works directly utilize weakly-supervised base model to generate instance-level pseudo-labels for training the fully-supervised detection head. We argue that the noise in pseudo-labels would interfere with the learning of fully-supervised detection head, leading to significant performance leakage. Issues with noisy labels include:(1) inaccurate boundary localization; (2) undetected short action clips; (3) multiple adjacent segments incorrectly detected as one segment. To target these issues, we introduce a two-stage noisy label learning strategy to harness every potential useful signal in noisy labels. First, we propose a frame-level pseudo-label generation model with a context-aware denoising algorithm to refine the boundaries. Second, we introduce an online-revised teacher-student framework with a missing instance compensation module and an ambiguous instance correction module to solve the short-action-missing and many-to-one problems. Besides, we apply a high-quality pseudo-label mining loss in our online-revised teacher-student framework to add different weights to the noisy labels to train more effectively. Our model outperforms the previous state-of-the-art method in detection accuracy and inference speed greatly upon the THUMOS14 and ActivityNet v1.2 benchmarks.
<div id='section'>Paperid: <span id='pid'>419, <a href='https://arxiv.org/pdf/2412.14295.pdf' target='_blank'>https://arxiv.org/pdf/2412.14295.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anna Manasyan, Maximilian Seitzer, Filip Radovic, Georg Martius, Andrii Zadaianchuk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14295">Temporally Consistent Object-Centric Learning by Contrasting Slots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsupervised object-centric learning from videos is a promising approach to extract structured representations from large, unlabeled collections of videos. To support downstream tasks like autonomous control, these representations must be both compositional and temporally consistent. Existing approaches based on recurrent processing often lack long-term stability across frames because their training objective does not enforce temporal consistency. In this work, we introduce a novel object-level temporal contrastive loss for video object-centric models that explicitly promotes temporal consistency. Our method significantly improves the temporal consistency of the learned object-centric representations, yielding more reliable video decompositions that facilitate challenging downstream tasks such as unsupervised object dynamics prediction. Furthermore, the inductive bias added by our loss strongly improves object discovery, leading to state-of-the-art results on both synthetic and real-world datasets, outperforming even weakly-supervised methods that leverage motion masks as additional cues.
<div id='section'>Paperid: <span id='pid'>420, <a href='https://arxiv.org/pdf/2411.08466.pdf' target='_blank'>https://arxiv.org/pdf/2411.08466.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Quan Zhang, Jinwei Fang, Rui Yuan, Xi Tang, Yuxin Qi, Ke Zhang, Chun Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08466">Weakly Supervised Temporal Action Localization via Dual-Prior Collaborative Learning Guided by Multimodal Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent breakthroughs in Multimodal Large Language Models (MLLMs) have gained significant recognition within the deep learning community, where the fusion of the Video Foundation Models (VFMs) and Large Language Models(LLMs) has proven instrumental in constructing robust video understanding systems, effectively surmounting constraints associated with predefined visual tasks. These sophisticated MLLMs exhibit remarkable proficiency in comprehending videos, swiftly attaining unprecedented performance levels across diverse benchmarks. However, their operation demands substantial memory and computational resources, underscoring the continued importance of traditional models in video comprehension tasks. In this paper, we introduce a novel learning paradigm termed MLLM4WTAL. This paradigm harnesses the potential of MLLM to offer temporal action key semantics and complete semantic priors for conventional Weakly-supervised Temporal Action Localization (WTAL) methods. MLLM4WTAL facilitates the enhancement of WTAL by leveraging MLLM guidance. It achieves this by integrating two distinct modules: Key Semantic Matching (KSM) and Complete Semantic Reconstruction (CSR). These modules work in tandem to effectively address prevalent issues like incomplete and over-complete outcomes common in WTAL methods. Rigorous experiments are conducted to validate the efficacy of our proposed approach in augmenting the performance of various heterogeneous WTAL models.
<div id='section'>Paperid: <span id='pid'>421, <a href='https://arxiv.org/pdf/2409.05199.pdf' target='_blank'>https://arxiv.org/pdf/2409.05199.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giannis Karamanolakis, Daniel Hsu, Luis Gravano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.05199">Interactive Machine Teaching by Labeling Rules and Instances</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised learning aims to reduce the cost of labeling data by using expert-designed labeling rules. However, existing methods require experts to design effective rules in a single shot, which is difficult in the absence of proper guidance and tooling. Therefore, it is still an open question whether experts should spend their limited time writing rules or instead providing instance labels via active learning. In this paper, we investigate how to exploit an expert's limited time to create effective supervision. First, to develop practical guidelines for rule creation, we conduct an exploratory analysis of diverse collections of existing expert-designed rules and find that rule precision is more important than coverage across datasets. Second, we compare rule creation to individual instance labeling via active learning and demonstrate the importance of both across 6 datasets. Third, we propose an interactive learning framework, INTERVAL, that achieves efficiency by automatically extracting candidate rules based on rich patterns (e.g., by prompting a language model), and effectiveness by soliciting expert feedback on both candidate rules and individual instances. Across 6 datasets, INTERVAL outperforms state-of-the-art weakly supervised approaches by 7% in F1. Furthermore, it requires as few as 10 queries for expert feedback to reach F1 values that existing active learning methods cannot match even with 100 queries.
<div id='section'>Paperid: <span id='pid'>422, <a href='https://arxiv.org/pdf/2409.03236.pdf' target='_blank'>https://arxiv.org/pdf/2409.03236.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenglizhao Chen, Xinyu Liu, Mengke Song, Luming Li, Xu Yu, Shanchen Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.03236">Unveiling Context-Related Anomalies: Knowledge Graph Empowered Decoupling of Scene and Action for Human-Related Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detecting anomalies in human-related videos is crucial for surveillance applications. Current methods primarily include appearance-based and action-based techniques. Appearance-based methods rely on low-level visual features such as color, texture, and shape. They learn a large number of pixel patterns and features related to known scenes during training, making them effective in detecting anomalies within these familiar contexts. However, when encountering new or significantly changed scenes, i.e., unknown scenes, they often fail because existing SOTA methods do not effectively capture the relationship between actions and their surrounding scenes, resulting in low generalization. In contrast, action-based methods focus on detecting anomalies in human actions but are usually less informative because they tend to overlook the relationship between actions and their scenes, leading to incorrect detection. For instance, the normal event of running on the beach and the abnormal event of running on the street might both be considered normal due to the lack of scene information. In short, current methods struggle to integrate low-level visual and high-level action features, leading to poor anomaly detection in varied and complex scenes. To address this challenge, we propose a novel decoupling-based architecture for human-related video anomaly detection (DecoAD). DecoAD significantly improves the integration of visual and action features through the decoupling and interweaving of scenes and actions, thereby enabling a more intuitive and accurate understanding of complex behaviors and scenes. DecoAD supports fully supervised, weakly supervised, and unsupervised settings.
<div id='section'>Paperid: <span id='pid'>423, <a href='https://arxiv.org/pdf/2512.06171.pdf' target='_blank'>https://arxiv.org/pdf/2512.06171.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jessica Plassmann, Nicolas Schuler, Michael Schuth, Georg von Freymann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.06171">Automated Annotation of Shearographic Measurements Enabling Weakly Supervised Defect Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Shearography is an interferometric technique sensitive to surface displacement gradients, providing high sensitivity for detecting subsurface defects in safety-critical components. A key limitation to industrial adoption is the lack of high-quality annotated datasets, since manual labeling remains labor-intensive, subjective, and difficult to standardize. We introduce an automated workflow that generates defect annotations from shearography measurements using deep learning, producing high-resolution segmentation and bounding-box labels. Evaluation against expert-labeled data demonstrates sufficient accuracy to enable weakly supervised training, reducing manual effort and supporting scalable dataset creation for robust defect detection.
<div id='section'>Paperid: <span id='pid'>424, <a href='https://arxiv.org/pdf/2512.05922.pdf' target='_blank'>https://arxiv.org/pdf/2512.05922.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Khang Le, Anh Mai Vu, Thi Kim Trang Vo, Ha Thach, Ngoc Bui Lam Quang, Thanh-Huy Nguyen, Minh H. N. Le, Zhu Han, Chandra Mohan, Hien Van Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.05922">LPD: Learnable Prototypes with Diversity Regularization for Weakly Supervised Histopathology Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation (WSSS) in histopathology reduces pixel-level labeling by learning from image-level labels, but it is hindered by inter-class homogeneity, intra-class heterogeneity, and CAM-induced region shrinkage (global pooling-based class activation maps whose activations highlight only the most distinctive areas and miss nearby class regions). Recent works address these challenges by constructing a clustering prototype bank and then refining masks in a separate stage; however, such two-stage pipelines are costly, sensitive to hyperparameters, and decouple prototype discovery from segmentation learning, limiting their effectiveness and efficiency. We propose a cluster-free, one-stage learnable-prototype framework with diversity regularization to enhance morphological intra-class heterogeneity coverage. Our approach achieves state-of-the-art (SOTA) performance on BCSS-WSSS, outperforming prior methods in mIoU and mDice. Qualitative segmentation maps show sharper boundaries and fewer mislabels, and activation heatmaps further reveal that, compared with clustering-based prototypes, our learnable prototypes cover more diverse and complementary regions within each class, providing consistent qualitative evidence for their effectiveness.
<div id='section'>Paperid: <span id='pid'>425, <a href='https://arxiv.org/pdf/2511.15396.pdf' target='_blank'>https://arxiv.org/pdf/2511.15396.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simon Boeder, Fabian Gigengack, Simon Roesler, Holger Caesar, Benjamin Risse
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.15396">ShelfOcc: Native 3D Supervision beyond LiDAR for Vision-Based Occupancy Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent progress in self- and weakly supervised occupancy estimation has largely relied on 2D projection or rendering-based supervision, which suffers from geometric inconsistencies and severe depth bleeding. We thus introduce ShelfOcc, a vision-only method that overcomes these limitations without relying on LiDAR. ShelfOcc brings supervision into native 3D space by generating metrically consistent semantic voxel labels from video, enabling true 3D supervision without any additional sensors or manual 3D annotations. While recent vision-based 3D geometry foundation models provide a promising source of prior knowledge, they do not work out of the box as a prediction due to sparse or noisy and inconsistent geometry, especially in dynamic driving scenes. Our method introduces a dedicated framework that mitigates these issues by filtering and accumulating static geometry consistently across frames, handling dynamic content and propagating semantic information into a stable voxel representation. This data-centric shift in supervision for weakly/shelf-supervised occupancy estimation allows the use of essentially any SOTA occupancy model architecture without relying on LiDAR data. We argue that such high-quality supervision is essential for robust occupancy learning and constitutes an important complementary avenue to architectural innovation. On the Occ3D-nuScenes benchmark, ShelfOcc substantially outperforms all previous weakly/shelf-supervised methods (up to a 34% relative improvement), establishing a new data-driven direction for LiDAR-free 3D scene understanding.
<div id='section'>Paperid: <span id='pid'>426, <a href='https://arxiv.org/pdf/2511.13276.pdf' target='_blank'>https://arxiv.org/pdf/2511.13276.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Noam Tsfaty, Avishai Weizman, Liav Cohen, Moshe Tshuva, Yehudit Aperstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13276">Recognition of Abnormal Events in Surveillance Videos using Weakly Supervised Dual-Encoder Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address the challenge of detecting rare and diverse anomalies in surveillance videos using only video-level supervision. Our dual-backbone framework combines convolutional and transformer representations through top-k pooling, achieving 90.7% area under the curve (AUC) on the UCF-Crime dataset.
<div id='section'>Paperid: <span id='pid'>427, <a href='https://arxiv.org/pdf/2511.12269.pdf' target='_blank'>https://arxiv.org/pdf/2511.12269.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rupam Mukherjee, Rajkumar Daniel, Soujanya Hazra, Shirin Dasgupta, Subhamoy Mandal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.12269">RAA-MIL: A Novel Framework for Classification of Oral Cytology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cytology is a valuable tool for early detection of oral squamous cell carcinoma (OSCC). However, manual examination of cytology whole slide images (WSIs) is slow, subjective, and depends heavily on expert pathologists. To address this, we introduce the first weakly supervised deep learning framework for patient-level diagnosis of oral cytology whole slide images, leveraging the newly released Oral Cytology Dataset [1], which provides annotated cytology WSIs from ten medical centres across India. Each patient case is represented as a bag of cytology patches and assigned a diagnosis label (Healthy, Benign, Oral Potentially Malignant Disorders (OPMD), OSCC) by an in-house expert pathologist. These patient-level weak labels form a new extension to the dataset. We evaluate a baseline multiple-instance learning (MIL) model and a proposed Region-Affinity Attention MIL (RAA-MIL) that models spatial relationships between regions within each slide. The RAA-MIL achieves an average accuracy of 72.7%, weighted F1-score of 0.69 on an unseen test set, outperforming the baseline. This study establishes the first patient-level weakly supervised benchmark for oral cytology and moves toward reliable AI-assisted digital pathology.
<div id='section'>Paperid: <span id='pid'>428, <a href='https://arxiv.org/pdf/2510.12827.pdf' target='_blank'>https://arxiv.org/pdf/2510.12827.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md. Nayeem, Md Shamse Tabrej, Kabbojit Jit Deb, Shaonti Goswami, Md. Azizul Hakim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.12827">Automatic Speech Recognition in the Modern Era: Architectures, Training, and Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic Speech Recognition (ASR) has undergone a profound transformation over the past decade, driven by advances in deep learning. This survey provides a comprehensive overview of the modern era of ASR, charting its evolution from traditional hybrid systems, such as Gaussian Mixture Model-Hidden Markov Models (GMM-HMMs) and Deep Neural Network-HMMs (DNN-HMMs), to the now-dominant end-to-end neural architectures. We systematically review the foundational end-to-end paradigms: Connectionist Temporal Classification (CTC), attention-based encoder-decoder models, and the Recurrent Neural Network Transducer (RNN-T), which established the groundwork for fully integrated speech-to-text systems. We then detail the subsequent architectural shift towards Transformer and Conformer models, which leverage self-attention to capture long-range dependencies with high computational efficiency. A central theme of this survey is the parallel revolution in training paradigms. We examine the progression from fully supervised learning, augmented by techniques like SpecAugment, to the rise of self-supervised learning (SSL) with foundation models such as wav2vec 2.0, which drastically reduce the reliance on transcribed data. Furthermore, we analyze the impact of largescale, weakly supervised models like Whisper, which achieve unprecedented robustness through massive data diversity. The paper also covers essential ecosystem components, including key datasets and benchmarks (e.g., LibriSpeech, Switchboard, CHiME), standard evaluation metrics (e.g., Word Error Rate), and critical considerations for real-world deployment, such as streaming inference, on-device efficiency, and the ethical imperatives of fairness and robustness. We conclude by outlining open challenges and future research directions.
<div id='section'>Paperid: <span id='pid'>429, <a href='https://arxiv.org/pdf/2510.04477.pdf' target='_blank'>https://arxiv.org/pdf/2510.04477.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soo Yong Kim, Suin Cho, Vincent-Daniel Yun, Gyeongyeon Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04477">MedCLM: Learning to Localize and Reason via a CoT-Curriculum in Medical Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Bridging clinical diagnostic reasoning with AI remains a central challenge in medical imaging. We introduce MedCLM, an automated pipeline that converts detection datasets into large-scale medical visual question answering (VQA) data with Chain-of-Thought (CoT) reasoning by linking lesion boxes to organ segmentation and structured rationales. These contextual signals enable medical vision-language models to generate question-answer pairs with step-by-step reasoning. To utilize this data effectively, we propose an Integrated CoT-Curriculum Strategy composed of an Easy stage with explicit lesion boxes for visual grounding, a Medium stage that encourages implicit localization, and a Hard stage for weakly supervised reasoning. Experimental results demonstrate that MedCLM attains state-of-the-art performance on several medical VQA benchmarks, providing a scalable framework for developing clinically aligned medical vision-language models.
<div id='section'>Paperid: <span id='pid'>430, <a href='https://arxiv.org/pdf/2509.10184.pdf' target='_blank'>https://arxiv.org/pdf/2509.10184.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leen Almajed, Abeer ALdayel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10184">Incongruent Positivity: When Miscalibrated Positivity Undermines Online Supportive Conversations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In emotionally supportive conversations, well-intended positivity can sometimes misfire, leading to responses that feel dismissive, minimizing, or unrealistically optimistic. We examine this phenomenon of incongruent positivity as miscalibrated expressions of positive support in both human and LLM generated responses. To this end, we collected real user-assistant dialogues from Reddit across a range of emotional intensities and generated additional responses using large language models for the same context. We categorize these conversations by intensity into two levels: Mild, which covers relationship tension and general advice, and Severe, which covers grief and anxiety conversations. This level of categorization enables a comparative analysis of how supportive responses vary across lower and higher stakes contexts. Our analysis reveals that LLMs are more prone to unrealistic positivity through dismissive and minimizing tone, particularly in high-stakes contexts. To further study the underlying dimensions of this phenomenon, we finetune LLMs on datasets with strong and weak emotional reactions. Moreover, we developed a weakly supervised multilabel classifier ensemble (DeBERTa and MentalBERT) that shows improved detection of incongruent positivity types across two sorts of concerns (Mild and Severe). Our findings shed light on the need to move beyond merely generating generic positive responses and instead study the congruent support measures to balance positive affect with emotional acknowledgment. This approach offers insights into aligning large language models with affective expectations in the online supportive dialogue, paving the way toward context-aware and trust preserving online conversation systems.
<div id='section'>Paperid: <span id='pid'>431, <a href='https://arxiv.org/pdf/2509.04491.pdf' target='_blank'>https://arxiv.org/pdf/2509.04491.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinnian Zhao, Hugo Van Hamme
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04491">Refining Transcripts With TV Subtitles by Prompt-Based Weakly Supervised Training of ASR</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study proposes a novel approach to using TV subtitles within a weakly supervised (WS) Automatic Speech Recognition (ASR) framework. Although TV subtitles are readily available, their imprecise alignment with corresponding audio limits their applicability as supervised targets for verbatim transcription. Rather than using subtitles as direct supervision signals, our method reimagines them as context-rich prompts. This design enables the model to handle discrepancies between spoken audio and subtitle text. Instead, generated pseudo transcripts become the primary targets, with subtitles acting as guiding cues for iterative refinement. To further enhance the process, we introduce a weighted attention mechanism that emphasizes relevant subtitle tokens during inference. Our experiments demonstrate significant improvements in transcription accuracy, highlighting the effectiveness of the proposed method in refining transcripts. These enhanced pseudo-labeled datasets provide high-quality foundational resources for training robust ASR systems.
<div id='section'>Paperid: <span id='pid'>432, <a href='https://arxiv.org/pdf/2509.01214.pdf' target='_blank'>https://arxiv.org/pdf/2509.01214.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yizhe Yuan, Bingsen Xue, Bangzheng Pu, Chengxiang Wang, Cheng Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01214">PRINTER:Deformation-Aware Adversarial Learning for Virtual IHC Staining with In Situ Fidelity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tumor spatial heterogeneity analysis requires precise correlation between Hematoxylin and Eosin H&E morphology and immunohistochemical (IHC) biomarker expression, yet current methods suffer from spatial misalignment in consecutive sections, severely compromising in situ pathological interpretation. In order to obtain a more accurate virtual staining pattern, We propose PRINTER, a weakly-supervised framework that integrates PRototype-drIven content and staiNing patTERn decoupling and deformation-aware adversarial learning strategies designed to accurately learn IHC staining patterns while preserving H&E staining details. Our approach introduces three key innovations: (1) A prototype-driven staining pattern transfer with explicit content-style decoupling; and (2) A cyclic registration-synthesis framework GapBridge that bridges H&E and IHC domains through deformable structural alignment, where registered features guide cross-modal style transfer while synthesized outputs iteratively refine the registration;(3) Deformation-Aware Adversarial Learning: We propose a training framework where a generator and deformation-aware registration network jointly adversarially optimize a style-focused discriminator. Extensive experiments demonstrate that PRINTER effectively achieves superior performance in preserving H&E staining details and virtual staining fidelity, outperforming state-of-the-art methods. Our work provides a robust and scalable solution for virtual staining, advancing the field of computational pathology.
<div id='section'>Paperid: <span id='pid'>433, <a href='https://arxiv.org/pdf/2508.12290.pdf' target='_blank'>https://arxiv.org/pdf/2508.12290.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chor Boon Tan, Conghui Hu, Gim Hee Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12290">CLAIR: CLIP-Aided Weakly Supervised Zero-Shot Cross-Domain Image Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent growth of large foundation models that can easily generate pseudo-labels for huge quantity of unlabeled data makes unsupervised Zero-Shot Cross-Domain Image Retrieval (UZS-CDIR) less relevant. In this paper, we therefore turn our attention to weakly supervised ZS-CDIR (WSZS-CDIR) with noisy pseudo labels generated by large foundation models such as CLIP. To this end, we propose CLAIR to refine the noisy pseudo-labels with a confidence score from the similarity between the CLIP text and image features. Furthermore, we design inter-instance and inter-cluster contrastive losses to encode images into a class-aware latent space, and an inter-domain contrastive loss to alleviate domain discrepancies. We also learn a novel cross-domain mapping function in closed-form, using only CLIP text embeddings to project image features from one domain to another, thereby further aligning the image features for retrieval. Finally, we enhance the zero-shot generalization ability of our CLAIR to handle novel categories by introducing an extra set of learnable prompts. Extensive experiments are carried out using TUBerlin, Sketchy, Quickdraw, and DomainNet zero-shot datasets, where our CLAIR consistently shows superior performance compared to existing state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>434, <a href='https://arxiv.org/pdf/2505.18368.pdf' target='_blank'>https://arxiv.org/pdf/2505.18368.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yike Zhang, Jack H. Noble
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18368">Weakly-supervised Mamba-Based Mastoidectomy Shape Prediction for Cochlear Implant Surgery Using 3D T-Distribution Loss</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cochlear implant surgery is a treatment for individuals with severe hearing loss. It involves inserting an array of electrodes inside the cochlea to electrically stimulate the auditory nerve and restore hearing sensation. A crucial step in this procedure is mastoidectomy, a surgical intervention that removes part of the mastoid region of the temporal bone, providing a critical pathway to the cochlea for electrode placement. Accurate prediction of the mastoidectomy region from preoperative imaging assists presurgical planning, reduces surgical risks, and improves surgical outcomes. In previous work, a self-supervised network was introduced to predict the mastoidectomy region using only preoperative CT scans. While promising, the method suffered from suboptimal robustness, limiting its practical application. To address this limitation, we propose a novel weakly-supervised Mamba-based framework to predict accurate mastoidectomy regions directly from preoperative CT scans. Our approach utilizes a 3D T-Distribution loss function inspired by the Student-t distribution, which effectively handles the complex geometric variability inherent in mastoidectomy shapes. Weak supervision is achieved using the segmentation results from the prior self-supervised network to eliminate the need for manual data cleaning or labeling throughout the training process. The proposed method is extensively evaluated against state-of-the-art approaches, demonstrating superior performance in predicting accurate and clinically relevant mastoidectomy regions. Our findings highlight the robustness and efficiency of the weakly-supervised learning framework with the proposed novel 3D T-Distribution loss.
<div id='section'>Paperid: <span id='pid'>435, <a href='https://arxiv.org/pdf/2505.13911.pdf' target='_blank'>https://arxiv.org/pdf/2505.13911.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruijie Zhao, Zuopeng Tan, Xiao Xue, Longfei Zhao, Bing Li, Zicheng Liao, Ying Ming, Jiaru Wang, Ran Xiao, Sirong Piao, Rui Zhao, Qiqi Xu, Wei Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13911">Bronchovascular Tree-Guided Weakly Supervised Learning Method for Pulmonary Segment Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pulmonary segment segmentation is crucial for cancer localization and surgical planning. However, the pixel-wise annotation of pulmonary segments is laborious, as the boundaries between segments are indistinguishable in medical images. To this end, we propose a weakly supervised learning (WSL) method, termed Anatomy-Hierarchy Supervised Learning (AHSL), which consults the precise clinical anatomical definition of pulmonary segments to perform pulmonary segment segmentation. Since pulmonary segments reside within the lobes and are determined by the bronchovascular tree, i.e., artery, airway and vein, the design of the loss function is founded on two principles. First, segment-level labels are utilized to directly supervise the output of the pulmonary segments, ensuring that they accurately encompass the appropriate bronchovascular tree. Second, lobe-level supervision indirectly oversees the pulmonary segment, ensuring their inclusion within the corresponding lobe. Besides, we introduce a two-stage segmentation strategy that incorporates bronchovascular priori information. Furthermore, a consistency loss is proposed to enhance the smoothness of segment boundaries, along with an evaluation metric designed to measure the smoothness of pulmonary segment boundaries. Visual inspection and evaluation metrics from experiments conducted on a private dataset demonstrate the effectiveness of our method.
<div id='section'>Paperid: <span id='pid'>436, <a href='https://arxiv.org/pdf/2505.09615.pdf' target='_blank'>https://arxiv.org/pdf/2505.09615.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yung-Hsuan Lai, Janek Ebbers, Yu-Chiang Frank Wang, FranÃ§ois Germain, Michael Jeffrey Jones, Moitreya Chatterjee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09615">UWAV: Uncertainty-weighted Weakly-supervised Audio-Visual Video Parsing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-Visual Video Parsing (AVVP) entails the challenging task of localizing both uni-modal events (i.e., those occurring exclusively in either the visual or acoustic modality of a video) and multi-modal events (i.e., those occurring in both modalities concurrently). Moreover, the prohibitive cost of annotating training data with the class labels of all these events, along with their start and end times, imposes constraints on the scalability of AVVP techniques unless they can be trained in a weakly-supervised setting, where only modality-agnostic, video-level labels are available in the training data. To this end, recently proposed approaches seek to generate segment-level pseudo-labels to better guide model training. However, the absence of inter-segment dependencies when generating these pseudo-labels and the general bias towards predicting labels that are absent in a segment limit their performance. This work proposes a novel approach towards overcoming these weaknesses called Uncertainty-weighted Weakly-supervised Audio-visual Video Parsing (UWAV). Additionally, our innovative approach factors in the uncertainty associated with these estimated pseudo-labels and incorporates a feature mixup based training regularization for improved training. Empirical results show that UWAV outperforms state-of-the-art methods for the AVVP task on multiple metrics, across two different datasets, attesting to its effectiveness and generalizability.
<div id='section'>Paperid: <span id='pid'>437, <a href='https://arxiv.org/pdf/2504.14300.pdf' target='_blank'>https://arxiv.org/pdf/2504.14300.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Liang, Hao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14300">Learning and Generating Diverse Residential Load Patterns Using GAN with Weakly-Supervised Training and Weight Selection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The scarcity of high-quality residential load data can pose obstacles for decarbonizing the residential sector as well as effective grid planning and operation. The above challenges have motivated research into generating synthetic load data, but existing methods faced limitations in terms of scalability, diversity, and similarity. This paper proposes a Generative Adversarial Network-based Synthetic Residential Load Pattern (RLP-GAN) generation model, a novel weakly-supervised GAN framework, leveraging an over-complete autoencoder to capture dependencies within complex and diverse load patterns and learn household-level data distribution at scale. We incorporate a model weight selection method to address the mode collapse problem and generate load patterns with high diversity. We develop a holistic evaluation method to validate the effectiveness of RLP-GAN using real-world data of 417 households. The results demonstrate that RLP-GAN outperforms state-of-the-art models in capturing temporal dependencies and generating load patterns with higher similarity to real data. Furthermore, we have publicly released the RLP-GAN generated synthetic dataset, which comprises one million synthetic residential load pattern profiles.
<div id='section'>Paperid: <span id='pid'>438, <a href='https://arxiv.org/pdf/2503.13693.pdf' target='_blank'>https://arxiv.org/pdf/2503.13693.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eitan Shaar, Ariel Shaulov, Gal Chechik, Lior Wolf
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13693">Adapting to the Unknown: Training-Free Audio-Visual Event Perception with Dynamic Thresholds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the domain of audio-visual event perception, which focuses on the temporal localization and classification of events across distinct modalities (audio and visual), existing approaches are constrained by the vocabulary available in their training data. This limitation significantly impedes their capacity to generalize to novel, unseen event categories. Furthermore, the annotation process for this task is labor-intensive, requiring extensive manual labeling across modalities and temporal segments, limiting the scalability of current methods. Current state-of-the-art models ignore the shifts in event distributions over time, reducing their ability to adjust to changing video dynamics. Additionally, previous methods rely on late fusion to combine audio and visual information. While straightforward, this approach results in a significant loss of multimodal interactions. To address these challenges, we propose Audio-Visual Adaptive Video Analysis ($\text{AV}^2\text{A}$), a model-agnostic approach that requires no further training and integrates a score-level fusion technique to retain richer multimodal interactions. $\text{AV}^2\text{A}$ also includes a within-video label shift algorithm, leveraging input video data and predictions from prior frames to dynamically adjust event distributions for subsequent frames. Moreover, we present the first training-free, open-vocabulary baseline for audio-visual event perception, demonstrating that $\text{AV}^2\text{A}$ achieves substantial improvements over naive training-free baselines. We demonstrate the effectiveness of $\text{AV}^2\text{A}$ on both zero-shot and weakly-supervised state-of-the-art methods, achieving notable improvements in performance metrics over existing approaches.
<div id='section'>Paperid: <span id='pid'>439, <a href='https://arxiv.org/pdf/2502.18883.pdf' target='_blank'>https://arxiv.org/pdf/2502.18883.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanfu Yan, Viet Duong, Huajie Shao, Denys Poshyvanyk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18883">Towards More Trustworthy Deep Code Models by Enabling Out-of-Distribution Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Numerous machine learning (ML) models have been developed, including those for software engineering (SE) tasks, under the assumption that training and testing data come from the same distribution. However, training and testing distributions often differ, as training datasets rarely encompass the entire distribution, while testing distribution tends to shift over time. Hence, when confronted with out-of-distribution (OOD) instances that differ from the training data, a reliable and trustworthy SE ML model must be capable of detecting them to either abstain from making predictions, or potentially forward these OODs to appropriate models handling other categories or tasks.
  In this paper, we develop two types of SE-specific OOD detection models, unsupervised and weakly-supervised OOD detection for code. The unsupervised OOD detection approach is trained solely on in-distribution samples while the weakly-supervised approach utilizes a tiny number of OOD samples to further enhance the detection performance in various OOD scenarios. Extensive experimental results demonstrate that our proposed methods significantly outperform the baselines in detecting OOD samples from four different scenarios simultaneously and also positively impact a main code understanding task.
<div id='section'>Paperid: <span id='pid'>440, <a href='https://arxiv.org/pdf/2502.17288.pdf' target='_blank'>https://arxiv.org/pdf/2502.17288.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simon Boeder, Fabian Gigengack, Benjamin Risse
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17288">GaussianFlowOcc: Sparse and Weakly Supervised Occupancy Estimation using Gaussian Splatting and Temporal Flow</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Occupancy estimation has become a prominent task in 3D computer vision, particularly within the autonomous driving community. In this paper, we present a novel approach to occupancy estimation, termed GaussianFlowOcc, which is inspired by Gaussian Splatting and replaces traditional dense voxel grids with a sparse 3D Gaussian representation. Our efficient model architecture based on a Gaussian Transformer significantly reduces computational and memory requirements by eliminating the need for expensive 3D convolutions used with inefficient voxel-based representations that predominantly represent empty 3D spaces. GaussianFlowOcc effectively captures scene dynamics by estimating temporal flow for each Gaussian during the overall network training process, offering a straightforward solution to a complex problem that is often neglected by existing methods. Moreover, GaussianFlowOcc is designed for scalability, as it employs weak supervision and does not require costly dense 3D voxel annotations based on additional data (e.g., LiDAR). Through extensive experimentation, we demonstrate that GaussianFlowOcc significantly outperforms all previous methods for weakly supervised occupancy estimation on the nuScenes dataset while featuring an inference speed that is 50 times faster than current SOTA.
<div id='section'>Paperid: <span id='pid'>441, <a href='https://arxiv.org/pdf/2502.12484.pdf' target='_blank'>https://arxiv.org/pdf/2502.12484.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junrui Wen, Yifei Li, Bart Selman, Kun He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.12484">LocalEscaper: A Weakly-supervised Framework with Regional Reconstruction for Scalable Neural TSP Solvers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural solvers have shown significant potential in solving the Traveling Salesman Problem (TSP), yet current approaches face significant challenges. Supervised learning (SL)-based solvers require large amounts of high-quality labeled data, while reinforcement learning (RL)-based solvers, though less dependent on such data, often suffer from inefficiencies. To address these limitations, we propose LocalEscaper, a novel weakly-supervised learning framework for large-scale TSP. LocalEscaper effectively combines the advantages of both SL and RL, enabling effective training on datasets with low-quality labels. To further enhance solution quality, we introduce a regional reconstruction strategy, which is the key technique of this paper and mitigates the local-optima problem common in existing local reconstruction methods. Experimental results on both synthetic and real-world datasets demonstrate that LocalEscaper outperforms existing neural solvers, achieving remarkable results.
<div id='section'>Paperid: <span id='pid'>442, <a href='https://arxiv.org/pdf/2502.05129.pdf' target='_blank'>https://arxiv.org/pdf/2502.05129.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Van Brunt, Justin Kay, Timm Haucke, Pietro Perona, Grant Van Horn, Sara Beery
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05129">Counting Fish with Temporal Representations of Sonar Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate estimates of salmon escapement - the number of fish migrating upstream to spawn - are key data for conservation and fishery management. Existing methods for salmon counting using high-resolution imaging sonar hardware are non-invasive and compatible with computer vision processing. Prior work in this area has utilized object detection and tracking based methods for automated salmon counting. However, these techniques remain inaccessible to many sonar deployment sites due to limited compute and connectivity in the field. We propose an alternative lightweight computer vision method for fish counting based on analyzing echograms - temporal representations that compress several hundred frames of imaging sonar video into a single image. We predict upstream and downstream counts within 200-frame time windows directly from echograms using a ResNet-18 model, and propose a set of domain-specific image augmentations and a weakly-supervised training protocol to further improve results. We achieve a count error of 23% on representative data from the Kenai River in Alaska, demonstrating the feasibility of our approach.
<div id='section'>Paperid: <span id='pid'>443, <a href='https://arxiv.org/pdf/2502.03212.pdf' target='_blank'>https://arxiv.org/pdf/2502.03212.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jakob Poncelet, Hugo Van hamme
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03212">Leveraging Broadcast Media Subtitle Transcripts for Automatic Speech Recognition and Subtitling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent advancement of speech recognition technology has been driven by large-scale datasets and attention-based architectures, but many challenges still remain, especially for low-resource languages and dialects. This paper explores the integration of weakly supervised transcripts from TV subtitles into automatic speech recognition (ASR) systems, aiming to improve both verbatim transcriptions and automatically generated subtitles. To this end, verbatim data and subtitles are regarded as different domains or languages, due to their distinct characteristics. We propose and compare several end-to-end architectures that are designed to jointly model both modalities with separate or shared encoders and decoders. The proposed methods are able to jointly generate a verbatim transcription and a subtitle. Evaluation on Flemish (Belgian Dutch) demonstrates that a model with cascaded encoders and separate decoders allows to represent the differences between the two data types most efficiently while improving on both domains. Despite differences in domain and linguistic variations, combining verbatim transcripts with subtitle data leads to notable ASR improvements without the need for extensive preprocessing. Additionally, experiments with a large-scale subtitle dataset show the scalability of the proposed approach. The methods not only improve ASR accuracy but also generate subtitles that closely match standard written text, offering several potential applications.
<div id='section'>Paperid: <span id='pid'>444, <a href='https://arxiv.org/pdf/2501.04666.pdf' target='_blank'>https://arxiv.org/pdf/2501.04666.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nannan Li, Kevin J. Shih, Bryan A. Plummer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04666">Enhancing Virtual Try-On with Synthetic Pairs and Error-Aware Noise Scheduling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Given an isolated garment image in a canonical product view and a separate image of a person, the virtual try-on task aims to generate a new image of the person wearing the target garment. Prior virtual try-on works face two major challenges in achieving this goal: a) the paired (human, garment) training data has limited availability; b) generating textures on the human that perfectly match that of the prompted garment is difficult, often resulting in distorted text and faded textures. Our work explores ways to tackle these issues through both synthetic data as well as model refinement. We introduce a garment extraction model that generates (human, synthetic garment) pairs from a single image of a clothed individual. The synthetic pairs can then be used to augment the training of virtual try-on. We also propose an Error-Aware Refinement-based SchrÃ¶dinger Bridge (EARSB) that surgically targets localized generation errors for correcting the output of a base virtual try-on model. To identify likely errors, we propose a weakly-supervised error classifier that localizes regions for refinement, subsequently augmenting the SchrÃ¶dinger Bridge's noise schedule with its confidence heatmap. Experiments on VITON-HD and DressCode-Upper demonstrate that our synthetic data augmentation enhances the performance of prior work, while EARSB improves the overall image quality. In user studies, our model is preferred by the users in an average of 59% of cases.
<div id='section'>Paperid: <span id='pid'>445, <a href='https://arxiv.org/pdf/2412.00426.pdf' target='_blank'>https://arxiv.org/pdf/2412.00426.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ayoub Hammal, Benno Uthayasooriyar, Caio Corro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.00426">Few-Shot Domain Adaptation for Named-Entity Recognition via Joint Constrained k-Means and Subspace Selection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Named-entity recognition (NER) is a task that typically requires large annotated datasets, which limits its applicability across domains with varying entity definitions. This paper addresses few-shot NER, aiming to transfer knowledge to new domains with minimal supervision. Unlike previous approaches that rely solely on limited annotated data, we propose a weakly supervised algorithm that combines small labeled datasets with large amounts of unlabeled data. Our method extends the k-means algorithm with label supervision, cluster size constraints and domain-specific discriminative subspace selection. This unified framework achieves state-of-the-art results in few-shot NER on several English datasets.
<div id='section'>Paperid: <span id='pid'>446, <a href='https://arxiv.org/pdf/2411.18475.pdf' target='_blank'>https://arxiv.org/pdf/2411.18475.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuze Wang, Aoran Hu, Ji Qi, Yang Liu, Chao Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18475">Weakly Supervised Framework Considering Multi-temporal Information for Large-scale Cropland Mapping with Satellite Imagery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately mapping large-scale cropland is crucial for agricultural production management and planning. Currently, the combination of remote sensing data and deep learning techniques has shown outstanding performance in cropland mapping. However, those approaches require massive precise labels, which are labor-intensive. To reduce the label cost, this study presented a weakly supervised framework considering multi-temporal information for large-scale cropland mapping. Specifically, we extract high-quality labels according to their consistency among global land cover (GLC) products to construct the supervised learning signal. On the one hand, to alleviate the overfitting problem caused by the model's over-trust of remaining errors in high-quality labels, we encode the similarity/aggregation of cropland in the visual/spatial domain to construct the unsupervised learning signal, and take it as the regularization term to constrain the supervised part. On the other hand, to sufficiently leverage the plentiful information in the samples without high-quality labels, we also incorporate the unsupervised learning signal in these samples, enriching the diversity of the feature space. After that, to capture the phenological features of croplands, we introduce dense satellite image time series (SITS) to extend the proposed framework in the temporal dimension. We also visualized the high dimensional phenological features to uncover how multi-temporal information benefits cropland extraction, and assessed the method's robustness under conditions of data scarcity. The proposed framework has been experimentally validated for strong adaptability across three study areas (Hunan Province, Southeast France, and Kansas) in large-scale cropland mapping, and the internal mechanism and temporal generalizability are also investigated.
<div id='section'>Paperid: <span id='pid'>447, <a href='https://arxiv.org/pdf/2411.11636.pdf' target='_blank'>https://arxiv.org/pdf/2411.11636.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiman Li, Jiayue Zhao, Shaolei Liu, Xiaokun Dai, Chenxi Zhang, Zhijian Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11636">SP${ }^3$ : Superpixel-propagated pseudo-label learning for weakly semi-supervised medical image segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning-based medical image segmentation helps assist diagnosis and accelerate the treatment process while the model training usually requires large-scale dense annotation datasets. Weakly semi-supervised medical image segmentation is an essential application because it only requires a small amount of scribbles and a large number of unlabeled data to train the model, which greatly reduces the clinician's effort to fully annotate images. To handle the inadequate supervisory information challenge in weakly semi-supervised segmentation (WSSS), a SuperPixel-Propagated Pseudo-label (SP${}^3$) learning method is proposed, using the structural information contained in superpixel for supplemental information. Specifically, the annotation of scribbles is propagated to superpixels and thus obtains a dense annotation for supervised training. Since the quality of pseudo-labels is limited by the low-quality annotation, the beneficial superpixels selected by dynamic thresholding are used to refine pseudo-labels. Furthermore, aiming to alleviate the negative impact of noise in pseudo-label, superpixel-level uncertainty is incorporated to guide the pseudo-label supervision for stable learning. Our method achieves state-of-the-art performance on both tumor and organ segmentation datasets under the WSSS setting, using only 3\% of the annotation workload compared to fully supervised methods and attaining approximately 80\% Dice score. Additionally, our method outperforms eight weakly and semi-supervised methods under both weakly supervised and semi-supervised settings. Results of extensive experiments validate the effectiveness and annotation efficiency of our weakly semi-supervised segmentation, which can assist clinicians in achieving automated segmentation for organs or tumors quickly and ultimately benefit patients.
<div id='section'>Paperid: <span id='pid'>448, <a href='https://arxiv.org/pdf/2409.19600.pdf' target='_blank'>https://arxiv.org/pdf/2409.19600.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayu Hu, Senlin Shu, Beibei Li, Tao Xiang, Zhongshi He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19600">An Unbiased Risk Estimator for Partial Label Learning with Augmented Classes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partial Label Learning (PLL) is a typical weakly supervised learning task, which assumes each training instance is annotated with a set of candidate labels containing the ground-truth label. Recent PLL methods adopt identification-based disambiguation to alleviate the influence of false positive labels and achieve promising performance. However, they require all classes in the test set to have appeared in the training set, ignoring the fact that new classes will keep emerging in real applications. To address this issue, in this paper, we focus on the problem of Partial Label Learning with Augmented Class (PLLAC), where one or more augmented classes are not visible in the training stage but appear in the inference stage. Specifically, we propose an unbiased risk estimator with theoretical guarantees for PLLAC, which estimates the distribution of augmented classes by differentiating the distribution of known classes from unlabeled data and can be equipped with arbitrary PLL loss functions. Besides, we provide a theoretical analysis of the estimation error bound of the estimator, which guarantees the convergence of the empirical risk minimizer to the true risk minimizer as the number of training data tends to infinity. Furthermore, we add a risk-penalty regularization term in the optimization objective to alleviate the influence of the over-fitting issue caused by negative empirical risk. Extensive experiments on benchmark, UCI and real-world datasets demonstrate the effectiveness of the proposed approach.
<div id='section'>Paperid: <span id='pid'>449, <a href='https://arxiv.org/pdf/2409.18434.pdf' target='_blank'>https://arxiv.org/pdf/2409.18434.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siru Li, Ziyang Hong, Yushuai Chen, Liang Hu, Jiahu Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18434">Get It For Free: Radar Segmentation without Expert Labels and Its Application in Odometry and Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel weakly supervised semantic segmentation method for radar segmentation, where the existing LiDAR semantic segmentation models are employed to generate semantic labels, which then serve as supervision signals for training a radar semantic segmentation model. The obtained radar semantic segmentation model outperforms LiDAR-based models, providing more consistent and robust segmentation under all-weather conditions, particularly in the snow, rain and fog. To mitigate potential errors in LiDAR semantic labels, we design a dedicated refinement scheme that corrects erroneous labels based on structural features and distribution patterns. The semantic information generated by our radar segmentation model is used in two downstream tasks, achieving significant performance improvements. In large-scale radar-based localization using OpenStreetMap, it leads to localization error reduction by 20.55\% over prior methods. For the odometry task, it improves translation accuracy by 16.4\% compared to the second-best method, securing the first place in the radar odometry competition at the Radar in Robotics workshop of ICRA 2024, Japan
<div id='section'>Paperid: <span id='pid'>450, <a href='https://arxiv.org/pdf/2409.09616.pdf' target='_blank'>https://arxiv.org/pdf/2409.09616.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cagri Gungor, Adriana Kovashka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09616">Enhancing Weakly-Supervised Object Detection on Static Images through (Hallucinated) Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While motion has garnered attention in various tasks, its potential as a modality for weakly-supervised object detection (WSOD) in static images remains unexplored. Our study introduces an approach to enhance WSOD methods by integrating motion information. This method involves leveraging hallucinated motion from static images to improve WSOD on image datasets, utilizing a Siamese network for enhanced representation learning with motion, addressing camera motion through motion normalization, and selectively training images based on object motion. Experimental validation on the COCO and YouTube-BB datasets demonstrates improvements over a state-of-the-art method.
<div id='section'>Paperid: <span id='pid'>451, <a href='https://arxiv.org/pdf/2512.22203.pdf' target='_blank'>https://arxiv.org/pdf/2512.22203.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiang Guo, Rubo Zhang, Bingbing Zhang, Junjie Liu, Jianqing Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.22203">TCFormer: A 5M-Parameter Transformer with Density-Guided Aggregation for Weakly-Supervised Crowd Counting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Crowd counting typically relies on labor-intensive point-level annotations and computationally intensive backbones, restricting its scalability and deployment in resource-constrained environments. To address these challenges, this paper proposes the TCFormer, a tiny, ultra-lightweight, weakly-supervised transformer-based crowd counting framework with only 5 million parameters that achieves competitive performance. Firstly, a powerful yet efficient vision transformer is adopted as the feature extractor, the global context-aware capabilities of which provides semantic meaningful crowd features with a minimal memory footprint. Secondly, to compensate for the lack of spatial supervision, we design a feature aggregation mechanism termed the Learnable Density-Weighted Averaging module. This module dynamically re-weights local tokens according to predicted density scores, enabling the network to adaptively modulate regional features based on their specific density characteristics without the need for additional annotations. Furthermore, this paper introduces a density-level classification loss, which discretizes crowd density into distinct grades, thereby regularizing the training process and enhancing the model's classification power across varying levels of crowd density. Therefore, although TCformer is trained under a weakly-supervised paradigm utilizing only image-level global counts, the joint optimization of count and density-level losses enables the framework to achieve high estimation accuracy. Extensive experiments on four benchmarks including ShanghaiTech A/B, UCF-QNRF, and NWPU datasets demonstrate that our approach strikes a superior trade-off between parameter efficiency and counting accuracy and can be a good solution for crowd counting tasks in edge devices.
<div id='section'>Paperid: <span id='pid'>452, <a href='https://arxiv.org/pdf/2512.15061.pdf' target='_blank'>https://arxiv.org/pdf/2512.15061.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pandega Abyan Zumarsyah, Igi Ardiyanto, Hanung Adi Nugroho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.15061">Meta-learners for few-shot weakly-supervised optic disc and cup segmentation on fundus images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study develops meta-learners for few-shot weakly-supervised segmentation (FWS) to address the challenge of optic disc (OD) and optic cup (OC) segmentation for glaucoma diagnosis with limited labeled fundus images. We significantly improve existing meta-learners by introducing Omni meta-training which balances data usage and diversifies the number of shots. We also develop their efficient versions that reduce computational costs. In addition, we develop sparsification techniques that generate more customizable and representative scribbles and other sparse labels. After evaluating multiple datasets, we find that Omni and efficient versions outperform the original versions, with the best meta-learner being Efficient Omni ProtoSeg (EO-ProtoSeg). It achieves intersection over union (IoU) scores of 88.15% for OD and 71.17% for OC on the REFUGE dataset using just one sparsely labeled image, outperforming few-shot and semi-supervised methods which require more labeled images. Its best performance reaches 86.80% for OD and 71.78%for OC on DRISHTIGS, 88.21% for OD and 73.70% for OC on REFUGE, 80.39% for OD and 52.65% for OC on REFUGE. EO-ProtoSeg is comparable to unsupervised domain adaptation methods yet much lighter with less than two million parameters and does not require any retraining.
<div id='section'>Paperid: <span id='pid'>453, <a href='https://arxiv.org/pdf/2511.17392.pdf' target='_blank'>https://arxiv.org/pdf/2511.17392.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runxun Zhang, Yizhou Liu, Li Dongrui, Bo XU, Jingwei Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17392">MorphSeek: Fine-grained Latent Representation-Level Policy Optimization for Deformable Image Registration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deformable image registration (DIR) remains a fundamental yet challenging problem in medical image analysis, largely due to the prohibitively high-dimensional deformation space of dense displacement fields and the scarcity of voxel-level supervision. Existing reinforcement learning frameworks often project this space into coarse, low-dimensional representations, limiting their ability to capture spatially variant deformations. We propose MorphSeek, a fine-grained representation-level policy optimization paradigm that reformulates DIR as a spatially continuous optimization process in the latent feature space. MorphSeek introduces a stochastic Gaussian policy head atop the encoder to model a distribution over latent features, facilitating efficient exploration and coarse-to-fine refinement. The framework integrates unsupervised warm-up with weakly supervised fine-tuning through Group Relative Policy Optimization, where multi-trajectory sampling stabilizes training and improves label efficiency. Across three 3D registration benchmarks (OASIS brain MRI, LiTS liver CT, and Abdomen MR-CT), MorphSeek achieves consistent Dice improvements over competitive baselines while maintaining high label efficiency with minimal parameter cost and low step-level latency overhead. Beyond optimizer specifics, MorphSeek advances a representation-level policy learning paradigm that achieves spatially coherent and data-efficient deformation optimization, offering a principled, backbone-agnostic, and optimizer-agnostic solution for scalable visual alignment in high-dimensional settings.
<div id='section'>Paperid: <span id='pid'>454, <a href='https://arxiv.org/pdf/2511.13204.pdf' target='_blank'>https://arxiv.org/pdf/2511.13204.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junhee Lee, ChaeBeen Bang, MyoungChul Kim, MyeongAh Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13204">RefineVAD: Semantic-Guided Feature Recalibration for Weakly Supervised Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-Supervised Video Anomaly Detection aims to identify anomalous events using only video-level labels, balancing annotation efficiency with practical applicability. However, existing methods often oversimplify the anomaly space by treating all abnormal events as a single category, overlooking the diverse semantic and temporal characteristics intrinsic to real-world anomalies. Inspired by how humans perceive anomalies, by jointly interpreting temporal motion patterns and semantic structures underlying different anomaly types, we propose RefineVAD, a novel framework that mimics this dual-process reasoning. Our framework integrates two core modules. The first, Motion-aware Temporal Attention and Recalibration (MoTAR), estimates motion salience and dynamically adjusts temporal focus via shift-based attention and global Transformer-based modeling. The second, Category-Oriented Refinement (CORE), injects soft anomaly category priors into the representation space by aligning segment-level features with learnable category prototypes through cross-attention. By jointly leveraging temporal dynamics and semantic structure, explicitly models both "how" motion evolves and "what" semantic category it resembles. Extensive experiments on WVAD benchmark validate the effectiveness of RefineVAD and highlight the importance of integrating semantic context to guide feature refinement toward anomaly-relevant patterns.
<div id='section'>Paperid: <span id='pid'>455, <a href='https://arxiv.org/pdf/2510.25134.pdf' target='_blank'>https://arxiv.org/pdf/2510.25134.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingdong Cai, Charith Abhayaratne
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.25134">Region-CAM: Towards Accurate Object Regions in Class Activation Maps for Weakly Supervised Learning Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Class Activation Mapping (CAM) methods are widely applied in weakly supervised learning tasks due to their ability to highlight object regions. However, conventional CAM methods highlight only the most discriminative regions of the target. These highlighted regions often fail to cover the entire object and are frequently misaligned with object boundaries, thereby limiting the performance of downstream weakly supervised learning tasks, particularly Weakly Supervised Semantic Segmentation (WSSS), which demands pixel-wise accurate activation maps to get the best results. To alleviate the above problems, we propose a novel activation method, Region-CAM. Distinct from network feature weighting approaches, Region-CAM generates activation maps by extracting semantic information maps (SIMs) and performing semantic information propagation (SIP) by considering both gradients and features in each of the stages of the baseline classification model. Our approach highlights a greater proportion of object regions while ensuring activation maps to have precise boundaries that align closely with object edges. Region-CAM achieves 60.12% and 58.43% mean intersection over union (mIoU) using the baseline model on the PASCAL VOC training and validation datasets, respectively, which are improvements of 13.61% and 13.13% over the original CAM (46.51% and 45.30%). On the MS COCO validation set, Region-CAM achieves 36.38%, a 16.23% improvement over the original CAM (20.15%). We also demonstrate the superiority of Region-CAM in object localization tasks, using the ILSVRC2012 validation set. Region-CAM achieves 51.7% in Top-1 Localization accuracy Loc1. Compared with LayerCAM, an activation method designed for weakly supervised object localization, Region-CAM achieves 4.5% better performance in Loc1.
<div id='section'>Paperid: <span id='pid'>456, <a href='https://arxiv.org/pdf/2510.12182.pdf' target='_blank'>https://arxiv.org/pdf/2510.12182.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Youngju Yoo, Seho Kim, Changick Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.12182">BEEP3D: Box-Supervised End-to-End Pseudo-Mask Generation for 3D Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D instance segmentation is crucial for understanding complex 3D environments, yet fully supervised methods require dense point-level annotations, resulting in substantial annotation costs and labor overhead. To mitigate this, box-level annotations have been explored as a weaker but more scalable form of supervision. However, box annotations inherently introduce ambiguity in overlapping regions, making accurate point-to-instance assignment challenging. Recent methods address this ambiguity by generating pseudo-masks through training a dedicated pseudo-labeler in an additional training stage. However, such two-stage pipelines often increase overall training time and complexity, hinder end-to-end optimization. To overcome these challenges, we propose BEEP3D-Box-supervised End-to-End Pseudo-mask generation for 3D instance segmentation. BEEP3D adopts a student-teacher framework, where the teacher model serves as a pseudo-labeler and is updated by the student model via an Exponential Moving Average. To better guide the teacher model to generate precise pseudo-masks, we introduce an instance center-based query refinement that enhances position query localization and leverages features near instance centers. Additionally, we design two novel losses-query consistency loss and masked feature consistency loss-to align semantic and geometric signals between predictions and pseudo-masks. Extensive experiments on ScanNetV2 and S3DIS datasets demonstrate that BEEP3D achieves competitive or superior performance compared to state-of-the-art weakly supervised methods while remaining computationally efficient.
<div id='section'>Paperid: <span id='pid'>457, <a href='https://arxiv.org/pdf/2508.18958.pdf' target='_blank'>https://arxiv.org/pdf/2508.18958.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matteo Contini, Victor Illien, Sylvain Poulain, Serge Bernard, Julien Barde, Sylvain Bonhommeau, Alexis Joly
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18958">The point is the mask: scaling coral reef segmentation with weak supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monitoring coral reefs at large spatial scales remains an open challenge, essential for assessing ecosystem health and informing conservation efforts. While drone-based aerial imagery offers broad spatial coverage, its limited resolution makes it difficult to reliably distinguish fine-scale classes, such as coral morphotypes. At the same time, obtaining pixel-level annotations over large spatial extents is costly and labor-intensive, limiting the scalability of deep learning-based segmentation methods for aerial imagery. We present a multi-scale weakly supervised semantic segmentation framework that addresses this challenge by transferring fine-scale ecological information from underwater imagery to aerial data. Our method enables large-scale coral reef mapping from drone imagery with minimal manual annotation, combining classification-based supervision, spatial interpolation and self-distillation techniques. We demonstrate the efficacy of the approach, enabling large-area segmentation of coral morphotypes and demonstrating flexibility for integrating new classes. This study presents a scalable, cost-effective methodology for high-resolution reef monitoring, combining low-cost data collection, weakly supervised deep learning and multi-scale remote sensing.
<div id='section'>Paperid: <span id='pid'>458, <a href='https://arxiv.org/pdf/2508.11826.pdf' target='_blank'>https://arxiv.org/pdf/2508.11826.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dehn Xu, Tim Katzke, Emmanuel MÃ¼ller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11826">From Pixels to Graphs: Deep Graph-Level Anomaly Detection on Dermoscopic Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph Neural Networks (GNNs) have emerged as a powerful approach for graph-based machine learning tasks. Previous work applied GNNs to image-derived graph representations for various downstream tasks such as classification or anomaly detection. These transformations include segmenting images, extracting features from segments, mapping them to nodes, and connecting them. However, to the best of our knowledge, no study has rigorously compared the effectiveness of the numerous potential image-to-graph transformation approaches for GNN-based graph-level anomaly detection (GLAD). In this study, we systematically evaluate the efficacy of multiple segmentation schemes, edge construction strategies, and node feature sets based on color, texture, and shape descriptors to produce suitable image-derived graph representations to perform graph-level anomaly detection. We conduct extensive experiments on dermoscopic images using state-of-the-art GLAD models, examining performance and efficiency in purely unsupervised, weakly supervised, and fully supervised regimes. Our findings reveal, for example, that color descriptors contribute the best standalone performance, while incorporating shape and texture features consistently enhances detection efficacy. In particular, our best unsupervised configuration using OCGTL achieves a competitive AUC-ROC score of up to 0.805 without relying on pretrained backbones like comparable image-based approaches. With the inclusion of sparse labels, the performance increases substantially to 0.872 and with full supervision to 0.914 AUC-ROC.
<div id='section'>Paperid: <span id='pid'>459, <a href='https://arxiv.org/pdf/2508.08095.pdf' target='_blank'>https://arxiv.org/pdf/2508.08095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chun Wang, Chenyang Liu, Wenze Xu, Weihong Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08095">Dual Information Speech Language Models for Emotional Conversations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conversational systems relying on text-based large language models (LLMs) often overlook paralinguistic cues, essential for understanding emotions and intentions. Speech-language models (SLMs), which use speech as input, are emerging as a promising solution. However, SLMs built by extending frozen LLMs struggle to capture paralinguistic information and exhibit reduced context understanding. We identify entangled information and improper training strategies as key issues. To address these issues, we propose two heterogeneous adapters and suggest a weakly supervised training strategy. Our approach disentangles paralinguistic and linguistic information, enabling SLMs to interpret speech through structured representations. It also preserves contextual understanding by avoiding the generation of task-specific vectors through controlled randomness. This approach trains only the adapters on common datasets, ensuring parameter and data efficiency. Experiments demonstrate competitive performance in emotional conversation tasks, showcasing the model's ability to effectively integrate both paralinguistic and linguistic information within contextual settings.
<div id='section'>Paperid: <span id='pid'>460, <a href='https://arxiv.org/pdf/2508.05019.pdf' target='_blank'>https://arxiv.org/pdf/2508.05019.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sadia Kamal, Tim Oates, Joy Wan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05019">Skin-SOAP: A Weakly Supervised Framework for Generating Structured SOAP Notes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skin carcinoma is the most prevalent form of cancer globally, accounting for over $8 billion in annual healthcare expenditures. Early diagnosis, accurate and timely treatment are critical to improving patient survival rates. In clinical settings, physicians document patient visits using detailed SOAP (Subjective, Objective, Assessment, and Plan) notes. However, manually generating these notes is labor-intensive and contributes to clinician burnout. In this work, we propose skin-SOAP, a weakly supervised multimodal framework to generate clinically structured SOAP notes from limited inputs, including lesion images and sparse clinical text. Our approach reduces reliance on manual annotations, enabling scalable, clinically grounded documentation while alleviating clinician burden and reducing the need for large annotated data. Our method achieves performance comparable to GPT-4o, Claude, and DeepSeek Janus Pro across key clinical relevance metrics. To evaluate this clinical relevance, we introduce two novel metrics MedConceptEval and Clinical Coherence Score (CCS) which assess semantic alignment with expert medical concepts and input features, respectively.
<div id='section'>Paperid: <span id='pid'>461, <a href='https://arxiv.org/pdf/2508.03724.pdf' target='_blank'>https://arxiv.org/pdf/2508.03724.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jia Li, Yapeng Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03724">From Waveforms to Pixels: A Survey on Audio-Visual Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-Visual Segmentation (AVS) aims to identify and segment sound-producing objects in videos by leveraging both visual and audio modalities. It has emerged as a significant research area in multimodal perception, enabling fine-grained object-level understanding. In this survey, we present a comprehensive overview of the AVS field, covering its problem formulation, benchmark datasets, evaluation metrics, and the progression of methodologies. We analyze a wide range of approaches, including architectures for unimodal and multimodal encoding, key strategies for audio-visual fusion, and various decoder designs. Furthermore, we examine major training paradigms, from fully supervised learning to weakly supervised and training-free methods. Notably, we provide an extensive comparison of AVS methods across standard benchmarks, highlighting the impact of different architectural choices, fusion strategies, and training paradigms on performance. Finally, we outline the current challenges, such as limited temporal modeling, modality bias toward vision, lack of robustness in complex environments, and high computational demands, and propose promising future directions, including improving temporal reasoning and multimodal fusion, leveraging foundation models for better generalization and few-shot learning, reducing reliance on labeled data through selfand weakly supervised learning, and incorporating higher-level reasoning for more intelligent AVS systems.
<div id='section'>Paperid: <span id='pid'>462, <a href='https://arxiv.org/pdf/2508.02844.pdf' target='_blank'>https://arxiv.org/pdf/2508.02844.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anghong Du, Nay Aung, Theodoros N. Arvanitis, Stefan K. Piechnik, Joao A C Lima, Steffen E. Petersen, Le Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02844">RefineSeg: Dual Coarse-to-Fine Learning for Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-quality pixel-level annotations of medical images are essential for supervised segmentation tasks, but obtaining such annotations is costly and requires medical expertise. To address this challenge, we propose a novel coarse-to-fine segmentation framework that relies entirely on coarse-level annotations, encompassing both target and complementary drawings, despite their inherent noise. The framework works by introducing transition matrices in order to model the inaccurate and incomplete regions in the coarse annotations. By jointly training on multiple sets of coarse annotations, it progressively refines the network's outputs and infers the true segmentation distribution, achieving a robust approximation of precise labels through matrix-based modeling. To validate the flexibility and effectiveness of the proposed method, we demonstrate the results on two public cardiac imaging datasets, ACDC and MSCMRseg, and further evaluate its performance on the UK Biobank dataset. Experimental results indicate that our approach surpasses the state-of-the-art weakly supervised methods and closely matches the fully supervised approach.
<div id='section'>Paperid: <span id='pid'>463, <a href='https://arxiv.org/pdf/2507.06848.pdf' target='_blank'>https://arxiv.org/pdf/2507.06848.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joelle Hanna, Damian Borth
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06848">Know Your Attention Maps: Class-specific Token Masking for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Semantic Segmentation (WSSS) is a challenging problem that has been extensively studied in recent years. Traditional approaches often rely on external modules like Class Activation Maps to highlight regions of interest and generate pseudo segmentation masks. In this work, we propose an end-to-end method that directly utilizes the attention maps learned by a Vision Transformer (ViT) for WSSS. We propose training a sparse ViT with multiple [CLS] tokens (one for each class), using a random masking strategy to promote [CLS] token - class assignment. At inference time, we aggregate the different self-attention maps of each [CLS] token corresponding to the predicted labels to generate pseudo segmentation masks. Our proposed approach enhances the interpretability of self-attention maps and ensures accurate class assignments. Extensive experiments on two standard benchmarks and three specialized datasets demonstrate that our method generates accurate pseudo-masks, outperforming related works. Those pseudo-masks can be used to train a segmentation model which achieves results comparable to fully-supervised models, significantly reducing the need for fine-grained labeled data.
<div id='section'>Paperid: <span id='pid'>464, <a href='https://arxiv.org/pdf/2507.01721.pdf' target='_blank'>https://arxiv.org/pdf/2507.01721.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongwen Zhang, Yuri Boykov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01721">Soft Self-labeling and Potts Relaxations for Weakly-Supervised Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider weakly supervised segmentation where only a fraction of pixels have ground truth labels (scribbles) and focus on a self-labeling approach optimizing relaxations of the standard unsupervised CRF/Potts loss on unlabeled pixels. While WSSS methods can directly optimize such losses via gradient descent, prior work suggests that higher-order optimization can improve network training by introducing hidden pseudo-labels and powerful CRF sub-problem solvers, e.g. graph cut. However, previously used hard pseudo-labels can not represent class uncertainty or errors, which motivates soft self-labeling. We derive a principled auxiliary loss and systematically evaluate standard and new CRF relaxations (convex and non-convex), neighborhood systems, and terms connecting network predictions with soft pseudo-labels. We also propose a general continuous sub-problem solver. Using only standard architectures, soft self-labeling consistently improves scribble-based training and outperforms significantly more complex specialized WSSS systems. It can outperform full pixel-precise supervision. Our general ideas apply to other weakly-supervised problems/systems.
<div id='section'>Paperid: <span id='pid'>465, <a href='https://arxiv.org/pdf/2506.10328.pdf' target='_blank'>https://arxiv.org/pdf/2506.10328.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sadia Kamal, Tim Oates, Joy Wan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10328">Towards Scalable SOAP Note Generation: A Weakly Supervised Multimodal Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skin carcinoma is the most prevalent form of cancer globally, accounting for over $8 billion in annual healthcare expenditures. In clinical settings, physicians document patient visits using detailed SOAP (Subjective, Objective, Assessment, and Plan) notes. However, manually generating these notes is labor-intensive and contributes to clinician burnout. In this work, we propose a weakly supervised multimodal framework to generate clinically structured SOAP notes from limited inputs, including lesion images and sparse clinical text. Our approach reduces reliance on manual annotations, enabling scalable, clinically grounded documentation while alleviating clinician burden and reducing the need for large annotated data. Our method achieves performance comparable to GPT-4o, Claude, and DeepSeek Janus Pro across key clinical relevance metrics. To evaluate clinical quality, we introduce two novel metrics MedConceptEval and Clinical Coherence Score (CCS) which assess semantic alignment with expert medical concepts and input features, respectively.
<div id='section'>Paperid: <span id='pid'>466, <a href='https://arxiv.org/pdf/2506.07076.pdf' target='_blank'>https://arxiv.org/pdf/2506.07076.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyi Wu, Haohong Wang, Aggelos K. Katsaggelos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07076">Harmony-Aware Music-driven Motion Synthesis with Perceptual Constraint on UGC Datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the popularity of video-based user-generated content (UGC) on social media, harmony, as dictated by human perceptual principles, is critical in assessing the rhythmic consistency of audio-visual UGCs for better user engagement. In this work, we propose a novel harmony-aware GAN framework, following a specifically designed harmony evaluation strategy to enhance rhythmic synchronization in the automatic music-to-motion synthesis using a UGC dance dataset. This harmony strategy utilizes refined cross-modal beat detection to capture closely correlated audio and visual rhythms in an audio-visual pair. To mimic human attention mechanism, we introduce saliency-based beat weighting and interval-driven beat alignment, which ensures accurate harmony score estimation consistent with human perception. Building on this strategy, our model, employing efficient encoder-decoder and depth-lifting designs, is adversarially trained based on categorized musical meter segments to generate realistic and rhythmic 3D human motions. We further incorporate our harmony evaluation strategy as a weakly supervised perceptual constraint to flexibly guide the synchronized audio-visual rhythms during the generation process. Experimental results show that our proposed model significantly outperforms other leading music-to-motion methods in rhythmic harmony, both quantitatively and qualitatively, even with limited UGC training data. Live samples 15 can be watched at: https://youtu.be/tWwz7yq4aUs
<div id='section'>Paperid: <span id='pid'>467, <a href='https://arxiv.org/pdf/2506.03570.pdf' target='_blank'>https://arxiv.org/pdf/2506.03570.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lin Sun, Chuang Liu, Xiaofeng Ma, Tao Yang, Weijia Lu, Ning Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03570">FreePRM: Training Process Reward Models Without Ground Truth Process Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in Large Language Models (LLMs) have demonstrated that Process Reward Models (PRMs) play a crucial role in enhancing model performance. However, training PRMs typically requires step-level labels, either manually annotated or automatically generated, which can be costly and difficult to obtain at scale. To address this challenge, we introduce FreePRM, a weakly supervised framework for training PRMs without access to ground-truth step-level labels. FreePRM first generates pseudo step-level labels based on the correctness of final outcome, and then employs Buffer Probability to eliminate impact of noise inherent in pseudo labeling. Experimental results show that FreePRM achieves an average F1 score of 53.0% on ProcessBench, outperforming fully supervised PRM trained on Math-Shepherd by +24.1%. Compared to other open-source PRMs, FreePRM outperforms upon RLHFlow-PRM-Mistral-8B (28.4%) by +24.6%, EurusPRM (31.3%) by +21.7%, and Skywork-PRM-7B (42.1%) by +10.9%. This work introduces a new paradigm in PRM training, significantly reducing reliance on costly step-level annotations while maintaining strong performance.
<div id='section'>Paperid: <span id='pid'>468, <a href='https://arxiv.org/pdf/2506.02451.pdf' target='_blank'>https://arxiv.org/pdf/2506.02451.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pratheeksha Nair, Reihaneh Rabbany
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02451">Weak Supervision for Real World Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Node classification in real world graphs often suffers from label scarcity and noise, especially in high stakes domains like human trafficking detection and misinformation monitoring. While direct supervision is limited, such graphs frequently contain weak signals, noisy or indirect cues, that can still inform learning. We propose WSNET, a novel weakly supervised graph contrastive learning framework that leverages these weak signals to guide robust representation learning. WSNET integrates graph structure, node features, and multiple noisy supervision sources through a contrastive objective tailored for weakly labeled data. Across three real world datasets and synthetic benchmarks with controlled noise, WSNET consistently outperforms state of the art contrastive and noisy label learning methods by up to 15% in F1 score. Our results highlight the effectiveness of contrastive learning under weak supervision and the promise of exploiting imperfect labels in graph based settings.
<div id='section'>Paperid: <span id='pid'>469, <a href='https://arxiv.org/pdf/2505.09955.pdf' target='_blank'>https://arxiv.org/pdf/2505.09955.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaeho Kim, Seulki Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09955">TransPL: VQ-Code Transition Matrices for Pseudo-Labeling of Time Series Unsupervised Domain Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsupervised domain adaptation (UDA) for time series data remains a critical challenge in deep learning, with traditional pseudo-labeling strategies failing to capture temporal patterns and channel-wise shifts between domains, producing sub-optimal pseudo-labels. As such, we introduce TransPL, a novel approach that addresses these limitations by modeling the joint distribution $P(\mathbf{X}, y)$ of the source domain through code transition matrices, where the codes are derived from vector quantization (VQ) of time series patches. Our method constructs class- and channel-wise code transition matrices from the source domain and employs Bayes' rule for target domain adaptation, generating pseudo-labels based on channel-wise weighted class-conditional likelihoods. TransPL offers three key advantages: explicit modeling of temporal transitions and channel-wise shifts between different domains, versatility towards different UDA scenarios (e.g., weakly-supervised UDA), and explainable pseudo-label generation. We validate TransPL's effectiveness through extensive analysis on four time series UDA benchmarks and confirm that it consistently outperforms state-of-the-art pseudo-labeling methods by a strong margin (6.1% accuracy improvement, 4.9% F1 improvement), while providing interpretable insights into the domain adaptation process through its learned code transition matrices.
<div id='section'>Paperid: <span id='pid'>470, <a href='https://arxiv.org/pdf/2505.09011.pdf' target='_blank'>https://arxiv.org/pdf/2505.09011.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Antonio Candito, Matthew D Blackledge, Richard Holbrey, Nuria Porta, Ana Ribeiro, Fabio Zugni, Luca D'Erme, Francesca Castagnoli, Alina Dragan, Ricardo Donners, Christina Messiou, Nina Tunariu, Dow-Mu Koh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09011">Signal-based AI-driven software solution for automated quantification of metastatic bone disease and treatment response assessment using Whole-Body Diffusion-Weighted MRI (WB-DWI) biomarkers in Advanced Prostate Cancer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We developed an AI-driven software solution to quantify metastatic bone disease from WB-DWI scans. Core technologies include: (i) a weakly-supervised Residual U-Net model generating a skeleton probability map to isolate bone; (ii) a statistical framework for WB-DWI intensity normalisation, obtaining a signal-normalised b=900s/mm^2 (b900) image; and (iii) a shallow convolutional neural network that processes outputs from (i) and (ii) to generate a mask of suspected bone lesions, characterised by higher b900 signal intensity due to restricted water diffusion. This mask is applied to the gADC map to extract TDV and gADC statistics. We tested the tool using expert-defined metastatic bone disease delineations on 66 datasets, assessed repeatability of imaging biomarkers (N=10), and compared software-based response assessment with a construct reference standard based on clinical, laboratory and imaging assessments (N=118). Dice score between manual and automated delineations was 0.6 for lesions within pelvis and spine, with an average surface distance of 2mm. Relative differences for log-transformed TDV (log-TDV) and median gADC were below 9% and 5%, respectively. Repeatability analysis showed coefficients of variation of 4.57% for log-TDV and 3.54% for median gADC, with intraclass correlation coefficients above 0.9. The software achieved 80.5% accuracy, 84.3% sensitivity, and 85.7% specificity in assessing response to treatment compared to the construct reference standard. Computation time generating a mask averaged 90 seconds per scan. Our software enables reproducible TDV and gADC quantification from WB-DWI scans for monitoring metastatic bone disease response, thus providing potentially useful measurements for clinical decision-making in APC patients.
<div id='section'>Paperid: <span id='pid'>471, <a href='https://arxiv.org/pdf/2504.09430.pdf' target='_blank'>https://arxiv.org/pdf/2504.09430.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruiwen Ding, Lin Li, Rajath Soans, Tosha Shah, Radha Krishnan, Marc Alexander Sze, Sasha Lukyanov, Yash Deshpande, Antong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09430">Predicting ulcer in H&E images of inflammatory bowel disease using domain-knowledge-driven graph neural network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inflammatory bowel disease (IBD) involves chronic inflammation of the digestive tract, with treatment options often burdened by adverse effects. Identifying biomarkers for personalized treatment is crucial. While immune cells play a key role in IBD, accurately identifying ulcer regions in whole slide images (WSIs) is essential for characterizing these cells and exploring potential therapeutics. Multiple instance learning (MIL) approaches have advanced WSI analysis but they lack spatial context awareness. In this work, we propose a weakly-supervised model called DomainGCN that employs a graph convolution neural network (GCN) and incorporates domain-specific knowledge of ulcer features, specifically, the presence of epithelium, lymphocytes, and debris for WSI-level ulcer prediction in IBD. We demonstrate that DomainGCN outperforms various state-of-the-art (SOTA) MIL methods and show the added value of domain knowledge.
<div id='section'>Paperid: <span id='pid'>472, <a href='https://arxiv.org/pdf/2504.05700.pdf' target='_blank'>https://arxiv.org/pdf/2504.05700.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seth Z. Zhao, Reza Ghoddoosian, Isht Dwivedi, Nakul Agarwal, Behzad Dariush
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.05700">Pose-Aware Weakly-Supervised Action Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding human behavior is an important problem in the pursuit of visual intelligence. A challenge in this endeavor is the extensive and costly effort required to accurately label action segments. To address this issue, we consider learning methods that demand minimal supervision for segmentation of human actions in long instructional videos. Specifically, we introduce a weakly-supervised framework that uniquely incorporates pose knowledge during training while omitting its use during inference, thereby distilling pose knowledge pertinent to each action component. We propose a pose-inspired contrastive loss as a part of the whole weakly-supervised framework which is trained to distinguish action boundaries more effectively. Our approach, validated through extensive experiments on representative datasets, outperforms previous state-of-the-art (SOTA) in segmenting long instructional videos under both online and offline settings. Additionally, we demonstrate the framework's adaptability to various segmentation backbones and pose extractors across different datasets.
<div id='section'>Paperid: <span id='pid'>473, <a href='https://arxiv.org/pdf/2503.21616.pdf' target='_blank'>https://arxiv.org/pdf/2503.21616.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahui Chen, Yang Huan, Runhua Shi, Chanfan Ding, Xiaoqi Mo, Siyu Xiong, Yinong He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.21616">Audio-driven Gesture Generation via Deviation Feature in the Latent Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gestures are essential for enhancing co-speech communication, offering visual emphasis and complementing verbal interactions. While prior work has concentrated on point-level motion or fully supervised data-driven methods, we focus on co-speech gestures, advocating for weakly supervised learning and pixel-level motion deviations. We introduce a weakly supervised framework that learns latent representation deviations, tailored for co-speech gesture video generation. Our approach employs a diffusion model to integrate latent motion features, enabling more precise and nuanced gesture representation. By leveraging weakly supervised deviations in latent space, we effectively generate hand gestures and mouth movements, crucial for realistic video production. Experiments show our method significantly improves video quality, surpassing current state-of-the-art techniques.
<div id='section'>Paperid: <span id='pid'>474, <a href='https://arxiv.org/pdf/2503.14395.pdf' target='_blank'>https://arxiv.org/pdf/2503.14395.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Wang, Ruirui Liu, Yu Lei, Michael J. Baine, Tian Liu, Yang Lei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14395">Weakly Supervised Spatial Implicit Neural Representation Learning for 3D MRI-Ultrasound Deformable Image Registration in HDR Prostate Brachytherapy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Purpose: Accurate 3D MRI-ultrasound (US) deformable registration is critical for real-time guidance in high-dose-rate (HDR) prostate brachytherapy. We present a weakly supervised spatial implicit neural representation (SINR) method to address modality differences and pelvic anatomy challenges.
  Methods: The framework uses sparse surface supervision from MRI/US segmentations instead of dense intensity matching. SINR models deformations as continuous spatial functions, with patient-specific surface priors guiding a stationary velocity field for biologically plausible deformations. Validation included 20 public Prostate-MRI-US-Biopsy cases and 10 institutional HDR cases, evaluated via Dice similarity coefficient (DSC), mean surface distance (MSD), and 95% Hausdorff distance (HD95).
  Results: The proposed method achieved robust registration. For the public dataset, prostate DSC was $0.93 \pm 0.05$, MSD $0.87 \pm 0.10$ mm, and HD95 $1.58 \pm 0.37$ mm. For the institutional dataset, prostate CTV achieved DSC $0.88 \pm 0.09$, MSD $1.21 \pm 0.38$ mm, and HD95 $2.09 \pm 1.48$ mm. Bladder and rectum performance was lower due to ultrasound's limited field of view. Visual assessments confirmed accurate alignment with minimal discrepancies.
  Conclusion: This study introduces a novel weakly supervised SINR-based approach for 3D MRI-US deformable registration. By leveraging sparse surface supervision and spatial priors, it achieves accurate, robust, and computationally efficient registration, enhancing real-time image guidance in HDR prostate brachytherapy and improving treatment precision.
<div id='section'>Paperid: <span id='pid'>475, <a href='https://arxiv.org/pdf/2501.07020.pdf' target='_blank'>https://arxiv.org/pdf/2501.07020.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anh Thi-Hoang Nguyen, Dung Ha Nguyen, Kiet Van Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07020">ViSoLex: An Open-Source Repository for Vietnamese Social Media Lexical Normalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>ViSoLex is an open-source system designed to address the unique challenges of lexical normalization for Vietnamese social media text. The platform provides two core services: Non-Standard Word (NSW) Lookup and Lexical Normalization, enabling users to retrieve standard forms of informal language and standardize text containing NSWs. ViSoLex's architecture integrates pre-trained language models and weakly supervised learning techniques to ensure accurate and efficient normalization, overcoming the scarcity of labeled data in Vietnamese. This paper details the system's design, functionality, and its applications for researchers and non-technical users. Additionally, ViSoLex offers a flexible, customizable framework that can be adapted to various datasets and research requirements. By publishing the source code, ViSoLex aims to contribute to the development of more robust Vietnamese natural language processing tools and encourage further research in lexical normalization. Future directions include expanding the system's capabilities for additional languages and improving the handling of more complex non-standard linguistic patterns.
<div id='section'>Paperid: <span id='pid'>476, <a href='https://arxiv.org/pdf/2501.05933.pdf' target='_blank'>https://arxiv.org/pdf/2501.05933.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Olivier Morelle, Justus Bisten, Maximilian W. M. Wintergerst, Robert P. Finger, Thomas Schultz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.05933">Weakly Supervised Segmentation of Hyper-Reflective Foci with Compact Convolutional Transformers and SAM2</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised segmentation has the potential to greatly reduce the annotation effort for training segmentation models for small structures such as hyper-reflective foci (HRF) in optical coherence tomography (OCT). However, most weakly supervised methods either involve a strong downsampling of input images, or only achieve localization at a coarse resolution, both of which are unsatisfactory for small structures. We propose a novel framework that increases the spatial resolution of a traditional attention-based Multiple Instance Learning (MIL) approach by using Layer-wise Relevance Propagation (LRP) to prompt the Segment Anything Model (SAM~2), and increases recall with iterative inference. Moreover, we demonstrate that replacing MIL with a Compact Convolutional Transformer (CCT), which adds a positional encoding, and permits an exchange of information between different regions of the OCT image, leads to a further and substantial increase in segmentation accuracy.
<div id='section'>Paperid: <span id='pid'>477, <a href='https://arxiv.org/pdf/2412.12829.pdf' target='_blank'>https://arxiv.org/pdf/2412.12829.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elena Bueno-Benito, Mariella Dimiccoli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12829">2by2: Weakly-Supervised Learning for Global Action Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a simple yet effective approach for the poorly investigated task of global action segmentation, aiming at grouping frames capturing the same action across videos of different activities. Unlike the case of videos depicting all the same activity, the temporal order of actions is not roughly shared among all videos, making the task even more challenging. We propose to use activity labels to learn, in a weakly-supervised fashion, action representations suitable for global action segmentation. For this purpose, we introduce a triadic learning approach for video pairs, to ensure intra-video action discrimination, as well as inter-video and inter-activity action association. For the backbone architecture, we use a Siamese network based on sparse transformers that takes as input video pairs and determine whether they belong to the same activity. The proposed approach is validated on two challenging benchmark datasets: Breakfast and YouTube Instructions, outperforming state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>478, <a href='https://arxiv.org/pdf/2412.07384.pdf' target='_blank'>https://arxiv.org/pdf/2412.07384.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Florin Condrea, Saikiran Rapaka, Marius Leordeanu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07384">Label up: Learning Pulmonary Embolism Segmentation from Image Level Annotation through Model Explainability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pulmonary Embolisms (PE) are a leading cause of cardiovascular death. Computed tomographic pulmonary angiography (CTPA) stands as the gold standard for diagnosing pulmonary embolisms (PE) and there has been a lot of interest in developing AI-based models for assisting in PE diagnosis. Performance of these algorithms has been hindered by the scarcity of annotated data, especially those with fine-grained delineation of the thromboembolic burden. In this paper we attempt to address this issue by introducing a weakly supervised learning pipeline, that leverages model explainability to generate fine-grained (pixel level) masks for embolisms starting from more coarse-grained (binary, image level) PE annotations. Furthermore, we show that training models using the automatically generated pixel annotations yields good PE localization performance. We demonstrate the effectiveness of our pipeline on the large-scale, multi-center RSPECT augmented dataset for PE detection and localization.
<div id='section'>Paperid: <span id='pid'>479, <a href='https://arxiv.org/pdf/2412.02250.pdf' target='_blank'>https://arxiv.org/pdf/2412.02250.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Javier UreÃ±a Santiago, Thomas StrÃ¶hle, Antonio RodrÃ­guez-SÃ¡nchez, Ruth Breu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02250">Vision Transformers for Weakly-Supervised Microorganism Enumeration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Microorganism enumeration is an essential task in many applications, such as assessing contamination levels or ensuring health standards when evaluating surface cleanliness. However, it's traditionally performed by human-supervised methods that often require manual counting, making it tedious and time-consuming. Previous research suggests automating this task using computer vision and machine learning methods, primarily through instance segmentation or density estimation techniques. This study conducts a comparative analysis of vision transformers (ViTs) for weakly-supervised counting in microorganism enumeration, contrasting them with traditional architectures such as ResNet and investigating ViT-based models such as TransCrowd. We trained different versions of ViTs as the architectural backbone for feature extraction using four microbiology datasets to determine potential new approaches for total microorganism enumeration in images. Results indicate that while ResNets perform better overall, ViTs performance demonstrates competent results across all datasets, opening up promising lines of research in microorganism enumeration. This comparative study contributes to the field of microbial image analysis by presenting innovative approaches to the recurring challenge of microorganism enumeration and by highlighting the capabilities of ViTs in the task of regression counting.
<div id='section'>Paperid: <span id='pid'>480, <a href='https://arxiv.org/pdf/2410.09999.pdf' target='_blank'>https://arxiv.org/pdf/2410.09999.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sandeep Sricharan Mukku, Abinesh Kanagarajan, Pushpendu Ghosh, Chetan Aggarwal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.09999">Leveraging Customer Feedback for Multi-modal Insight Extraction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Businesses can benefit from customer feedback in different modalities, such as text and images, to enhance their products and services. However, it is difficult to extract actionable and relevant pairs of text segments and images from customer feedback in a single pass. In this paper, we propose a novel multi-modal method that fuses image and text information in a latent space and decodes it to extract the relevant feedback segments using an image-text grounded text decoder. We also introduce a weakly-supervised data generation technique that produces training data for this task. We evaluate our model on unseen data and demonstrate that it can effectively mine actionable insights from multi-modal customer feedback, outperforming the existing baselines by $14$ points in F1 score.
<div id='section'>Paperid: <span id='pid'>481, <a href='https://arxiv.org/pdf/2410.04789.pdf' target='_blank'>https://arxiv.org/pdf/2410.04789.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>MÃ³nica Apellaniz Portos, Roberto Labadie-Tamayo, Claudius Stemmler, Erwin Feyersinger, Andreas Babic, Franziska Bruckner, VrÃ¤Ã¤th Ãhner, Matthias Zeppelzauer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.04789">Analysis of Hybrid Compositions in Animation Film with Weakly Supervised Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present an approach for the analysis of hybrid visual compositions in animation in the domain of ephemeral film. We combine ideas from semi-supervised and weakly supervised learning to train a model that can segment hybrid compositions without requiring pre-labeled segmentation masks. We evaluate our approach on a set of ephemeral films from 13 film archives. Results demonstrate that the proposed learning strategy yields a performance close to a fully supervised baseline. On a qualitative level the performed analysis provides interesting insights on hybrid compositions in animation film.
<div id='section'>Paperid: <span id='pid'>482, <a href='https://arxiv.org/pdf/2409.20467.pdf' target='_blank'>https://arxiv.org/pdf/2409.20467.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dung Ha Nguyen, Anh Thi Hoang Nguyen, Kiet Van Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.20467">A Weakly Supervised Data Labeling Framework for Machine Lexical Normalization in Vietnamese Social Media</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study introduces an innovative automatic labeling framework to address the challenges of lexical normalization in social media texts for low-resource languages like Vietnamese. Social media data is rich and diverse, but the evolving and varied language used in these contexts makes manual labeling labor-intensive and expensive. To tackle these issues, we propose a framework that integrates semi-supervised learning with weak supervision techniques. This approach enhances the quality of training dataset and expands its size while minimizing manual labeling efforts. Our framework automatically labels raw data, converting non-standard vocabulary into standardized forms, thereby improving the accuracy and consistency of the training data. Experimental results demonstrate the effectiveness of our weak supervision framework in normalizing Vietnamese text, especially when utilizing Pre-trained Language Models. The proposed framework achieves an impressive F1-score of 82.72% and maintains vocabulary integrity with an accuracy of up to 99.22%. Additionally, it effectively handles undiacritized text under various conditions. This framework significantly enhances natural language normalization quality and improves the accuracy of various NLP tasks, leading to an average accuracy increase of 1-3%.
<div id='section'>Paperid: <span id='pid'>483, <a href='https://arxiv.org/pdf/2409.15491.pdf' target='_blank'>https://arxiv.org/pdf/2409.15491.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyu Su, Yongxin Guo, Robert Wesolowski, Gary Tozbikian, Nathaniel S. O'Connell, M. Khalid Khan Niazi, Metin N. Gurcan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.15491">Computational Pathology for Accurate Prediction of Breast Cancer Recurrence: Development and Validation of a Deep Learning-based Tool</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate recurrence risk stratification is crucial for optimizing treatment plans for breast cancer patients. Current prognostic tools like Oncotype DX (ODX) offer valuable genomic insights for HR+/HER2- patients but are limited by cost and accessibility, particularly in underserved populations. In this study, we present Deep-BCR-Auto, a deep learning-based computational pathology approach that predicts breast cancer recurrence risk from routine H&E-stained whole slide images (WSIs). Our methodology was validated on two independent cohorts: the TCGA-BRCA dataset and an in-house dataset from The Ohio State University (OSU). Deep-BCR-Auto demonstrated robust performance in stratifying patients into low- and high-recurrence risk categories. On the TCGA-BRCA dataset, the model achieved an area under the receiver operating characteristic curve (AUROC) of 0.827, significantly outperforming existing weakly supervised models (p=0.041). In the independent OSU dataset, Deep-BCR-Auto maintained strong generalizability, achieving an AUROC of 0.832, along with 82.0% accuracy, 85.0% specificity, and 67.7% sensitivity. These findings highlight the potential of computational pathology as a cost-effective alternative for recurrence risk assessment, broadening access to personalized treatment strategies. This study underscores the clinical utility of integrating deep learning-based computational pathology into routine pathological assessment for breast cancer prognosis across diverse clinical settings.
<div id='section'>Paperid: <span id='pid'>484, <a href='https://arxiv.org/pdf/2409.01676.pdf' target='_blank'>https://arxiv.org/pdf/2409.01676.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenyang Hu, Gaetan Frusque, Tianyang Wang, Fulei Chu, Olga Fink
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01676">Classifier-Free Diffusion-Based Weakly-Supervised Approach for Health Indicator Derivation in Rotating Machines: Advancing Early Fault Detection and Condition Monitoring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deriving health indicators of rotating machines is crucial for their maintenance. However, this process is challenging for the prevalent adopted intelligent methods since they may take the whole data distributions, not only introducing noise interference but also lacking the explainability. To address these issues, we propose a diffusion-based weakly-supervised approach for deriving health indicators of rotating machines, enabling early fault detection and continuous monitoring of condition evolution. This approach relies on a classifier-free diffusion model trained using healthy samples and a few anomalies. This model generates healthy samples. and by comparing the differences between the original samples and the generated ones in the envelope spectrum, we construct an anomaly map that clearly identifies faults. Health indicators are then derived, which can explain the fault types and mitigate noise interference. Comparative studies on two cases demonstrate that the proposed method offers superior health monitoring effectiveness and robustness compared to baseline models.
<div id='section'>Paperid: <span id='pid'>485, <a href='https://arxiv.org/pdf/2601.20626.pdf' target='_blank'>https://arxiv.org/pdf/2601.20626.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>F. D. Amaro, R. Antonietti, E. Baracchini, L. Benussi, C. Capoccia, M. Caponero, L. G. M. de Carvalho, G. Cavoto, I. A. Costa, A. Croce, M. D'Astolfo, G. D'Imperio, G. Dho, E. Di Marco, J. M. F. dos Santos, D. Fiorina, F. Iacoangeli, Z. Islam, E. Kemp, H. P. Lima, G. Maccarrone, R. D. P. Mano, D. J. G. Marques, G. Mazzitelli, P. Meloni, A. Messina, C. M. B. Monteiro, R. A. Nobrega, G. M. Oppedisano, I. F. Pains, E. Paoletti, F. Petrucci, S. Piacentini, D. Pierluigi, D. Pinci, F. Renga, A. Russo, G. Saviano, P. A. O. C. Silva, N. J. Spooner, R. Tesauro, S. Tomassini, D. Tozzi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.20626">Trigger Optimization and Event Classification for Dark Matter Searches in the CYGNO Experiment Using Machine Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The CYGNO experiment employs an optical-readout Time Projection Chamber (TPC) to search for rare low-energy interactions using finely resolved scintillation images. While the optical readout provides rich topological information, it produces large, sparse megapixel images that challenge real-time triggering, data reduction, and background discrimination. We summarize two complementary machine-learning approaches developed within CYGNO. First, we present a fast and fully unsupervised strategy for online data reduction based on reconstruction-based anomaly detection. A convolutional autoencoder trained exclusively on pedestal images (i.e. frames acquired with GEM amplification disabled) learns the detector noise morphology and highlights particle-induced structures through localized reconstruction residuals, from which compact Regions of Interest (ROIs) are extracted. On real prototype data, the selected configuration retains (93.0 +/- 0.2)% of reconstructed signal intensity while discarding (97.8 +/- 0.1)% of the image area, with ~25 ms per-frame inference time on a consumer GPU. Second, we report a weakly supervised application of the Classification Without Labels (CWoLa) framework to data acquired with an Americium--Beryllium neutron source. Using only mixed AmBe and standard datasets (no event-level labels), a convolutional classifier learns to identify nuclear-recoil-like topologies. The achieved performance approaches the theoretical limit imposed by the mixture composition and isolates a high-score population with compact, approximately circular morphologies consistent with nuclear recoils.
<div id='section'>Paperid: <span id='pid'>486, <a href='https://arxiv.org/pdf/2601.04744.pdf' target='_blank'>https://arxiv.org/pdf/2601.04744.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingyuan Li, Mengyue Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.04744">Semi-Supervised Diseased Detection from Speech Dialogues with Multi-Level Data Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detecting medical conditions from speech acoustics is fundamentally a weakly-supervised learning problem: a single, often noisy, session-level label must be linked to nuanced patterns within a long, complex audio recording. This task is further hampered by severe data scarcity and the subjective nature of clinical annotations. While semi-supervised learning (SSL) offers a viable path to leverage unlabeled data, existing audio methods often fail to address the core challenge that pathological traits are not uniformly expressed in a patient's speech. We propose a novel, audio-only SSL framework that explicitly models this hierarchy by jointly learning from frame-level, segment-level, and session-level representations within unsegmented clinical dialogues. Our end-to-end approach dynamically aggregates these multi-granularity features and generates high-quality pseudo-labels to efficiently utilize unlabeled data. Extensive experiments show the framework is model-agnostic, robust across languages and conditions, and highly data-efficient-achieving, for instance, 90\% of fully-supervised performance using only 11 labeled samples. This work provides a principled approach to learning from weak, far-end supervision in medical speech analysis.
<div id='section'>Paperid: <span id='pid'>487, <a href='https://arxiv.org/pdf/2512.16700.pdf' target='_blank'>https://arxiv.org/pdf/2512.16700.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>John M. Statheros, Hairong Wang, Richard Klein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.16700">CLARiTy: A Vision Transformer for Multi-Label Classification and Weakly-Supervised Localization of Chest X-ray Pathologies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The interpretation of chest X-rays (CXRs) poses significant challenges, particularly in achieving accurate multi-label pathology classification and spatial localization. These tasks demand different levels of annotation granularity but are frequently constrained by the scarcity of region-level (dense) annotations. We introduce CLARiTy (Class Localizing and Attention Refining Image Transformer), a vision transformer-based model for joint multi-label classification and weakly-supervised localization of thoracic pathologies. CLARiTy employs multiple class-specific tokens to generate discriminative attention maps, and a SegmentCAM module for foreground segmentation and background suppression using explicit anatomical priors. Trained on image-level labels from the NIH ChestX-ray14 dataset, it leverages distillation from a ConvNeXtV2 teacher for efficiency. Evaluated on the official NIH split, the CLARiTy-S-16-512 (a configuration of CLARiTy), achieves competitive classification performance across 14 pathologies, and state-of-the-art weakly-supervised localization performance on 8 pathologies, outperforming prior methods by 50.7%. In particular, pronounced gains occur for small pathologies like nodules and masses. The lower-resolution variant of CLARiTy, CLARiTy-S-16-224, offers high efficiency while decisively surpassing baselines, thereby having the potential for use in low-resource settings. An ablation study confirms contributions of SegmentCAM, DINO pretraining, orthogonal class token loss, and attention pooling. CLARiTy advances beyond CNN-ViT hybrids by harnessing ViT self-attention for global context and class-specific localization, refined through convolutional background suppression for precise, noise-reduced heatmaps.
<div id='section'>Paperid: <span id='pid'>488, <a href='https://arxiv.org/pdf/2511.16343.pdf' target='_blank'>https://arxiv.org/pdf/2511.16343.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chi-Han Chen, Chieh-Ming Chen, Wen-Huang Cheng, Ching-Chun Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.16343">Aerial View River Landform Video segmentation: A Weakly Supervised Context-aware Temporal Consistency Distillation Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The study of terrain and landform classification through UAV remote sensing diverges significantly from ground vehicle patrol tasks. Besides grappling with the complexity of data annotation and ensuring temporal consistency, it also confronts the scarcity of relevant data and the limitations imposed by the effective range of many technologies. This research substantiates that, in aerial positioning tasks, both the mean Intersection over Union (mIoU) and temporal consistency (TC) metrics are of paramount importance. It is demonstrated that fully labeled data is not the optimal choice, as selecting only key data lacks the enhancement in TC, leading to failures. Hence, a teacher-student architecture, coupled with key frame selection and key frame updating algorithms, is proposed. This framework successfully performs weakly supervised learning and TC knowledge distillation, overcoming the deficiencies of traditional TC training in aerial tasks. The experimental results reveal that our method utilizing merely 30\% of labeled data, concurrently elevates mIoU and temporal consistency ensuring stable localization of terrain objects. Result demo : https://gitlab.com/prophet.ai.inc/drone-based-riverbed-inspection
<div id='section'>Paperid: <span id='pid'>489, <a href='https://arxiv.org/pdf/2511.15393.pdf' target='_blank'>https://arxiv.org/pdf/2511.15393.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kunyu Zhang, Mingxuan Wang, Xiangjie Shi, Haoxing Xu, Chao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.15393">EVA-Net: Interpretable Anomaly Detection for Brain Health via Learning Continuous Aging Prototypes from One-Class EEG Cohorts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The brain age is a key indicator of brain health. While electroencephalography (EEG) is a practical tool for this task, existing models struggle with the common challenge of imperfect medical data, such as learning a ``normal'' baseline from weakly supervised, healthy-only cohorts. This is a critical anomaly detection task for identifying disease, but standard models are often black boxes lacking an interpretable structure. We propose EVA-Net, a novel framework that recasts brain age as an interpretable anomaly detection problem. EVA-Net uses an efficient, sparsified-attention Transformer to model long EEG sequences. To handle noise and variability in imperfect data, it employs a Variational Information Bottleneck to learn a robust, compressed representation. For interpretability, this representation is aligned to a continuous prototype network that explicitly learns the normative healthy aging manifold. Trained on 1297 healthy subjects, EVA-Net achieves state-of-the-art accuracy. We validated its anomaly detection capabilities on an unseen cohort of 27 MCI and AD patients. This pathological group showed significantly higher brain-age gaps and a novel Prototype Alignment Error, confirming their deviation from the healthy manifold. EVA-Net provides an interpretable framework for healthcare intelligence using imperfect medical data.
<div id='section'>Paperid: <span id='pid'>490, <a href='https://arxiv.org/pdf/2511.04892.pdf' target='_blank'>https://arxiv.org/pdf/2511.04892.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vasileios Magoulianitis, Catherine A. Alexander, Jiaxin Yang, C. -C. Jay Kuo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.04892">LG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nuclei segmentation is the cornerstone task in histology image reading, shedding light on the underlying molecular patterns and leading to disease or cancer diagnosis. Yet, it is a laborious task that requires expertise from trained physicians. The large nuclei variability across different organ tissues and acquisition processes challenges the automation of this task. On the other hand, data annotations are expensive to obtain, and thus, Deep Learning (DL) models are challenged to generalize to unseen organs or different domains. This work proposes Local-to-Global NuSegHop (LG-NuSegHop), a self-supervised pipeline developed on prior knowledge of the problem and molecular biology. There are three distinct modules: (1) a set of local processing operations to generate a pseudolabel, (2) NuSegHop a novel data-driven feature extraction model and (3) a set of global operations to post-process the predictions of NuSegHop. Notably, even though the proposed pipeline uses { no manually annotated training data} or domain adaptation, it maintains a good generalization performance on other datasets. Experiments in three publicly available datasets show that our method outperforms other self-supervised and weakly supervised methods while having a competitive standing among fully supervised methods. Remarkably, every module within LG-NuSegHop is transparent and explainable to physicians.
<div id='section'>Paperid: <span id='pid'>491, <a href='https://arxiv.org/pdf/2508.15646.pdf' target='_blank'>https://arxiv.org/pdf/2508.15646.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Swann Emilien CÃ©leste Destouches, Jesse Lahaye, Laurent Valentin Jospin, Jan Skaloud
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15646">Weakly-Supervised Learning for Tree Instances Segmentation in Airborne Lidar Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tree instance segmentation of airborne laser scanning (ALS) data is of utmost importance for forest monitoring, but remains challenging due to variations in the data caused by factors such as sensor resolution, vegetation state at acquisition time, terrain characteristics, etc. Moreover, obtaining a sufficient amount of precisely labeled data to train fully supervised instance segmentation methods is expensive. To address these challenges, we propose a weakly supervised approach where labels of an initial segmentation result obtained either by a non-finetuned model or a closed form algorithm are provided as a quality rating by a human operator. The labels produced during the quality assessment are then used to train a rating model, whose task is to classify a segmentation output into the same classes as specified by the human operator. Finally, the segmentation model is finetuned using feedback from the rating model. This in turn improves the original segmentation model by 34\% in terms of correctly identified tree instances while considerably reducing the number of non-tree instances predicted. Challenges still remain in data over sparsely forested regions characterized by small trees (less than two meters in height) or within complex surroundings containing shrubs, boulders, etc. which can be confused as trees where the performance of the proposed method is reduced.
<div id='section'>Paperid: <span id='pid'>492, <a href='https://arxiv.org/pdf/2508.09665.pdf' target='_blank'>https://arxiv.org/pdf/2508.09665.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmed Alharbi, Hai Dong, Xun Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09665">Social-Sensor Identity Cloning Detection Using Weakly Supervised Deep Forest and Cryptographic Authentication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent years have witnessed a rising trend in social-sensor cloud identity cloning incidents. However, existing approaches suffer from unsatisfactory performance, a lack of solutions for detecting duplicated accounts, and a lack of large-scale evaluations on real-world datasets. We introduce a novel method for detecting identity cloning in social-sensor cloud service providers. Our proposed technique consists of two primary components: 1) a similar identity detection method and 2) a cryptography-based authentication protocol. Initially, we developed a weakly supervised deep forest model to identify similar identities using non-privacy-sensitive user profile features provided by the service. Subsequently, we designed a cryptography-based authentication protocol to verify whether similar identities were generated by the same provider. Our extensive experiments on a large real-world dataset demonstrate the feasibility and superior performance of our technique compared to current state-of-the-art identity clone detection methods.
<div id='section'>Paperid: <span id='pid'>493, <a href='https://arxiv.org/pdf/2508.08912.pdf' target='_blank'>https://arxiv.org/pdf/2508.08912.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mahmoud Salhab, Shameed Sait, Mohammad Abusheikh, Hasan Abusheikh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08912">Munsit at NADI 2025 Shared Task 2: Pushing the Boundaries of Multidialectal Arabic ASR with Weakly Supervised Pretraining and Continual Supervised Fine-tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic speech recognition (ASR) plays a vital role in enabling natural human-machine interaction across applications such as virtual assistants, industrial automation, customer support, and real-time transcription. However, developing accurate ASR systems for low-resource languages like Arabic remains a significant challenge due to limited labeled data and the linguistic complexity introduced by diverse dialects. In this work, we present a scalable training pipeline that combines weakly supervised learning with supervised fine-tuning to develop a robust Arabic ASR model. In the first stage, we pretrain the model on 15,000 hours of weakly labeled speech covering both Modern Standard Arabic (MSA) and various Dialectal Arabic (DA) variants. In the subsequent stage, we perform continual supervised fine-tuning using a mixture of filtered weakly labeled data and a small, high-quality annotated dataset. Our approach achieves state-of-the-art results, ranking first in the multi-dialectal Arabic ASR challenge. These findings highlight the effectiveness of weak supervision paired with fine-tuning in overcoming data scarcity and delivering high-quality ASR for low-resource, dialect-rich languages.
<div id='section'>Paperid: <span id='pid'>494, <a href='https://arxiv.org/pdf/2508.06819.pdf' target='_blank'>https://arxiv.org/pdf/2508.06819.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ayaan Nooruddin Siddiqui, Mahnoor Zaidi, Ayesha Nazneen Shahbaz, Priyadarshini Chatterjee, Krishnan Menon Iyer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06819">VesselRW: Weakly Supervised Subcutaneous Vessel Segmentation via Learned Random Walk Propagation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The task of parsing subcutaneous vessels in clinical images is often hindered by the high cost and limited availability of ground truth data, as well as the challenge of low contrast and noisy vessel appearances across different patients and imaging modalities. In this work, we propose a novel weakly supervised training framework specifically designed for subcutaneous vessel segmentation. This method utilizes low-cost, sparse annotations such as centerline traces, dot markers, or short scribbles to guide the learning process. These sparse annotations are expanded into dense probabilistic supervision through a differentiable random walk label propagation model, which integrates vesselness cues and tubular continuity priors driven by image data. The label propagation process results in per-pixel hitting probabilities and uncertainty estimates, which are incorporated into an uncertainty-weighted loss function to prevent overfitting in ambiguous areas. Notably, the label propagation model is trained jointly with a CNN-based segmentation network, allowing the system to learn vessel boundaries and continuity constraints without the need for explicit edge supervision. Additionally, we introduce a topology-aware regularizer that encourages centerline connectivity and penalizes irrelevant branches, further enhancing clinical applicability. Our experiments on clinical subcutaneous imaging datasets demonstrate that our approach consistently outperforms both naive sparse-label training and traditional dense pseudo-labeling methods, yielding more accurate vascular maps and better-calibrated uncertainty, which is crucial for clinical decision-making. This method significantly reduces the annotation workload while maintaining clinically relevant vessel topology.
<div id='section'>Paperid: <span id='pid'>495, <a href='https://arxiv.org/pdf/2507.22002.pdf' target='_blank'>https://arxiv.org/pdf/2507.22002.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yida Tao, Yen-Chia Hsu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22002">Bridging Synthetic and Real-World Domains: A Human-in-the-Loop Weakly-Supervised Framework for Industrial Toxic Emission Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industrial smoke segmentation is critical for air-quality monitoring and environmental protection but is often hampered by the high cost and scarcity of pixel-level annotations in real-world settings. We introduce CEDANet, a human-in-the-loop, class-aware domain adaptation framework that uniquely integrates weak, citizen-provided video-level labels with adversarial feature alignment. Specifically, we refine pseudo-labels generated by a source-trained segmentation model using citizen votes, and employ class-specific domain discriminators to transfer rich source-domain representations to the industrial domain. Comprehensive experiments on SMOKE5K and custom IJmond datasets demonstrate that CEDANet achieves an F1-score of 0.414 and a smoke-class IoU of 0.261 with citizen feedback, vastly outperforming the baseline model, which scored 0.083 and 0.043 respectively. This represents a five-fold increase in F1-score and a six-fold increase in smoke-class IoU. Notably, CEDANet with citizen-constrained pseudo-labels achieves performance comparable to the same architecture trained on limited 100 fully annotated images with F1-score of 0.418 and IoU of 0.264, demonstrating its ability to reach small-sampled fully supervised-level accuracy without target-domain annotations. Our research validates the scalability and cost-efficiency of combining citizen science with weakly supervised domain adaptation, offering a practical solution for complex, data-scarce environmental monitoring applications.
<div id='section'>Paperid: <span id='pid'>496, <a href='https://arxiv.org/pdf/2507.03292.pdf' target='_blank'>https://arxiv.org/pdf/2507.03292.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pattaramanee Arsomngern, Sasikarn Khwanmuang, Matthias NieÃner, Supasorn Suwajanakorn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03292">Zero-shot Inexact CAD Model Alignment from a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One practical approach to infer 3D scene structure from a single image is to retrieve a closely matching 3D model from a database and align it with the object in the image. Existing methods rely on supervised training with images and pose annotations, which limits them to a narrow set of object categories. To address this, we propose a weakly supervised 9-DoF alignment method for inexact 3D models that requires no pose annotations and generalizes to unseen categories. Our approach derives a novel feature space based on foundation features that ensure multi-view consistency and overcome symmetry ambiguities inherent in foundation features using a self-supervised triplet loss. Additionally, we introduce a texture-invariant pose refinement technique that performs dense alignment in normalized object coordinates, estimated through the enhanced feature space. We conduct extensive evaluations on the real-world ScanNet25k dataset, where our method outperforms SOTA weakly supervised baselines by +4.3% mean alignment accuracy and is the only weakly supervised approach to surpass the supervised ROCA by +2.7%. To assess generalization, we introduce SUN2CAD, a real-world test set with 20 novel object categories, where our method achieves SOTA results without prior training on them.
<div id='section'>Paperid: <span id='pid'>497, <a href='https://arxiv.org/pdf/2506.16745.pdf' target='_blank'>https://arxiv.org/pdf/2506.16745.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi-Ying Sun, Wan-Lei Zhao, Yi-Bo Miao, Chong-Wah Ngo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.16745">Class Agnostic Instance-level Descriptor for Visual Instance Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the great success of the deep features in content-based image retrieval, the visual instance search remains challenging due to the lack of effective instance level feature representation. Supervised or weakly supervised object detection methods are not among the options due to their poor performance on the unknown object categories. In this paper, based on the feature set output from self-supervised ViT, the instance level region discovery is modeled as detecting the compact feature subsets in a hierarchical fashion. The hierarchical decomposition results in a hierarchy of feature subsets. The non-leaf nodes and leaf nodes on the hierarchy correspond to the various instance regions in an image of different semantic scales. The hierarchical decomposition well addresses the problem of object embedding and occlusions, which are widely observed in the real scenarios. The features derived from the nodes on the hierarchy make up a comprehensive representation for the latent instances in the image. Our instance-level descriptor remains effective on both the known and unknown object categories. Empirical studies on three instance search benchmarks show that it outperforms state-of-the-art methods considerably.
<div id='section'>Paperid: <span id='pid'>498, <a href='https://arxiv.org/pdf/2506.02166.pdf' target='_blank'>https://arxiv.org/pdf/2506.02166.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arnav Rustagi, Satvik Bajpai, Nimrat Kaur, Siddharth Siddharth
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02166">Dhvani: A Weakly-supervised Phonemic Error Detection and Personalized Feedback System for Hindi</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computer-Assisted Pronunciation Training (CAPT) has been extensively studied for English. However, there remains a critical gap in its application to Indian languages with a base of 1.5 billion speakers. Pronunciation tools tailored to Indian languages are strikingly lacking despite the fact that millions learn them every year. With over 600 million speakers and being the fourth most-spoken language worldwide, improving Hindi pronunciation is a vital first step toward addressing this gap. This paper proposes 1) Dhvani -- a novel CAPT system for Hindi, 2) synthetic speech generation for Hindi mispronunciations, and 3) a novel methodology for providing personalized feedback to learners. While the system often interacts with learners using Devanagari graphemes, its core analysis targets phonemic distinctions, leveraging Hindi's highly phonetic orthography to analyze mispronounced speech and provide targeted feedback.
<div id='section'>Paperid: <span id='pid'>499, <a href='https://arxiv.org/pdf/2504.14860.pdf' target='_blank'>https://arxiv.org/pdf/2504.14860.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyi Liu, Yangcen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14860">Bridge the Gap: From Weak to Full Supervision for Temporal Action Localization with PseudoFormer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised Temporal Action Localization (WTAL) has achieved notable success but still suffers from a lack of temporal annotations, leading to a performance and framework gap compared with fully-supervised methods. While recent approaches employ pseudo labels for training, three key challenges: generating high-quality pseudo labels, making full use of different priors, and optimizing training methods with noisy labels remain unresolved. Due to these perspectives, we propose PseudoFormer, a novel two-branch framework that bridges the gap between weakly and fully-supervised Temporal Action Localization (TAL). We first introduce RickerFusion, which maps all predicted action proposals to a global shared space to generate pseudo labels with better quality. Subsequently, we leverage both snippet-level and proposal-level labels with different priors from the weak branch to train the regression-based model in the full branch. Finally, the uncertainty mask and iterative refinement mechanism are applied for training with noisy pseudo labels. PseudoFormer achieves state-of-the-art WTAL results on the two commonly used benchmarks, THUMOS14 and ActivityNet1.3. Besides, extensive ablation studies demonstrate the contribution of each component of our method.
<div id='section'>Paperid: <span id='pid'>500, <a href='https://arxiv.org/pdf/2504.12254.pdf' target='_blank'>https://arxiv.org/pdf/2504.12254.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mahmoud Salhab, Marwan Elghitany, Shameed Sait, Syed Sibghat Ullah, Mohammad Abusheikh, Hasan Abusheikh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12254">Advancing Arabic Speech Recognition Through Large-Scale Weakly Supervised Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic speech recognition (ASR) is crucial for human-machine interaction in diverse applications like conversational agents, industrial robotics, call center automation, and automated subtitling. However, developing high-performance ASR models remains challenging, particularly for low-resource languages like Arabic, due to the scarcity of large, labeled speech datasets, which are costly and labor-intensive to produce. In this work, we employ weakly supervised learning to train an Arabic ASR model using the Conformer architecture. Our model is trained from scratch on 15,000 hours of weakly annotated speech data covering both Modern Standard Arabic (MSA) and Dialectal Arabic (DA), eliminating the need for costly manual transcriptions. Despite the absence of human-verified labels, our approach achieves state-of-the-art (SOTA) results in Arabic ASR, surpassing both open and closed-source models on standard benchmarks. By demonstrating the effectiveness of weak supervision as a scalable, cost-efficient alternative to traditional supervised approaches, paving the way for improved ASR systems in low resource settings.
<div id='section'>Paperid: <span id='pid'>501, <a href='https://arxiv.org/pdf/2504.10613.pdf' target='_blank'>https://arxiv.org/pdf/2504.10613.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xing David Wang, Ulf Leser
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10613">Enhancing Document Retrieval for Curating N-ary Relations in Knowledge Bases</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Curation of biomedical knowledge bases (KBs) relies on extracting accurate multi-entity relational facts from the literature - a process that remains largely manual and expert-driven. An essential step in this workflow is retrieving documents that can support or complete partially observed n-ary relations. We present a neural retrieval model designed to assist KB curation by identifying documents that help fill in missing relation arguments and provide relevant contextual evidence.
  To reduce dependence on scarce gold-standard training data, we exploit existing KB records to construct weakly supervised training sets. Our approach introduces two key technical contributions: (i) a layered contrastive loss that enables learning from noisy and incomplete relational structures, and (ii) a balanced sampling strategy that generates high-quality negatives from diverse KB records. On two biomedical retrieval benchmarks, our approach achieves state-of-the-art performance, outperforming strong baselines in NDCG@10 by 5.7 and 3.7 percentage points, respectively.
<div id='section'>Paperid: <span id='pid'>502, <a href='https://arxiv.org/pdf/2503.22668.pdf' target='_blank'>https://arxiv.org/pdf/2503.22668.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sindhu B Hegde, K R Prajwal, Taein Kwon, Andrew Zisserman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22668">Understanding Co-speech Gestures in-the-wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Co-speech gestures play a vital role in non-verbal communication. In this paper, we introduce a new framework for co-speech gesture understanding in the wild. Specifically, we propose three new tasks and benchmarks to evaluate a model's capability to comprehend gesture-speech-text associations: (i) gesture based retrieval, (ii) gesture word spotting, and (iii) active speaker detection using gestures. We present a new approach that learns a tri-modal video-gesture-speech-text representation to solve these tasks. By leveraging a combination of global phrase contrastive loss and local gesture-word coupling loss, we demonstrate that a strong gesture representation can be learned in a weakly supervised manner from videos in the wild. Our learned representations outperform previous methods, including large vision-language models (VLMs). Further analysis reveals that speech and text modalities capture distinct gesture related signals, underscoring the advantages of learning a shared tri-modal embedding space. The dataset, model, and code are available at: https://www.robots.ox.ac.uk/~vgg/research/jegal.
<div id='section'>Paperid: <span id='pid'>503, <a href='https://arxiv.org/pdf/2502.17836.pdf' target='_blank'>https://arxiv.org/pdf/2502.17836.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Nawaz, Basma Nasir, Tehseen Zia, Zawar Hussain, Catarina Moreira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17836">TagGAN: A Generative Model for Data Tagging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Precise identification and localization of disease-specific features at the pixel-level are particularly important for early diagnosis, disease progression monitoring, and effective treatment in medical image analysis. However, conventional diagnostic AI systems lack decision transparency and cannot operate well in environments where there is a lack of pixel-level annotations. In this study, we propose a novel Generative Adversarial Networks (GANs)-based framework, TagGAN, which is tailored for weakly-supervised fine-grained disease map generation from purely image-level labeled data. TagGAN generates a pixel-level disease map during domain translation from an abnormal image to a normal representation. Later, this map is subtracted from the input abnormal image to convert it into its normal counterpart while preserving all the critical anatomical details. Our method is first to generate fine-grained disease maps to visualize disease lesions in a weekly supervised setting without requiring pixel-level annotations. This development enhances the interpretability of diagnostic AI by providing precise visualizations of disease-specific regions. It also introduces automated binary mask generation to assist radiologists. Empirical evaluations carried out on the benchmark datasets, CheXpert, TBX11K, and COVID-19, demonstrate the capability of TagGAN to outperform current top models in accurately identifying disease-specific pixels. This outcome highlights the capability of the proposed model to tag medical images, significantly reducing the workload for radiologists by eliminating the need for binary masks during training.
<div id='section'>Paperid: <span id='pid'>504, <a href='https://arxiv.org/pdf/2502.17824.pdf' target='_blank'>https://arxiv.org/pdf/2502.17824.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Basma Nasir, Tehseen Zia, Muhammad Nawaz, Catarina Moreira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17824">Weakly Supervised Pixel-Level Annotation with Visual Interpretability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical image annotation is essential for diagnosing diseases, yet manual annotation is time-consuming, costly, and prone to variability among experts. To address these challenges, we propose an automated explainable annotation system that integrates ensemble learning, visual explainability, and uncertainty quantification. Our approach combines three pre-trained deep learning models - ResNet50, EfficientNet, and DenseNet - enhanced with XGrad-CAM for visual explanations and Monte Carlo Dropout for uncertainty quantification. This ensemble mimics the consensus of multiple radiologists by intersecting saliency maps from models that agree on the diagnosis while uncertain predictions are flagged for human review. We evaluated our system using the TBX11K medical imaging dataset and a Fire segmentation dataset, demonstrating its robustness across different domains. Experimental results show that our method outperforms baseline models, achieving 93.04% accuracy on TBX11K and 96.4% accuracy on the Fire dataset. Moreover, our model produces precise pixel-level annotations despite being trained with only image-level labels, achieving Intersection over Union IoU scores of 36.07% and 64.7%, respectively. By enhancing the accuracy and interpretability of image annotations, our approach offers a reliable and transparent solution for medical diagnostics and other image analysis tasks.
<div id='section'>Paperid: <span id='pid'>505, <a href='https://arxiv.org/pdf/2501.04582.pdf' target='_blank'>https://arxiv.org/pdf/2501.04582.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Miaoyang He, Shuyong Gao, Tsui Qin Mok, Weifeng Ge, Wengqiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04582">Boosting Salient Object Detection with Knowledge Distillated from Large Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Salient Object Detection (SOD) aims to identify and segment prominent regions within a scene. Traditional models rely on manually annotated pseudo labels with precise pixel-level accuracy, which is time-consuming. We developed a low-cost, high-precision annotation method by leveraging large foundation models to address the challenges. Specifically, we use a weakly supervised approach to guide large models in generating pseudo-labels through textual prompts. Since large models do not effectively focus on the salient regions of images, we manually annotate a subset of text to fine-tune the model. Based on this approach, which enables precise and rapid generation of pseudo-labels, we introduce a new dataset, BDS-TR. Compared to the previous DUTS-TR dataset, BDS-TR is more prominent in scale and encompasses a wider variety of categories and scenes. This expansion will enhance our model's applicability across a broader range of scenarios and provide a more comprehensive foundational dataset for future SOD research. Additionally, we present an edge decoder based on dynamic upsampling, which focuses on object edges while gradually recovering image feature resolution. Comprehensive experiments on five benchmark datasets demonstrate that our method significantly outperforms state-of-the-art approaches and also surpasses several existing fully-supervised SOD methods. The code and results will be made available.
<div id='section'>Paperid: <span id='pid'>506, <a href='https://arxiv.org/pdf/2412.14561.pdf' target='_blank'>https://arxiv.org/pdf/2412.14561.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jintao Huang, Yiu-ming Cheung, Chi-man Vong, Wenbin Qian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14561">GBRIP: Granular Ball Representation for Imbalanced Partial Label Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partial label learning (PLL) is a complicated weakly supervised multi-classification task compounded by class imbalance. Currently, existing methods only rely on inter-class pseudo-labeling from inter-class features, often overlooking the significant impact of the intra-class imbalanced features combined with the inter-class. To address these limitations, we introduce Granular Ball Representation for Imbalanced PLL (GBRIP), a novel framework for imbalanced PLL. GBRIP utilizes coarse-grained granular ball representation and multi-center loss to construct a granular ball-based nfeature space through unsupervised learning, effectively capturing the feature distribution within each class. GBRIP mitigates the impact of confusing features by systematically refining label disambiguation and estimating imbalance distributions. The novel multi-center loss function enhances learning by emphasizing the relationships between samples and their respective centers within the granular balls. Extensive experiments on standard benchmarks demonstrate that GBRIP outperforms existing state-of-the-art methods, offering a robust solution to the challenges of imbalanced PLL.
<div id='section'>Paperid: <span id='pid'>507, <a href='https://arxiv.org/pdf/2412.12331.pdf' target='_blank'>https://arxiv.org/pdf/2412.12331.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>PhÃºc H. Le Khac, Graham Healy, Alan F. Smeaton
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12331">Efficient Object-centric Representation Learning with Pre-trained Geometric Prior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses key challenges in object-centric representation learning of video. While existing approaches struggle with complex scenes, we propose a novel weakly-supervised framework that emphasises geometric understanding and leverages pre-trained vision models to enhance object discovery. Our method introduces an efficient slot decoder specifically designed for object-centric learning, enabling effective representation of multi-object scenes without requiring explicit depth information. Results on synthetic video benchmarks with increasing complexity in terms of objects and their movement, object occlusion and camera motion demonstrate that our approach achieves comparable performance to supervised methods while maintaining computational efficiency. This advances the field towards more practical applications in complex real-world scenarios.
<div id='section'>Paperid: <span id='pid'>508, <a href='https://arxiv.org/pdf/2412.11467.pdf' target='_blank'>https://arxiv.org/pdf/2412.11467.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuyang Xie, Yan Yang, Yankai Yu, Jie Wang, Yongquan Jiang, Xiao Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11467">Exploring Temporal Event Cues for Dense Video Captioning in Cyclic Co-learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dense video captioning aims to detect and describe all events in untrimmed videos. This paper presents a dense video captioning network called Multi-Concept Cyclic Learning (MCCL), which aims to: (1) detect multiple concepts at the frame level, using these concepts to enhance video features and provide temporal event cues; and (2) design cyclic co-learning between the generator and the localizer within the captioning network to promote semantic perception and event localization. Specifically, we perform weakly supervised concept detection for each frame, and the detected concept embeddings are integrated into the video features to provide event cues. Additionally, video-level concept contrastive learning is introduced to obtain more discriminative concept embeddings. In the captioning network, we establish a cyclic co-learning strategy where the generator guides the localizer for event localization through semantic matching, while the localizer enhances the generator's event semantic perception through location matching, making semantic perception and event localization mutually beneficial. MCCL achieves state-of-the-art performance on the ActivityNet Captions and YouCook2 datasets. Extensive experiments demonstrate its effectiveness and interpretability.
<div id='section'>Paperid: <span id='pid'>509, <a href='https://arxiv.org/pdf/2411.13528.pdf' target='_blank'>https://arxiv.org/pdf/2411.13528.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>James Willoughby, Irina Voiculescu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13528">Entropy Bootstrapping for Weakly Supervised Nuclei Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Microscopy structure segmentation, such as detecting cells or nuclei, generally requires a human to draw a ground truth contour around each instance. Weakly supervised approaches (e.g. consisting of only single point labels) have the potential to reduce this workload significantly. Our approach uses individual point labels for an entropy estimation to approximate an underlying distribution of cell pixels. We infer full cell masks from this distribution, and use Mask-RCNN to produce an instance segmentation output. We compare this point--annotated approach with training on the full ground truth masks. We show that our method achieves a comparatively good level of performance, despite a 95% reduction in pixel labels.
<div id='section'>Paperid: <span id='pid'>510, <a href='https://arxiv.org/pdf/2410.19332.pdf' target='_blank'>https://arxiv.org/pdf/2410.19332.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianning Chi, Zelan Li, Huixuan Wu, Wenjun Zhang, Ying Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.19332">Beyond Point Annotation: A Weakly Supervised Network Guided by Multi-Level Labels Generated from Four-Point Annotation for Thyroid Nodule Segmentation in Ultrasound Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised methods typically guided the pixel-wise training by comparing the predictions to single-level labels containing diverse segmentation-related information at once, but struggled to represent delicate feature differences between nodule and background regions and confused incorrect information, resulting in underfitting or overfitting in the segmentation predictions. In this work, we propose a weakly-supervised network that generates multi-level labels from four-point annotation to refine diverse constraints for delicate nodule segmentation. The Distance-Similarity Fusion Prior referring to the points annotations filters out information irrelevant to nodules. The bounding box and pure foreground/background labels, generated from the point annotation, guarantee the rationality of the prediction in the arrangement of target localization and the spatial distribution of target/background regions, respectively. Our proposed network outperforms existing weakly-supervised methods on two public datasets with respect to the accuracy and robustness, improving the applicability of deep-learning based segmentation in the clinical practice of thyroid nodule diagnosis.
<div id='section'>Paperid: <span id='pid'>511, <a href='https://arxiv.org/pdf/2601.18891.pdf' target='_blank'>https://arxiv.org/pdf/2601.18891.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ghazaleh Serati, Samuel Foucher, Jerome Theau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.18891">Weakly supervised framework for wildlife detection and counting in challenging Arctic environments: a case study on caribou (Rangifer tarandus)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Caribou across the Arctic has declined in recent decades, motivating scalable and accurate monitoring approaches to guide evidence-based conservation actions and policy decisions. Manual interpretation from this imagery is labor-intensive and error-prone, underscoring the need for automatic and reliable detection across varying scenes. Yet, such automatic detection is challenging due to severe background heterogeneity, dominant empty terrain (class imbalance), small or occluded targets, and wide variation in density and scale. To make the detection model (HerdNet) more robust to these challenges, a weakly supervised patch-level pretraining based on a detection network's architecture is proposed. The detection dataset includes five caribou herds distributed across Alaska. By learning from empty vs. non-empty labels in this dataset, the approach produces early weakly supervised knowledge for enhanced detection compared to HerdNet, which is initialized from generic weights. Accordingly, the patch-based pretrain network attained high accuracy on multi-herd imagery (2017) and on an independent year's (2019) test sets (F1: 93.7%/92.6%, respectively), enabling reliable mapping of regions containing animals to facilitate manual counting on large aerial imagery. Transferred to detection, initialization from weakly supervised pretraining yielded consistent gains over ImageNet weights on both positive patches (F1: 92.6%/93.5% vs. 89.3%/88.6%), and full-image counting (F1: 95.5%/93.3% vs. 91.5%/90.4%). Remaining limitations are false positives from animal-like background clutter and false negatives related to low animal density occlusions. Overall, pretraining on coarse labels prior to detection makes it possible to rely on weakly-supervised pretrained weights even when labeled data are limited, achieving results comparable to generic-weight initialization.
<div id='section'>Paperid: <span id='pid'>512, <a href='https://arxiv.org/pdf/2601.14718.pdf' target='_blank'>https://arxiv.org/pdf/2601.14718.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiyang Fu, Hui Li, Wangyu Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.14718">Context Patch Fusion With Class Token Enhancement for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Semantic Segmentation (WSSS), which relies only on image-level labels, has attracted significant attention for its cost-effectiveness and scalability. Existing methods mainly enhance inter-class distinctions and employ data augmentation to mitigate semantic ambiguity and reduce spurious activations. However, they often neglect the complex contextual dependencies among image patches, resulting in incomplete local representations and limited segmentation accuracy. To address these issues, we propose the Context Patch Fusion with Class Token Enhancement (CPF-CTE) framework, which exploits contextual relations among patches to enrich feature representations and improve segmentation. At its core, the Contextual-Fusion Bidirectional Long Short-Term Memory (CF-BiLSTM) module captures spatial dependencies between patches and enables bidirectional information flow, yielding a more comprehensive understanding of spatial correlations. This strengthens feature learning and segmentation robustness. Moreover, we introduce learnable class tokens that dynamically encode and refine class-specific semantics, enhancing discriminative capability. By effectively integrating spatial and semantic cues, CPF-CTE produces richer and more accurate representations of image content. Extensive experiments on PASCAL VOC 2012 and MS COCO 2014 validate that CPF-CTE consistently surpasses prior WSSS methods.
<div id='section'>Paperid: <span id='pid'>513, <a href='https://arxiv.org/pdf/2601.06572.pdf' target='_blank'>https://arxiv.org/pdf/2601.06572.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huyen Khanh Vo, Isabel Valera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.06572">Hellinger Multimodal Variational Autoencoders</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal variational autoencoders (VAEs) are widely used for weakly supervised generative learning with multiple modalities. Predominant methods aggregate unimodal inference distributions using either a product of experts (PoE), a mixture of experts (MoE), or their combinations to approximate the joint posterior. In this work, we revisit multimodal inference through the lens of probabilistic opinion pooling, an optimization-based approach. We start from Hölder pooling with $α=0.5$, which corresponds to the unique symmetric member of the $α\text{-divergence}$ family, and derive a moment-matching approximation, termed Hellinger. We then leverage such an approximation to propose HELVAE, a multimodal VAE that avoids sub-sampling, yielding an efficient yet effective model that: (i) learns more expressive latent representations as additional modalities are observed; and (ii) empirically achieves better trade-offs between generative coherence and quality, outperforming state-of-the-art multimodal VAE models.
<div id='section'>Paperid: <span id='pid'>514, <a href='https://arxiv.org/pdf/2512.19550.pdf' target='_blank'>https://arxiv.org/pdf/2512.19550.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Naresh Manwani, M Elamparithy, Tanish Taneja
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.19550">DFORD: Directional Feedback based Online Ordinal Regression Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce directional feedback in the ordinal regression setting, in which the learner receives feedback on whether the predicted label is on the left or the right side of the actual label. This is a weak supervision setting for ordinal regression compared to the full information setting, where the learner can access the labels. We propose an online algorithm for ordinal regression using directional feedback. The proposed algorithm uses an exploration-exploitation scheme to learn from directional feedback efficiently. Furthermore, we introduce its kernel-based variant to learn non-linear ordinal regression models in an online setting. We use a truncation trick to make the kernel implementation more memory efficient. The proposed algorithm maintains the ordering of the thresholds in the expected sense. Moreover, it achieves the expected regret of $\mathcal{O}(\log T)$. We compare our approach with a full information and a weakly supervised algorithm for ordinal regression on synthetic and real-world datasets. The proposed approach, which learns using directional feedback, performs comparably (sometimes better) to its full information counterpart.
<div id='section'>Paperid: <span id='pid'>515, <a href='https://arxiv.org/pdf/2512.11057.pdf' target='_blank'>https://arxiv.org/pdf/2512.11057.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marshal Ashif Shawkat, Moidul Hasan, Taufiq Hasan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.11057">Weakly Supervised Tuberculosis Localization in Chest X-rays through Knowledge Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tuberculosis (TB) remains one of the leading causes of mortality worldwide, particularly in resource-limited countries. Chest X-ray (CXR) imaging serves as an accessible and cost-effective diagnostic tool but requires expert interpretation, which is often unavailable. Although machine learning models have shown high performance in TB classification, they often depend on spurious correlations and fail to generalize. Besides, building large datasets featuring high-quality annotations for medical images demands substantial resources and input from domain specialists, and typically involves several annotators reaching agreement, which results in enormous financial and logistical expenses. This study repurposes knowledge distillation technique to train CNN models reducing spurious correlations and localize TB-related abnormalities without requiring bounding-box annotations. By leveraging a teacher-student framework with ResNet50 architecture, the proposed method trained on TBX11k dataset achieve impressive 0.2428 mIOU score. Experimental results further reveal that the student model consistently outperforms the teacher, underscoring improved robustness and potential for broader clinical deployment in diverse settings.
<div id='section'>Paperid: <span id='pid'>516, <a href='https://arxiv.org/pdf/2512.05364.pdf' target='_blank'>https://arxiv.org/pdf/2512.05364.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ananth Hariharan, David Mortensen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.05364">Transformer-Enabled Diachronic Analysis of Vedic Sanskrit: Neural Methods for Quantifying Types of Language Change</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study demonstrates how hybrid neural-symbolic methods can yield significant new insights into the evolution of a morphologically rich, low-resource language. We challenge the naive assumption that linguistic change is simplification by quantitatively analyzing over 2,000 years of Sanskrit, demonstrating how weakly-supervised hybrid methods can yield new insights into the evolution of morphologically rich, low-resource languages. Our approach addresses data scarcity through weak supervision, using 100+ high-precision regex patterns to generate pseudo-labels for fine-tuning a multilingual BERT. We then fuse symbolic and neural outputs via a novel confidence-weighted ensemble, creating a system that is both scalable and interpretable. Applying this framework to a 1.47-million-word diachronic corpus, our ensemble achieves a 52.4% overall feature detection rate. Our findings reveal that Sanskrit's overall morphological complexity does not decrease but is instead dynamically redistributed: while earlier verbal features show cyclical patterns of decline, complexity shifts to other domains, evidenced by a dramatic expansion in compounding and the emergence of new philosophical terminology. Critically, our system produces well-calibrated uncertainty estimates, with confidence strongly correlating with accuracy (Pearson r = 0.92) and low overall calibration error (ECE = 0.043), bolstering the reliability of these findings for computational philology.
<div id='section'>Paperid: <span id='pid'>517, <a href='https://arxiv.org/pdf/2512.02344.pdf' target='_blank'>https://arxiv.org/pdf/2512.02344.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyuan Sun, Yongping Zhang, Hongcheng Zeng, Yamin Wang, Wei Yang, Wanting Yang, Jie Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.02344">A multi-weight self-matching visual explanation for cnns on sar images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, convolutional neural networks (CNNs) have achieved significant success in various synthetic aperture radar (SAR) tasks. However, the complexity and opacity of their internal mechanisms hinder the fulfillment of high-reliability requirements, thereby limiting their application in SAR. Improving the interpretability of CNNs is thus of great importance for their development and deployment in SAR. In this paper, a visual explanation method termed multi-weight self-matching class activation mapping (MS-CAM) is proposed. MS-CAM matches SAR images with the feature maps and corresponding gradients extracted by the CNN, and combines both channel-wise and element-wise weights to visualize the decision basis learned by the model in SAR images. Extensive experiments conducted on a self-constructed SAR target classification dataset demonstrate that MS-CAM more accurately highlights the network's regions of interest and captures detailed target feature information, thereby enhancing network interpretability. Furthermore, the feasibility of applying MS-CAM to weakly-supervised obiect localization is validated. Key factors affecting localization accuracy, such as pixel thresholds, are analyzed in depth to inform future work.
<div id='section'>Paperid: <span id='pid'>518, <a href='https://arxiv.org/pdf/2511.18012.pdf' target='_blank'>https://arxiv.org/pdf/2511.18012.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaying Zhou, Qingchao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.18012">State and Scene Enhanced Prototypes for Weakly Supervised Open-Vocabulary Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-Vocabulary Object Detection (OVOD) aims to generalize object recognition to novel categories, while Weakly Supervised OVOD (WS-OVOD) extends this by combining box-level annotations with image-level labels. Despite recent progress, two critical challenges persist in this setting. First, existing semantic prototypes, even when enriched by LLMs, are static and limited, failing to capture the rich intra-class visual variations induced by different object states (e.g., a cat's pose). Second, the standard pseudo-box generation introduces a semantic mismatch between visual region proposals (which contain context) and object-centric text embeddings. To tackle these issues, we introduce two complementary prototype enhancement strategies. To capture intra-class variations in appearance and state, we propose the State-Enhanced Semantic Prototypes (SESP), which generates state-aware textual descriptions (e.g., "a sleeping cat") to capture diverse object appearances, yielding more discriminative prototypes. Building on this, we further introduce Scene-Augmented Pseudo Prototypes (SAPP) to address the semantic mismatch. SAPP incorporates contextual semantics (e.g., "cat lying on sofa") and utilizes a soft alignment mechanism to promote contextually consistent visual-textual representations. By integrating SESP and SAPP, our method effectively enhances both the richness of semantic prototypes and the visual-textual alignment, achieving notable improvements.
<div id='section'>Paperid: <span id='pid'>519, <a href='https://arxiv.org/pdf/2511.03361.pdf' target='_blank'>https://arxiv.org/pdf/2511.03361.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gabriel Pirlogeanu, Alexandru-Lucian Georgescu, Horia Cucu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.03361">Open Source State-Of-the-Art Solution for Romanian Speech Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we present a new state-of-the-art Romanian Automatic Speech Recognition (ASR) system based on NVIDIA's FastConformer architecture--explored here for the first time in the context of Romanian. We train our model on a large corpus of, mostly, weakly supervised transcriptions, totaling over 2,600 hours of speech. Leveraging a hybrid decoder with both Connectionist Temporal Classification (CTC) and Token-Duration Transducer (TDT) branches, we evaluate a range of decoding strategies including greedy, ALSD, and CTC beam search with a 6-gram token-level language model. Our system achieves state-of-the-art performance across all Romanian evaluation benchmarks, including read, spontaneous, and domain-specific speech, with up to 27% relative WER reduction compared to previous best-performing systems. In addition to improved transcription accuracy, our approach demonstrates practical decoding efficiency, making it suitable for both research and deployment in low-latency ASR applications.
<div id='section'>Paperid: <span id='pid'>520, <a href='https://arxiv.org/pdf/2511.02576.pdf' target='_blank'>https://arxiv.org/pdf/2511.02576.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alix de Langlais, Benjamin Billot, Théo Aguilar Vidal, Marc-Olivier Gauci, Hervé Delingette
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.02576">Resource-efficient Automatic Refinement of Segmentations via Weak Supervision from Light Feedback</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Delineating anatomical regions is a key task in medical image analysis. Manual segmentation achieves high accuracy but is labor-intensive and prone to variability, thus prompting the development of automated approaches. Recently, a breadth of foundation models has enabled automated segmentations across diverse anatomies and imaging modalities, but these may not always meet the clinical accuracy standards. While segmentation refinement strategies can improve performance, current methods depend on heavy user interactions or require fully supervised segmentations for training. Here, we present SCORE (Segmentation COrrection from Regional Evaluations), a weakly supervised framework that learns to refine mask predictions only using light feedback during training. Specifically, instead of relying on dense training image annotations, SCORE introduces a novel loss that leverages region-wise quality scores and over/under-segmentation error labels. We demonstrate SCORE on humerus CT scans, where it considerably improves initial predictions from TotalSegmentator, and achieves performance on par with existing refinement methods, while greatly reducing their supervision requirements and annotation time. Our code is available at: https://gitlab.inria.fr/adelangl/SCORE.
<div id='section'>Paperid: <span id='pid'>521, <a href='https://arxiv.org/pdf/2507.21959.pdf' target='_blank'>https://arxiv.org/pdf/2507.21959.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheyuan Zhang, Yen-chia Hsu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21959">Mitigating Spurious Correlations in Weakly Supervised Semantic Segmentation via Cross-architecture Consistency Regularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scarcity of pixel-level labels is a significant challenge in practical scenarios. In specific domains like industrial smoke, acquiring such detailed annotations is particularly difficult and often requires expert knowledge. To alleviate this, weakly supervised semantic segmentation (WSSS) has emerged as a promising approach. However, due to the supervision gap and inherent bias in models trained with only image level labels, existing WSSS methods suffer from limitations such as incomplete foreground coverage, inaccurate object boundaries, and spurious correlations, especially in our domain, where emissions are always spatially coupled with chimneys.
  Previous solutions typically rely on additional priors or external knowledge to mitigate these issues, but they often lack scalability and fail to address the model's inherent bias toward co-occurring context. To address this, we propose a novel WSSS framework that directly targets the co-occurrence problem without relying on external supervision. Unlike prior methods that adopt a single network, we employ a teacher-student framework that combines CNNs and ViTs. We introduce a knowledge transfer loss that enforces cross-architecture consistency by aligning internal representations. Additionally, we incorporate post-processing techniques to address partial coverage and further improve pseudo mask quality.
<div id='section'>Paperid: <span id='pid'>522, <a href='https://arxiv.org/pdf/2507.21587.pdf' target='_blank'>https://arxiv.org/pdf/2507.21587.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheyuan Zhang, Wang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21587">Emerging Trends in Pseudo-Label Refinement for Weakly Supervised Semantic Segmentation with Image-Level Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unlike fully supervised semantic segmentation, weakly supervised semantic segmentation (WSSS) relies on weaker forms of supervision to perform dense prediction tasks. Among the various types of weak supervision, WSSS with image level annotations is considered both the most challenging and the most practical, attracting significant research attention. Therefore, in this review, we focus on WSSS with image level annotations. Additionally, this review concentrates on mainstream research directions, deliberately omitting less influential branches.
  Given the rapid development of new methods and the limitations of existing surveys in capturing recent trends, there is a pressing need for an updated and comprehensive review. Our goal is to fill this gap by synthesizing the latest advancements and state-of-the-art techniques in WSSS with image level labels.
  Basically, we provide a comprehensive review of recent advancements in WSSS with image level labels, categorizing existing methods based on the types and levels of additional supervision involved. We also examine the challenges of applying advanced methods to domain specific datasets in WSSS,a topic that remains underexplored. Finally, we discuss the current challenges, evaluate the limitations of existing approaches, and outline several promising directions for future research. This review is intended for researchers who are already familiar with the fundamental concepts of WSSS and are seeking to deepen their understanding of current advances and methodological innovations.
<div id='section'>Paperid: <span id='pid'>523, <a href='https://arxiv.org/pdf/2507.16849.pdf' target='_blank'>https://arxiv.org/pdf/2507.16849.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi-Shan Chu, Hsuan-Cheng Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16849">Post-Disaster Affected Area Segmentation with a Vision Transformer (ViT)-based EVAP Model using Sentinel-2 and Formosat-5 Imagery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a vision transformer (ViT)-based deep learning framework to refine disaster-affected area segmentation from remote sensing imagery, aiming to support and enhance the Emergent Value Added Product (EVAP) developed by the Taiwan Space Agency (TASA). The process starts with a small set of manually annotated regions. We then apply principal component analysis (PCA)-based feature space analysis and construct a confidence index (CI) to expand these labels, producing a weakly supervised training set. These expanded labels are then used to train ViT-based encoder-decoder models with multi-band inputs from Sentinel-2 and Formosat-5 imagery. Our architecture supports multiple decoder variants and multi-stage loss strategies to improve performance under limited supervision. During the evaluation, model predictions are compared with higher-resolution EVAP output to assess spatial coherence and segmentation consistency. Case studies on the 2022 Poyang Lake drought and the 2023 Rhodes wildfire demonstrate that our framework improves the smoothness and reliability of segmentation results, offering a scalable approach for disaster mapping when accurate ground truth is unavailable.
<div id='section'>Paperid: <span id='pid'>524, <a href='https://arxiv.org/pdf/2507.06013.pdf' target='_blank'>https://arxiv.org/pdf/2507.06013.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kushal Gajjar, Harshit Sikchi, Arpit Singh Gautam, Marc Hammons, Saurabh Jha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06013">CogniSQL-R1-Zero: Lightweight Reinforced Reasoning for Efficient SQL Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Translating natural language into SQL (Text-to-SQL) remains a core challenge at the intersection of language understanding and structured data access. Although large language models (LLMs) have improved fluency, generating correct and executable SQL, especially for complex queries, continues to be challenging. We introduce CogniSQL-R1-Zero, a reinforcement learning (RL) framework and model that produces accurate SQL using a lightweight reward signal based on execution correctness and format-tag compliance. By avoiding intermediate supervision, hybrid pipelines and complex reward shaping, our method encourages stable learning and stronger alignment with the ultimate task objective-producing executable programs. CogniSQL-R1-Zero achieves state-of-the-art execution accuracy on Text2SQL benchmark; BIRD bench, outperforming prior supervised and instruction-tuned baselines including SFT CodeS-7B, DeepSeek-Coder 236B, and Mistral 123B-despite being trained on a significantly smaller 7B backbone. This result underscores the scalability and efficiency of our RL-based approach when trained on just four NVIDIA A100 GPUs (40 GB VRAM each). To support further research in efficient and interpretable Text-to-SQL modeling, we release two curated datasets: (i) a collection of 5,024 reasoning traces with varying context lengths, and (ii) a positive-sampled corpus of 36,356 corpus of weakly supervised queries, each annotated with six semantically diverse reasoning paths. Together, these contributions advance scalable, execution-aligned Text-to-SQL generation.
<div id='section'>Paperid: <span id='pid'>525, <a href='https://arxiv.org/pdf/2507.02308.pdf' target='_blank'>https://arxiv.org/pdf/2507.02308.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pei Guo, Ryan Farrell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02308">LMPNet for Weakly-supervised Keypoint Discovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we explore the task of semantic object keypoint discovery weakly-supervised by only category labels. This is achieved by transforming discriminatively-trained intermediate layer filters into keypoint detectors. We begin by identifying three preferred characteristics of keypoint detectors: (i) spatially sparse activations, (ii) consistency and (iii) diversity. Instead of relying on hand-crafted loss terms, a novel computationally-efficient leaky max pooling (LMP) layer is proposed to explicitly encourage final conv-layer filters to learn "non-repeatable local patterns" that are well aligned with object keypoints. Informed by visualizations, a simple yet effective selection strategy is proposed to ensure consistent filter activations and attention mask-out is then applied to force the network to distribute its attention to the whole object instead of just the most discriminative region. For the final keypoint prediction, a learnable clustering layer is proposed to group keypoint proposals into keypoint predictions. The final model, named LMPNet, is highly interpretable in that it directly manipulates network filters to detect predefined concepts. Our experiments show that LMPNet can (i) automatically discover semantic keypoints that are robust to object pose and (ii) achieves strong prediction accuracy comparable to a supervised pose estimation model.
<div id='section'>Paperid: <span id='pid'>526, <a href='https://arxiv.org/pdf/2506.13095.pdf' target='_blank'>https://arxiv.org/pdf/2506.13095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Wang, Shiwei Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13095">Learning Event Completeness for Weakly Supervised Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised video anomaly detection (WS-VAD) is tasked with pinpointing temporal intervals containing anomalous events within untrimmed videos, utilizing only video-level annotations. However, a significant challenge arises due to the absence of dense frame-level annotations, often leading to incomplete localization in existing WS-VAD methods. To address this issue, we present a novel LEC-VAD, Learning Event Completeness for Weakly Supervised Video Anomaly Detection, which features a dual structure designed to encode both category-aware and category-agnostic semantics between vision and language. Within LEC-VAD, we devise semantic regularities that leverage an anomaly-aware Gaussian mixture to learn precise event boundaries, thereby yielding more complete event instances. Besides, we develop a novel memory bank-based prototype learning mechanism to enrich concise text descriptions associated with anomaly-event categories. This innovation bolsters the text's expressiveness, which is crucial for advancing WS-VAD. Our LEC-VAD demonstrates remarkable advancements over the current state-of-the-art methods on two benchmark datasets XD-Violence and UCF-Crime.
<div id='section'>Paperid: <span id='pid'>527, <a href='https://arxiv.org/pdf/2505.23586.pdf' target='_blank'>https://arxiv.org/pdf/2505.23586.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyong Wang, Charith Abhayaratne
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23586">Weakly-supervised Localization of Manipulated Image Regions Using Multi-resolution Learned Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The explosive growth of digital images and the widespread availability of image editing tools have made image manipulation detection an increasingly critical challenge. Current deep learning-based manipulation detection methods excel in achieving high image-level classification accuracy, they often fall short in terms of interpretability and localization of manipulated regions. Additionally, the absence of pixel-wise annotations in real-world scenarios limits the existing fully-supervised manipulation localization techniques. To address these challenges, we propose a novel weakly-supervised approach that integrates activation maps generated by image-level manipulation detection networks with segmentation maps from pre-trained models. Specifically, we build on our previous image-level work named WCBnet to produce multi-view feature maps which are subsequently fused for coarse localization. These coarse maps are then refined using detailed segmented regional information provided by pre-trained segmentation models (such as DeepLab, SegmentAnything and PSPnet), with Bayesian inference employed to enhance the manipulation localization. Experimental results demonstrate the effectiveness of our approach, highlighting the feasibility to localize image manipulations without relying on pixel-level labels.
<div id='section'>Paperid: <span id='pid'>528, <a href='https://arxiv.org/pdf/2505.15438.pdf' target='_blank'>https://arxiv.org/pdf/2505.15438.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianyuan Guo, Peike Li, Trevor Cohn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.15438">Bridging Sign and Spoken Languages: Pseudo Gloss Generation for Sign Language Translation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sign Language Translation (SLT) aims to map sign language videos to spoken language text. A common approach relies on gloss annotations as an intermediate representation, decomposing SLT into two sub-tasks: video-to-gloss recognition and gloss-to-text translation. While effective, this paradigm depends on expert-annotated gloss labels, which are costly and rarely available in existing datasets, limiting its scalability. To address this challenge, we propose a gloss-free pseudo gloss generation framework that eliminates the need for human-annotated glosses while preserving the structured intermediate representation. Specifically, we prompt a Large Language Model (LLM) with a few example text-gloss pairs using in-context learning to produce draft sign glosses from spoken language text. To enhance the correspondence between LLM-generated pseudo glosses and the sign sequences in video, we correct the ordering in the pseudo glosses for better alignment via a weakly supervised learning process. This reordering facilitates the incorporation of auxiliary alignment objectives, and allows for the use of efficient supervision via a Connectionist Temporal Classification (CTC) loss. We train our SLT mode, which consists of a vision encoder and a translator, through a three-stage pipeline, which progressively narrows the modality gap between sign language and spoken language. Despite its simplicity, our approach outperforms previous state-of-the-art gloss-free frameworks on two SLT benchmarks and achieves competitive results compared to gloss-based methods.
<div id='section'>Paperid: <span id='pid'>529, <a href='https://arxiv.org/pdf/2505.10781.pdf' target='_blank'>https://arxiv.org/pdf/2505.10781.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Minkwan Kim, Soeun Lee, Byeongkeun Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10781">Completely Weakly Supervised Class-Incremental Learning for Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work addresses the task of completely weakly supervised class-incremental learning for semantic segmentation to learn segmentation for both base and additional novel classes using only image-level labels. While class-incremental semantic segmentation (CISS) is crucial for handling diverse and newly emerging objects in the real world, traditional CISS methods require expensive pixel-level annotations for training. To overcome this limitation, partially weakly-supervised approaches have recently been proposed. However, to the best of our knowledge, this is the first work to introduce a completely weakly-supervised method for CISS. To achieve this, we propose to generate robust pseudo-labels by combining pseudo-labels from a localizer and a sequence of foundation models based on their uncertainty. Moreover, to mitigate catastrophic forgetting, we introduce an exemplar-guided data augmentation method that generates diverse images containing both previous and novel classes with guidance. Finally, we conduct experiments in three common experimental settings: 15-5 VOC, 10-10 VOC, and COCO-to-VOC, and in two scenarios: disjoint and overlap. The experimental results demonstrate that our completely weakly supervised method outperforms even partially weakly supervised methods in the 15-5 VOC and 10-10 VOC settings while achieving competitive accuracy in the COCO-to-VOC setting.
<div id='section'>Paperid: <span id='pid'>530, <a href='https://arxiv.org/pdf/2505.07440.pdf' target='_blank'>https://arxiv.org/pdf/2505.07440.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rituraj Singh, Sachin Pawar, Girish Palshikar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07440">Matching Tasks with Industry Groups for Augmenting Commonsense Knowledge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Commonsense knowledge bases (KB) are a source of specialized knowledge that is widely used to improve machine learning applications. However, even for a large KB such as ConceptNet, capturing explicit knowledge from each industry domain is challenging. For example, only a few samples of general {\em tasks} performed by various industries are available in ConceptNet. Here, a task is a well-defined knowledge-based volitional action to achieve a particular goal. In this paper, we aim to fill this gap and present a weakly-supervised framework to augment commonsense KB with tasks carried out by various industry groups (IG). We attempt to {\em match} each task with one or more suitable IGs by training a neural model to learn task-IG affinity and apply clustering to select the top-k tasks per IG. We extract a total of 2339 triples of the form $\langle IG, is~capable~of, task \rangle$ from two publicly available news datasets for 24 IGs with the precision of 0.86. This validates the reliability of the extracted task-IG pairs that can be directly added to existing KBs.
<div id='section'>Paperid: <span id='pid'>531, <a href='https://arxiv.org/pdf/2505.04150.pdf' target='_blank'>https://arxiv.org/pdf/2505.04150.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Yamaoka, Weng Ian Chan, Shigeto Seno, Soichiro Fukada, Hideo Matsuda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04150">Learning from Similarity Proportion Loss for Classifying Skeletal Muscle Recovery Stages</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Evaluating the regeneration process of damaged muscle tissue is a fundamental analysis in muscle research to measure experimental effect sizes and uncover mechanisms behind muscle weakness due to aging and disease. The conventional approach to assessing muscle tissue regeneration involves whole-slide imaging and expert visual inspection of the recovery stages based on the morphological information of cells and fibers. There is a need to replace these tasks with automated methods incorporating machine learning techniques to ensure a quantitative and objective analysis. Given the limited availability of fully labeled data, a possible approach is Learning from Label Proportions (LLP), a weakly supervised learning method using class label proportions. However, current LLP methods have two limitations: (1) they cannot adapt the feature extractor for muscle tissues, and (2) they treat the classes representing recovery stages and cell morphological changes as nominal, resulting in the loss of ordinal information. To address these issues, we propose Ordinal Scale Learning from Similarity Proportion (OSLSP), which uses a similarity proportion loss derived from two bag combinations. OSLSP can update the feature extractor by using class proportion attention to the ordinal scale of the class. Our model with OSLSP outperforms large-scale pre-trained and fine-tuning models in classification tasks of skeletal muscle recovery stages.
<div id='section'>Paperid: <span id='pid'>532, <a href='https://arxiv.org/pdf/2504.14302.pdf' target='_blank'>https://arxiv.org/pdf/2504.14302.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yogev Kriger, Shai Fine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14302">Learning to Score</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Common machine learning settings range from supervised tasks, where accurately labeled data is accessible, through semi-supervised and weakly-supervised tasks, where target labels are scant or noisy, to unsupervised tasks where labels are unobtainable. In this paper we study a scenario where the target labels are not available but additional related information is at hand. This information, referred to as Side Information, is either correlated with the unknown labels or imposes constraints on the feature space. We formulate the problem as an ensemble of three semantic components: representation learning, side information and metric learning. The proposed scoring model is advantageous for multiple use-cases. For example, in the healthcare domain it can be used to create a severity score for diseases where the symptoms are known but the criteria for the disease progression are not well defined. We demonstrate the utility of the suggested scoring system on well-known benchmark data-sets and bio-medical patient records.
<div id='section'>Paperid: <span id='pid'>533, <a href='https://arxiv.org/pdf/2504.13927.pdf' target='_blank'>https://arxiv.org/pdf/2504.13927.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>F. Herrera, U. A. Rozikov, M. V. Velasco
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13927">Ising Models with Hidden Markov Structure: Applications to Probabilistic Inference in Machine Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we investigate tree-indexed Markov chains (Gibbs measures) defined by a Hamiltonian that couples two Ising layers: hidden spins \(s(x) \in \{\pm 1\}\) and observed spins \(Ï(x) \in \{\pm 1\}\) on a Cayley tree. The Hamiltonian incorporates Ising interactions within each layer and site-wise emission couplings between layers, extending hidden Markov models to a bilayer Markov random field.
  Specifically, we explore translation-invariant Gibbs measures (TIGM) of this Hamiltonian on Cayley trees.
  Under certain explicit conditions on the model's parameters, we demonstrate that there can be up to three distinct TIGMs. Each of these measures represents an equilibrium state of the spin system. These measures provide a structured approach to inference on hierarchical data in machine learning. They have practical applications in tasks such as denoising, weakly supervised learning, and anomaly detection. The Cayley tree structure is particularly advantageous for exact inference due to its tractability.
<div id='section'>Paperid: <span id='pid'>534, <a href='https://arxiv.org/pdf/2504.13297.pdf' target='_blank'>https://arxiv.org/pdf/2504.13297.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andreas Lau Hansen, Lukas Wanzeck, Dim P. Papadopoulos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13297">Weak Cube R-CNN: Weakly Supervised 3D Detection using only 2D Bounding Boxes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monocular 3D object detection is an essential task in computer vision, and it has several applications in robotics and virtual reality. However, 3D object detectors are typically trained in a fully supervised way, relying extensively on 3D labeled data, which is labor-intensive and costly to annotate. This work focuses on weakly-supervised 3D detection to reduce data needs using a monocular method that leverages a singlecamera system over expensive LiDAR sensors or multi-camera setups. We propose a general model Weak Cube R-CNN, which can predict objects in 3D at inference time, requiring only 2D box annotations for training by exploiting the relationship between 2D projections of 3D cubes. Our proposed method utilizes pre-trained frozen foundation 2D models to estimate depth and orientation information on a training set. We use these estimated values as pseudo-ground truths during training. We design loss functions that avoid 3D labels by incorporating information from the external models into the loss. In this way, we aim to implicitly transfer knowledge from these large foundation 2D models without having access to 3D bounding box annotations. Experimental results on the SUN RGB-D dataset show increased performance in accuracy compared to an annotation time equalized Cube R-CNN baseline. While not precise for centimetre-level measurements, this method provides a strong foundation for further research.
<div id='section'>Paperid: <span id='pid'>535, <a href='https://arxiv.org/pdf/2504.04435.pdf' target='_blank'>https://arxiv.org/pdf/2504.04435.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tatiana Merkulova, Bharani Jayakumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04435">Evaluation framework for Image Segmentation Algorithms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a comprehensive evaluation framework for image segmentation algorithms, encompassing naive methods, machine learning approaches, and deep learning techniques. We begin by introducing the fundamental concepts and importance of image segmentation, and the role of interactive segmentation in enhancing accuracy. A detailed background theory section explores various segmentation methods, including thresholding, edge detection, region growing, feature extraction, random forests, support vector machines, convolutional neural networks, U-Net, and Mask R-CNN. The implementation and experimental setup are thoroughly described, highlighting three primary approaches: algorithm assisting user, user assisting algorithm, and hybrid methods. Evaluation metrics such as Intersection over Union (IoU), computation time, and user interaction time are employed to measure performance. A comparative analysis presents detailed results, emphasizing the strengths, limitations, and trade-offs of each method. The paper concludes with insights into the practical applicability of these approaches across various scenarios and outlines future work, focusing on expanding datasets, developing more representative approaches, integrating real-time feedback, and exploring weakly supervised and self-supervised learning paradigms to enhance segmentation accuracy and efficiency. Keywords: Image Segmentation, Interactive Segmentation, Machine Learning, Deep Learning, Computer Vision
<div id='section'>Paperid: <span id='pid'>536, <a href='https://arxiv.org/pdf/2503.23181.pdf' target='_blank'>https://arxiv.org/pdf/2503.23181.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sunoh Kim, Daeho Um
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.23181">Enhancing Weakly Supervised Video Grounding via Diverse Inference Strategies for Boundary and Prediction Selection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised video grounding aims to localize temporal boundaries relevant to a given query without explicit ground-truth temporal boundaries. While existing methods primarily use Gaussian-based proposals, they overlook the importance of (1) boundary prediction and (2) top-1 prediction selection during inference. In their boundary prediction, boundaries are simply set at half a standard deviation away from a Gaussian mean on both sides, which may not accurately capture the optimal boundaries. In the top-1 prediction process, these existing methods rely heavily on intersections with other proposals, without considering the varying quality of each proposal. To address these issues, we explore various inference strategies by introducing (1) novel boundary prediction methods to capture diverse boundaries from multiple Gaussians and (2) new selection methods that take proposal quality into account. Extensive experiments on the ActivityNet Captions and Charades-STA datasets validate the effectiveness of our inference strategies, demonstrating performance improvements without requiring additional training.
<div id='section'>Paperid: <span id='pid'>537, <a href='https://arxiv.org/pdf/2503.22856.pdf' target='_blank'>https://arxiv.org/pdf/2503.22856.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shanshan Bai, Anna Kruspe, Xiaoxiang Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22856">Generating Synthetic Oracle Datasets to Analyze Noise Impact: A Study on Building Function Classification Using Tweets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tweets provides valuable semantic context for earth observation tasks and serves as a complementary modality to remote sensing imagery. In building function classification (BFC), tweets are often collected using geographic heuristics and labeled via external databases, an inherently weakly supervised process that introduces both label noise and sentence level feature noise (e.g., irrelevant or uninformative tweets). While label noise has been widely studied, the impact of sentence level feature noise remains underexplored, largely due to the lack of clean benchmark datasets for controlled analysis. In this work, we propose a method for generating a synthetic oracle dataset using LLM, designed to contain only tweets that are both correctly labeled and semantically relevant to their associated buildings. This oracle dataset enables systematic investigation of noise impacts that are otherwise difficult to isolate in real-world data. To assess its utility, we compare model performance using Naive Bayes and mBERT classifiers under three configurations: real vs. synthetic training data, and cross-domain generalization. Results show that noise in real tweets significantly degrades the contextual learning capacity of mBERT, reducing its performance to that of a simple keyword-based model. In contrast, the clean synthetic dataset allows mBERT to learn effectively, outperforming Naive Bayes Bayes by a large margin. These findings highlight that addressing feature noise is more critical than model complexity in this task. Our synthetic dataset offers a novel experimental environment for future noise injection studies and is publicly available on GitHub.
<div id='section'>Paperid: <span id='pid'>538, <a href='https://arxiv.org/pdf/2503.13925.pdf' target='_blank'>https://arxiv.org/pdf/2503.13925.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Da Kuang, Guanwen Qiu, Junhyong Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13925">Reconstructing Cell Lineage Trees from Phenotypic Features with Metric Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>How a single fertilized cell gives rise to a complex array of specialized cell types in development is a central question in biology. The cells grow, divide, and acquire differentiated characteristics through poorly understood molecular processes. A key approach to studying developmental processes is to infer the tree graph of cell lineage division and differentiation histories, providing an analytical framework for dissecting individual cells' molecular decisions during replication and differentiation. Although genetically engineered lineage-tracing methods have advanced the field, they are either infeasible or ethically constrained in many organisms. In contrast, modern single-cell technologies can measure high-content molecular profiles (e.g., transcriptomes) in a wide range of biological systems.
  Here, we introduce CellTreeQM, a novel deep learning method based on transformer architectures that learns an embedding space with geometric properties optimized for tree-graph inference. By formulating lineage reconstruction as a tree-metric learning problem, we have systematically explored supervised, weakly supervised, and unsupervised training settings and present a Lineage Reconstruction Benchmark to facilitate comprehensive evaluation of our learning method. We benchmarked the method on (1) synthetic data modeled via Brownian motion with independent noise and spurious signals and (2) lineage-resolved single-cell RNA sequencing datasets. Experimental results show that CellTreeQM recovers lineage structures with minimal supervision and limited data, offering a scalable framework for uncovering cell lineage relationships in challenging animal models. To our knowledge, this is the first method to cast cell lineage inference explicitly as a metric learning task, paving the way for future computational models aimed at uncovering the molecular dynamics of cell lineage.
<div id='section'>Paperid: <span id='pid'>539, <a href='https://arxiv.org/pdf/2503.04342.pdf' target='_blank'>https://arxiv.org/pdf/2503.04342.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ivan Oleksiyuk, Svyatoslav Voloshynovskiy, Tobias Golling
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04342">TRANSIT your events into a new mass: Fast background interpolation for weakly-supervised anomaly searches</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a new model for conditional and continuous data morphing called TRansport Adversarial Network for Smooth InTerpolation (TRANSIT). We apply it to create a background data template for weakly-supervised searches at the LHC. The method smoothly transforms sideband events to match signal region mass distributions. We demonstrate the performance of TRANSIT using the LHC Olympics R\&D dataset. The model captures non-linear mass correlations of features and produces a template that offers a competitive anomaly sensitivity compared to state-of-the-art transport-based template generators. Moreover, the computational training time required for TRANSIT is an order of magnitude lower than that of competing deep learning methods. This makes it ideal for analyses that iterate over many signal regions and signal models. Unlike generative models, which must learn a full probability density distribution, i.e., the correlations between all the variables, the proposed transport model only has to learn a smooth conditional shift of the distribution. This allows for a simpler, more efficient residual architecture, enabling mass uncorrelated features to pass the network unchanged while the mass correlated features are adjusted accordingly. Furthermore, we show that the latent space of the model provides a set of mass decorrelated features useful for anomaly detection without background sculpting.
<div id='section'>Paperid: <span id='pid'>540, <a href='https://arxiv.org/pdf/2502.00629.pdf' target='_blank'>https://arxiv.org/pdf/2502.00629.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Wu, Hideki Nakayama
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.00629">Advanced Weakly-Supervised Formula Exploration for Neuro-Symbolic Mathematical Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, neuro-symbolic methods have become a popular and powerful approach that augments artificial intelligence systems with the capability to perform abstract, logical, and quantitative deductions with enhanced precision and controllability. Recent studies successfully performed symbolic reasoning by leveraging various machine learning models to explicitly or implicitly predict intermediate labels that provide symbolic instructions. However, these intermediate labels are not always prepared for every task as a part of training data, and pre-trained models, represented by Large Language Models (LLMs), also do not consistently generate valid symbolic instructions with their intrinsic knowledge. On the other hand, existing work developed alternative learning techniques that allow the learning system to autonomously uncover optimal symbolic instructions. Nevertheless, their performance also exhibits limitations when faced with relatively huge search spaces or more challenging reasoning problems. In view of this, in this work, we put forward an advanced practice for neuro-symbolic reasoning systems to explore the intermediate labels with weak supervision from problem inputs and final outputs. Our experiments on the Mathematics dataset illustrated the effectiveness of our proposals from multiple aspects.
<div id='section'>Paperid: <span id='pid'>541, <a href='https://arxiv.org/pdf/2501.03891.pdf' target='_blank'>https://arxiv.org/pdf/2501.03891.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyi Wu, Hong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.03891">Superpixel Boundary Correction for Weakly-Supervised Semantic Segmentation on Histopathology Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancement of deep learning, computational pathology has made significant progress in cancer diagnosis and subtyping. Tissue segmentation is a core challenge, essential for prognosis and treatment decisions. Weakly supervised semantic segmentation (WSSS) reduces the annotation requirement by using image-level labels instead of pixel-level ones. However, Class Activation Map (CAM)-based methods still suffer from low spatial resolution and unclear boundaries. To address these issues, we propose a multi-level superpixel correction algorithm that refines CAM boundaries using superpixel clustering and floodfill. Experimental results show that our method achieves great performance on breast cancer segmentation dataset with mIoU of 71.08%, significantly improving tumor microenvironment boundary delineation.
<div id='section'>Paperid: <span id='pid'>542, <a href='https://arxiv.org/pdf/2501.01658.pdf' target='_blank'>https://arxiv.org/pdf/2501.01658.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wang Lituan, Zhang Lei, Wang Yan, Wang Zhenbin, Zhang Zhenwei, Zhang Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01658">EAUWSeg: Eliminating annotation uncertainty in weakly-supervised medical image segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised medical image segmentation is gaining traction as it requires only rough annotations rather than accurate pixel-to-pixel labels, thereby reducing the workload for specialists. Although some progress has been made, there is still a considerable performance gap between the label-efficient methods and fully-supervised one, which can be attributed to the uncertainty nature of these weak labels. To address this issue, we propose a novel weak annotation method coupled with its learning framework EAUWSeg to eliminate the annotation uncertainty. Specifically, we first propose the Bounded Polygon Annotation (BPAnno) by simply labeling two polygons for a lesion. Then, the tailored learning mechanism that explicitly treat bounded polygons as two separated annotations is proposed to learn invariant feature by providing adversarial supervision signal for model training. Subsequently, a confidence-auxiliary consistency learner incorporates with a classification-guided confidence generator is designed to provide reliable supervision signal for pixels in uncertain region by leveraging the feature presentation consistency across pixels within the same category as well as class-specific information encapsulated in bounded polygons annotation. Experimental results demonstrate that EAUWSeg outperforms existing weakly-supervised segmentation methods. Furthermore, compared to fully-supervised counterparts, the proposed method not only delivers superior performance but also costs much less annotation workload. This underscores the superiority and effectiveness of our approach.
<div id='section'>Paperid: <span id='pid'>543, <a href='https://arxiv.org/pdf/2412.19563.pdf' target='_blank'>https://arxiv.org/pdf/2412.19563.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongbiao Gao, Xiangcheng Sun, Guohua Lv, Deng Yu, Sijiu Niu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19563">Reinforced Label Denoising for Weakly-Supervised Audio-Visual Video Parsing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-visual video parsing (AVVP) aims to recognize audio and visual event labels with precise temporal boundaries, which is quite challenging since audio or visual modality might include only one event label with only the overall video labels available. Existing label denoising models often treat the denoising process as a separate preprocessing step, leading to a disconnect between label denoising and AVVP tasks. To bridge this gap, we present a novel joint reinforcement learning-based label denoising approach (RLLD). This approach enables simultaneous training of both label denoising and video parsing models through a joint optimization strategy. We introduce a novel AVVP-validation and soft inter-reward feedback mechanism that directly guides the learning of label denoising policy. Extensive experiments on AVVP tasks demonstrate the superior performance of our proposed method compared to label denoising techniques. Furthermore, by incorporating our label denoising method into other AVVP models, we find that it can further enhance parsing results.
<div id='section'>Paperid: <span id='pid'>544, <a href='https://arxiv.org/pdf/2412.14870.pdf' target='_blank'>https://arxiv.org/pdf/2412.14870.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Isabelle Tingzon, Utku Can Ozturk, Ivan Dotu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14870">Large-scale School Mapping using Weakly Supervised Deep Learning for Universal School Connectivity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Improving global school connectivity is critical for ensuring inclusive and equitable quality education. To reliably estimate the cost of connecting schools, governments and connectivity providers require complete and accurate school location data - a resource that is often scarce in many low- and middle-income countries. To address this challenge, we propose a cost-effective, scalable approach to locating schools in high-resolution satellite images using weakly supervised deep learning techniques. Our best models, which combine vision transformers and convolutional neural networks, achieve AUPRC values above 0.96 across 10 pilot African countries. Leveraging explainable AI techniques, our approach can approximate the precise geographical coordinates of the school locations using only low-cost, classification-level annotations. To demonstrate the scalability of our method, we generate nationwide maps of school location predictions in African countries and present a detailed analysis of our results, using Senegal as our case study. Finally, we demonstrate the immediate usability of our work by introducing an interactive web mapping tool to streamline human-in-the-loop model validation efforts by government partners. This work successfully showcases the real-world utility of deep learning and satellite images for planning regional infrastructure and accelerating universal school connectivity.
<div id='section'>Paperid: <span id='pid'>545, <a href='https://arxiv.org/pdf/2412.11237.pdf' target='_blank'>https://arxiv.org/pdf/2412.11237.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Max Riffi-Aslett, Christina Fell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11237">On the Generalizability of Iterative Patch Selection for Memory-Efficient High-Resolution Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Classifying large images with small or tiny regions of interest (ROI) is challenging due to computational and memory constraints. Weakly supervised memory-efficient patch selectors have achieved results comparable with strongly supervised methods. However, low signal-to-noise ratios and low entropy attention still cause overfitting. We explore these issues using a novel testbed on a memory-efficient cross-attention transformer with Iterative Patch Selection (IPS) as the patch selection module. Our testbed extends the megapixel MNIST benchmark to four smaller O2I (object-to-image) ratios ranging from 0.01% to 0.14% while keeping the canvas size fixed and introducing a noise generation component based on BÃ©zier curves. Experimental results generalize the observations made on CNNs to IPS whereby the O2I threshold below which the classifier fails to generalize is affected by the training dataset size. We further observe that the magnitude of this interaction differs for each task of the Megapixel MNIST. For tasks "Maj" and "Top", the rate is at its highest, followed by tasks "Max" and "Multi" where in the latter, this rate is almost at 0. Moreover, results show that in a low data setting, tuning the patch size to be smaller relative to the ROI improves generalization, resulting in an improvement of + 15% for the megapixel MNIST and + 5% for the Swedish traffic signs dataset compared to the original object-to-patch ratios in IPS. Further outcomes indicate that the similarity between the thickness of the noise component and the digits in the megapixel MNIST gradually causes IPS to fail to generalize, contributing to previous suspicions.
<div id='section'>Paperid: <span id='pid'>546, <a href='https://arxiv.org/pdf/2412.00077.pdf' target='_blank'>https://arxiv.org/pdf/2412.00077.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nima Sedaghat, Tanawan Chatchadanoraset, Colin Orion Chandler, Ashish Mahabal, Maryam Eslami
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.00077">Selfish Evolution: Making Discoveries in Extreme Label Noise with the Help of Overfitting Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motivated by the scarcity of proper labels in an astrophysical application, we have developed a novel technique, called Selfish Evolution, which allows for the detection and correction of corrupted labels in a weakly supervised fashion. Unlike methods based on early stopping, we let the model train on the noisy dataset. Only then do we intervene and allow the model to overfit to individual samples. The ``evolution'' of the model during this process reveals patterns with enough information about the noisiness of the label, as well as its correct version. We train a secondary network on these spatiotemporal ``evolution cubes'' to correct potentially corrupted labels. We incorporate the technique in a closed-loop fashion, allowing for automatic convergence towards a mostly clean dataset, without presumptions about the state of the network in which we intervene. We evaluate on the main task of the Supernova-hunting dataset but also demonstrate efficiency on the more standard MNIST dataset.
<div id='section'>Paperid: <span id='pid'>547, <a href='https://arxiv.org/pdf/2411.19547.pdf' target='_blank'>https://arxiv.org/pdf/2411.19547.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dihong Gong, Pu Lu, Zelong Wang, Meng Zhou, Xiuqiang He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19547">Training Agents with Weakly Supervised Feedback from Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) offer a promising basis for creating agents that can tackle complex tasks through iterative environmental interaction. Existing methods either require these agents to mimic expert-provided trajectories or rely on definitive environmental feedback for reinforcement learning which limits their application to specific scenarios like gaming or code generation. This paper introduces a novel training method for LLM-based agents using weakly supervised signals from a critic LLM, bypassing the need for expert trajectories or definitive feedback. Our agents are trained in iterative manner, where they initially generate trajectories through environmental interaction. Subsequently, a critic LLM selects a subset of good trajectories, which are then used to update the agents, enabling them to generate improved trajectories in the next iteration. Extensive tests on the API-bank dataset show consistent improvement in our agents' capabilities and comparable performance to GPT-4, despite using open-source models with much fewer parameters.
<div id='section'>Paperid: <span id='pid'>548, <a href='https://arxiv.org/pdf/2411.18915.pdf' target='_blank'>https://arxiv.org/pdf/2411.18915.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vishnou Vinayagame, Gregory Senay, Luis MartÃ­
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18915">MATATA: Weakly Supervised End-to-End MAthematical Tool-Augmented Reasoning for Tabular Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Business documents often contain substantial tabular and textual information with numerical values, requiring mathematical reasoning for effective document understanding. While Small Language Models (SLMs) still struggle at this task, tool-augmented multi-step agents perform better, at the cost of relying on closed-source or larger models, external data, or extensive prompt-engineering. This work introduces MATATA, a novel weakly supervised end-to-end approach to train multi-step reasoning language agents for document tabular applications. MATATA presents an annotation-free paradigm for each agent to enhance 3.8B/8B SLMs. During its two-stage training, MATATA uses the final outcome of the multi-step reasoning chain as weak supervision. This approach avoids having to individually supervise each intermediate agent in the reasoning chain. By employing an adaptive planner and shared tools across different datasets, MATATA shows robust performance. Experiments demonstrate that MATATA achieves state-of-the-art on FinQA, and on TAT-QA among reasoning methods based on open-source SLMs. Although being SLM-based, MATATA closely matches GPT-4-based frameworks on TabMWP. This novel weakly supervised approach enables training an end-to-end multi-step reasoning agent without intermediate supervision, supporting future developments of cost-effective powerful agentic systems.
<div id='section'>Paperid: <span id='pid'>549, <a href='https://arxiv.org/pdf/2411.08755.pdf' target='_blank'>https://arxiv.org/pdf/2411.08755.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sareh Soltani Nejad, Anwar Haque
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08755">Weakly-Supervised Anomaly Detection in Surveillance Videos Based on Two-Stream I3D Convolution Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The widespread implementation of urban surveillance systems has necessitated more sophisticated techniques for anomaly detection to ensure enhanced public safety. This paper presents a significant advancement in the field of anomaly detection through the application of Two-Stream Inflated 3D (I3D) Convolutional Networks. These networks substantially outperform traditional 3D Convolutional Networks (C3D) by more effectively extracting spatial and temporal features from surveillance videos, thus improving the precision of anomaly detection. Our research advances the field by implementing a weakly supervised learning framework based on Multiple Instance Learning (MIL), which uniquely conceptualizes surveillance videos as collections of 'bags' that contain instances (video clips). Each instance is innovatively processed through a ranking mechanism that prioritizes clips based on their potential to display anomalies. This novel strategy not only enhances the accuracy and precision of anomaly detection but also significantly diminishes the dependency on extensive manual annotations. Moreover, through meticulous optimization of model settings, including the choice of optimizer, our approach not only establishes new benchmarks in the performance of anomaly detection systems but also offers a scalable and efficient solution for real-world surveillance applications. This paper contributes significantly to the field of computer vision by delivering a more adaptable, efficient, and context-aware anomaly detection system, which is poised to redefine practices in urban surveillance.
<div id='section'>Paperid: <span id='pid'>550, <a href='https://arxiv.org/pdf/2411.03082.pdf' target='_blank'>https://arxiv.org/pdf/2411.03082.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Irum Mehboob, Li Sun, Alireza Astegarpanah, Rustam Stolkin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03082">Self-supervised cross-modality learning for uncertainty-aware object detection and recognition in applications which lack pre-labelled training data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper shows how an uncertainty-aware, deep neural network can be trained to detect, recognise and localise objects in 2D RGB images, in applications lacking annotated train-ng datasets. We propose a self-supervising teacher-student pipeline, in which a relatively simple teacher classifier, trained with only a few labelled 2D thumbnails, automatically processes a larger body of unlabelled RGB-D data to teach a student network based on a modified YOLOv3 architecture. Firstly, 3D object detection with back projection is used to automatically extract and teach 2D detection and localisation information to the student network. Secondly, a weakly supervised 2D thumbnail classifier, with minimal training on a small number of hand-labelled images, is used to teach object category recognition. Thirdly, we use a Gaussian Process GP to encode and teach a robust uncertainty estimation functionality, so that the student can output confidence scores with each categorization. The resulting student significantly outperforms the same YOLO architecture trained directly on the same amount of labelled data. Our GP-based approach yields robust and meaningful uncertainty estimations for complex industrial object classifications. The end-to-end network is also capable of real-time processing, needed for robotics applications. Our method can be applied to many important industrial tasks, where labelled datasets are typically unavailable. In this paper, we demonstrate an example of detection, localisation, and object category recognition of nuclear mixed-waste materials in highly cluttered and unstructured scenes. This is critical for robotic sorting and handling of legacy nuclear waste, which poses complex environmental remediation challenges in many nuclearised nations.
<div id='section'>Paperid: <span id='pid'>551, <a href='https://arxiv.org/pdf/2410.07460.pdf' target='_blank'>https://arxiv.org/pdf/2410.07460.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Wen, Evgenia Roussinova, Olivier Brina, Paolo Machi, Mohamed Bouri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07460">Generalizing Segmentation Foundation Model Under Sim-to-real Domain-shift for Guidewire Segmentation in X-ray Fluoroscopy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Guidewire segmentation during endovascular interventions holds the potential to significantly enhance procedural accuracy, improving visualization and providing critical feedback that can support both physicians and robotic systems in navigating complex vascular pathways. Unlike supervised segmentation networks, which need many expensive expert-annotated labels, sim-to-real domain adaptation approaches utilize synthetic data from simulations, offering a cost-effective solution. The success of models like Segment-Anything (SAM) has driven advancements in image segmentation foundation models with strong zero/few-shot generalization through prompt engineering. However, they struggle with medical images like X-ray fluoroscopy and the domain-shifts of the data. Given the challenges of acquiring annotation and the accessibility of labeled simulation data, we propose a sim-to-real domain adaption framework with a coarse-to-fine strategy to adapt SAM to X-ray fluoroscopy guidewire segmentation without any annotation on the target domain. We first generate the pseudo-labels by utilizing a simple source image style transfer technique that preserves the guidewire structure. Then, we develop a weakly supervised self-training architecture to fine-tune an end-to-end student SAM with the coarse labels by imposing consistency regularization and supervision from the teacher SAM network. We validate the effectiveness of the proposed method on a publicly available Cardiac dataset and an in-house Neurovascular dataset, where our method surpasses both pre-trained SAM and many state-of-the-art domain adaptation techniques by a large margin. Our code will be made public on GitHub soon.
<div id='section'>Paperid: <span id='pid'>552, <a href='https://arxiv.org/pdf/2409.01330.pdf' target='_blank'>https://arxiv.org/pdf/2409.01330.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Iulian Emil Tampu, Per Nyman, Christoforos Spyretos, Ida Blystad, Alia Shamikh, Gabriela Prochazka, Teresita DÃ­az de StÃ¥hl, Johanna Sandgren, Peter Lundberg, Neda Haj-Hosseini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01330">Pediatric brain tumor classification using digital histopathology and deep learning: evaluation of SOTA methods on a multi-center Swedish cohort</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Brain tumors are the most common solid tumors in children and young adults, but the scarcity of large histopathology datasets has limited the application of computational pathology in this group. This study implements two weakly supervised multiple-instance learning (MIL) approaches on patch-features obtained from state-of-the-art histology-specific foundation models to classify pediatric brain tumors in hematoxylin and eosin whole slide images (WSIs) from a multi-center Swedish cohort. WSIs from 540 subjects (age 8.5$\pm$4.9 years) diagnosed with brain tumor were gathered from the six Swedish university hospitals. Instance (patch)-level features were obtained from WSIs using three pre-trained feature extractors: ResNet50, UNI, and CONCH. Instances were aggregated using attention-based MIL (ABMIL) or clustering-constrained attention MIL (CLAM) for patient-level classification. Models were evaluated on three classification tasks based on the hierarchical classification of pediatric brain tumors: tumor category, family, and type. Model generalization was assessed by training on data from two of the centers and testing on data from four other centers. Model interpretability was evaluated through attention mapping. The highest classification performance was achieved using UNI features and ABMIL aggregation, with Matthew's correlation coefficient of 0.76$\pm$0.04, 0.63$\pm$0.04, and 0.60$\pm$0.05 for tumor category, family, and type classification, respectively. When evaluating generalization, models utilizing UNI and CONCH features outperformed those using ResNet50. However, the drop in performance from the in-site to out-of-site testing was similar across feature extractors. These results show the potential of state-of-the-art computational pathology methods in diagnosing pediatric brain tumors at different hierarchical levels with fair generalizability on a multi-center national dataset.
<div id='section'>Paperid: <span id='pid'>553, <a href='https://arxiv.org/pdf/2512.01145.pdf' target='_blank'>https://arxiv.org/pdf/2512.01145.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Riyadh Mohammed Almushrafy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.01145">Weakly Supervised Continuous Micro-Expression Intensity Estimation Using Temporal Deep Neural Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Micro-facial expressions are brief and involuntary facial movements that reflect genuine emotional states. While most prior work focuses on classifying discrete micro-expression categories, far fewer studies address the continuous evolution of intensity over time. Progress in this direction is limited by the lack of frame-level intensity labels, which makes fully supervised regression impractical. We propose a unified framework for continuous micro-expression intensity estimation using only weak temporal labels (onset, apex, offset). A simple triangular prior converts sparse temporal landmarks into dense pseudo-intensity trajectories, and a lightweight temporal regression model that combines a ResNet18 encoder with a bidirectional GRU predicts frame-wise intensity directly from image sequences. The method requires no frame-level annotation effort and is applied consistently across datasets through a single preprocessing and temporal alignment pipeline. Experiments on SAMM and CASME II show strong temporal agreement with the pseudo-intensity trajectories. On SAMM, the model reaches a Spearman correlation of 0.9014 and a Kendall correlation of 0.7999, outperforming a frame-wise baseline. On CASME II, it achieves up to 0.9116 and 0.8168, respectively, when trained without the apex-ranking term. Ablation studies confirm that temporal modeling and structured pseudo labels are central to capturing the rise-apex-fall dynamics of micro-facial movements. To our knowledge, this is the first unified approach for continuous micro-expression intensity estimation using only sparse temporal annotations.
<div id='section'>Paperid: <span id='pid'>554, <a href='https://arxiv.org/pdf/2511.07429.pdf' target='_blank'>https://arxiv.org/pdf/2511.07429.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hari Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.07429">Knowledge-Guided Textual Reasoning for Explainable Video Anomaly Detection via LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Text-based Explainable Video Anomaly Detection (TbVAD), a language-driven framework for weakly supervised video anomaly detection that performs anomaly detection and explanation entirely within the textual domain. Unlike conventional WSVAD models that rely on explicit visual features, TbVAD represents video semantics through language, enabling interpretable and knowledge-grounded reasoning. The framework operates in three stages: (1) transforming video content into fine-grained captions using a vision-language model, (2) constructing structured knowledge by organizing the captions into four semantic slots (action, object, context, environment), and (3) generating slot-wise explanations that reveal which semantic factors contribute most to the anomaly decision. We evaluate TbVAD on two public benchmarks, UCF-Crime and XD-Violence, demonstrating that textual knowledge reasoning provides interpretable and reliable anomaly detection for real-world surveillance scenarios.
<div id='section'>Paperid: <span id='pid'>555, <a href='https://arxiv.org/pdf/2510.25075.pdf' target='_blank'>https://arxiv.org/pdf/2510.25075.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Keisuke Imoto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.25075">Joint Analysis of Acoustic Scenes and Sound Events Based on Semi-Supervised Training of Sound Events With Partial Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Annotating time boundaries of sound events is labor-intensive, limiting the scalability of strongly supervised learning in audio detection. To reduce annotation costs, weakly-supervised learning with only clip-level labels has been widely adopted. As an alternative, partial label learning offers a cost-effective approach, where a set of possible labels is provided instead of exact weak annotations. However, partial label learning for audio analysis remains largely unexplored. Motivated by the observation that acoustic scenes provide contextual information for constructing a set of possible sound events, we utilize acoustic scene information to construct partial labels of sound events. On the basis of this idea, in this paper, we propose a multitask learning framework that jointly performs acoustic scene classification and sound event detection with partial labels of sound events. While reducing annotation costs, weakly-supervised and partial label learning often suffer from decreased detection performance due to lacking the precise event set and their temporal annotations. To better balance between annotation cost and detection performance, we also explore a semi-supervised framework that leverages both strong and partial labels. Moreover, to refine partial labels and achieve better model training, we propose a label refinement method based on self-distillation for the proposed approach with partial labels.
<div id='section'>Paperid: <span id='pid'>556, <a href='https://arxiv.org/pdf/2510.03606.pdf' target='_blank'>https://arxiv.org/pdf/2510.03606.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mattia Scardecchia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03606">Unsupervised Transformer Pre-Training for Images: Self-Distillation, Mean Teachers, and Random Crops</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in self-supervised learning (SSL) have made it possible to learn general-purpose visual features that capture both the high-level semantics and the fine-grained spatial structure of images. Most notably, the recent DINOv2 has established a new state of the art by surpassing weakly supervised methods (WSL) like OpenCLIP on most benchmarks. In this survey, we examine the core ideas behind its approach, multi-crop view augmentation and self-distillation with a mean teacher, and trace their development in previous work. We then compare the performance of DINO and DINOv2 with other SSL and WSL methods across various downstream tasks, and highlight some remarkable emergent properties of their learned features with transformer backbones. We conclude by briefly discussing DINOv2's limitations, its impact, and future research directions.
<div id='section'>Paperid: <span id='pid'>557, <a href='https://arxiv.org/pdf/2509.26145.pdf' target='_blank'>https://arxiv.org/pdf/2509.26145.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yukun Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26145">LMILAtt: A Deep Learning Model for Depression Detection from Social Media Users Enhanced by Multi-Instance Learning Based on Attention Mechanism</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Depression is a major global public health challenge and its early identification is crucial. Social media data provides a new perspective for depression detection, but existing methods face limitations such as insufficient accuracy, insufficient utilization of time series features, and high annotation costs. To this end, this study proposes the LMILAtt model, which innovatively integrates Long Short-Term Memory autoencoders and attention mechanisms: firstly, the temporal dynamic features of user tweets (such as depressive tendency evolution patterns) are extracted through unsupervised LSTM autoencoders. Secondly, the attention mechanism is used to dynamically weight key texts (such as early depression signals) and construct a multi-example learning architecture to improve the accuracy of user-level detection. Finally, the performance was verified on the WU3D dataset labeled by professional medicine. Experiments show that the model is significantly better than the baseline model in terms of accuracy, recall and F1 score. In addition, the weakly supervised learning strategy significantly reduces the cost of labeling and provides an efficient solution for large-scale social media depression screening.
<div id='section'>Paperid: <span id='pid'>558, <a href='https://arxiv.org/pdf/2509.14554.pdf' target='_blank'>https://arxiv.org/pdf/2509.14554.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoming Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14554">Generative Large Language Models for Knowledge Representation: A Systematic Review of Concept Map Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rise of generative large language models (LLMs) has opened new opportunities for automating knowledge representation through concept maps, a long-standing pedagogical tool valued for fostering meaningful learning and higher-order thinking. Traditional construction of concept maps is labor-intensive, requiring significant expertise and time, limiting their scalability in education. This review systematically synthesizes the emerging body of research on LLM-enabled concept map generation, focusing on two guiding questions: (a) What methods and technical features of LLMs are employed to construct concept maps? (b) What empirical evidence exists to validate their educational utility? Through a comprehensive search across major databases and AI-in-education conference proceedings, 28 studies meeting rigorous inclusion criteria were analyzed using thematic synthesis. Findings reveal six major methodological categories: human-in-the-loop systems, weakly supervised learning models, fine-tuned domain-specific LLMs, pre-trained LLMs with prompt engineering, hybrid systems integrating knowledge bases, and modular frameworks combining symbolic and statistical tools. Validation strategies ranged from quantitative metrics (precision, recall, F1-score, semantic similarity) to qualitative evaluations (expert review, learner feedback). Results indicate LLM-generated maps hold promise for scalable, adaptive, and pedagogically relevant knowledge visualization, though challenges remain regarding validity, interpretability, multilingual adaptability, and classroom integration. Future research should prioritize interdisciplinary co-design, empirical classroom trials, and alignment with instructional practices to realize their full educational potential.
<div id='section'>Paperid: <span id='pid'>559, <a href='https://arxiv.org/pdf/2509.08698.pdf' target='_blank'>https://arxiv.org/pdf/2509.08698.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thorsten Wittkopp
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08698">A layered architecture for log analysis in complex IT systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the evolving IT landscape, stability and reliability of systems are essential, yet their growing complexity challenges DevOps teams in implementation and maintenance. Log analysis, a core element of AIOps, provides critical insights into complex behaviors and failures. This dissertation introduces a three-layered architecture to support DevOps in failure resolution. The first layer, Log Investigation, performs autonomous log labeling and anomaly classification. We propose a method that labels log data without manual effort, enabling supervised training and precise evaluation of anomaly detection. Additionally, we define a taxonomy that groups anomalies into three categories, ensuring appropriate method selection. The second layer, Anomaly Detection, detects behaviors deviating from the norm. We propose a flexible Anomaly Detection method adaptable to unsupervised, weakly supervised, and supervised training. Evaluations on public and industry datasets show F1-scores between 0.98 and 1.0, ensuring reliable anomaly detection. The third layer, Root Cause Analysis, identifies minimal log sets describing failures, their origin, and event sequences. By balancing training data and identifying key services, our Root Cause Analysis method consistently detects 90-98% of root cause log lines within the top 10 candidates, providing actionable insights for mitigation. Our research addresses how log analysis methods can be designed and optimized to help DevOps resolve failures efficiently. By integrating these three layers, the architecture equips teams with robust methods to enhance IT system reliability.
<div id='section'>Paperid: <span id='pid'>560, <a href='https://arxiv.org/pdf/2508.15973.pdf' target='_blank'>https://arxiv.org/pdf/2508.15973.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minh-Tan Pham
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15973">Contributions to Label-Efficient Learning in Computer Vision and Remote Sensing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This manuscript presents a series of my selected contributions to the topic of label-efficient learning in computer vision and remote sensing. The central focus of this research is to develop and adapt methods that can learn effectively from limited or partially annotated data, and can leverage abundant unlabeled data in real-world applications. The contributions span both methodological developments and domain-specific adaptations, in particular addressing challenges unique to Earth observation data such as multi-modality, spatial resolution variability, and scene heterogeneity. The manuscript is organized around four main axes including (1) weakly supervised learning for object discovery and detection based on anomaly-aware representations learned from large amounts of background images; (2) multi-task learning that jointly trains on multiple datasets with disjoint annotations to improve performance on object detection and semantic segmentation; (3) self-supervised and supervised contrastive learning with multimodal data to enhance scene classification in remote sensing; and (4) few-shot learning for hierarchical scene classification using both explicit and implicit modeling of class hierarchies. These contributions are supported by extensive experimental results across natural and remote sensing datasets, reflecting the outcomes of several collaborative research projects. The manuscript concludes by outlining ongoing and future research directions focused on scaling and enhancing label-efficient learning for real-world applications.
<div id='section'>Paperid: <span id='pid'>561, <a href='https://arxiv.org/pdf/2507.00297.pdf' target='_blank'>https://arxiv.org/pdf/2507.00297.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Ifeoluwa Adelani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00297">Natural language processing for African languages</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in word embeddings and language models use large-scale, unlabelled data and self-supervised learning to boost NLP performance. Multilingual models, often trained on web-sourced data like Wikipedia, face challenges: few low-resource languages are included, their data is often noisy, and lack of labeled datasets makes it hard to evaluate performance outside high-resource languages like English. In this dissertation, we focus on languages spoken in Sub-Saharan Africa where all the indigenous languages in this region can be regarded as low-resourced in terms of the availability of labelled data for NLP tasks and unlabelled data found on the web. We analyse the noise in the publicly available corpora, and curate a high-quality corpus, demonstrating that the quality of semantic representations learned in word embeddings does not only depend on the amount of data but on the quality of pre-training data. We demonstrate empirically the limitations of word embeddings, and the opportunities the multilingual pre-trained language model (PLM) offers especially for languages unseen during pre-training and low-resource scenarios. We further study how to adapt and specialize multilingual PLMs to unseen African languages using a small amount of monolingual texts. To address the under-representation of the African languages in NLP research, we developed large scale human-annotated labelled datasets for 21 African languages in two impactful NLP tasks: named entity recognition and machine translation. We conduct an extensive empirical evaluation using state-of-the-art methods across supervised, weakly-supervised, and transfer learning settings.
<div id='section'>Paperid: <span id='pid'>562, <a href='https://arxiv.org/pdf/2505.09129.pdf' target='_blank'>https://arxiv.org/pdf/2505.09129.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09129">WSCIF: A Weakly-Supervised Color Intelligence Framework for Tactical Anomaly Detection in Surveillance Keyframes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The deployment of traditional deep learning models in high-risk security tasks in an unlabeled, data-non-exploitable video intelligence environment faces significant challenges. In this paper, we propose a lightweight anomaly detection framework based on color features for surveillance video clips in a high sensitivity tactical mission, aiming to quickly identify and interpret potential threat events under resource-constrained and data-sensitive conditions. The method fuses unsupervised KMeans clustering with RGB channel histogram modeling to achieve composite detection of structural anomalies and color mutation signals in key frames. The experiment takes an operation surveillance video occurring in an African country as a research sample, and successfully identifies multiple highly anomalous frames related to high-energy light sources, target presence, and reflective interference under the condition of no access to the original data. The results show that this method can be effectively used for tactical assassination warning, suspicious object screening and environmental drastic change monitoring with strong deployability and tactical interpretation value. The study emphasizes the importance of color features as low semantic battlefield signal carriers, and its battlefield intelligent perception capability will be further extended by combining graph neural networks and temporal modeling in the future.
<div id='section'>Paperid: <span id='pid'>563, <a href='https://arxiv.org/pdf/2505.09083.pdf' target='_blank'>https://arxiv.org/pdf/2505.09083.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dominic Zaun Eu Jones
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09083">Ornithologist: Towards Trustworthy "Reasoning" about Central Bank Communications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>I develop Ornithologist, a weakly-supervised textual classification system and measure the hawkishness and dovishness of central bank text. Ornithologist uses ``taxonomy-guided reasoning'', guiding a large language model with human-authored decision trees. This increases the transparency and explainability of the system and makes it accessible to non-experts. It also reduces hallucination risk. Since it requires less supervision than traditional classification systems, it can more easily be applied to other problems or sources of text (e.g. news) without much modification. Ornithologist measurements of hawkishness and dovishness of RBA communication carry information about the future of the cash rate path and of market expectations.
<div id='section'>Paperid: <span id='pid'>564, <a href='https://arxiv.org/pdf/2501.02021.pdf' target='_blank'>https://arxiv.org/pdf/2501.02021.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditya Prakash
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.02021">Weakly Supervised Learning on Large Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph classification plays a pivotal role in various domains, including pathology, where images can be represented as graphs. In this domain, images can be represented as graphs, where nodes might represent individual nuclei, and edges capture the spatial or functional relationships between them. Often, the overall label of the graph, such as a cancer type or disease state, is determined by patterns within smaller, localized regions of the image. This work introduces a weakly-supervised graph classification framework leveraging two subgraph extraction techniques: (1) Sliding-window approach (2) BFS-based approach. Subgraphs are processed using a Graph Attention Network (GAT), which employs attention mechanisms to identify the most informative subgraphs for classification. Weak supervision is achieved by propagating graph-level labels to subgraphs, eliminating the need for detailed subgraph annotations.
