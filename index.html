<!DOCTYPE html>
<html>
<head>
<title>arXiv Papers of Weakly Supervised Learning</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">


/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 10px 20px 10px 20px;
  max-width: 960px;
  margin: 0 auto;
}

span#pid {
  color:red;
  
}
span#filename{
  font-style: oblique;
}

span#title{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: italic;
  font-size: 20px;
  border:1px solid #B50;
}
span#abs{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: oblique;
  font-size: 18px;
}
</style>
</head>
<body><div id='title' style='font-size:1.3em; font-weight:bold;'>arXiv Papers of Weakly Supervised Learning</div><br>
<div id='section'>Paperid: <span id='pid'>1, <a href='https://arxiv.org/pdf/2512.22188.pdf' target='_blank'>https://arxiv.org/pdf/2512.22188.pdf</a></span>   <span><a href='https://github.com/lingxitong/HookMIL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xitong Ling, Minxi Ouyang, Xiaoxiao Li, Jiawen Li, Ying Chen, Yuxuan Sun, Xinrui Chen, Tian Guan, Xiaoping Liu, Yonghong He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.22188">HookMIL: Revisiting Context Modeling in Multiple Instance Learning for Computational Pathology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple Instance Learning (MIL) has enabled weakly supervised analysis of whole-slide images (WSIs) in computational pathology. However, traditional MIL approaches often lose crucial contextual information, while transformer-based variants, though more expressive, suffer from quadratic complexity and redundant computations. To address these limitations, we propose HookMIL, a context-aware and computationally efficient MIL framework that leverages compact, learnable hook tokens for structured contextual aggregation. These tokens can be initialized from (i) key-patch visual features, (ii) text embeddings from vision-language pathology models, and (iii) spatially grounded features from spatial transcriptomics-vision models. This multimodal initialization enables Hook Tokens to incorporate rich textual and spatial priors, accelerating convergence and enhancing representation quality. During training, Hook tokens interact with instances through bidirectional attention with linear complexity. To further promote specialization, we introduce a Hook Diversity Loss that encourages each token to focus on distinct histopathological patterns. Additionally, a hook-to-hook communication mechanism refines contextual interactions while minimizing redundancy. Extensive experiments on four public pathology datasets demonstrate that HookMIL achieves state-of-the-art performance, with improved computational efficiency and interpretability. Codes are available at https://github.com/lingxitong/HookMIL.
<div id='section'>Paperid: <span id='pid'>2, <a href='https://arxiv.org/pdf/2512.19990.pdf' target='_blank'>https://arxiv.org/pdf/2512.19990.pdf</a></span>   <span><a href='https://github.com/gpgpgp123/DDTM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Gao, Ke Li, Di Wang, Yongshan Zhu, Yiming Zhang, Xuemei Luo, Yifeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.19990">A Dual-Branch Local-Global Framework for Cross-Resolution Land Cover Mapping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-resolution land cover mapping aims to produce high-resolution semantic predictions from coarse or low-resolution supervision, yet the severe resolution mismatch makes effective learning highly challenging. Existing weakly supervised approaches often struggle to align fine-grained spatial structures with coarse labels, leading to noisy supervision and degraded mapping accuracy. To tackle this problem, we propose DDTM, a dual-branch weakly supervised framework that explicitly decouples local semantic refinement from global contextual reasoning. Specifically, DDTM introduces a diffusion-based branch to progressively refine fine-scale local semantics under coarse supervision, while a transformer-based branch enforces long-range contextual consistency across large spatial extents. In addition, we design a pseudo-label confidence evaluation module to mitigate noise induced by cross-resolution inconsistencies and to selectively exploit reliable supervisory signals. Extensive experiments demonstrate that DDTM establishes a new state-of-the-art on the Chesapeake Bay benchmark, achieving 66.52\% mIoU and substantially outperforming prior weakly supervised methods. The code is available at https://github.com/gpgpgp123/DDTM.
<div id='section'>Paperid: <span id='pid'>3, <a href='https://arxiv.org/pdf/2512.10353.pdf' target='_blank'>https://arxiv.org/pdf/2512.10353.pdf</a></span>   <span><a href='https://github.com/YihengLyu/TranSamba' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiheng Lyu, Lian Xu, Mohammed Bennamoun, Farid Boussaid, Coen Arrow, Girish Dwivedi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.10353">Hybrid Transformer-Mamba Architecture for Weakly Supervised Volumetric Medical Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation offers a label-efficient solution to train segmentation models for volumetric medical imaging. However, existing approaches often rely on 2D encoders that neglect the inherent volumetric nature of the data. We propose TranSamba, a hybrid Transformer-Mamba architecture designed to capture 3D context for weakly supervised volumetric medical segmentation. TranSamba augments a standard Vision Transformer backbone with Cross-Plane Mamba blocks, which leverage the linear complexity of state space models for efficient information exchange across neighboring slices. The information exchange enhances the pairwise self-attention within slices computed by the Transformer blocks, directly contributing to the attention maps for object localization. TranSamba achieves effective volumetric modeling with time complexity that scales linearly with the input volume depth and maintains constant memory usage for batch processing. Extensive experiments on three datasets demonstrate that TranSamba establishes new state-of-the-art performance, consistently outperforming existing methods across diverse modalities and pathologies. Our source code and trained models are openly accessible at: https://github.com/YihengLyu/TranSamba.
<div id='section'>Paperid: <span id='pid'>4, <a href='https://arxiv.org/pdf/2512.10031.pdf' target='_blank'>https://arxiv.org/pdf/2512.10031.pdf</a></span>   <span><a href='https://kaist-viclab.github.io/ABBSPO_site/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Woojin Lee, Hyugjae Chang, Jaeho Moon, Jaehyup Lee, Munchurl Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.10031">ABBSPO: Adaptive Bounding Box Scaling and Symmetric Prior based Orientation Prediction for Detecting Aerial Image Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised oriented object detection (WS-OOD) has gained attention as a cost-effective alternative to fully supervised methods, providing both efficiency and high accuracy. Among weakly supervised approaches, horizontal bounding box (HBox)-supervised OOD stands out for its ability to directly leverage existing HBox annotations while achieving the highest accuracy under weak supervision settings. This paper introduces adaptive bounding box scaling and symmetry-prior-based orientation prediction, called ABBSPO, a framework for WS-OOD. Our ABBSPO addresses limitations of previous HBox-supervised OOD methods, which compare ground truth (GT) HBoxes directly with the minimum circumscribed rectangles of predicted RBoxes, often leading to inaccurate scale estimation. To overcome this, we propose: (i) Adaptive Bounding Box Scaling (ABBS), which appropriately scales GT HBoxes to optimize for the size of each predicted RBox, ensuring more accurate scale prediction; and (ii) a Symmetric Prior Angle (SPA) loss that exploits inherent symmetry of aerial objects for self-supervised learning, resolving issues in previous methods where learning collapses when predictions for all three augmented views (original, rotated, and flipped) are consistently incorrect. Extensive experimental results demonstrate that ABBSPO achieves state-of-the-art performance, outperforming existing methods.
<div id='section'>Paperid: <span id='pid'>5, <a href='https://arxiv.org/pdf/2512.05996.pdf' target='_blank'>https://arxiv.org/pdf/2512.05996.pdf</a></span>   <span><a href='https://umfieldrobotics.github.io/FishDetector-R1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Liu, Jingyu Song, Vedanth Kallakuri, Katherine A. Skinner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.05996">FishDetector-R1: Unified MLLM-Based Framework with Reinforcement Fine-Tuning for Weakly Supervised Fish Detection, Segmentation, and Counting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Analyzing underwater fish imagery is critical for ecological monitoring but remains difficult due to visual degradation and costly annotations. We introduce FishDetector-R1, a unified MLLM-based framework for fish detection, segmentation, and counting under weak supervision. On the DeepFish dataset, our framework achieves substantial gains over baselines, improving AP by 20% and mIoU by 10%, while reducing MAE by 30% and GAME by 35%. These improvements stem from two key components: a novel detect-to-count prompt that enforces spatially consistent detections and counts, and Reinforcement Learning from Verifiable Reward (RLVR) with a complementary scalable paradigm leveraging sparse point labels. Ablation studies further validate the effectiveness of this reward design. Moreover, the improvement generalizes well to other underwater datasets, confirming strong cross-domain robustness. Overall, FishDetector-R1 provides a reliable and scalable solution for accurate marine visual understanding via weak supervision. The project page for FishDetector-R1 is https://umfieldrobotics.github.io/FishDetector-R1.
<div id='section'>Paperid: <span id='pid'>6, <a href='https://arxiv.org/pdf/2512.05113.pdf' target='_blank'>https://arxiv.org/pdf/2512.05113.pdf</a></span>   <span><a href='https://chien90190.github.io/splannequin/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao-Jen Chien, Yi-Chuan Huang, Chung-Ho Wu, Wei-Lun Chao, Yu-Lun Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.05113">Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthesizing high-fidelity frozen 3D scenes from monocular Mannequin-Challenge (MC) videos is a unique problem distinct from standard dynamic scene reconstruction. Instead of focusing on modeling motion, our goal is to create a frozen scene while strategically preserving subtle dynamics to enable user-controlled instant selection. To achieve this, we introduce a novel application of dynamic Gaussian splatting: the scene is modeled dynamically, which retains nearby temporal variation, and a static scene is rendered by fixing the model's time parameter. However, under this usage, monocular capture with sparse temporal supervision introduces artifacts like ghosting and blur for Gaussians that become unobserved or occluded at weakly supervised timestamps. We propose Splannequin, an architecture-agnostic regularization that detects two states of Gaussian primitives, hidden and defective, and applies temporal anchoring. Under predominantly forward camera motion, hidden states are anchored to their recent well-observed past states, while defective states are anchored to future states with stronger supervision. Our method integrates into existing dynamic Gaussian pipelines via simple loss terms, requires no architectural changes, and adds zero inference overhead. This results in markedly improved visual quality, enabling high-fidelity, user-selectable frozen-time renderings, validated by a 96% user preference. Project page: https://chien90190.github.io/splannequin/
<div id='section'>Paperid: <span id='pid'>7, <a href='https://arxiv.org/pdf/2512.02359.pdf' target='_blank'>https://arxiv.org/pdf/2512.02359.pdf</a></span>   <span><a href='https://github.com/zqyq/Weakly-MVCC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bin Li, Daijie Chen, Qi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.02359">WSCF-MVCC: Weakly-supervised Calibration-free Multi-view Crowd Counting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-view crowd counting can effectively mitigate occlusion issues that commonly arise in single-image crowd counting. Existing deep-learning multi-view crowd counting methods project different camera view images onto a common space to obtain ground-plane density maps, requiring abundant and costly crowd annotations and camera calibrations. Hence, calibration-free methods are proposed that do not require camera calibrations and scene-level crowd annotations. However, existing calibration-free methods still require expensive image-level crowd annotations for training the single-view counting module. Thus, in this paper, we propose a weakly-supervised calibration-free multi-view crowd counting method (WSCF-MVCC), directly using crowd count as supervision for the single-view counting module rather than density maps constructed from crowd annotations. Instead, a self-supervised ranking loss that leverages multi-scale priors is utilized to enhance the model's perceptual ability without additional annotation costs. What's more, the proposed model leverages semantic information to achieve a more accurate view matching and, consequently, a more precise scene-level crowd count estimation. The proposed method outperforms the state-of-the-art methods on three widely used multi-view counting datasets under weakly supervised settings, indicating that it is more suitable for practical deployment compared with calibrated methods. Code is released in https://github.com/zqyq/Weakly-MVCC.
<div id='section'>Paperid: <span id='pid'>8, <a href='https://arxiv.org/pdf/2512.01178.pdf' target='_blank'>https://arxiv.org/pdf/2512.01178.pdf</a></span>   <span><a href='https://github.com/Magicboomliu/VSRD_plus_plus' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihua Liu, Hiroki Sakuma, Masatoshi Okutomi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.01178">VSRD++: Autolabeling for 3D Object Detection via Instance-Aware Volumetric Silhouette Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monocular 3D object detection is a fundamental yet challenging task in 3D scene understanding. Existing approaches heavily depend on supervised learning with extensive 3D annotations, which are often acquired from LiDAR point clouds through labor-intensive labeling processes. To tackle this problem, we propose VSRD++, a novel weakly supervised framework for monocular 3D object detection that eliminates the reliance on 3D annotations and leverages neural-field-based volumetric rendering with weak 2D supervision. VSRD++ consists of a two-stage pipeline: multi-view 3D autolabeling and subsequent monocular 3D detector training. In the multi-view autolabeling stage, object surfaces are represented as signed distance fields (SDFs) and rendered as instance masks via the proposed instance-aware volumetric silhouette rendering. To optimize 3D bounding boxes, we decompose each instance's SDF into a cuboid SDF and a residual distance field (RDF) that captures deviations from the cuboid. To address the geometry inconsistency commonly observed in volume rendering methods applied to dynamic objects, we model the dynamic objects by including velocity into bounding box attributes as well as assigning confidence to each pseudo-label. Moreover, we also employ a 3D attribute initialization module to initialize the dynamic bounding box parameters. In the monocular 3D object detection phase, the optimized 3D bounding boxes serve as pseudo labels for training monocular 3D object detectors. Extensive experiments on the KITTI-360 dataset demonstrate that VSRD++ significantly outperforms existing weakly supervised approaches for monocular 3D object detection on both static and dynamic scenes. Code is available at https://github.com/Magicboomliu/VSRD_plus_plus
<div id='section'>Paperid: <span id='pid'>9, <a href='https://arxiv.org/pdf/2512.00130.pdf' target='_blank'>https://arxiv.org/pdf/2512.00130.pdf</a></span>   <span><a href='https://github.com/DanielaPlusPlus/LGCOAMix' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fadi Dornaika, Danyang Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.00130">Local and Global Context-and-Object-part-Aware Superpixel-based Data Augmentation for Deep Visual Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cutmix-based data augmentation, which uses a cut-and-paste strategy, has shown remarkable generalization capabilities in deep learning. However, existing methods primarily consider global semantics with image-level constraints, which excessively reduces attention to the discriminative local context of the class and leads to a performance improvement bottleneck. Moreover, existing methods for generating augmented samples usually involve cutting and pasting rectangular or square regions, resulting in a loss of object part information. To mitigate the problem of inconsistency between the augmented image and the generated mixed label, existing methods usually require double forward propagation or rely on an external pre-trained network for object centering, which is inefficient. To overcome the above limitations, we propose LGCOAMix, an efficient context-aware and object-part-aware superpixel-based grid blending method for data augmentation. To the best of our knowledge, this is the first time that a label mixing strategy using a superpixel attention approach has been proposed for cutmix-based data augmentation. It is the first instance of learning local features from discriminative superpixel-wise regions and cross-image superpixel contrasts. Extensive experiments on various benchmark datasets show that LGCOAMix outperforms state-of-the-art cutmix-based data augmentation methods on classification tasks, {and weakly supervised object location on CUB200-2011.} We have demonstrated the effectiveness of LGCOAMix not only for CNN networks, but also for Transformer networks. Source codes are available at https://github.com/DanielaPlusPlus/LGCOAMix.
<div id='section'>Paperid: <span id='pid'>10, <a href='https://arxiv.org/pdf/2511.14639.pdf' target='_blank'>https://arxiv.org/pdf/2511.14639.pdf</a></span>   <span><a href='https://github.com/Ace95/SLAM-AGS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Marco Acerbis, Swarnadip Chatterjee, Christophe Avenel, Joakim Lindblad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.14639">SLAM-AGS: Slide-Label Aware Multi-Task Pretraining Using Adaptive Gradient Surgery in Computational Cytology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computational cytology faces two major challenges: i) instance-level labels are unreliable and prohibitively costly to obtain, ii) witness rates are extremely low. We propose SLAM-AGS, a Slide-Label-Aware Multitask pretraining framework that jointly optimizes (i) a weakly supervised similarity objective on slide-negative patches and (ii) a self-supervised contrastive objective on slide-positive patches, yielding stronger performance on downstream tasks. To stabilize learning, we apply Adaptive Gradient Surgery to tackle conflicting task gradients and prevent model collapse. We integrate the pretrained encoder into an attention-based Multiple Instance Learning aggregator for bag-level prediction and attention-guided retrieval of the most abnormal instances in a bag. On a publicly available bone-marrow cytology dataset, with simulated witness rates from 10% down to 0.5%, SLAM-AGS improves bag-level F1-Score and Top 400 positive cell retrieval over other pretraining methods, with the largest gains at low witness rates, showing that resolving gradient interference enables stable pretraining and better performance on downstream tasks. To facilitate reproducibility, we share our complete implementation and evaluation framework as open source: https://github.com/Ace95/SLAM-AGS.
<div id='section'>Paperid: <span id='pid'>11, <a href='https://arxiv.org/pdf/2511.14250.pdf' target='_blank'>https://arxiv.org/pdf/2511.14250.pdf</a></span>   <span><a href='https://yoni-yaffe.github.io/count-the-notes' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonathan Yaffe, Ben Maman, Meinard Müller, Amit H. Bermano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.14250">Count The Notes: Histogram-Based Supervision for Automatic Music Transcription</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic Music Transcription (AMT) converts audio recordings into symbolic musical representations. Training deep neural networks (DNNs) for AMT typically requires strongly aligned training pairs with precise frame-level annotations. Since creating such datasets is costly and impractical for many musical contexts, weakly aligned approaches using segment-level annotations have gained traction. However, existing methods often rely on Dynamic Time Warping (DTW) or soft alignment loss functions, both of which still require local semantic correspondences, making them error-prone and computationally expensive. In this article, we introduce CountEM, a novel AMT framework that eliminates the need for explicit local alignment by leveraging note event histograms as supervision, enabling lighter computations and greater flexibility. Using an Expectation-Maximization (EM) approach, CountEM iteratively refines predictions based solely on note occurrence counts, significantly reducing annotation efforts while maintaining high transcription accuracy. Experiments on piano, guitar, and multi-instrument datasets demonstrate that CountEM matches or surpasses existing weakly supervised methods, improving AMT's robustness, scalability, and efficiency. Our project page is available at https://yoni-yaffe.github.io/count-the-notes.
<div id='section'>Paperid: <span id='pid'>12, <a href='https://arxiv.org/pdf/2511.13863.pdf' target='_blank'>https://arxiv.org/pdf/2511.13863.pdf</a></span>   <span><a href='https://krantiparida.github.io/projects/cs3.html' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kranti Kumar Parida, Omar Emara, Hazel Doughty, Dima Damen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13863">Segmenting Collision Sound Sources in Egocentric Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans excel at multisensory perception and can often recognise object properties from the sound of their interactions. Inspired by this, we propose the novel task of Collision Sound Source Segmentation (CS3), where we aim to segment the objects responsible for a collision sound in visual input (i.e. video frames from the collision clip), conditioned on the audio. This task presents unique challenges. Unlike isolated sound events, a collision sound arises from interactions between two objects, and the acoustic signature of the collision depends on both. We focus on egocentric video, where sounds are often clear, but the visual scene is cluttered, objects are small, and interactions are brief. To address these challenges, we propose a weakly-supervised method for audio-conditioned segmentation, utilising foundation models (CLIP and SAM2). We also incorporate egocentric cues, i.e. objects in hands, to find acting objects that can potentially be collision sound sources. Our approach outperforms competitive baselines by $3\times$ and $4.7\times$ in mIoU on two benchmarks we introduce for the CS3 task: EPIC-CS3 and Ego4D-CS3.
<div id='section'>Paperid: <span id='pid'>13, <a href='https://arxiv.org/pdf/2511.10334.pdf' target='_blank'>https://arxiv.org/pdf/2511.10334.pdf</a></span>   <span><a href='https://github.com/lessiYin/DSANet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenti Yin, Huaxin Zhang, Xiang Wang, Yuqing Lu, Yicheng Zhang, Bingquan Gong, Jialong Zuo, Li Yu, Changxin Gao, Nong Sang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.10334">Learning to Tell Apart: Weakly Supervised Video Anomaly Detection via Disentangled Semantic Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in weakly-supervised video anomaly detection have achieved remarkable performance by applying the multiple instance learning paradigm based on multimodal foundation models such as CLIP to highlight anomalous instances and classify categories. However, their objectives may tend to detect the most salient response segments, while neglecting to mine diverse normal patterns separated from anomalies, and are prone to category confusion due to similar appearance, leading to unsatisfactory fine-grained classification results. Therefore, we propose a novel Disentangled Semantic Alignment Network (DSANet) to explicitly separate abnormal and normal features from coarse-grained and fine-grained aspects, enhancing the distinguishability. Specifically, at the coarse-grained level, we introduce a self-guided normality modeling branch that reconstructs input video features under the guidance of learned normal prototypes, encouraging the model to exploit normality cues inherent in the video, thereby improving the temporal separation of normal patterns and anomalous events. At the fine-grained level, we present a decoupled contrastive semantic alignment mechanism, which first temporally decomposes each video into event-centric and background-centric components using frame-level anomaly scores and then applies visual-language contrastive learning to enhance class-discriminative representations. Comprehensive experiments on two standard benchmarks, namely XD-Violence and UCF-Crime, demonstrate that DSANet outperforms existing state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>14, <a href='https://arxiv.org/pdf/2511.10003.pdf' target='_blank'>https://arxiv.org/pdf/2511.10003.pdf</a></span>   <span><a href='https://github.com/liuxuexun/DBGroup' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuexun Liu, Xiaoxu Xu, Qiudan Zhang, Lin Ma, Xu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.10003">DBGroup: Dual-Branch Point Grouping for Weakly Supervised 3D Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised 3D instance segmentation is essential for 3D scene understanding, especially as the growing scale of data and high annotation costs associated with fully supervised approaches. Existing methods primarily rely on two forms of weak supervision: one-thing-one-click annotations and bounding box annotations, both of which aim to reduce labeling efforts. However, these approaches still encounter limitations, including labor-intensive annotation processes, high complexity, and reliance on expert annotators. To address these challenges, we propose \textbf{DBGroup}, a two-stage weakly supervised 3D instance segmentation framework that leverages scene-level annotations as a more efficient and scalable alternative. In the first stage, we introduce a Dual-Branch Point Grouping module to generate pseudo labels guided by semantic and mask cues extracted from multi-view images. To further improve label quality, we develop two refinement strategies: Granularity-Aware Instance Merging and Semantic Selection and Propagation. The second stage involves multi-round self-training on an end-to-end instance segmentation network using the refined pseudo-labels. Additionally, we introduce an Instance Mask Filter strategy to address inconsistencies within the pseudo labels. Extensive experiments demonstrate that DBGroup achieves competitive performance compared to sparse-point-level supervised 3D instance segmentation methods, while surpassing state-of-the-art scene-level supervised 3D semantic segmentation approaches. Code is available at https://github.com/liuxuexun/DBGroup.
<div id='section'>Paperid: <span id='pid'>15, <a href='https://arxiv.org/pdf/2511.05833.pdf' target='_blank'>https://arxiv.org/pdf/2511.05833.pdf</a></span>   <span><a href='https://github.com/Taixi-CHEN/TYrPPG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Taixi Chen, Yiu-ming Cheung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.05833">TYrPPG: Uncomplicated and Enhanced Learning Capability rPPG for Remote Heart Rate Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Remote photoplethysmography (rPPG) can remotely extract physiological signals from RGB video, which has many advantages in detecting heart rate, such as low cost and no invasion to patients. The existing rPPG model is usually based on the transformer module, which has low computation efficiency. Recently, the Mamba model has garnered increasing attention due to its efficient performance in natural language processing tasks, demonstrating potential as a substitute for transformer-based algorithms. However, the Mambaout model and its variants prove that the SSM module, which is the core component of the Mamba model, is unnecessary for the vision task. Therefore, we hope to prove the feasibility of using the Mambaout-based module to remotely learn the heart rate. Specifically, we propose a novel rPPG algorithm called uncomplicated and enhanced learning capability rPPG (TYrPPG). This paper introduces an innovative gated video understanding block (GVB) designed for efficient analysis of RGB videos. Based on the Mambaout structure, this block integrates 2D-CNN and 3D-CNN to enhance video understanding for analysis. In addition, we propose a comprehensive supervised loss function (CSL) to improve the model's learning capability, along with its weakly supervised variants. The experiments show that our TYrPPG can achieve state-of-the-art performance in commonly used datasets, indicating its prospects and superiority in remote heart rate estimation. The source code is available at https://github.com/Taixi-CHEN/TYrPPG.
<div id='section'>Paperid: <span id='pid'>16, <a href='https://arxiv.org/pdf/2511.00456.pdf' target='_blank'>https://arxiv.org/pdf/2511.00456.pdf</a></span>   <span><a href='https://github.com/kiranshahi/pneumonia-analysis' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kiran Shahi, Anup Bagale
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.00456">Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study proposes a weakly supervised deep learning framework for pneumonia classification and localization from chest X-rays, utilizing Grad-CAM explanations. Instead of costly pixel-level annotations, our approach uses image-level labels to generate clinically meaningful heatmaps that highlight regions affected by pneumonia. We evaluate seven pre-trained architectures and the Vision Transformer under identical training conditions, using focal loss and patient-wise splits to prevent data leakage. Experimental results suggest that all models achieved high accuracy (96-98%), with ResNet-18 and EfficientNet-B0 showing the best overall performance and MobileNet-V2 providing an efficient lightweight alternative. Grad-CAM heatmap visualizations confirm that the proposed models focus on clinically relevant lung regions, supporting the use of interpretable AI for radiological diagnostics. This work highlights the potential of weakly supervised, explainable models that enhance the transparency of pneumonia screening and clinical trust in AI-assisted screening.
<div id='section'>Paperid: <span id='pid'>17, <a href='https://arxiv.org/pdf/2510.22055.pdf' target='_blank'>https://arxiv.org/pdf/2510.22055.pdf</a></span>   <span><a href='https://github.com/VenkteshV/QuanTemp_Plus' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>V Venktesh, Deepali Prabhu, Avishek Anand
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.22055">A Benchmark for Open-Domain Numerical Fact-Checking Enhanced by Claim Decomposition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fact-checking numerical claims is critical as the presence of numbers provide mirage of veracity despite being fake potentially causing catastrophic impacts on society. The prior works in automatic fact verification do not primarily focus on natural numerical claims. A typical human fact-checker first retrieves relevant evidence addressing the different numerical aspects of the claim and then reasons about them to predict the veracity of the claim. Hence, the search process of a human fact-checker is a crucial skill that forms the foundation of the verification process. Emulating a real-world setting is essential to aid in the development of automated methods that encompass such skills. However, existing benchmarks employ heuristic claim decomposition approaches augmented with weakly supervised web search to collect evidences for verifying claims. This sometimes results in less relevant evidences and noisy sources with temporal leakage rendering a less realistic retrieval setting for claim verification. Hence, we introduce QuanTemp++: a dataset consisting of natural numerical claims, an open domain corpus, with the corresponding relevant evidence for each claim. The evidences are collected through a claim decomposition process approximately emulating the approach of human fact-checker and veracity labels ensuring there is no temporal leakage. Given this dataset, we also characterize the retrieval performance of key claim decomposition paradigms. Finally, we observe their effect on the outcome of the verification pipeline and draw insights. The code for data pipeline along with link to data can be found at https://github.com/VenkteshV/QuanTemp_Plus
<div id='section'>Paperid: <span id='pid'>18, <a href='https://arxiv.org/pdf/2510.21532.pdf' target='_blank'>https://arxiv.org/pdf/2510.21532.pdf</a></span>   <span><a href='https://github.com/rohban-lab/FrameShield' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mojtaba Nafez, Mobina Poulaei, Nikan Vasei, Bardia Soltani Moakhar, Mohammad Sabokrou, MohammadHossein Rohban
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.21532">FrameShield: Adversarially Robust Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Video Anomaly Detection (WSVAD) has achieved notable advancements, yet existing models remain vulnerable to adversarial attacks, limiting their reliability. Due to the inherent constraints of weak supervision, where only video-level labels are provided despite the need for frame-level predictions, traditional adversarial defense mechanisms, such as adversarial training, are not effective since video-level adversarial perturbations are typically weak and inadequate. To address this limitation, pseudo-labels generated directly from the model can enable frame-level adversarial training; however, these pseudo-labels are inherently noisy, significantly degrading performance. We therefore introduce a novel Pseudo-Anomaly Generation method called Spatiotemporal Region Distortion (SRD), which creates synthetic anomalies by applying severe augmentations to localized regions in normal videos while preserving temporal consistency. Integrating these precisely annotated synthetic anomalies with the noisy pseudo-labels substantially reduces label noise, enabling effective adversarial training. Extensive experiments demonstrate that our method significantly enhances the robustness of WSVAD models against adversarial attacks, outperforming state-of-the-art methods by an average of 71.0\% in overall AUROC performance across multiple benchmarks. The implementation and code are publicly available at https://github.com/rohban-lab/FrameShield.
<div id='section'>Paperid: <span id='pid'>19, <a href='https://arxiv.org/pdf/2510.21449.pdf' target='_blank'>https://arxiv.org/pdf/2510.21449.pdf</a></span>   <span><a href='https://github.com/YsTvT/MoniTor' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengtian Yang, Yue Feng, Yingshi Liu, Jingrou Zhang, Jie Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.21449">MoniTor: Exploiting Large Language Models with Instruction for Online Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Anomaly Detection (VAD) aims to locate unusual activities or behaviors within videos. Recently, offline VAD has garnered substantial research attention, which has been invigorated by the progress in large language models (LLMs) and vision-language models (VLMs), offering the potential for a more nuanced understanding of anomalies. However, online VAD has seldom received attention due to real-time constraints and computational intensity. In this paper, we introduce a novel Memory-based online scoring queue scheme for Training-free VAD (MoniTor), to address the inherent complexities in online VAD. Specifically, MoniTor applies a streaming input to VLMs, leveraging the capabilities of pre-trained large-scale models. To capture temporal dependencies more effectively, we incorporate a novel prediction mechanism inspired by Long Short-Term Memory (LSTM) networks. This ensures the model can effectively model past states and leverage previous predictions to identify anomalous behaviors. Thereby, it better understands the current frame. Moreover, we design a scoring queue and an anomaly prior to dynamically store recent scores and cover all anomalies in the monitoring scenario, providing guidance for LLMs to distinguish between normal and abnormal behaviors over time. We evaluate MoniTor on two large datasets (i.e., UCF-Crime and XD-Violence) containing various surveillance and real-world scenarios. The results demonstrate that MoniTor outperforms state-of-the-art methods and is competitive with weakly supervised methods without training. Code is available at https://github.com/YsTvT/MoniTor.
<div id='section'>Paperid: <span id='pid'>20, <a href='https://arxiv.org/pdf/2510.12385.pdf' target='_blank'>https://arxiv.org/pdf/2510.12385.pdf</a></span>   <span><a href='https://timschoonbeek.github.io/stormpsr' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tim J. Schoonbeek, Shao-Hsuan Hung, Dan Lehman, Hans Onvlee, Jacek Kustra, Peter H. N. de With, Fons van der Sommen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.12385">Learning to Recognize Correctly Completed Procedure Steps in Egocentric Assembly Videos through Spatio-Temporal Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Procedure step recognition (PSR) aims to identify all correctly completed steps and their sequential order in videos of procedural tasks. The existing state-of-the-art models rely solely on detecting assembly object states in individual video frames. By neglecting temporal features, model robustness and accuracy are limited, especially when objects are partially occluded. To overcome these limitations, we propose Spatio-Temporal Occlusion-Resilient Modeling for Procedure Step Recognition (STORM-PSR), a dual-stream framework for PSR that leverages both spatial and temporal features. The assembly state detection stream operates effectively with unobstructed views of the object, while the spatio-temporal stream captures both spatial and temporal features to recognize step completions even under partial occlusion. This stream includes a spatial encoder, pre-trained using a novel weakly supervised approach to capture meaningful spatial representations, and a transformer-based temporal encoder that learns how these spatial features relate over time. STORM-PSR is evaluated on the MECCANO and IndustReal datasets, reducing the average delay between actual and predicted assembly step completions by 11.2% and 26.1%, respectively, compared to prior methods. We demonstrate that this reduction in delay is driven by the spatio-temporal stream, which does not rely on unobstructed views of the object to infer completed steps. The code for STORM-PSR, along with the newly annotated MECCANO labels, is made publicly available at https://timschoonbeek.github.io/stormpsr .
<div id='section'>Paperid: <span id='pid'>21, <a href='https://arxiv.org/pdf/2510.10564.pdf' target='_blank'>https://arxiv.org/pdf/2510.10564.pdf</a></span>   <span><a href='https://github.com/lalunex/MGSD-WSS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liang Li, Zhou Yang, Xiaofei Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10564">Multi-Granularity Sequence Denoising with Weakly Supervised Signal for Sequential Recommendation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sequential recommendation aims to predict the next item based on user interests in historical interaction sequences. Historical interaction sequences often contain irrelevant noisy items, which significantly hinders the performance of recommendation systems. Existing research employs unsupervised methods that indirectly identify item-granularity irrelevant noise by predicting the ground truth item. Since these methods lack explicit noise labels, they are prone to misidentify users' interested items as noise. Additionally, while these methods focus on removing item-granularity noise driven by the ground truth item, they overlook interest-granularity noise, limiting their ability to perform broader denoising based on user interests. To address these issues, we propose Multi-Granularity Sequence Denoising with Weakly Supervised Signal for Sequential Recommendation(MGSD-WSS). MGSD-WSS first introduces the Multiple Gaussian Kernel Perceptron module to map the original and enhance sequence into a common representation space and utilizes weakly supervised signals to accurately identify noisy items in the historical interaction sequence. Subsequently, it employs the item-granularity denoising module with noise-weighted contrastive learning to obtain denoised item representations. Then, it extracts target interest representations from the ground truth item and applies noise-weighted contrastive learning to obtain denoised interest representations. Finally, based on the denoised item and interest representations, MGSD-WSS predicts the next item. Extensive experiments on five datasets demonstrate that the proposed method significantly outperforms state-of-the-art sequence recommendation and denoising models. Our code is available at https://github.com/lalunex/MGSD-WSS.
<div id='section'>Paperid: <span id='pid'>22, <a href='https://arxiv.org/pdf/2510.08052.pdf' target='_blank'>https://arxiv.org/pdf/2510.08052.pdf</a></span>   <span><a href='https://github.com/BheeshmSharma/RASALoRE-BMVC-2025/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bheeshm Sharma, Karthikeyan Jaganathan, Balamurugan Palaniappan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08052">RASALoRE: Region Aware Spatial Attention with Location-based Random Embeddings for Weakly Supervised Anomaly Detection in Brain MRI Scans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Anomaly detection (WSAD) in brain MRI scans is an important challenge useful to obtain quick and accurate detection of brain anomalies when precise pixel-level anomaly annotations are unavailable and only weak labels (e.g., slice-level) are available. In this work, we propose RASALoRE: Region Aware Spatial Attention with Location-based Random Embeddings, a novel two-stage WSAD framework. In the first stage, we introduce a Discriminative Dual Prompt Tuning (DDPT) mechanism that generates high-quality pseudo weak masks based on slice-level labels, serving as coarse localization cues. In the second stage, we propose a segmentation network with a region-aware spatial attention mechanism that relies on fixed location-based random embeddings. This design enables the model to effectively focus on anomalous regions. Our approach achieves state-of-the-art anomaly detection performance, significantly outperforming existing WSAD methods while utilizing less than 8 million parameters. Extensive evaluations on the BraTS20, BraTS21, BraTS23, and MSD datasets demonstrate a substantial performance improvement coupled with a significant reduction in computational complexity. Code is available at: https://github.com/BheeshmSharma/RASALoRE-BMVC-2025/.
<div id='section'>Paperid: <span id='pid'>23, <a href='https://arxiv.org/pdf/2510.05899.pdf' target='_blank'>https://arxiv.org/pdf/2510.05899.pdf</a></span>   <span><a href='https://github.com/jiesihu/Weak-ICL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiesi Hu, Yanwu Yang, Zhiyu Ye, Jinyan Zhou, Jianfeng Cao, Hanyang Peng, Ting Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05899">Efficient Universal Models for Medical Image Segmentation via Weakly Supervised In-Context Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Universal models for medical image segmentation, such as interactive and in-context learning (ICL) models, offer strong generalization but require extensive annotations. Interactive models need repeated user prompts for each image, while ICL relies on dense, pixel-level labels. To address this, we propose Weakly Supervised In-Context Learning (WS-ICL), a new ICL paradigm that leverages weak prompts (e.g., bounding boxes or points) instead of dense labels for context. This approach significantly reduces annotation effort by eliminating the need for fine-grained masks and repeated user prompting for all images. We evaluated the proposed WS-ICL model on three held-out benchmarks. Experimental results demonstrate that WS-ICL achieves performance comparable to regular ICL models at a significantly lower annotation cost. In addition, WS-ICL is highly competitive even under the interactive paradigm. These findings establish WS-ICL as a promising step toward more efficient and unified universal models for medical image segmentation. Our code and model are publicly available at https://github.com/jiesihu/Weak-ICL.
<div id='section'>Paperid: <span id='pid'>24, <a href='https://arxiv.org/pdf/2509.10308.pdf' target='_blank'>https://arxiv.org/pdf/2509.10308.pdf</a></span>   <span><a href='https://github.com/riskaudit/GraphCSVAE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Joshua Dimasaka, Christian GeiÃ, Robert Muir-Wood, Emily So
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10308">GraphCSVAE: Graph Categorical Structured Variational Autoencoder for Spatiotemporal Auditing of Physical Vulnerability Towards Sustainable Post-Disaster Risk Reduction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the aftermath of disasters, many institutions worldwide face challenges in continually monitoring changes in disaster risk, limiting the ability of key decision-makers to assess progress towards the UN Sendai Framework for Disaster Risk Reduction 2015-2030. While numerous efforts have substantially advanced the large-scale modeling of hazard and exposure through Earth observation and data-driven methods, progress remains limited in modeling another equally important yet challenging element of the risk equation: physical vulnerability. To address this gap, we introduce Graph Categorical Structured Variational Autoencoder (GraphCSVAE), a novel probabilistic data-driven framework for modeling physical vulnerability by integrating deep learning, graph representation, and categorical probabilistic inference, using time-series satellite-derived datasets and prior expert belief systems. We introduce a weakly supervised first-order transition matrix that reflects the changes in the spatiotemporal distribution of physical vulnerability in two disaster-stricken and socioeconomically disadvantaged areas: (1) the cyclone-impacted coastal Khurushkul community in Bangladesh and (2) the mudslide-affected city of Freetown in Sierra Leone. Our work reveals post-disaster regional dynamics in physical vulnerability, offering valuable insights into localized spatiotemporal auditing and sustainable strategies for post-disaster risk reduction.
<div id='section'>Paperid: <span id='pid'>25, <a href='https://arxiv.org/pdf/2509.08991.pdf' target='_blank'>https://arxiv.org/pdf/2509.08991.pdf</a></span>   <span><a href='https://github.com/magdalena-wysocki/ultron' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Magdalena Wysocki, Felix Duelmer, Ananya Bal, Nassir Navab, Mohammad Farid Azampour
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08991">UltrON: Ultrasound Occupancy Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In free-hand ultrasound imaging, sonographers rely on expertise to mentally integrate partial 2D views into 3D anatomical shapes. Shape reconstruction can assist clinicians in this process. Central to this task is the choice of shape representation, as it determines how accurately and efficiently the structure can be visualized, analyzed, and interpreted. Implicit representations, such as SDF and occupancy function, offer a powerful alternative to traditional voxel- or mesh-based methods by modeling continuous, smooth surfaces with compact storage, avoiding explicit discretization. Recent studies demonstrate that SDF can be effectively optimized using annotations derived from segmented B-mode ultrasound images. Yet, these approaches hinge on precise annotations, overlooking the rich acoustic information embedded in B-mode intensity. Moreover, implicit representation approaches struggle with the ultrasound's view-dependent nature and acoustic shadowing artifacts, which impair reconstruction. To address the problems resulting from occlusions and annotation dependency, we propose an occupancy-based representation and introduce \gls{UltrON} that leverages acoustic features to improve geometric consistency in weakly-supervised optimization regime. We show that these features can be obtained from B-mode images without additional annotation cost. Moreover, we propose a novel loss function that compensates for view-dependency in the B-mode images and facilitates occupancy optimization from multiview ultrasound. By incorporating acoustic properties, \gls{UltrON} generalizes to shapes of the same anatomy. We show that \gls{UltrON} mitigates the limitations of occlusions and sparse labeling and paves the way for more accurate 3D reconstruction. Code and dataset will be available at https://github.com/magdalena-wysocki/ultron.
<div id='section'>Paperid: <span id='pid'>26, <a href='https://arxiv.org/pdf/2509.08289.pdf' target='_blank'>https://arxiv.org/pdf/2509.08289.pdf</a></span>   <span><a href='https://github.com/gyl2565309278/DTH-CP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuelin Guo, Haoyu He, Zhiyuan Chen, Zitong Huang, Renhao Lu, Lu Shi, Zejun Wang, Weizhe Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08289">Dual-Thresholding Heatmaps to Cluster Proposals for Weakly Supervised Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised object detection (WSOD) has attracted significant attention in recent years, as it does not require box-level annotations. State-of-the-art methods generally adopt a multi-module network, which employs WSDDN as the multiple instance detection network module and multiple instance refinement modules to refine performance. However, these approaches suffer from three key limitations. First, existing methods tend to generate pseudo GT boxes that either focus only on discriminative parts, failing to capture the whole object, or cover the entire object but fail to distinguish between adjacent intra-class instances. Second, the foundational WSDDN architecture lacks a crucial background class representation for each proposal and exhibits a large semantic gap between its branches. Third, prior methods discard ignored proposals during optimization, leading to slow convergence. To address these challenges, we first design a heatmap-guided proposal selector (HGPS) algorithm, which utilizes dual thresholds on heatmaps to pre-select proposals, enabling pseudo GT boxes to both capture the full object extent and distinguish between adjacent intra-class instances. We then present a weakly supervised basic detection network (WSBDN), which augments each proposal with a background class representation and uses heatmaps for pre-supervision to bridge the semantic gap between matrices. At last, we introduce a negative certainty supervision loss on ignored proposals to accelerate convergence. Extensive experiments on the challenging PASCAL VOC 2007 and 2012 datasets demonstrate the effectiveness of our framework. We achieve mAP/mCorLoc scores of 58.5%/81.8% on VOC 2007 and 55.6%/80.5% on VOC 2012, performing favorably against the state-of-the-art WSOD methods. Our code is publicly available at https://github.com/gyl2565309278/DTH-CP.
<div id='section'>Paperid: <span id='pid'>27, <a href='https://arxiv.org/pdf/2509.07507.pdf' target='_blank'>https://arxiv.org/pdf/2509.07507.pdf</a></span>   <span><a href='https://github.com/CEA-LIST/MVAT' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/CEA-LIST/MVAT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Saad Lahlali, Alexandre Fournier Montgieux, Nicolas Granger, HervÃ© Le Borgne, Quoc Cuong Pham
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07507">MVAT: Multi-View Aware Teacher for Weakly Supervised 3D Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Annotating 3D data remains a costly bottleneck for 3D object detection, motivating the development of weakly supervised annotation methods that rely on more accessible 2D box annotations. However, relying solely on 2D boxes introduces projection ambiguities since a single 2D box can correspond to multiple valid 3D poses. Furthermore, partial object visibility under a single viewpoint setting makes accurate 3D box estimation difficult. We propose MVAT, a novel framework that leverages temporal multi-view present in sequential data to address these challenges. Our approach aggregates object-centric point clouds across time to build 3D object representations as dense and complete as possible. A Teacher-Student distillation paradigm is employed: The Teacher network learns from single viewpoints but targets are derived from temporally aggregated static objects. Then the Teacher generates high quality pseudo-labels that the Student learns to predict from a single viewpoint for both static and moving objects. The whole framework incorporates a multi-view 2D projection loss to enforce consistency between predicted 3D boxes and all available 2D annotations. Experiments on the nuScenes and Waymo Open datasets demonstrate that MVAT achieves state-of-the-art performance for weakly supervised 3D object detection, significantly narrowing the gap with fully supervised methods without requiring any 3D box annotations. % \footnote{Code available upon acceptance} Our code is available in our public repository (\href{https://github.com/CEA-LIST/MVAT}{code}).
<div id='section'>Paperid: <span id='pid'>28, <a href='https://arxiv.org/pdf/2508.19060.pdf' target='_blank'>https://arxiv.org/pdf/2508.19060.pdf</a></span>   <span><a href='https://github.com/blaz-r/SuperSimpleNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>BlaÅ¾ Rolih, Matic FuÄka, Danijel SkoÄaj
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19060">No Label Left Behind: A Unified Surface Defect Detection Model for all Supervision Regimes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Surface defect detection is a critical task across numerous industries, aimed at efficiently identifying and localising imperfections or irregularities on manufactured components. While numerous methods have been proposed, many fail to meet industrial demands for high performance, efficiency, and adaptability. Existing approaches are often constrained to specific supervision scenarios and struggle to adapt to the diverse data annotations encountered in real-world manufacturing processes, such as unsupervised, weakly supervised, mixed supervision, and fully supervised settings. To address these challenges, we propose SuperSimpleNet, a highly efficient and adaptable discriminative model built on the foundation of SimpleNet. SuperSimpleNet incorporates a novel synthetic anomaly generation process, an enhanced classification head, and an improved learning procedure, enabling efficient training in all four supervision scenarios, making it the first model capable of fully leveraging all available data annotations. SuperSimpleNet sets a new standard for performance across all scenarios, as demonstrated by its results on four challenging benchmark datasets. Beyond accuracy, it is very fast, achieving an inference time below 10 ms. With its ability to unify diverse supervision paradigms while maintaining outstanding speed and reliability, SuperSimpleNet represents a promising step forward in addressing real-world manufacturing challenges and bridging the gap between academic research and industrial applications. Code: https://github.com/blaz-r/SuperSimpleNet
<div id='section'>Paperid: <span id='pid'>29, <a href='https://arxiv.org/pdf/2508.17186.pdf' target='_blank'>https://arxiv.org/pdf/2508.17186.pdf</a></span>   <span><a href='https://github.com/zhenghuizhao/AdvCP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenghui Zhao, Chen Wu, Di Wang, Hongruixuan Chen, Cuiqun Chen, Zhuo Zheng, Bo Du, Liangpei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17186">Advancing Weakly-Supervised Change Detection in Satellite Images via Adversarial Class Prompting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-Supervised Change Detection (WSCD) aims to distinguish specific object changes (e.g., objects appearing or disappearing) from background variations (e.g., environmental changes due to light, weather, or seasonal shifts) in paired satellite images, relying only on paired image (i.e., image-level) classification labels. This technique significantly reduces the need for dense annotations required in fully-supervised change detection. However, as image-level supervision only indicates whether objects have changed in a scene, WSCD methods often misclassify background variations as object changes, especially in complex remote-sensing scenarios. In this work, we propose an Adversarial Class Prompting (AdvCP) method to address this co-occurring noise problem, including two phases: a) Adversarial Prompt Mining: After each training iteration, we introduce adversarial prompting perturbations, using incorrect one-hot image-level labels to activate erroneous feature mappings. This process reveals co-occurring adversarial samples under weak supervision, namely background variation features that are likely to be misclassified as object changes. b) Adversarial Sample Rectification: We integrate these adversarially prompt-activated pixel samples into training by constructing an online global prototype. This prototype is built from an exponentially weighted moving average of the current batch and all historical training data. Our AdvCP can be seamlessly integrated into current WSCD methods without adding additional inference cost. Experiments on ConvNet, Transformer, and Segment Anything Model (SAM)-based baselines demonstrate significant performance enhancements. Furthermore, we demonstrate the generalizability of AdvCP to other multi-class weakly-supervised dense prediction scenarios. Code is available at https://github.com/zhenghuizhao/AdvCP
<div id='section'>Paperid: <span id='pid'>30, <a href='https://arxiv.org/pdf/2508.16159.pdf' target='_blank'>https://arxiv.org/pdf/2508.16159.pdf</a></span>   <span><a href='https://github.com/jarch-ma/TLG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Ma, Guo-Sen Xie, Fang Zhao, Zechao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16159">Through the Looking Glass: A Dual Perspective on Weakly-Supervised Few-Shot Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Meta-learning aims to uniformly sample homogeneous support-query pairs, characterized by the same categories and similar attributes, and extract useful inductive biases through identical network architectures. However, this identical network design results in over-semantic homogenization. To address this, we propose a novel homologous but heterogeneous network. By treating support-query pairs as dual perspectives, we introduce heterogeneous visual aggregation (HA) modules to enhance complementarity while preserving semantic commonality. To further reduce semantic noise and amplify the uniqueness of heterogeneous semantics, we design a heterogeneous transfer (HT) module. Finally, we propose heterogeneous CLIP (HC) textual information to enhance the generalization capability of multimodal models. In the weakly-supervised few-shot semantic segmentation (WFSS) task, with only 1/24 of the parameters of existing state-of-the-art models, TLG achieves a 13.2\% improvement on Pascal-5\textsuperscript{i} and a 9.7\% improvement on COCO-20\textsuperscript{i}. To the best of our knowledge, TLG is also the first weakly supervised (image-level) model that outperforms fully supervised (pixel-level) models under the same backbone architectures. The code is available at https://github.com/jarch-ma/TLG.
<div id='section'>Paperid: <span id='pid'>31, <a href='https://arxiv.org/pdf/2508.12023.pdf' target='_blank'>https://arxiv.org/pdf/2508.12023.pdf</a></span>   <span><a href='https://github.com/SFI-Visual-Intelligence/wiselvam.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Durgesh Kumar Singh, Qing Cao, Sarina Thomas, AhcÃ¨ne Boubekki, Robert Jenssen, Michael Kampffmeyer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12023">WiseLVAM: A Novel Framework For Left Ventricle Automatic Measurements</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Clinical guidelines recommend performing left ventricular (LV) linear measurements in B-mode echocardiographic images at the basal level -- typically at the mitral valve leaflet tips -- and aligned perpendicular to the LV long axis along a virtual scanline (SL). However, most automated methods estimate landmarks directly from B-mode images for the measurement task, where even small shifts in predicted points along the LV walls can lead to significant measurement errors, reducing their clinical reliability. A recent semi-automatic method, EnLVAM, addresses this limitation by constraining landmark prediction to a clinician-defined SL and training on generated Anatomical Motion Mode (AMM) images to predict LV landmarks along the same. To enable full automation, a contour-aware SL placement approach is proposed in this work, in which the LV contour is estimated using a weakly supervised B-mode landmark detector. SL placement is then performed by inferring the LV long axis and the basal level- mimicking clinical guidelines. Building on this foundation, we introduce \textit{WiseLVAM} -- a novel, fully automated yet manually adaptable framework for automatically placing the SL and then automatically performing the LV linear measurements in the AMM mode. \textit{WiseLVAM} utilizes the structure-awareness from B-mode images and the motion-awareness from AMM mode to enhance robustness and accuracy with the potential to provide a practical solution for the routine clinical application. The source code is publicly available at https://github.com/SFI-Visual-Intelligence/wiselvam.git.
<div id='section'>Paperid: <span id='pid'>32, <a href='https://arxiv.org/pdf/2508.10256.pdf' target='_blank'>https://arxiv.org/pdf/2508.10256.pdf</a></span>   <span><a href='https://github.com/nantonzhang/Awesome-Crack-Detection' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinan Zhang, Haolin Wang, Yung-An Hsieh, Zhongyu Yang, Anthony Yezzi, Yi-Chang Tsai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10256">Deep Learning for Crack Detection: A Review of Learning Paradigms, Generalizability, and Datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Crack detection plays a crucial role in civil infrastructures, including inspection of pavements, buildings, etc., and deep learning has significantly advanced this field in recent years. While numerous technical and review papers exist in this domain, emerging trends are reshaping the landscape. These shifts include transitions in learning paradigms (from fully supervised learning to semi-supervised, weakly-supervised, unsupervised, few-shot, domain adaptation and fine-tuning foundation models), improvements in generalizability (from single-dataset performance to cross-dataset evaluation), and diversification in dataset acquisition (from RGB images to specialized sensor-based data). In this review, we systematically analyze these trends and highlight representative works. Additionally, we introduce a new annotated dataset collected with 3D laser scans, 3DCrack, to support future research and conduct extensive benchmarking experiments to establish baselines for commonly used deep learning methodologies, including recent foundation models. Our findings provide insights into the evolving methodologies and future directions in deep learning-based crack detection. Project page: https://github.com/nantonzhang/Awesome-Crack-Detection
<div id='section'>Paperid: <span id='pid'>33, <a href='https://arxiv.org/pdf/2508.07298.pdf' target='_blank'>https://arxiv.org/pdf/2508.07298.pdf</a></span>   <span><a href='https://github.com/Senyh/SynMatch' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiqiang Shen, Peng Cao, Xiaoli Liu, Jinzhu Yang, Osmar R. Zaiane
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07298">SynMatch: Rethinking Consistency in Medical Image Segmentation with Sparse Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Label scarcity remains a major challenge in deep learning-based medical image segmentation. Recent studies use strong-weak pseudo supervision to leverage unlabeled data. However, performance is often hindered by inconsistencies between pseudo labels and their corresponding unlabeled images. In this work, we propose \textbf{SynMatch}, a novel framework that sidesteps the need for improving pseudo labels by synthesizing images to match them instead. Specifically, SynMatch synthesizes images using texture and shape features extracted from the same segmentation model that generates the corresponding pseudo labels for unlabeled images. This design enables the generation of highly consistent synthesized-image-pseudo-label pairs without requiring any training parameters for image synthesis. We extensively evaluate SynMatch across diverse medical image segmentation tasks under semi-supervised learning (SSL), weakly-supervised learning (WSL), and barely-supervised learning (BSL) settings with increasingly limited annotations. The results demonstrate that SynMatch achieves superior performance, especially in the most challenging BSL setting. For example, it outperforms the recent strong-weak pseudo supervision-based method by 29.71\% and 10.05\% on the polyp segmentation task with 5\% and 10\% scribble annotations, respectively. The code will be released at https://github.com/Senyh/SynMatch.
<div id='section'>Paperid: <span id='pid'>34, <a href='https://arxiv.org/pdf/2508.06485.pdf' target='_blank'>https://arxiv.org/pdf/2508.06485.pdf</a></span>   <span><a href='https://github.com/Sofianebouaziz1/WGAST.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sofiane Bouaziz, Adel Hafiane, Raphael Canals, Rachid Nedjai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06485">WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Urbanization, climate change, and agricultural stress are increasing the demand for precise and timely environmental monitoring. Land Surface Temperature (LST) is a key variable in this context and is retrieved from remote sensing satellites. However, these systems face a trade-off between spatial and temporal resolution. While spatio-temporal fusion methods offer promising solutions, few have addressed the estimation of daily LST at 10 m resolution. In this study, we present WGAST, a Weakly-Supervised Generative Network for Daily 10 m LST Estimation via Spatio-Temporal Fusion of Terra MODIS, Landsat 8, and Sentinel-2. WGAST is the first end-to-end deep learning framework designed for this task. It adopts a conditional generative adversarial architecture, with a generator composed of four stages: feature extraction, fusion, LST reconstruction, and noise suppression. The first stage employs a set of encoders to extract multi-level latent representations from the inputs, which are then fused in the second stage using cosine similarity, normalization, and temporal attention mechanisms. The third stage decodes the fused features into high-resolution LST, followed by a Gaussian filter to suppress high-frequency noise. Training follows a weakly supervised strategy based on physical averaging principles and reinforced by a PatchGAN discriminator. Experiments demonstrate that WGAST outperforms existing methods in both quantitative and qualitative evaluations. Compared to the best-performing baseline, on average, WGAST reduces RMSE by 17.18% and improves SSIM by 11.00%. Furthermore, WGAST is robust to cloud-induced LST and effectively captures fine-scale thermal patterns, as validated against 33 ground-based sensors. The code is available at https://github.com/Sofianebouaziz1/WGAST.git.
<div id='section'>Paperid: <span id='pid'>35, <a href='https://arxiv.org/pdf/2508.04943.pdf' target='_blank'>https://arxiv.org/pdf/2508.04943.pdf</a></span>   <span><a href='https://github.com/XZPKU/TRKT.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhu Xu, Ting Lei, Zhimin Li, Guan Wang, Qingchao Chen, Yuxin Peng, Yang liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04943">TRKT: Weakly Supervised Dynamic Scene Graph Generation with Temporal-enhanced Relation-aware Knowledge Transferring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic Scene Graph Generation (DSGG) aims to create a scene graph for each video frame by detecting objects and predicting their relationships. Weakly Supervised DSGG (WS-DSGG) reduces annotation workload by using an unlocalized scene graph from a single frame per video for training. Existing WS-DSGG methods depend on an off-the-shelf external object detector to generate pseudo labels for subsequent DSGG training. However, detectors trained on static, object-centric images struggle in dynamic, relation-aware scenarios required for DSGG, leading to inaccurate localization and low-confidence proposals. To address the challenges posed by external object detectors in WS-DSGG, we propose a Temporal-enhanced Relation-aware Knowledge Transferring (TRKT) method, which leverages knowledge to enhance detection in relation-aware dynamic scenarios. TRKT is built on two key components:(1)Relation-aware knowledge mining: we first employ object and relation class decoders that generate category-specific attention maps to highlight both object regions and interactive areas. Then we propose an Inter-frame Attention Augmentation strategy that exploits optical flow for neighboring frames to enhance the attention maps, making them motion-aware and robust to motion blur. This step yields relation- and motion-aware knowledge mining for WS-DSGG. (2) we introduce a Dual-stream Fusion Module that integrates category-specific attention maps into external detections to refine object localization and boost confidence scores for object proposals. Extensive experiments demonstrate that TRKT achieves state-of-the-art performance on Action Genome dataset. Our code is avaliable at https://github.com/XZPKU/TRKT.git.
<div id='section'>Paperid: <span id='pid'>36, <a href='https://arxiv.org/pdf/2508.03201.pdf' target='_blank'>https://arxiv.org/pdf/2508.03201.pdf</a></span>   <span><a href='https://github.com/I2-Multimedia-Lab/AlignCAT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yidan Wang, Chenyi Zhuang, Wutao Liu, Pan Gao, Nicu Sebe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03201">AlignCAT: Visual-Linguistic Alignment of Category and Attributefor Weakly Supervised Visual Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised visual grounding (VG) aims to locate objects in images based on text descriptions. Despite significant progress, existing methods lack strong cross-modal reasoning to distinguish subtle semantic differences in text expressions due to category-based and attribute-based ambiguity. To address these challenges, we introduce AlignCAT, a novel query-based semantic matching framework for weakly supervised VG. To enhance visual-linguistic alignment, we propose a coarse-grained alignment module that utilizes category information and global context, effectively mitigating interference from category-inconsistent objects. Subsequently, a fine-grained alignment module leverages descriptive information and captures word-level text features to achieve attribute consistency. By exploiting linguistic cues to their fullest extent, our proposed AlignCAT progressively filters out misaligned visual queries and enhances contrastive learning efficiency. Extensive experiments on three VG benchmarks, namely RefCOCO, RefCOCO+, and RefCOCOg, verify the superiority of AlignCAT against existing weakly supervised methods on two VG tasks. Our code is available at: https://github.com/I2-Multimedia-Lab/AlignCAT.
<div id='section'>Paperid: <span id='pid'>37, <a href='https://arxiv.org/pdf/2508.01310.pdf' target='_blank'>https://arxiv.org/pdf/2508.01310.pdf</a></span>   <span><a href='https://github.com/riskaudit/GraphVSSM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Joshua Dimasaka, Christian GeiÃ, Emily So
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01310">GraphVSSM: Graph Variational State-Space Model for Probabilistic Spatiotemporal Inference of Dynamic Exposure and Vulnerability for Regional Disaster Resilience Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Regional disaster resilience quantifies the changing nature of physical risks to inform policy instruments ranging from local immediate recovery to international sustainable development. While many existing state-of-practice methods have greatly advanced the dynamic mapping of exposure and hazard, our understanding of large-scale physical vulnerability has remained static, costly, limited, region-specific, coarse-grained, overly aggregated, and inadequately calibrated. With the significant growth in the availability of time-series satellite imagery and derived products for exposure and hazard, we focus our work on the equally important yet challenging element of the risk equation: physical vulnerability. We leverage machine learning methods that flexibly capture spatial contextual relationships, limited temporal observations, and uncertainty in a unified probabilistic spatiotemporal inference framework. We therefore introduce Graph Variational State-Space Model (GraphVSSM), a novel modular spatiotemporal approach that uniquely integrates graph deep learning, state-space modeling, and variational inference using time-series data and prior expert belief systems in a weakly supervised or coarse-to-fine-grained manner. We present three major results: a city-wide demonstration in Quezon City, Philippines; an investigation of sudden changes in the cyclone-impacted coastal Khurushkul community (Bangladesh) and mudslide-affected Freetown (Sierra Leone); and an open geospatial dataset, METEOR 2.5D, that spatiotemporally enhances the existing global static dataset for UN Least Developed Countries (2020). Beyond advancing regional disaster resilience assessment and improving our understanding global disaster risk reduction progress, our method also offers a probabilistic deep learning approach, contributing to broader urban studies that require compositional data analysis in weak supervision.
<div id='section'>Paperid: <span id='pid'>38, <a href='https://arxiv.org/pdf/2508.01250.pdf' target='_blank'>https://arxiv.org/pdf/2508.01250.pdf</a></span>   <span><a href='https://github.com/CVI-SZU/DisFaceRep' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/CVI-SZU/DisFaceRep' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoqin Wang, Xianxu Hou, Meidan Ding, Junliang Chen, Kaijun Deng, Jinheng Xie, Linlin Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01250">DisFaceRep: Representation Disentanglement for Co-occurring Facial Components in Weakly Supervised Face Parsing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face parsing aims to segment facial images into key components such as eyes, lips, and eyebrows. While existing methods rely on dense pixel-level annotations, such annotations are expensive and labor-intensive to obtain. To reduce annotation cost, we introduce Weakly Supervised Face Parsing (WSFP), a new task setting that performs dense facial component segmentation using only weak supervision, such as image-level labels and natural language descriptions. WSFP introduces unique challenges due to the high co-occurrence and visual similarity of facial components, which lead to ambiguous activations and degraded parsing performance. To address this, we propose DisFaceRep, a representation disentanglement framework designed to separate co-occurring facial components through both explicit and implicit mechanisms. Specifically, we introduce a co-occurring component disentanglement strategy to explicitly reduce dataset-level bias, and a text-guided component disentanglement loss to guide component separation using language supervision implicitly. Extensive experiments on CelebAMask-HQ, LaPa, and Helen demonstrate the difficulty of WSFP and the effectiveness of DisFaceRep, which significantly outperforms existing weakly supervised semantic segmentation methods. The code will be released at \href{https://github.com/CVI-SZU/DisFaceRep}{\textcolor{cyan}{https://github.com/CVI-SZU/DisFaceRep}}.
<div id='section'>Paperid: <span id='pid'>39, <a href='https://arxiv.org/pdf/2508.00312.pdf' target='_blank'>https://arxiv.org/pdf/2508.00312.pdf</a></span>   <span><a href='https://github.com/Sumutan/GV-VAD.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Suhang Cai, Xiaohao Peng, Chong Wang, Xiaojie Cai, Jiangbo Qian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00312">GV-VAD : Exploring Video Generation for Weakly-Supervised Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video anomaly detection (VAD) plays a critical role in public safety applications such as intelligent surveillance. However, the rarity, unpredictability, and high annotation cost of real-world anomalies make it difficult to scale VAD datasets, which limits the performance and generalization ability of existing models. To address this challenge, we propose a generative video-enhanced weakly-supervised video anomaly detection (GV-VAD) framework that leverages text-conditioned video generation models to produce semantically controllable and physically plausible synthetic videos. These virtual videos are used to augment training data at low cost. In addition, a synthetic sample loss scaling strategy is utilized to control the influence of generated synthetic samples for efficient training. The experiments show that the proposed framework outperforms state-of-the-art methods on UCF-Crime datasets. The code is available at https://github.com/Sumutan/GV-VAD.git.
<div id='section'>Paperid: <span id='pid'>40, <a href='https://arxiv.org/pdf/2507.20976.pdf' target='_blank'>https://arxiv.org/pdf/2507.20976.pdf</a></span>   <span><a href='https://humansensinglab.github.io/AGenDA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Fang, Minhyek Jeon, Zheyang Qin, Stanislav Panev, Celso de Melo, Shuowen Hu, Shayok Chakraborty, Fernando De la Torre
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20976">Adapting Vehicle Detectors for Aerial Imagery to Unseen Domains with Weak Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detecting vehicles in aerial imagery is a critical task with applications in traffic monitoring, urban planning, and defense intelligence. Deep learning methods have provided state-of-the-art (SOTA) results for this application. However, a significant challenge arises when models trained on data from one geographic region fail to generalize effectively to other areas. Variability in factors such as environmental conditions, urban layouts, road networks, vehicle types, and image acquisition parameters (e.g., resolution, lighting, and angle) leads to domain shifts that degrade model performance. This paper proposes a novel method that uses generative AI to synthesize high-quality aerial images and their labels, improving detector training through data augmentation. Our key contribution is the development of a multi-stage, multi-modal knowledge transfer framework utilizing fine-tuned latent diffusion models (LDMs) to mitigate the distribution gap between the source and target environments. Extensive experiments across diverse aerial imagery domains show consistent performance improvements in AP50 over supervised learning on source domain data, weakly supervised adaptation methods, unsupervised domain adaptation methods, and open-set object detectors by 4-23%, 6-10%, 7-40%, and more than 50%, respectively. Furthermore, we introduce two newly annotated aerial datasets from New Zealand and Utah to support further research in this field. Project page is available at: https://humansensinglab.github.io/AGenDA
<div id='section'>Paperid: <span id='pid'>41, <a href='https://arxiv.org/pdf/2507.18677.pdf' target='_blank'>https://arxiv.org/pdf/2507.18677.pdf</a></span>   <span><a href='https://github.com/SiyuMU/Loaded2UnNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyu Mu, Wei Xuan Chan, Choon Hwai Yap
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18677">HeartUnloadNet: A Weakly-Supervised Cycle-Consistent Graph Network for Predicting Unloaded Cardiac Geometry from Diastolic States</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The unloaded cardiac geometry (i.e., the state of the heart devoid of luminal pressure) serves as a valuable zero-stress and zero-strain reference and is critical for personalized biomechanical modeling of cardiac function, to understand both healthy and diseased physiology and to predict the effects of cardiac interventions. However, estimating the unloaded geometry from clinical images remains a challenging task. Traditional approaches rely on inverse finite element (FE) solvers that require iterative optimization and are computationally expensive. In this work, we introduce HeartUnloadNet, a deep learning framework that predicts the unloaded left ventricular (LV) shape directly from the end diastolic (ED) mesh while explicitly incorporating biophysical priors. The network accepts a mesh of arbitrary size along with physiological parameters such as ED pressure, myocardial stiffness scale, and fiber helix orientation, and outputs the corresponding unloaded mesh. It adopts a graph attention architecture and employs a cycle-consistency strategy to enable bidirectional (loading and unloading) prediction, allowing for partial self-supervision that improves accuracy and reduces the need for large training datasets. Trained and tested on 20,700 FE simulations across diverse LV geometries and physiological conditions, HeartUnloadNet achieves sub-millimeter accuracy, with an average DSC of 0.986 and HD of 0.083 cm, while reducing inference time to just 0.02 seconds per case, over 10^5 times faster and significantly more accurate than traditional inverse FE solvers. Ablation studies confirm the effectiveness of the architecture. Notably, the cycle-consistent design enables the model to maintain a DSC of 97% even with as few as 200 training samples. This work thus presents a scalable and accurate surrogate for inverse FE solvers, supporting real-time clinical applications in the future.
<div id='section'>Paperid: <span id='pid'>42, <a href='https://arxiv.org/pdf/2507.10935.pdf' target='_blank'>https://arxiv.org/pdf/2507.10935.pdf</a></span>   <span><a href='https://github.com/tongshw/GeoDistill' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaowen Tong, Zimin Xia, Alexandre Alahi, Xuming He, Yujiao Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10935">GeoDistill: Geometry-Guided Self-Distillation for Weakly Supervised Cross-View Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-view localization, the task of estimating a camera's 3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with satellite images, is crucial for large-scale outdoor applications like autonomous navigation and augmented reality. Existing methods often rely on fully supervised learning, which requires costly ground-truth pose annotations. In this work, we propose GeoDistill, a Geometry guided weakly supervised self distillation framework that uses teacher-student learning with Field-of-View (FoV)-based masking to enhance local feature learning for robust cross-view localization. In GeoDistill, the teacher model localizes a panoramic image, while the student model predicts locations from a limited FoV counterpart created by FoV-based masking. By aligning the student's predictions with those of the teacher, the student focuses on key features like lane lines and ignores textureless regions, such as roads. This results in more accurate predictions and reduced uncertainty, regardless of whether the query images are panoramas or limited FoV images. Our experiments show that GeoDistill significantly improves localization performance across different frameworks. Additionally, we introduce a novel orientation estimation network that predicts relative orientation without requiring precise planar position ground truth. GeoDistill provides a scalable and efficient solution for real-world cross-view localization challenges. Code and model can be found at https://github.com/tongshw/GeoDistill.
<div id='section'>Paperid: <span id='pid'>43, <a href='https://arxiv.org/pdf/2507.07578.pdf' target='_blank'>https://arxiv.org/pdf/2507.07578.pdf</a></span>   <span><a href='https://github.com/ChunyanWang1/DGKD-WLSS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunyan Wang, Dong Zhang, Jinhui Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07578">Diffusion-Guided Knowledge Distillation for Weakly-Supervised Low-Light Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised semantic segmentation aims to assign category labels to each pixel using weak annotations, significantly reducing manual annotation costs. Although existing methods have achieved remarkable progress in well-lit scenarios, their performance significantly degrades in low-light environments due to two fundamental limitations: severe image quality degradation (e.g., low contrast, noise, and color distortion) and the inherent constraints of weak supervision. These factors collectively lead to unreliable class activation maps and semantically ambiguous pseudo-labels, ultimately compromising the model's ability to learn discriminative feature representations. To address these problems, we propose Diffusion-Guided Knowledge Distillation for Weakly-Supervised Low-light Semantic Segmentation (DGKD-WLSS), a novel framework that synergistically combines Diffusion-Guided Knowledge Distillation (DGKD) with Depth-Guided Feature Fusion (DGF2). DGKD aligns normal-light and low-light features via diffusion-based denoising and knowledge distillation, while DGF2 integrates depth maps as illumination-invariant geometric priors to enhance structural feature learning. Extensive experiments demonstrate the effectiveness of DGKD-WLSS, which achieves state-of-the-art performance in weakly supervised semantic segmentation tasks under low-light conditions. The source codes have been released at:https://github.com/ChunyanWang1/DGKD-WLSS.
<div id='section'>Paperid: <span id='pid'>44, <a href='https://arxiv.org/pdf/2507.04839.pdf' target='_blank'>https://arxiv.org/pdf/2507.04839.pdf</a></span>   <span><a href='https://github.com/fraunhoferhhi/RIPE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Johannes KÃ¼nzel, Anna Hilsmann, Peter Eisert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04839">RIPE: Reinforcement Learning on Unlabeled Image Pairs for Robust Keypoint Extraction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce RIPE, an innovative reinforcement learning-based framework for weakly-supervised training of a keypoint extractor that excels in both detection and description tasks. In contrast to conventional training regimes that depend heavily on artificial transformations, pre-generated models, or 3D data, RIPE requires only a binary label indicating whether paired images represent the same scene. This minimal supervision significantly expands the pool of training data, enabling the creation of a highly generalized and robust keypoint extractor.
  RIPE utilizes the encoder's intermediate layers for the description of the keypoints with a hyper-column approach to integrate information from different scales. Additionally, we propose an auxiliary loss to enhance the discriminative capability of the learned descriptors.
  Comprehensive evaluations on standard benchmarks demonstrate that RIPE simplifies data preparation while achieving competitive performance compared to state-of-the-art techniques, marking a significant advancement in robust keypoint extraction and description. To support further research, we have made our code publicly available at https://github.com/fraunhoferhhi/RIPE.
<div id='section'>Paperid: <span id='pid'>45, <a href='https://arxiv.org/pdf/2507.02751.pdf' target='_blank'>https://arxiv.org/pdf/2507.02751.pdf</a></span>   <span><a href='https://github.com/VisionXLab/PWOOD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingxin Liu, Peiyuan Zhang, Yuan Liu, Wei Zhang, Yue Zhou, Ning Liao, Ziyang Gong, Junwei Luo, Zhirui Wang, Yi Yu, Xue Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02751">Partial Weakly-Supervised Oriented Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing demand for oriented object detection (OOD) across various domains has driven significant research in this area. However, the high cost of dataset annotation remains a major concern. Current mainstream OOD algorithms can be mainly categorized into three types: (1) fully supervised methods using complete oriented bounding box (OBB) annotations, (2) semi-supervised methods using partial OBB annotations, and (3) weakly supervised methods using weak annotations such as horizontal boxes or points. However, these algorithms inevitably increase the cost of models in terms of annotation speed or annotation cost. To address this issue, we propose:(1) the first Partial Weakly-Supervised Oriented Object Detection (PWOOD) framework based on partially weak annotations (horizontal boxes or single points), which can efficiently leverage large amounts of unlabeled data, significantly outperforming weakly supervised algorithms trained with partially weak annotations, also offers a lower cost solution; (2) Orientation-and-Scale-aware Student (OS-Student) model capable of learning orientation and scale information with only a small amount of orientation-agnostic or scale-agnostic weak annotations; and (3) Class-Agnostic Pseudo-Label Filtering strategy (CPF) to reduce the model's sensitivity to static filtering thresholds. Comprehensive experiments on DOTA-v1.0/v1.5/v2.0 and DIOR datasets demonstrate that our PWOOD framework performs comparably to, or even surpasses, traditional semi-supervised algorithms.
<div id='section'>Paperid: <span id='pid'>46, <a href='https://arxiv.org/pdf/2507.02743.pdf' target='_blank'>https://arxiv.org/pdf/2507.02743.pdf</a></span>   <span><a href='https://github.com/Minimel/box-prompt-learning-VFM.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>MÃ©lanie Gaillochet, Mehrdad Noori, Sahar Dastani, Christian Desrosiers, HervÃ© Lombaert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02743">Prompt learning with bounding box constraints for medical image segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pixel-wise annotations are notoriously labourious and costly to obtain in the medical domain. To mitigate this burden, weakly supervised approaches based on bounding box annotations-much easier to acquire-offer a practical alternative. Vision foundation models have recently shown noteworthy segmentation performance when provided with prompts such as points or bounding boxes. Prompt learning exploits these models by adapting them to downstream tasks and automating segmentation, thereby reducing user intervention. However, existing prompt learning approaches depend on fully annotated segmentation masks. This paper proposes a novel framework that combines the representational power of foundation models with the annotation efficiency of weakly supervised segmentation. More specifically, our approach automates prompt generation for foundation models using only bounding box annotations. Our proposed optimization scheme integrates multiple constraints derived from box annotations with pseudo-labels generated by the prompted foundation model. Extensive experiments across multimodal datasets reveal that our weakly supervised method achieves an average Dice score of 84.90% in a limited data setting, outperforming existing fully-supervised and weakly-supervised approaches. The code is available at https://github.com/Minimel/box-prompt-learning-VFM.git
<div id='section'>Paperid: <span id='pid'>47, <a href='https://arxiv.org/pdf/2507.01384.pdf' target='_blank'>https://arxiv.org/pdf/2507.01384.pdf</a></span>   <span><a href='https://github.com/WangLY136/MUG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Langyu Wang, Bingke Zhu, Yingying Chen, Yiyuan Zhang, Ming Tang, Jinqiao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01384">MUG: Pseudo Labeling Augmented Audio-Visual Mamba Network for Audio-Visual Video Parsing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The weakly-supervised audio-visual video parsing (AVVP) aims to predict all modality-specific events and locate their temporal boundaries. Despite significant progress, due to the limitations of the weakly-supervised and the deficiencies of the model architecture, existing methods are lacking in simultaneously improving both the segment-level prediction and the event-level prediction. In this work, we propose a audio-visual Mamba network with pseudo labeling aUGmentation (MUG) for emphasising the uniqueness of each segment and excluding the noise interference from the alternate modalities. Specifically, we annotate some of the pseudo-labels based on previous work. Using unimodal pseudo-labels, we perform cross-modal random combinations to generate new data, which can enhance the model's ability to parse various segment-level event combinations. For feature processing and interaction, we employ a audio-visual mamba network. The AV-Mamba enhances the ability to perceive different segments and excludes additional modal noise while sharing similar modal information. Our extensive experiments demonstrate that MUG improves state-of-the-art results on LLP dataset in all metrics (e.g,, gains of 2.1% and 1.2% in terms of visual Segment-level and audio Segment-level metrics). Our code is available at https://github.com/WangLY136/MUG.
<div id='section'>Paperid: <span id='pid'>48, <a href='https://arxiv.org/pdf/2506.23648.pdf' target='_blank'>https://arxiv.org/pdf/2506.23648.pdf</a></span>   <span><a href='https://github.com/cskdstz/MReg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Liu, Yuhao Huang, Lian Liu, Chengrui Zhang, Haotian Lin, Tong Han, Zhiyuan Zhu, Yanlin Chen, Yuerui Chen, Dong Ni, Zhongshan Gou, Xin Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23648">MReg: A Novel Regression Model with MoE-based Video Feature Mining for Mitral Regurgitation Diagnosis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Color Doppler echocardiography is a crucial tool for diagnosing mitral regurgitation (MR). Recent studies have explored intelligent methods for MR diagnosis to minimize user dependence and improve accuracy. However, these approaches often fail to align with clinical workflow and may lead to suboptimal accuracy and interpretability. In this study, we introduce an automated MR diagnosis model (MReg) developed on the 4-chamber cardiac color Doppler echocardiography video (A4C-CDV). It follows comprehensive feature mining strategies to detect MR and assess its severity, considering clinical realities. Our contribution is threefold. First, we formulate the MR diagnosis as a regression task to capture the continuity and ordinal relationships between categories. Second, we design a feature selection and amplification mechanism to imitate the sonographer's diagnostic logic for accurate MR grading. Third, inspired by the Mixture-of-Experts concept, we introduce a feature summary module to extract the category-level features, enhancing the representational capacity for more accurate grading. We trained and evaluated our proposed MReg on a large in-house A4C-CDV dataset comprising 1868 cases with three graded regurgitation labels. Compared to other weakly supervised video anomaly detection and supervised classification methods, MReg demonstrated superior performance in MR diagnosis. Our code is available at: https://github.com/cskdstz/MReg.
<div id='section'>Paperid: <span id='pid'>49, <a href='https://arxiv.org/pdf/2506.22505.pdf' target='_blank'>https://arxiv.org/pdf/2506.22505.pdf</a></span>   <span><a href='GitHub' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/bakerhassan/WSOS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hassan Baker, Matthew S. Emigh, Austin J. Brockmeier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22505">Weakly Supervised Object Segmentation by Background Conditional Divergence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As a computer vision task, automatic object segmentation remains challenging in specialized image domains without massive labeled data, such as synthetic aperture sonar images, remote sensing, biomedical imaging, etc. In any domain, obtaining pixel-wise segmentation masks is expensive. In this work, we propose a method for training a masking network to perform binary object segmentation using weak supervision in the form of image-wise presence or absence of an object of interest, which provides less information but may be obtained more quickly from manual or automatic labeling. A key step in our method is that the segmented objects can be placed into background-only images to create realistic, images of the objects with counterfactual backgrounds. To create a contrast between the original and counterfactual background images, we propose to first cluster the background-only images, and then during learning create counterfactual images that blend objects segmented from their original source backgrounds to backgrounds chosen from a targeted cluster. One term in the training loss is the divergence between these counterfactual images and the real object images with backgrounds of the target cluster. The other term is a supervised loss for background-only images. While an adversarial critic could provide the divergence, we use sample-based divergences. We conduct experiments on side-scan and synthetic aperture sonar in which our approach succeeds compared to previous unsupervised segmentation baselines that were only tested on natural images. Furthermore, to show generality we extend our experiments to natural images, obtaining reasonable performance with our method that avoids pretrained networks, generative networks, and adversarial critics. The basecode for this work can be found at \href{GitHub}{https://github.com/bakerhassan/WSOS}.
<div id='section'>Paperid: <span id='pid'>50, <a href='https://arxiv.org/pdf/2506.09022.pdf' target='_blank'>https://arxiv.org/pdf/2506.09022.pdf</a></span>   <span><a href='https://github.com/mahmoodlab/MIL-Lab' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Shao, Richard J. Chen, Andrew H. Song, Joel Runevic, Ming Y. Lu, Tong Ding, Faisal Mahmood
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09022">Do Multiple Instance Learning Models Transfer?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple Instance Learning (MIL) is a cornerstone approach in computational pathology (CPath) for generating clinically meaningful slide-level embeddings from gigapixel tissue images. However, MIL often struggles with small, weakly supervised clinical datasets. In contrast to fields such as NLP and conventional computer vision, where transfer learning is widely used to address data scarcity, the transferability of MIL models remains poorly understood. In this study, we systematically evaluate the transfer learning capabilities of pretrained MIL models by assessing 11 models across 21 pretraining tasks for morphological and molecular subtype prediction. Our results show that pretrained MIL models, even when trained on different organs than the target task, consistently outperform models trained from scratch. Moreover, pretraining on pancancer datasets enables strong generalization across organs and tasks, outperforming slide foundation models while using substantially less pretraining data. These findings highlight the robust adaptability of MIL models and demonstrate the benefits of leveraging transfer learning to boost performance in CPath. Lastly, we provide a resource which standardizes the implementation of MIL models and collection of pretrained model weights on popular CPath tasks, available at https://github.com/mahmoodlab/MIL-Lab
<div id='section'>Paperid: <span id='pid'>51, <a href='https://arxiv.org/pdf/2506.06589.pdf' target='_blank'>https://arxiv.org/pdf/2506.06589.pdf</a></span>   <span><a href='https://github.com/jacqueline-he/precise-information-control' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jacqueline He, Howard Yen, Margaret Li, Shuyue Stella Li, Zhiyuan Zeng, Weijia Shi, Yulia Tsvetkov, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06589">Precise Information Control in Long-Form Text Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A central challenge in modern language models (LMs) is intrinsic hallucination: the generation of information that is plausible but unsubstantiated relative to input context. To study this problem, we propose Precise Information Control (PIC), a new task formulation that requires models to generate long-form outputs grounded in a provided set of short self-contained statements, known as verifiable claims, without adding any unsupported ones. For comprehensiveness, PIC includes a full setting that tests a model's ability to include exactly all input claims, and a partial setting that requires the model to selectively incorporate only relevant claims. We present PIC-Bench, a benchmark of eight long-form generation tasks (e.g., summarization, biography generation) adapted to the PIC setting, where LMs are supplied with well-formed, verifiable input claims. Our evaluation of a range of open and proprietary LMs on PIC-Bench reveals that, surprisingly, state-of-the-art LMs still intrinsically hallucinate in over 70% of outputs. To alleviate this lack of faithfulness, we introduce a post-training framework, using a weakly supervised preference data construction method, to train an 8B PIC-LM with stronger PIC ability--improving from 69.1% to 91.0% F1 in the full PIC setting. When integrated into end-to-end factual generation pipelines, PIC-LM improves exact match recall by 17.1% on ambiguous QA with retrieval, and factual precision by 30.5% on a birthplace verification task, underscoring the potential of precisely grounded generation.
<div id='section'>Paperid: <span id='pid'>52, <a href='https://arxiv.org/pdf/2506.05895.pdf' target='_blank'>https://arxiv.org/pdf/2506.05895.pdf</a></span>   <span><a href='https://github.com/adrienpetralia/CamAL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Adrien Petralia, Paul Boniol, Philippe Charpentier, Themis Palpanas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05895">Few Labels are all you need: A Weakly Supervised Framework for Appliance Localization in Smart-Meter Series</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Improving smart grid system management is crucial in the fight against climate change, and enabling consumers to play an active role in this effort is a significant challenge for electricity suppliers. In this regard, millions of smart meters have been deployed worldwide in the last decade, recording the main electricity power consumed in individual households. This data produces valuable information that can help them reduce their electricity footprint; nevertheless, the collected signal aggregates the consumption of the different appliances running simultaneously in the house, making it difficult to apprehend. Non-Intrusive Load Monitoring (NILM) refers to the challenge of estimating the power consumption, pattern, or on/off state activation of individual appliances using the main smart meter signal. Recent methods proposed to tackle this task are based on a fully supervised deep-learning approach that requires both the aggregate signal and the ground truth of individual appliance power. However, such labels are expensive to collect and extremely scarce in practice, as they require conducting intrusive surveys in households to monitor each appliance. In this paper, we introduce CamAL, a weakly supervised approach for appliance pattern localization that only requires information on the presence of an appliance in a household to be trained. CamAL merges an ensemble of deep-learning classifiers combined with an explainable classification method to be able to localize appliance patterns. Our experimental evaluation, conducted on 4 real-world datasets, demonstrates that CamAL significantly outperforms existing weakly supervised baselines and that current SotA fully supervised NILM approaches require significantly more labels to reach CamAL performances. The source of our experiments is available at: https://github.com/adrienpetralia/CamAL. This paper appeared in ICDE 2025.
<div id='section'>Paperid: <span id='pid'>53, <a href='https://arxiv.org/pdf/2505.24824.pdf' target='_blank'>https://arxiv.org/pdf/2505.24824.pdf</a></span>   <span><a href='https://github.com/Archiel19/FRAx4.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Marta LÃ³pez-Rauhut, Hongyu Zhou, Mathieu Aubry, Loic Landrieu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24824">Segmenting France Across Four Centuries</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Historical maps offer an invaluable perspective into territory evolution across past centuries--long before satellite or remote sensing technologies existed. Deep learning methods have shown promising results in segmenting historical maps, but publicly available datasets typically focus on a single map type or period, require extensive and costly annotations, and are not suited for nationwide, long-term analyses. In this paper, we introduce a new dataset of historical maps tailored for analyzing large-scale, long-term land use and land cover evolution with limited annotations. Spanning metropolitan France (548,305 km^2), our dataset contains three map collections from the 18th, 19th, and 20th centuries. We provide both comprehensive modern labels and 22,878 km^2 of manually annotated historical labels for the 18th and 19th century maps. Our dataset illustrates the complexity of the segmentation task, featuring stylistic inconsistencies, interpretive ambiguities, and significant landscape changes (e.g., marshlands disappearing in favor of forests). We assess the difficulty of these challenges by benchmarking three approaches: a fully-supervised model trained with historical labels, and two weakly-supervised models that rely only on modern annotations. The latter either use the modern labels directly or first perform image-to-image translation to address the stylistic gap between historical and contemporary maps. Finally, we discuss how these methods can support long-term environment monitoring, offering insights into centuries of landscape transformation. Our official project repository is publicly available at https://github.com/Archiel19/FRAx4.git.
<div id='section'>Paperid: <span id='pid'>54, <a href='https://arxiv.org/pdf/2505.24103.pdf' target='_blank'>https://arxiv.org/pdf/2505.24103.pdf</a></span>   <span><a href='https://github.com/woyut/WSAG-PLSP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Peiran Xu, Yadong Mu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24103">Weakly-Supervised Affordance Grounding Guided by Part-Level Semantic Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we focus on the task of weakly supervised affordance grounding, where a model is trained to identify affordance regions on objects using human-object interaction images and egocentric object images without dense labels. Previous works are mostly built upon class activation maps, which are effective for semantic segmentation but may not be suitable for locating actions and functions. Leveraging recent advanced foundation models, we develop a supervised training pipeline based on pseudo labels. The pseudo labels are generated from an off-the-shelf part segmentation model, guided by a mapping from affordance to part names. Furthermore, we introduce three key enhancements to the baseline model: a label refining stage, a fine-grained feature alignment process, and a lightweight reasoning module. These techniques harness the semantic knowledge of static objects embedded in off-the-shelf foundation models to improve affordance learning, effectively bridging the gap between objects and actions. Extensive experiments demonstrate that the performance of the proposed model has achieved a breakthrough improvement over existing methods. Our codes are available at https://github.com/woyut/WSAG-PLSP .
<div id='section'>Paperid: <span id='pid'>55, <a href='https://arxiv.org/pdf/2505.22028.pdf' target='_blank'>https://arxiv.org/pdf/2505.22028.pdf</a></span>   <span><a href='https://github.com/Speechless-10308/WSC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zi-Hao Zhou, Jun-Jie Wang, Tong Wei, Min-Ling Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22028">Weakly-Supervised Contrastive Learning for Imprecise Class Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contrastive learning has achieved remarkable success in learning effective representations, with supervised contrastive learning often outperforming self-supervised approaches. However, in real-world scenarios, data annotations are often ambiguous or inaccurate, meaning that class labels may not reliably indicate whether two examples belong to the same class. This limitation restricts the applicability of supervised contrastive learning. To address this challenge, we introduce the concept of ``continuous semantic similarity'' to define positive and negative pairs. Instead of directly relying on imprecise class labels, we measure the semantic similarity between example pairs, which quantifies how closely they belong to the same category by iteratively refining weak supervisory signals. Based on this concept, we propose a graph-theoretic framework for weakly-supervised contrastive learning, where semantic similarity serves as the graph weights. Our framework is highly versatile and can be applied to many weakly-supervised learning scenarios. We demonstrate its effectiveness through experiments in two common settings, i.e., noisy label and partial label learning, where existing methods can be easily integrated to significantly improve performance. Theoretically, we establish an error bound for our approach, showing that it can approximate supervised contrastive learning under mild conditions. The implementation code is available at https://github.com/Speechless-10308/WSC.
<div id='section'>Paperid: <span id='pid'>56, <a href='https://arxiv.org/pdf/2505.19190.pdf' target='_blank'>https://arxiv.org/pdf/2505.19190.pdf</a></span>   <span><a href='https://github.com/Raina-Xin/I2MoE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayi Xin, Sukwon Yun, Jie Peng, Inyoung Choi, Jenna L. Ballard, Tianlong Chen, Qi Long
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19190">I2MoE: Interpretable Multimodal Interaction-aware Mixture-of-Experts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modality fusion is a cornerstone of multimodal learning, enabling information integration from diverse data sources. However, vanilla fusion methods are limited by (1) inability to account for heterogeneous interactions between modalities and (2) lack of interpretability in uncovering the multimodal interactions inherent in the data. To this end, we propose I2MoE (Interpretable Multimodal Interaction-aware Mixture of Experts), an end-to-end MoE framework designed to enhance modality fusion by explicitly modeling diverse multimodal interactions, as well as providing interpretation on a local and global level. First, I2MoE utilizes different interaction experts with weakly supervised interaction losses to learn multimodal interactions in a data-driven way. Second, I2MoE deploys a reweighting model that assigns importance scores for the output of each interaction expert, which offers sample-level and dataset-level interpretation. Extensive evaluation of medical and general multimodal datasets shows that I2MoE is flexible enough to be combined with different fusion techniques, consistently improves task performance, and provides interpretation across various real-world scenarios. Code is available at https://github.com/Raina-Xin/I2MoE.
<div id='section'>Paperid: <span id='pid'>57, <a href='https://arxiv.org/pdf/2505.19022.pdf' target='_blank'>https://arxiv.org/pdf/2505.19022.pdf</a></span>   <span><a href='https://github.com/Kamino666/RethinkingVAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihao Liu, Xiaoyu Wu, Wenna Li, Linlin Yang, Shengjin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19022">Rethinking Metrics and Benchmarks of Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Anomaly Detection (VAD), which aims to detect anomalies that deviate from expectation, has attracted increasing attention in recent years. Existing advancements in VAD primarily focus on model architectures and training strategies, while devoting insufficient attention to evaluation metrics and benchmarks. In this paper, we rethink VAD evaluation methods through comprehensive analyses, revealing three critical limitations in current practices: 1) existing metrics are significantly influenced by single annotation bias; 2) current metrics fail to reward early detection of anomalies; 3) available benchmarks lack the capability to evaluate scene overfitting of fully/weakly-supervised algorithms. To address these limitations, we propose three novel evaluation methods: first, we establish probabilistic AUC/AP (Prob-AUC/AP) metrics utlizing multi-round annotations to mitigate single annotation bias; second, we develop a Latency-aware Average Precision (LaAP) metric that rewards early and accurate anomaly detection; and finally, we introduce two hard normal benchmarks (UCF-HN, MSAD-HN) with videos specifically designed to evaluate scene overfitting. We report performance comparisons of ten state-of-the-art VAD approaches using our proposed evaluation methods, providing novel perspectives for future VAD model development. We release our data and code in https://github.com/Kamino666/RethinkingVAD.
<div id='section'>Paperid: <span id='pid'>58, <a href='https://arxiv.org/pdf/2505.18686.pdf' target='_blank'>https://arxiv.org/pdf/2505.18686.pdf</a></span>   <span><a href='https://github.com/MRUIL/WeakMCN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Liu, Silin Cheng, Xinwei He, Sebastien Ourselin, Lei Tan, Gen Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18686">WeakMCN: Multi-task Collaborative Network for Weakly Supervised Referring Expression Comprehension and Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised referring expression comprehension(WREC) and segmentation(WRES) aim to learn object grounding based on a given expression using weak supervision signals like image-text pairs. While these tasks have traditionally been modeled separately, we argue that they can benefit from joint learning in a multi-task framework. To this end, we propose WeakMCN, a novel multi-task collaborative network that effectively combines WREC and WRES with a dual-branch architecture. Specifically, the WREC branch is formulated as anchor-based contrastive learning, which also acts as a teacher to supervise the WRES branch. In WeakMCN, we propose two innovative designs to facilitate multi-task collaboration, namely Dynamic Visual Feature Enhancement(DVFE) and Collaborative Consistency Module(CCM). DVFE dynamically combines various pre-trained visual knowledge to meet different task requirements, while CCM promotes cross-task consistency from the perspective of optimization. Extensive experimental results on three popular REC and RES benchmarks, i.e., RefCOCO, RefCOCO+, and RefCOCOg, consistently demonstrate performance gains of WeakMCN over state-of-the-art single-task alternatives, e.g., up to 3.91% and 13.11% on RefCOCO for WREC and WRES tasks, respectively. Furthermore, experiments also validate the strong generalization ability of WeakMCN in both semi-supervised REC and RES settings against existing methods, e.g., +8.94% for semi-REC and +7.71% for semi-RES on 1% RefCOCO. The code is publicly available at https://github.com/MRUIL/WeakMCN.
<div id='section'>Paperid: <span id='pid'>59, <a href='https://arxiv.org/pdf/2505.17982.pdf' target='_blank'>https://arxiv.org/pdf/2505.17982.pdf</a></span>   <span><a href='https://github.com/bryanwong17/HiVE-MIL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bryan Wong, Jong Woo Kim, Huazhu Fu, Mun Yong Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17982">Few-Shot Learning from Gigapixel Images via Hierarchical Vision-Language Alignment and Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language models (VLMs) have recently been integrated into multiple instance learning (MIL) frameworks to address the challenge of few-shot, weakly supervised classification of whole slide images (WSIs). A key trend involves leveraging multi-scale information to better represent hierarchical tissue structures. However, existing methods often face two key limitations: (1) insufficient modeling of interactions within the same modalities across scales (e.g., 5x and 20x) and (2) inadequate alignment between visual and textual modalities on the same scale. To address these gaps, we propose HiVE-MIL, a hierarchical vision-language framework that constructs a unified graph consisting of (1) parent-child links between coarse (5x) and fine (20x) visual/textual nodes to capture hierarchical relationships, and (2) heterogeneous intra-scale edges linking visual and textual nodes on the same scale. To further enhance semantic consistency, HiVE-MIL incorporates a two-stage, text-guided dynamic filtering mechanism that removes weakly correlated patch-text pairs, and introduces a hierarchical contrastive loss to align textual semantics across scales. Extensive experiments on TCGA breast, lung, and kidney cancer datasets demonstrate that HiVE-MIL consistently outperforms both traditional MIL and recent VLM-based MIL approaches, achieving gains of up to 4.1% in macro F1 under 16-shot settings. Our results demonstrate the value of jointly modeling hierarchical structure and multimodal alignment for efficient and scalable learning from limited pathology data. The code is available at https://github.com/bryanwong17/HiVE-MIL
<div id='section'>Paperid: <span id='pid'>60, <a href='https://arxiv.org/pdf/2505.16399.pdf' target='_blank'>https://arxiv.org/pdf/2505.16399.pdf</a></span>   <span><a href='https://github.com/dengq7/Sketchy-3DIS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qian Deng, Le Hui, Jin Xie, Jian Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16399">Sketchy Bounding-box Supervision for 3D Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Bounding box supervision has gained considerable attention in weakly supervised 3D instance segmentation. While this approach alleviates the need for extensive point-level annotations, obtaining accurate bounding boxes in practical applications remains challenging. To this end, we explore the inaccurate bounding box, named sketchy bounding box, which is imitated through perturbing ground truth bounding box by adding scaling, translation, and rotation. In this paper, we propose Sketchy-3DIS, a novel weakly 3D instance segmentation framework, which jointly learns pseudo labeler and segmentator to improve the performance under the sketchy bounding-box supervisions. Specifically, we first propose an adaptive box-to-point pseudo labeler that adaptively learns to assign points located in the overlapped parts between two sketchy bounding boxes to the correct instance, resulting in compact and pure pseudo instance labels. Then, we present a coarse-to-fine instance segmentator that first predicts coarse instances from the entire point cloud and then learns fine instances based on the region of coarse instances. Finally, by using the pseudo instance labels to supervise the instance segmentator, we can gradually generate high-quality instances through joint training. Extensive experiments show that our method achieves state-of-the-art performance on both the ScanNetV2 and S3DIS benchmarks, and even outperforms several fully supervised methods using sketchy bounding boxes. Code is available at https://github.com/dengq7/Sketchy-3DIS.
<div id='section'>Paperid: <span id='pid'>61, <a href='https://arxiv.org/pdf/2505.13905.pdf' target='_blank'>https://arxiv.org/pdf/2505.13905.pdf</a></span>   <span><a href='https://github.com/CLASS-Lab/4D-ROLLS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruihan Liu, Xiaoyi Wu, Xijun Chen, Liang Hu, Yunjiang Lou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13905">4D-ROLLS: 4D Radar Occupancy Learning via LiDAR Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A comprehensive understanding of 3D scenes is essential for autonomous vehicles (AVs), and among various perception tasks, occupancy estimation plays a central role by providing a general representation of drivable and occupied space. However, most existing occupancy estimation methods rely on LiDAR or cameras, which perform poorly in degraded environments such as smoke, rain, snow, and fog. In this paper, we propose 4D-ROLLS, the first weakly supervised occupancy estimation method for 4D radar using the LiDAR point cloud as the supervisory signal. Specifically, we introduce a method for generating pseudo-LiDAR labels, including occupancy queries and LiDAR height maps, as multi-stage supervision to train the 4D radar occupancy estimation model. Then the model is aligned with the occupancy map produced by LiDAR, fine-tuning its accuracy in occupancy estimation. Extensive comparative experiments validate the exceptional performance of 4D-ROLLS. Its robustness in degraded environments and effectiveness in cross-dataset training are qualitatively demonstrated. The model is also seamlessly transferred to downstream tasks BEV segmentation and point cloud occupancy prediction, highlighting its potential for broader applications. The lightweight network enables 4D-ROLLS model to achieve fast inference speeds at about 30 Hz on a 4060 GPU. The code of 4D-ROLLS will be made available at https://github.com/CLASS-Lab/4D-ROLLS.
<div id='section'>Paperid: <span id='pid'>62, <a href='https://arxiv.org/pdf/2505.12911.pdf' target='_blank'>https://arxiv.org/pdf/2505.12911.pdf</a></span>   <span><a href='https://github.com/sapeirone/hiero' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Simone Alberto Peirone, Francesca Pistilli, Giuseppe Averta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12911">HiERO: understanding the hierarchy of human behavior enhances reasoning on egocentric videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human activities are particularly complex and variable, and this makes challenging for deep learning models to reason about them. However, we note that such variability does have an underlying structure, composed of a hierarchy of patterns of related actions. We argue that such structure can emerge naturally from unscripted videos of human activities, and can be leveraged to better reason about their content. We present HiERO, a weakly-supervised method to enrich video segments features with the corresponding hierarchical activity threads. By aligning video clips with their narrated descriptions, HiERO infers contextual, semantic and temporal reasoning with an hierarchical architecture. We prove the potential of our enriched features with multiple video-text alignment benchmarks (EgoMCQ, EgoNLQ) with minimal additional training, and in zero-shot for procedure learning tasks (EgoProceL and Ego4D Goal-Step). Notably, HiERO achieves state-of-the-art performance in all the benchmarks, and for procedure learning tasks it outperforms fully-supervised methods by a large margin (+12.5% F1 on EgoProceL) in zero shot. Our results prove the relevance of using knowledge of the hierarchy of human activities for multiple reasoning tasks in egocentric vision.
<div id='section'>Paperid: <span id='pid'>63, <a href='https://arxiv.org/pdf/2505.09263.pdf' target='_blank'>https://arxiv.org/pdf/2505.09263.pdf</a></span>   <span><a href='https://github.com/gaobb/AnoGen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guan Gui, Bin-Bin Gao, Jun Liu, Chengjie Wang, Yunsheng Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09263">Few-Shot Anomaly-Driven Generation for Anomaly Classification and Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anomaly detection is a practical and challenging task due to the scarcity of anomaly samples in industrial inspection. Some existing anomaly detection methods address this issue by synthesizing anomalies with noise or external data. However, there is always a large semantic gap between synthetic and real-world anomalies, resulting in weak performance in anomaly detection. To solve the problem, we propose a few-shot Anomaly-driven Generation (AnoGen) method, which guides the diffusion model to generate realistic and diverse anomalies with only a few real anomalies, thereby benefiting training anomaly detection models. Specifically, our work is divided into three stages. In the first stage, we learn the anomaly distribution based on a few given real anomalies and inject the learned knowledge into an embedding. In the second stage, we use the embedding and given bounding boxes to guide the diffusion model to generate realistic and diverse anomalies on specific objects (or textures). In the final stage, we propose a weakly-supervised anomaly detection method to train a more powerful model with generated anomalies. Our method builds upon DRAEM and DesTSeg as the foundation model and conducts experiments on the commonly used industrial anomaly detection dataset, MVTec. The experiments demonstrate that our generated anomalies effectively improve the model performance of both anomaly classification and segmentation tasks simultaneously, \eg, DRAEM and DseTSeg achieved a 5.8\% and 1.5\% improvement in AU-PR metric on segmentation task, respectively. The code and generated anomalous data are available at https://github.com/gaobb/AnoGen.
<div id='section'>Paperid: <span id='pid'>64, <a href='https://arxiv.org/pdf/2505.07817.pdf' target='_blank'>https://arxiv.org/pdf/2505.07817.pdf</a></span>   <span><a href='https://kahnchana.github.io/LangToMo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kanchana Ranasinghe, Xiang Li, E-Ro Nguyen, Cristina Mata, Jongwoo Park, Michael S Ryoo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07817">Pixel Motion as Universal Representation for Robot Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present LangToMo, a vision-language-action framework structured as a dual-system architecture that uses pixel motion forecasts as intermediate representations. Our high-level System 2, an image diffusion model, generates text-conditioned pixel motion sequences from a single frame to guide robot control. Pixel motion-a universal, interpretable, and motion-centric representation-can be extracted from videos in a weakly-supervised manner, enabling diffusion model training on any video-caption data. Treating generated pixel motion as learned universal representations, our low level System 1 module translates these into robot actions via motion-to-action mapping functions, which can be either hand-crafted or learned with minimal supervision. System 2 operates as a high-level policy applied at sparse temporal intervals, while System 1 acts as a low-level policy at dense temporal intervals. This hierarchical decoupling enables flexible, scalable, and generalizable robot control under both unsupervised and supervised settings, bridging the gap between language, motion, and action. Checkout https://kahnchana.github.io/LangToMo
<div id='section'>Paperid: <span id='pid'>65, <a href='https://arxiv.org/pdf/2505.03207.pdf' target='_blank'>https://arxiv.org/pdf/2505.03207.pdf</a></span>   <span><a href='https://github.com/xyt-ml/PLC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yutong Xie, Fuchao Yang, Yuheng Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03207">Partial Label Clustering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partial label learning (PLL) is a significant weakly supervised learning framework, where each training example corresponds to a set of candidate labels and only one label is the ground-truth label. For the first time, this paper investigates the partial label clustering problem, which takes advantage of the limited available partial labels to improve the clustering performance. Specifically, we first construct a weight matrix of examples based on their relationships in the feature space and disambiguate the candidate labels to estimate the ground-truth label based on the weight matrix. Then, we construct a set of must-link and cannot-link constraints based on the disambiguation results. Moreover, we propagate the initial must-link and cannot-link constraints based on an adversarial prior promoted dual-graph learning approach. Finally, we integrate weight matrix construction, label disambiguation, and pairwise constraints propagation into a joint model to achieve mutual enhancement. We also theoretically prove that a better disambiguated label matrix can help improve clustering performance. Comprehensive experiments demonstrate our method realizes superior performance when comparing with state-of-the-art constrained clustering methods, and outperforms PLL and semi-supervised PLL methods when only limited samples are annotated. The code is publicly available at https://github.com/xyt-ml/PLC.
<div id='section'>Paperid: <span id='pid'>66, <a href='https://arxiv.org/pdf/2505.02179.pdf' target='_blank'>https://arxiv.org/pdf/2505.02179.pdf</a></span>   <span><a href='https://github.com/modadundun/ProDisc-VAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Zhu, Qi Yu, Xinru Dong, Shiyu Li, Yue Liu, Jinlong Jiang, Lei Shu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02179">ProDisc-VAD: An Efficient System for Weakly-Supervised Anomaly Detection in Video Surveillance Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised video anomaly detection (WS-VAD) using Multiple Instance Learning (MIL) suffers from label ambiguity, hindering discriminative feature learning. We propose ProDisc-VAD, an efficient framework tackling this via two synergistic components. The Prototype Interaction Layer (PIL) provides controlled normality modeling using a small set of learnable prototypes, establishing a robust baseline without being overwhelmed by dominant normal data. The Pseudo-Instance Discriminative Enhancement (PIDE) loss boosts separability by applying targeted contrastive learning exclusively to the most reliable extreme-scoring instances (highest/lowest scores). ProDisc-VAD achieves strong AUCs (97.98% ShanghaiTech, 87.12% UCF-Crime) using only 0.4M parameters, over 800x fewer than recent ViT-based methods like VadCLIP. Code is available at https://github.com/modadundun/ProDisc-VAD.
<div id='section'>Paperid: <span id='pid'>67, <a href='https://arxiv.org/pdf/2504.19357.pdf' target='_blank'>https://arxiv.org/pdf/2504.19357.pdf</a></span>   <span><a href='https://github.com/diku-dk/credanno' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Lu, Chong Yin, Silvia Ingala, Kenny Erleben, Michael Bachmann Nielsen, Sune Darkner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.19357">MERA: Multimodal and Multiscale Self-Explanatory Model with Considerably Reduced Annotation for Lung Nodule Diagnosis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Lung cancer, a leading cause of cancer-related deaths globally, emphasises the importance of early detection for better patient outcomes. Pulmonary nodules, often early indicators of lung cancer, necessitate accurate, timely diagnosis. Despite Explainable Artificial Intelligence (XAI) advances, many existing systems struggle providing clear, comprehensive explanations, especially with limited labelled data. This study introduces MERA, a Multimodal and Multiscale self-Explanatory model designed for lung nodule diagnosis with considerably Reduced Annotation requirements. MERA integrates unsupervised and weakly supervised learning strategies (self-supervised learning techniques and Vision Transformer architecture for unsupervised feature extraction) and a hierarchical prediction mechanism leveraging sparse annotations via semi-supervised active learning in the learned latent space. MERA explains its decisions on multiple levels: model-level global explanations via semantic latent space clustering, instance-level case-based explanations showing similar instances, local visual explanations via attention maps, and concept explanations using critical nodule attributes. Evaluations on the public LIDC dataset show MERA's superior diagnostic accuracy and self-explainability. With only 1% annotated samples, MERA achieves diagnostic accuracy comparable to or exceeding state-of-the-art methods requiring full annotation. The model's inherent design delivers comprehensive, robust, multilevel explanations aligned closely with clinical practice, enhancing trustworthiness and transparency. Demonstrated viability of unsupervised and weakly supervised learning lowers the barrier to deploying diagnostic AI in broader medical domains. Our complete code is open-source available: https://github.com/diku-dk/credanno.
<div id='section'>Paperid: <span id='pid'>68, <a href='https://arxiv.org/pdf/2504.17379.pdf' target='_blank'>https://arxiv.org/pdf/2504.17379.pdf</a></span>   <span><a href='https://github.com/tueimage/GABMIL' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/tueimage/GABMIL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hassan Keshvarikhojasteh, Mihail Tifrea, Sibylle Hess, Josien P. W. Pluim, Mitko Veta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.17379">A Spatially-Aware Multiple Instance Learning Framework for Digital Pathology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple instance learning (MIL) is a promising approach for weakly supervised classification in pathology using whole slide images (WSIs). However, conventional MIL methods such as Attention-Based Deep Multiple Instance Learning (ABMIL) typically disregard spatial interactions among patches that are crucial to pathological diagnosis. Recent advancements, such as Transformer based MIL (TransMIL), have incorporated spatial context and inter-patch relationships. However, it remains unclear whether explicitly modeling patch relationships yields similar performance gains in ABMIL, which relies solely on Multi-Layer Perceptrons (MLPs). In contrast, TransMIL employs Transformer-based layers, introducing a fundamental architectural shift at the cost of substantially increased computational complexity. In this work, we enhance the ABMIL framework by integrating interaction-aware representations to address this question. Our proposed model, Global ABMIL (GABMIL), explicitly captures inter-instance dependencies while preserving computational efficiency. Experimental results on two publicly available datasets for tumor subtyping in breast and lung cancers demonstrate that GABMIL achieves up to a 7 percentage point improvement in AUPRC and a 5 percentage point increase in the Kappa score over ABMIL, with minimal or no additional computational overhead. These findings underscore the importance of incorporating patch interactions within MIL frameworks. Our code is available at \href{https://github.com/tueimage/GABMIL}{\texttt{GABMIL}}.
<div id='section'>Paperid: <span id='pid'>69, <a href='https://arxiv.org/pdf/2504.14783.pdf' target='_blank'>https://arxiv.org/pdf/2504.14783.pdf</a></span>   <span><a href='https://github.com/ChongQingNoSubway/MILDropout' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhui Zhu, Peijie Qiu, Xiwen Chen, Zhangsihao Yang, Aristeidis Sotiras, Abolfazl Razi, Yalin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14783">How Effective Can Dropout Be in Multiple Instance Learning ?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple Instance Learning (MIL) is a popular weakly-supervised method for various applications, with a particular interest in histological whole slide image (WSI) classification. Due to the gigapixel resolution of WSI, applications of MIL in WSI typically necessitate a two-stage training scheme: first, extract features from the pre-trained backbone and then perform MIL aggregation. However, it is well-known that this suboptimal training scheme suffers from "noisy" feature embeddings from the backbone and inherent weak supervision, hindering MIL from learning rich and generalizable features. However, the most commonly used technique (i.e., dropout) for mitigating this issue has yet to be explored in MIL. In this paper, we empirically explore how effective the dropout can be in MIL. Interestingly, we observe that dropping the top-k most important instances within a bag leads to better performance and generalization even under noise attack. Based on this key observation, we propose a novel MIL-specific dropout method, termed MIL-Dropout, which systematically determines which instances to drop. Experiments on five MIL benchmark datasets and two WSI datasets demonstrate that MIL-Dropout boosts the performance of current MIL methods with a negligible computational cost. The code is available at https://github.com/ChongQingNoSubway/MILDropout.
<div id='section'>Paperid: <span id='pid'>70, <a href='https://arxiv.org/pdf/2504.11368.pdf' target='_blank'>https://arxiv.org/pdf/2504.11368.pdf</a></span>   <span><a href='https://github.com/jingkunchen/FGI.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingkun Chen, Haoran Duan, Xiao Zhang, Boyan Gao, Tao Tan, Vicente Grau, Jungong Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11368">From Gaze to Insight: Bridging Human Visual Attention and Vision Language Model Explanation for Weakly-Supervised Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical image segmentation remains challenging due to the high cost of pixel-level annotations for training. In the context of weak supervision, clinician gaze data captures regions of diagnostic interest; however, its sparsity limits its use for segmentation. In contrast, vision-language models (VLMs) provide semantic context through textual descriptions but lack the explanation precision required. Recognizing that neither source alone suffices, we propose a teacher-student framework that integrates both gaze and language supervision, leveraging their complementary strengths. Our key insight is that gaze data indicates where clinicians focus during diagnosis, while VLMs explain why those regions are significant. To implement this, the teacher model first learns from gaze points enhanced by VLM-generated descriptions of lesion morphology, establishing a foundation for guiding the student model. The teacher then directs the student through three strategies: (1) Multi-scale feature alignment to fuse visual cues with textual semantics; (2) Confidence-weighted consistency constraints to focus on reliable predictions; (3) Adaptive masking to limit error propagation in uncertain areas. Experiments on the Kvasir-SEG, NCI-ISBI, and ISIC datasets show that our method achieves Dice scores of 80.78%, 80.53%, and 84.22%, respectively-improving 3-5% over gaze baselines without increasing the annotation burden. By preserving correlations among predictions, gaze data, and lesion descriptions, our framework also maintains clinical interpretability. This work illustrates how integrating human visual attention with AI-generated semantic context can effectively overcome the limitations of individual weak supervision signals, thereby advancing the development of deployable, annotation-efficient medical AI systems. Code is available at: https://github.com/jingkunchen/FGI.git.
<div id='section'>Paperid: <span id='pid'>71, <a href='https://arxiv.org/pdf/2504.11014.pdf' target='_blank'>https://arxiv.org/pdf/2504.11014.pdf</a></span>   <span><a href='https://ies0411.github.io/GATE3D/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Eunsoo Im, Changhyun Jee, Jung Kwon Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11014">GATE3D: Generalized Attention-based Task-synergized Estimation in 3D*</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emerging trend in computer vision emphasizes developing universal models capable of simultaneously addressing multiple diverse tasks. Such universality typically requires joint training across multi-domain datasets to ensure effective generalization. However, monocular 3D object detection presents unique challenges in multi-domain training due to the scarcity of datasets annotated with accurate 3D ground-truth labels, especially beyond typical road-based autonomous driving contexts. To address this challenge, we introduce a novel weakly supervised framework leveraging pseudo-labels. Current pretrained models often struggle to accurately detect pedestrians in non-road environments due to inherent dataset biases. Unlike generalized image-based 2D object detection models, achieving similar generalization in monocular 3D detection remains largely unexplored. In this paper, we propose GATE3D, a novel framework designed specifically for generalized monocular 3D object detection via weak supervision. GATE3D effectively bridges domain gaps by employing consistency losses between 2D and 3D predictions. Remarkably, our model achieves competitive performance on the KITTI benchmark as well as on an indoor-office dataset collected by us to evaluate the generalization capabilities of our framework. Our results demonstrate that GATE3D significantly accelerates learning from limited annotated data through effective pre-training strategies, highlighting substantial potential for broader impacts in robotics, augmented reality, and virtual reality applications. Project page: https://ies0411.github.io/GATE3D/
<div id='section'>Paperid: <span id='pid'>72, <a href='https://arxiv.org/pdf/2504.09881.pdf' target='_blank'>https://arxiv.org/pdf/2504.09881.pdf</a></span>   <span><a href='https://github.com/chenshunpeng/FoL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Changwei Wang, Shunpeng Chen, Yukun Song, Rongtao Xu, Zherui Zhang, Jiguang Zhang, Haoran Yang, Yu Zhang, Kexue Fu, Shide Du, Zhiwei Xu, Longxiang Gao, Li Guo, Shibiao Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09881">Focus on Local: Finding Reliable Discriminative Regions for Visual Place Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Place Recognition (VPR) is aimed at predicting the location of a query image by referencing a database of geotagged images. For VPR task, often fewer discriminative local regions in an image produce important effects while mundane background regions do not contribute or even cause perceptual aliasing because of easy overlap. However, existing methods lack precisely modeling and full exploitation of these discriminative regions. In this paper, we propose the Focus on Local (FoL) approach to stimulate the performance of image retrieval and re-ranking in VPR simultaneously by mining and exploiting reliable discriminative local regions in images and introducing pseudo-correlation supervision. First, we design two losses, Extraction-Aggregation Spatial Alignment Loss (SAL) and Foreground-Background Contrast Enhancement Loss (CEL), to explicitly model reliable discriminative local regions and use them to guide the generation of global representations and efficient re-ranking. Second, we introduce a weakly-supervised local feature training strategy based on pseudo-correspondences obtained from aggregating global features to alleviate the lack of local correspondences ground truth for the VPR task. Third, we suggest an efficient re-ranking pipeline that is efficiently and precisely based on discriminative region guidance. Finally, experimental results show that our FoL achieves the state-of-the-art on multiple VPR benchmarks in both image retrieval and re-ranking stages and also significantly outperforms existing two-stage VPR methods in terms of computational efficiency. Code and models are available at https://github.com/chenshunpeng/FoL
<div id='section'>Paperid: <span id='pid'>73, <a href='https://arxiv.org/pdf/2504.03096.pdf' target='_blank'>https://arxiv.org/pdf/2504.03096.pdf</a></span>   <span><a href='https://siatheindochinese.github.io/sia_act_page/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhen Hao Sia, Yogesh Singh Rawat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03096">Scaling Open-Vocabulary Action Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we focus on scaling open-vocabulary action detection. Existing approaches for action detection are predominantly limited to closed-set scenarios and rely on complex, parameter-heavy architectures. Extending these models to the open-vocabulary setting poses two key challenges: (1) the lack of large-scale datasets with many action classes for robust training, and (2) parameter-heavy adaptations to a pretrained vision-language contrastive model to convert it for detection, risking overfitting the additional non-pretrained parameters to base action classes. Firstly, we introduce an encoder-only multimodal model for video action detection, reducing the reliance on parameter-heavy additions for video action detection. Secondly, we introduce a simple weakly supervised training strategy to exploit an existing closed-set action detection dataset for pretraining. Finally, we depart from the ill-posed base-to-novel benchmark used by prior works in open-vocabulary action detection and devise a new benchmark to evaluate on existing closed-set action detection datasets without ever using them for training, showing novel results to serve as baselines for future work. Our code is available at https://siatheindochinese.github.io/sia_act_page/ .
<div id='section'>Paperid: <span id='pid'>74, <a href='https://arxiv.org/pdf/2503.11439.pdf' target='_blank'>https://arxiv.org/pdf/2503.11439.pdf</a></span>   <span><a href='https://github.com/shjo-april/COIN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sanghyun Jo, Seo Jin Lee, Seungwoo Lee, Seohyung Hong, Hyungseok Seo, Kyungsu Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11439">COIN: Confidence Score-Guided Distillation for Annotation-Free Cell Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cell instance segmentation (CIS) is crucial for identifying individual cell morphologies in histopathological images, providing valuable insights for biological and medical research. While unsupervised CIS (UCIS) models aim to reduce the heavy reliance on labor-intensive image annotations, they fail to accurately capture cell boundaries, causing missed detections and poor performance. Recognizing the absence of error-free instances as a key limitation, we present COIN (COnfidence score-guided INstance distillation), a novel annotation-free framework with three key steps: (1) Increasing the sensitivity for the presence of error-free instances via unsupervised semantic segmentation with optimal transport, leveraging its ability to discriminate spatially minor instances, (2) Instance-level confidence scoring to measure the consistency between model prediction and refined mask and identify highly confident instances, offering an alternative to ground truth annotations, and (3) Progressive expansion of confidence with recursive self-distillation. Extensive experiments across six datasets show COIN outperforming existing UCIS methods, even surpassing semi- and weakly-supervised approaches across all metrics on the MoNuSeg and TNBC datasets. The code is available at https://github.com/shjo-april/COIN.
<div id='section'>Paperid: <span id='pid'>75, <a href='https://arxiv.org/pdf/2503.11032.pdf' target='_blank'>https://arxiv.org/pdf/2503.11032.pdf</a></span>   <span><a href='https://github.com/zhang-lilin/WSCAT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lilin Zhang, Chengpei Wu, Ning Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11032">Weakly Supervised Contrastive Adversarial Training for Learning Robust Features from Semi-supervised Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing adversarial training (AT) methods often suffer from incomplete perturbation, meaning that not all non-robust features are perturbed when generating adversarial examples (AEs). This results in residual correlations between non-robust features and labels, leading to suboptimal learning of robust features. However, achieving complete perturbation, i.e., perturbing as many non-robust features as possible, is challenging due to the difficulty in distinguishing robust and non-robust features and the sparsity of labeled data. To address these challenges, we propose a novel approach called Weakly Supervised Contrastive Adversarial Training (WSCAT). WSCAT ensures complete perturbation for improved learning of robust features by disrupting correlations between non-robust features and labels through complete AE generation over partially labeled data, grounded in information theory. Extensive theoretical analysis and comprehensive experiments on widely adopted benchmarks validate the superiority of WSCAT. Our code is available at https://github.com/zhang-lilin/WSCAT.
<div id='section'>Paperid: <span id='pid'>76, <a href='https://arxiv.org/pdf/2503.07982.pdf' target='_blank'>https://arxiv.org/pdf/2503.07982.pdf</a></span>   <span><a href='https://github.com/shjo-april/DiffEGG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sanghyun Jo, Ziseok Lee, Wooyeol Lee, Kyungsu Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.07982">DiffEGG: Diffusion-Driven Edge Generation as a Pixel-Annotation-Free Alternative for Instance Annotation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving precise panoptic segmentation relies on pixel-wise instance annotations, but obtaining such datasets is costly. Unsupervised instance segmentation (UIS) eliminates annotation requirements but struggles with adjacent instance merging and single-instance fragmentation, largely due to the limitations of DINO-based backbones which lack strong instance separation cues. Weakly-supervised panoptic segmentation (WPS) reduces annotation costs using sparse labels (e.g., points, boxes), yet these annotations remain expensive and introduce human bias and boundary errors. To address these challenges, we propose DiffEGG (Diffusion-Driven EdGe Generation), a fully annotation-free method that extracts instance-aware features from pretrained diffusion models to generate precise instance edge maps. Unlike DINO-based UIS methods, diffusion models inherently capture fine-grained, instance-aware features, enabling more precise boundary delineation. For WPS, DiffEGG eliminates annotation costs and human bias by operating without any form of manual supervision, addressing the key limitations of prior best methods. Additionally, we introduce RIP, a post-processing technique that fuses DiffEGG's edge maps with segmentation masks in a task-agnostic manner. RIP allows DiffEGG to be seamlessly integrated into various segmentation frameworks. When applied to UIS, DiffEGG and RIP achieve an average $+4.4\text{ AP}$ improvement over prior best UIS methods. When combined with weakly-supervised semantic segmentation (WSS), DiffEGG enables WPS without instance annotations, outperforming prior best point-supervised WPS methods by $+1.7\text{ PQ}$. These results demonstrate that DiffEGG's edge maps serve as a cost-effective, annotation-free alternative to instance annotations, significantly improving segmentation without human intervention. Code is available at https://github.com/shjo-april/DiffEGG.
<div id='section'>Paperid: <span id='pid'>77, <a href='https://arxiv.org/pdf/2503.07635.pdf' target='_blank'>https://arxiv.org/pdf/2503.07635.pdf</a></span>   <span><a href='https://github.com/WissingChen/CRA-GQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weixing Chen, Yang Liu, Binglin Chen, Jiandong Su, Yongsen Zheng, Liang Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.07635">Cross-modal Causal Relation Alignment for Video Question Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video question grounding (VideoQG) requires models to answer the questions and simultaneously infer the relevant video segments to support the answers. However, existing VideoQG methods usually suffer from spurious cross-modal correlations, leading to a failure to identify the dominant visual scenes that align with the intended question. Moreover, vision-language models exhibit unfaithful generalization performance and lack robustness on challenging downstream tasks such as VideoQG. In this work, we propose a novel VideoQG framework named Cross-modal Causal Relation Alignment (CRA), to eliminate spurious correlations and improve the causal consistency between question-answering and video temporal grounding. Our CRA involves three essential components: i) Gaussian Smoothing Grounding (GSG) module for estimating the time interval via cross-modal attention, which is de-noised by an adaptive Gaussian filter, ii) Cross-Modal Alignment (CMA) enhances the performance of weakly supervised VideoQG by leveraging bidirectional contrastive learning between estimated video segments and QA features, iii) Explicit Causal Intervention (ECI) module for multimodal deconfounding, which involves front-door intervention for vision and back-door intervention for language. Extensive experiments on two VideoQG datasets demonstrate the superiority of our CRA in discovering visually grounded content and achieving robust question reasoning. Codes are available at https://github.com/WissingChen/CRA-GQA.
<div id='section'>Paperid: <span id='pid'>78, <a href='https://arxiv.org/pdf/2503.04106.pdf' target='_blank'>https://arxiv.org/pdf/2503.04106.pdf</a></span>   <span><a href='https://github.com/wanghr64/WeakMedSAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Wang, Lian Huai, Wenbin Li, Lei Qi, Xingqun Jiang, Yinghuan Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04106">WeakMedSAM: Weakly-Supervised Medical Image Segmentation via SAM with Sub-Class Exploration and Prompt Affinity Mining</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We have witnessed remarkable progress in foundation models in vision tasks. Currently, several recent works have utilized the segmenting anything model (SAM) to boost the segmentation performance in medical images, where most of them focus on training an adaptor for fine-tuning a large amount of pixel-wise annotated medical images following a fully supervised manner. In this paper, to reduce the labeling cost, we investigate a novel weakly-supervised SAM-based segmentation model, namely WeakMedSAM. Specifically, our proposed WeakMedSAM contains two modules: 1) to mitigate severe co-occurrence in medical images, a sub-class exploration module is introduced to learn accurate feature representations. 2) to improve the quality of the class activation maps, our prompt affinity mining module utilizes the prompt capability of SAM to obtain an affinity map for random-walk refinement. Our method can be applied to any SAM-like backbone, and we conduct experiments with SAMUS and EfficientSAM. The experimental results on three popularly-used benchmark datasets, i.e., BraTS 2019, AbdomenCT-1K, and MSD Cardiac dataset, show the promising results of our proposed WeakMedSAM. Our code is available at https://github.com/wanghr64/WeakMedSAM.
<div id='section'>Paperid: <span id='pid'>79, <a href='https://arxiv.org/pdf/2503.03562.pdf' target='_blank'>https://arxiv.org/pdf/2503.03562.pdf</a></span>   <span><a href='https://guyao2023.github.io/Phys-AD/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenqiao Li, Yao Gu, Xintao Chen, Xiaohao Xu, Ming Hu, Xiaonan Huang, Yingna Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.03562">Towards Visual Discrimination and Reasoning of Real-World Physical Dynamics: Physics-Grounded Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans detect real-world object anomalies by perceiving, interacting, and reasoning based on object-conditioned physical knowledge. The long-term goal of Industrial Anomaly Detection (IAD) is to enable machines to autonomously replicate this skill. However, current IAD algorithms are largely developed and tested on static, semantically simple datasets, which diverge from real-world scenarios where physical understanding and reasoning are essential. To bridge this gap, we introduce the Physics Anomaly Detection (Phys-AD) dataset, the first large-scale, real-world, physics-grounded video dataset for industrial anomaly detection. Collected using a real robot arm and motor, Phys-AD provides a diverse set of dynamic, semantically rich scenarios. The dataset includes more than 6400 videos across 22 real-world object categories, interacting with robot arms and motors, and exhibits 47 types of anomalies. Anomaly detection in Phys-AD requires visual reasoning, combining both physical knowledge and video content to determine object abnormality. We benchmark state-of-the-art anomaly detection methods under three settings: unsupervised AD, weakly-supervised AD, and video-understanding AD, highlighting their limitations in handling physics-grounded anomalies. Additionally, we introduce the Physics Anomaly Explanation (PAEval) metric, designed to assess the ability of visual-language foundation models to not only detect anomalies but also provide accurate explanations for their underlying physical causes. Our project is available at https://guyao2023.github.io/Phys-AD/.
<div id='section'>Paperid: <span id='pid'>80, <a href='https://arxiv.org/pdf/2502.21109.pdf' target='_blank'>https://arxiv.org/pdf/2502.21109.pdf</a></span>   <span><a href='https://github.com/DIAGNijmegen/tumor-percentage-mil-regression' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Marina D'Amato, Jeroen van der Laak, Francesco Ciompi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.21109">"No negatives needed": weakly-supervised regression for interpretable tumor detection in whole-slide histopathology images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate tumor detection in digital pathology whole-slide images (WSIs) is crucial for cancer diagnosis and treatment planning. Multiple Instance Learning (MIL) has emerged as a widely used approach for weakly-supervised tumor detection with large-scale data without the need for manual annotations. However, traditional MIL methods often depend on classification tasks that require tumor-free cases as negative examples, which are challenging to obtain in real-world clinical workflows, especially for surgical resection specimens. We address this limitation by reformulating tumor detection as a regression task, estimating tumor percentages from WSIs, a clinically available target across multiple cancer types. In this paper, we provide an analysis of the proposed weakly-supervised regression framework by applying it to multiple organs, specimen types and clinical scenarios. We characterize the robustness of our framework to tumor percentage as a noisy regression target, and introduce a novel concept of amplification technique to improve tumor detection sensitivity when learning from small tumor regions. Finally, we provide interpretable insights into the model's predictions by analyzing visual attention and logit maps. Our code is available at https://github.com/DIAGNijmegen/tumor-percentage-mil-regression.
<div id='section'>Paperid: <span id='pid'>81, <a href='https://arxiv.org/pdf/2502.20838.pdf' target='_blank'>https://arxiv.org/pdf/2502.20838.pdf</a></span>   <span><a href='https://github.com/Ragib-Amin-Nihal/DSMIL-Loc' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ragib Amin Nihal, Benjamin Yen, Runwu Shi, Kazuhiro Nakadai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20838">Weakly Supervised Multiple Instance Learning for Whale Call Detection and Temporal Localization in Long-Duration Passive Acoustic Monitoring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Marine ecosystem monitoring via Passive Acoustic Monitoring (PAM) generates vast data, but deep learning often requires precise annotations and short segments. We introduce DSMIL-LocNet, a Multiple Instance Learning framework for whale call detection and localization using only bag-level labels. Our dual-stream model processes 2-30 minute audio segments, leveraging spectral and temporal features with attention-based instance selection. Tests on Antarctic whale data show longer contexts improve classification (F1: 0.8-0.9) while medium instances ensure localization precision (0.65-0.70). This suggests MIL can enhance scalable marine monitoring. Code: https://github.com/Ragib-Amin-Nihal/DSMIL-Loc
<div id='section'>Paperid: <span id='pid'>82, <a href='https://arxiv.org/pdf/2502.19707.pdf' target='_blank'>https://arxiv.org/pdf/2502.19707.pdf</a></span>   <span><a href='https://github.com/bluehenglee/MLI-MSC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianning Chi, Zelan Li, Geng Lin, MingYang Sun, Xiaosheng Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19707">Weakly Supervised Segmentation Framework for Thyroid Nodule Based on High-confidence Labels and High-rationality Losses</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised segmentation methods can delineate thyroid nodules in ultrasound images efficiently using training data with coarse labels, but suffer from: 1) low-confidence pseudo-labels that follow topological priors, introducing significant label noise, and 2) low-rationality loss functions that rigidly compare segmentation with labels, ignoring discriminative information for nodules with diverse and complex shapes. To solve these issues, we clarify the objective and references for weakly supervised ultrasound image segmentation, presenting a framework with high-confidence pseudo-labels to represent topological and anatomical information and high-rationality losses to capture multi-level discriminative features. Specifically, we fuse geometric transformations of four-point annotations and MedSAM model results prompted by specific annotations to generate high-confidence box, foreground, and background labels. Our high-rationality learning strategy includes: 1) Alignment loss measuring spatial consistency between segmentation and box label, and topological continuity within the foreground label, guiding the network to perceive nodule location; 2) Contrastive loss pulling features from labeled foreground regions while pushing features from labeled foreground and background regions, guiding the network to learn nodule and background feature distribution; 3) Prototype correlation loss measuring consistency between correlation maps derived by comparing features with foreground and background prototypes, refining uncertain regions to accurate nodule edges. Experimental results show that our method achieves state-of-the-art performance on the TN3K and DDTI datasets. The code is available at https://github.com/bluehenglee/MLI-MSC.
<div id='section'>Paperid: <span id='pid'>83, <a href='https://arxiv.org/pdf/2502.15885.pdf' target='_blank'>https://arxiv.org/pdf/2502.15885.pdf</a></span>   <span><a href='https://github.com/AIGeeksGroup/DOEI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongjie Zhu, Zeyu Zhang, Guansong Pang, Xu Wang, Shimin Wen, Yu Bai, Daji Ergu, Ying Cai, Yang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.15885">DOEI: Dual Optimization of Embedding Information for Attention-Enhanced Class Activation Maps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation (WSSS) typically utilizes limited semantic annotations to obtain initial Class Activation Maps (CAMs). However, due to the inadequate coupling between class activation responses and semantic information in high-dimensional space, the CAM is prone to object co-occurrence or under-activation, resulting in inferior recognition accuracy. To tackle this issue, we propose DOEI, Dual Optimization of Embedding Information, a novel approach that reconstructs embedding representations through semantic-aware attention weight matrices to optimize the expression capability of embedding information. Specifically, DOEI amplifies tokens with high confidence and suppresses those with low confidence during the class-to-patch interaction. This alignment of activation responses with semantic information strengthens the propagation and decoupling of target features, enabling the generated embeddings to more accurately represent target features in high-level semantic space. In addition, we propose a hybrid-feature alignment module in DOEI that combines RGB values, embedding-guided features, and self-attention weights to increase the reliability of candidate tokens. Comprehensive experiments show that DOEI is an effective plug-and-play module that empowers state-of-the-art visual transformer-based WSSS models to significantly improve the quality of CAMs and segmentation performance on popular benchmarks, including PASCAL VOC (+3.6%, +1.5%, +1.2% mIoU) and MS COCO (+1.2%, +1.6% mIoU). Code will be available at https://github.com/AIGeeksGroup/DOEI.
<div id='section'>Paperid: <span id='pid'>84, <a href='https://arxiv.org/pdf/2502.15152.pdf' target='_blank'>https://arxiv.org/pdf/2502.15152.pdf</a></span>   <span><a href='https://github.com/psychofict/CW-BASS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ebenezer Tarubinga, Jenifer Kalafatovich, Seong-Whan Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.15152">CW-BASS: Confidence-Weighted Boundary-Aware Learning for Semi-Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semi-supervised semantic segmentation (SSSS) aims to improve segmentation performance by utilizing large amounts of unlabeled data with limited labeled samples. Existing methods often suffer from coupling, where over-reliance on initial labeled data leads to suboptimal learning; confirmation bias, where incorrect predictions reinforce themselves repeatedly; and boundary blur caused by limited boundary-awareness and ambiguous edge cues. To address these issues, we propose CW-BASS, a novel framework for SSSS. In order to mitigate the impact of incorrect predictions, we assign confidence weights to pseudo-labels. Additionally, we leverage boundary-delineation techniques, which, despite being extensively explored in weakly-supervised semantic segmentation (WSSS), remain underutilized in SSSS. Specifically, our method: (1) reduces coupling via a confidence-weighted loss that adjusts pseudo-label influence based on their predicted confidence scores, (2) mitigates confirmation bias with a dynamic thresholding mechanism that learns to filter out pseudo-labels based on model performance, (3) tackles boundary blur using a boundary-aware module to refine segmentation near object edges, and (4) reduces label noise through a confidence decay strategy that progressively refines pseudo-labels during training. Extensive experiments on Pascal VOC 2012 and Cityscapes demonstrate that CW-BASS achieves state-of-the-art performance. Notably, CW-BASS achieves a 65.9% mIoU on Cityscapes under a challenging and underexplored 1/30 (3.3%) split (100 images), highlighting its effectiveness in limited-label settings. Our code is available at https://github.com/psychofict/CW-BASS.
<div id='section'>Paperid: <span id='pid'>85, <a href='https://arxiv.org/pdf/2502.10294.pdf' target='_blank'>https://arxiv.org/pdf/2502.10294.pdf</a></span>   <span><a href='https://github.com/anpc849/QMaxViT-Unet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Thien B. Nguyen-Tat, Hoang-An Vo, Phuoc-Sang Dang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10294">QMaxViT-Unet+: A Query-Based MaxViT-Unet with Edge Enhancement for Scribble-Supervised Segmentation of Medical Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The deployment of advanced deep learning models for medical image segmentation is often constrained by the requirement for extensively annotated datasets. Weakly-supervised learning, which allows less precise labels, has become a promising solution to this challenge. Building on this approach, we propose QMaxViT-Unet+, a novel framework for scribble-supervised medical image segmentation. This framework is built on the U-Net architecture, with the encoder and decoder replaced by Multi-Axis Vision Transformer (MaxViT) blocks. These blocks enhance the model's ability to learn local and global features efficiently. Additionally, our approach integrates a query-based Transformer decoder to refine features and an edge enhancement module to compensate for the limited boundary information in the scribble label. We evaluate the proposed QMaxViT-Unet+ on four public datasets focused on cardiac structures, colorectal polyps, and breast cancer: ACDC, MS-CMRSeg, SUN-SEG, and BUSI. Evaluation metrics include the Dice similarity coefficient (DSC) and the 95th percentile of Hausdorff distance (HD95). Experimental results show that QMaxViT-Unet+ achieves 89.1\% DSC and 1.316mm HD95 on ACDC, 88.4\% DSC and 2.226mm HD95 on MS-CMRSeg, 71.4\% DSC and 4.996mm HD95 on SUN-SEG, and 69.4\% DSC and 50.122mm HD95 on BUSI. These results demonstrate that our method outperforms existing approaches in terms of accuracy, robustness, and efficiency while remaining competitive with fully-supervised learning approaches. This makes it ideal for medical image analysis, where high-quality annotations are often scarce and require significant effort and expense. The code is available at: https://github.com/anpc849/QMaxViT-Unet
<div id='section'>Paperid: <span id='pid'>86, <a href='https://arxiv.org/pdf/2502.10263.pdf' target='_blank'>https://arxiv.org/pdf/2502.10263.pdf</a></span>   <span><a href='https://github.com/worldbank/ai4data-use' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aivin V. Solatorio, Rafael Macalaba, James Liounis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10263">Large Language Models and Synthetic Data for Monitoring Dataset Mentions in Research Papers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking how data is mentioned and used in research papers provides critical insights for improving data discoverability, quality, and production. However, manually identifying and classifying dataset mentions across vast academic literature is resource-intensive and not scalable. This paper presents a machine learning framework that automates dataset mention detection across research domains by leveraging large language models (LLMs), synthetic data, and a two-stage fine-tuning process. We employ zero-shot extraction from research papers, an LLM-as-a-Judge for quality assessment, and a reasoning agent for refinement to generate a weakly supervised synthetic dataset. The Phi-3.5-mini instruct model is pre-fine-tuned on this dataset, followed by fine-tuning on a manually annotated subset. At inference, a ModernBERT-based classifier efficiently filters dataset mentions, reducing computational overhead while maintaining high recall. Evaluated on a held-out manually annotated sample, our fine-tuned model outperforms NuExtract-v1.5 and GLiNER-large-v2.1 in dataset extraction accuracy. Our results highlight how LLM-generated synthetic data can effectively address training data scarcity, improving generalization in low-resource settings. This framework offers a pathway toward scalable monitoring of dataset usage, enhancing transparency, and supporting researchers, funders, and policymakers in identifying data gaps and strengthening data accessibility for informed decision-making.
<div id='section'>Paperid: <span id='pid'>87, <a href='https://arxiv.org/pdf/2502.09874.pdf' target='_blank'>https://arxiv.org/pdf/2502.09874.pdf</a></span>   <span><a href='https://github.com/LQY404/FrGNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Ling, Wenxiao Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09874">FrGNet: A fourier-guided weakly-supervised framework for nuclear instance segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nuclear instance segmentation has played a critical role in pathology image analysis. The main challenges arise from the difficulty in accurately segmenting instances and the high cost of precise mask-level annotations for fully-supervised training.In this work, we propose a fourier guidance framework for solving the weakly-supervised nuclear instance segmentation problem. In this framework, we construct a fourier guidance module to fuse the priori information into the training process of the model, which facilitates the model to capture the relevant features of the nuclear. Meanwhile, in order to further improve the model's ability to represent the features of nuclear, we propose the guide-based instance level contrastive module. This module makes full use of the framework's own properties and guide information to effectively enhance the representation features of nuclear. We show on two public datasets that our model can outperform current SOTA methods under fully-supervised design, and in weakly-supervised experiments, with only a small amount of labeling our model still maintains close to the performance under full supervision.In addition, we also perform generalization experiments on a private dataset, and without any labeling, our model is able to segment nuclear images that have not been seen during training quite effectively. As open science, all codes and pre-trained models are available at https://github.com/LQY404/FrGNet.
<div id='section'>Paperid: <span id='pid'>88, <a href='https://arxiv.org/pdf/2502.09471.pdf' target='_blank'>https://arxiv.org/pdf/2502.09471.pdf</a></span>   <span><a href='https://github.com/VisionXLab/whollywood' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/VisionXLab/whollywood-jittor' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Yu, Xue Yang, Yansheng Li, Zhenjun Han, Feipeng Da, Junchi Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09471">Wholly-WOOD: Wholly Leveraging Diversified-quality Labels for Weakly-supervised Oriented Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately estimating the orientation of visual objects with compact rotated bounding boxes (RBoxes) has become a prominent demand, which challenges existing object detection paradigms that only use horizontal bounding boxes (HBoxes). To equip the detectors with orientation awareness, supervised regression/classification modules have been introduced at the high cost of rotation annotation. Meanwhile, some existing datasets with oriented objects are already annotated with horizontal boxes or even single points. It becomes attractive yet remains open for effectively utilizing weaker single point and horizontal annotations to train an oriented object detector (OOD). We develop Wholly-WOOD, a weakly-supervised OOD framework, capable of wholly leveraging various labeling forms (Points, HBoxes, RBoxes, and their combination) in a unified fashion. By only using HBox for training, our Wholly-WOOD achieves performance very close to that of the RBox-trained counterpart on remote sensing and other areas, significantly reducing the tedious efforts on labor-intensive annotation for oriented objects. The source codes are available at https://github.com/VisionXLab/whollywood (PyTorch-based) and https://github.com/VisionXLab/whollywood-jittor (Jittor-based).
<div id='section'>Paperid: <span id='pid'>89, <a href='https://arxiv.org/pdf/2502.04268.pdf' target='_blank'>https://arxiv.org/pdf/2502.04268.pdf</a></span>   <span><a href='https://github.com/VisionXLab/point2rbox-v2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Yu, Botao Ren, Peiyuan Zhang, Mingxin Liu, Junwei Luo, Shaofeng Zhang, Feipeng Da, Junchi Yan, Xue Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04268">Point2RBox-v2: Rethinking Point-supervised Oriented Object Detection with Spatial Layout Among Instances</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapidly increasing demand for oriented object detection (OOD), recent research involving weakly-supervised detectors for learning OOD from point annotations has gained great attention. In this paper, we rethink this challenging task setting with the layout among instances and present Point2RBox-v2. At the core are three principles: 1) Gaussian overlap loss. It learns an upper bound for each instance by treating objects as 2D Gaussian distributions and minimizing their overlap. 2) Voronoi watershed loss. It learns a lower bound for each instance through watershed on Voronoi tessellation. 3) Consistency loss. It learns the size/rotation variation between two output sets with respect to an input image and its augmented view. Supplemented by a few devised techniques, e.g. edge loss and copy-paste, the detector is further enhanced. To our best knowledge, Point2RBox-v2 is the first approach to explore the spatial layout among instances for learning point-supervised OOD. Our solution is elegant and lightweight, yet it is expected to give a competitive performance especially in densely packed scenes: 62.61%/86.15%/34.71% on DOTA/HRSC/FAIR1M. Code is available at https://github.com/VisionXLab/point2rbox-v2.
<div id='section'>Paperid: <span id='pid'>90, <a href='https://arxiv.org/pdf/2502.03118.pdf' target='_blank'>https://arxiv.org/pdf/2502.03118.pdf</a></span>   <span><a href='https://github.com/yanwenCi/Tell2Reg.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wen Yan, Qianye Yang, Shiqi Huang, Yipei Wang, Shonit Punwani, Mark Emberton, Vasilis Stavrinides, Yipeng Hu, Dean Barratt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03118">Tell2Reg: Establishing spatial correspondence between images by the same language prompts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spatial correspondence can be represented by pairs of segmented regions, such that the image registration networks aim to segment corresponding regions rather than predicting displacement fields or transformation parameters. In this work, we show that such a corresponding region pair can be predicted by the same language prompt on two different images using the pre-trained large multimodal models based on GroundingDINO and SAM. This enables a fully automated and training-free registration algorithm, potentially generalisable to a wide range of image registration tasks. In this paper, we present experimental results using one of the challenging tasks, registering inter-subject prostate MR images, which involves both highly variable intensity and morphology between patients. Tell2Reg is training-free, eliminating the need for costly and time-consuming data curation and labelling that was previously required for this registration task. This approach outperforms unsupervised learning-based registration methods tested, and has a performance comparable to weakly-supervised methods. Additional qualitative results are also presented to suggest that, for the first time, there is a potential correlation between language semantics and spatial correspondence, including the spatial invariance in language-prompted regions and the difference in language prompts between the obtained local and global correspondences. Code is available at https://github.com/yanwenCi/Tell2Reg.git.
<div id='section'>Paperid: <span id='pid'>91, <a href='https://arxiv.org/pdf/2502.00240.pdf' target='_blank'>https://arxiv.org/pdf/2502.00240.pdf</a></span>   <span><a href='https://github.com/YasminZhang/ADCR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yasi Zhang, Oscar Leong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.00240">Learning Difference-of-Convex Regularizers for Inverse Problems: A Flexible Framework with Theoretical Guarantees</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning effective regularization is crucial for solving ill-posed inverse problems, which arise in a wide range of scientific and engineering applications. While data-driven methods that parameterize regularizers using deep neural networks have demonstrated strong empirical performance, they often result in highly nonconvex formulations that lack theoretical guarantees. Recent work has shown that incorporating structured nonconvexity into neural network-based regularizers, such as weak convexity, can strike a balance between empirical performance and theoretical tractability. In this paper, we demonstrate that a broader class of nonconvex functions, difference-of-convex (DC) functions, can yield improved empirical performance while retaining strong convergence guarantees. The DC structure enables the use of well-established optimization algorithms, such as the Difference-of-Convex Algorithm (DCA) and a Proximal Subgradient Method (PSM), which extend beyond standard gradient descent. Furthermore, we provide theoretical insights into the conditions under which optimal regularizers can be expressed as DC functions. Extensive experiments on computed tomography (CT) reconstruction tasks show that our approach achieves strong performance across sparse and limited-view settings, consistently outperforming other weakly supervised learned regularizers. Our code is available at \url{https://github.com/YasminZhang/ADCR}.
<div id='section'>Paperid: <span id='pid'>92, <a href='https://arxiv.org/pdf/2501.17053.pdf' target='_blank'>https://arxiv.org/pdf/2501.17053.pdf</a></span>   <span><a href='https://akash2907.github.io/cospal_webpage' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Akash Kumar, Zsolt Kira, Yogesh Singh Rawat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.17053">Contextual Self-paced Learning for Weakly Supervised Spatio-Temporal Video Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we focus on Weakly Supervised Spatio-Temporal Video Grounding (WSTVG). It is a multimodal task aimed at localizing specific subjects spatio-temporally based on textual queries without bounding box supervision. Motivated by recent advancements in multi-modal foundation models for grounding tasks, we first explore the potential of state-of-the-art object detection models for WSTVG. Despite their robust zero-shot capabilities, our adaptation reveals significant limitations, including inconsistent temporal predictions, inadequate understanding of complex queries, and challenges in adapting to difficult scenarios. We propose CoSPaL (Contextual Self-Paced Learning), a novel approach which is designed to overcome these limitations. CoSPaL integrates three core components: (1) Tubelet Phrase Grounding (TPG), which introduces spatio-temporal prediction by linking textual queries to tubelets; (2) Contextual Referral Grounding (CRG), which improves comprehension of complex queries by extracting contextual information to refine object identification over time; and (3) Self-Paced Scene Understanding (SPS), a training paradigm that progressively increases task difficulty, enabling the model to adapt to complex scenarios by transitioning from coarse to fine-grained understanding.
<div id='section'>Paperid: <span id='pid'>93, <a href='https://arxiv.org/pdf/2501.15326.pdf' target='_blank'>https://arxiv.org/pdf/2501.15326.pdf</a></span>   <span><a href='https://ntlm1686.github.io/raso' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiajie Li, Brian R Quaranto, Chenhui Xu, Ishan Mishra, Ruiyang Qin, Dancheng Liu, Peter C W Kim, Jinjun Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15326">Recognize Any Surgical Object: Unleashing the Power of Weakly-Supervised Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present RASO, a foundation model designed to Recognize Any Surgical Object, offering robust open-set recognition capabilities across a broad range of surgical procedures and object classes, in both surgical images and videos. RASO leverages a novel weakly-supervised learning framework that generates tag-image-text pairs automatically from large-scale unannotated surgical lecture videos, significantly reducing the need for manual annotations. Our scalable data generation pipeline gathers 2,200 surgical procedures and produces 3.6 million tag annotations across 2,066 unique surgical tags. Our experiments show that RASO achieves improvements of 2.9 mAP, 4.5 mAP, 10.6 mAP, and 7.2 mAP on four standard surgical benchmarks, respectively, in zero-shot settings, and surpasses state-of-the-art models in supervised surgical action recognition tasks. Code, model, and demo are available at https://ntlm1686.github.io/raso.
<div id='section'>Paperid: <span id='pid'>94, <a href='https://arxiv.org/pdf/2501.13426.pdf' target='_blank'>https://arxiv.org/pdf/2501.13426.pdf</a></span>   <span><a href='https://github.com/zxk688' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jian Wang, Xiaokang Zhang, Xianping Ma, Weikang Yu, Pedram Ghamisi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13426">Auto-Prompting SAM for Weakly Supervised Landslide Extraction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised landslide extraction aims to identify landslide regions from remote sensing data using models trained with weak labels, particularly image-level labels. However, it is often challenged by the imprecise boundaries of the extracted objects due to the lack of pixel-wise supervision and the properties of landslide objects. To tackle these issues, we propose a simple yet effective method by auto-prompting the Segment Anything Model (SAM), i.e., APSAM. Instead of depending on high-quality class activation maps (CAMs) for pseudo-labeling or fine-tuning SAM, our method directly yields fine-grained segmentation masks from SAM inference through prompt engineering. Specifically, it adaptively generates hybrid prompts from the CAMs obtained by an object localization network. To provide sufficient information for SAM prompting, an adaptive prompt generation (APG) algorithm is designed to fully leverage the visual patterns of CAMs, enabling the efficient generation of pseudo-masks for landslide extraction. These informative prompts are able to identify the extent of landslide areas (box prompts) and denote the centers of landslide objects (point prompts), guiding SAM in landslide segmentation. Experimental results on high-resolution aerial and satellite datasets demonstrate the effectiveness of our method, achieving improvements of at least 3.0\% in F1 score and 3.69\% in IoU compared to other state-of-the-art methods. The source codes and datasets will be available at https://github.com/zxk688.
<div id='section'>Paperid: <span id='pid'>95, <a href='https://arxiv.org/pdf/2501.07496.pdf' target='_blank'>https://arxiv.org/pdf/2501.07496.pdf</a></span>   <span><a href='https://github.com/xjpp2016/MAVD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenping Jin, Li Zhu, Jing Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07496">Aligning First, Then Fusing: A Novel Weakly Supervised Multimodal Violence Detection Method</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised violence detection refers to the technique of training models to identify violent segments in videos using only video-level labels. Among these approaches, multimodal violence detection, which integrates modalities such as audio and optical flow, holds great potential. Existing methods in this domain primarily focus on designing multimodal fusion models to address modality discrepancies. In contrast, we take a different approach; leveraging the inherent discrepancies across modalities in violence event representation to propose a novel multimodal semantic feature alignment method. This method sparsely maps the semantic features of local, transient, and less informative modalities ( such as audio and optical flow ) into the more informative RGB semantic feature space. Through an iterative process, the method identifies the suitable no-zero feature matching subspace and aligns the modality-specific event representations based on this subspace, enabling the full exploitation of information from all modalities during the subsequent modality fusion stage. Building on this, we design a new weakly supervised violence detection framework that consists of unimodal multiple-instance learning for extracting unimodal semantic features, multimodal alignment, multimodal fusion, and final detection. Experimental results on benchmark datasets demonstrate the effectiveness of our method, achieving an average precision (AP) of 86.07% on the XD-Violence dataset. Our code is available at https://github.com/xjpp2016/MAVD.
<div id='section'>Paperid: <span id='pid'>96, <a href='https://arxiv.org/pdf/2501.04934.pdf' target='_blank'>https://arxiv.org/pdf/2501.04934.pdf</a></span>   <span><a href='https://github.com/zhenghuizhao/Plug-and-Play-DISep-for-Change-Detection' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenghui Zhao, Chen Wu, Lixiang Ru, Di Wang, Hongruixuan Chen, Cuiqun Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04934">Plug-and-Play DISep: Separating Dense Instances for Scene-to-Pixel Weakly-Supervised Change Detection in High-Resolution Remote Sensing Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing Weakly-Supervised Change Detection (WSCD) methods often encounter the problem of "instance lumping" under scene-level supervision, particularly in scenarios with a dense distribution of changed instances (i.e., changed objects). In these scenarios, unchanged pixels between changed instances are also mistakenly identified as changed, causing multiple changes to be mistakenly viewed as one. In practical applications, this issue prevents the accurate quantification of the number of changes. To address this issue, we propose a Dense Instance Separation (DISep) method as a plug-and-play solution, refining pixel features from a unified instance perspective under scene-level supervision. Specifically, our DISep comprises a three-step iterative training process: 1) Instance Localization: We locate instance candidate regions for changed pixels using high-pass class activation maps. 2) Instance Retrieval: We identify and group these changed pixels into different instance IDs through connectivity searching. Then, based on the assigned instance IDs, we extract corresponding pixel-level features on a per-instance basis. 3) Instance Separation: We introduce a separation loss to enforce intra-instance pixel consistency in the embedding space, thereby ensuring separable instance feature representations. The proposed DISep adds only minimal training cost and no inference cost. It can be seamlessly integrated to enhance existing WSCD methods. We achieve state-of-the-art performance by enhancing {three Transformer-based and four ConvNet-based methods} on the LEVIR-CD, WHU-CD, DSIFN-CD, SYSU-CD, and CDD datasets. Additionally, our DISep can be used to improve fully-supervised change detection methods. Code is available at https://github.com/zhenghuizhao/Plug-and-Play-DISep-for-Change-Detection.
<div id='section'>Paperid: <span id='pid'>97, <a href='https://arxiv.org/pdf/2501.04440.pdf' target='_blank'>https://arxiv.org/pdf/2501.04440.pdf</a></span>   <span><a href='https://github.com/zhasion/RSAR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Zhang, Xue Yang, Yuxuan Li, Jian Yang, Ming-Ming Cheng, Xiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04440">RSAR: Restricted State Angle Resolver and Rotated SAR Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rotated object detection has made significant progress in the optical remote sensing. However, advancements in the Synthetic Aperture Radar (SAR) field are laggard behind, primarily due to the absence of a large-scale dataset. Annotating such a dataset is inefficient and costly. A promising solution is to employ a weakly supervised model (e.g., trained with available horizontal boxes only) to generate pseudo-rotated boxes for reference before manual calibration. Unfortunately, the existing weakly supervised models exhibit limited accuracy in predicting the object's angle. Previous works attempt to enhance angle prediction by using angle resolvers that decouple angles into cosine and sine encodings. In this work, we first reevaluate these resolvers from a unified perspective of dimension mapping and expose that they share the same shortcomings: these methods overlook the unit cycle constraint inherent in these encodings, easily leading to prediction biases. To address this issue, we propose the Unit Cycle Resolver, which incorporates a unit circle constraint loss to improve angle prediction accuracy. Our approach can effectively improve the performance of existing state-of-the-art weakly supervised methods and even surpasses fully supervised models on existing optical benchmarks (i.e., DOTA-v1.0 dataset). With the aid of UCR, we further annotate and introduce RSAR, the largest multi-class rotated SAR object detection dataset to date. Extensive experiments on both RSAR and optical datasets demonstrate that our UCR enhances angle prediction accuracy. Our dataset and code can be found at: https://github.com/zhasion/RSAR.
<div id='section'>Paperid: <span id='pid'>98, <a href='https://arxiv.org/pdf/2412.20924.pdf' target='_blank'>https://arxiv.org/pdf/2412.20924.pdf</a></span>   <span><a href='https://github.com/Vison307/HisynSeg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijie Fang, Yifeng Wang, Peizhang Xie, Zhi Wang, Yongbing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20924">HisynSeg: Weakly-Supervised Histopathological Image Segmentation via Image-Mixing Synthesis and Consistency Regularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tissue semantic segmentation is one of the key tasks in computational pathology. To avoid the expensive and laborious acquisition of pixel-level annotations, a wide range of studies attempt to adopt the class activation map (CAM), a weakly-supervised learning scheme, to achieve pixel-level tissue segmentation. However, CAM-based methods are prone to suffer from under-activation and over-activation issues, leading to poor segmentation performance. To address this problem, we propose a novel weakly-supervised semantic segmentation framework for histopathological images based on image-mixing synthesis and consistency regularization, dubbed HisynSeg. Specifically, synthesized histopathological images with pixel-level masks are generated for fully-supervised model training, where two synthesis strategies are proposed based on Mosaic transformation and BÃ©zier mask generation. Besides, an image filtering module is developed to guarantee the authenticity of the synthesized images. In order to further avoid the model overfitting to the occasional synthesis artifacts, we additionally propose a novel self-supervised consistency regularization, which enables the real images without segmentation masks to supervise the training of the segmentation model. By integrating the proposed techniques, the HisynSeg framework successfully transforms the weakly-supervised semantic segmentation problem into a fully-supervised one, greatly improving the segmentation accuracy. Experimental results on three datasets prove that the proposed method achieves a state-of-the-art performance. Code is available at https://github.com/Vison307/HisynSeg.
<div id='section'>Paperid: <span id='pid'>99, <a href='https://arxiv.org/pdf/2412.19418.pdf' target='_blank'>https://arxiv.org/pdf/2412.19418.pdf</a></span>   <span><a href='https://github.com/heyuanpengpku/GUEF/tree/main' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanpeng He, Lijian Li, Tianxiang Zhan, Wenpin Jiao, Chi-Man Pun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19418">Generalized Uncertainty-Based Evidential Fusion with Hybrid Multi-Head Attention for Weak-Supervised Temporal Action Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised temporal action localization (WS-TAL) is a task of targeting at localizing complete action instances and categorizing them with video-level labels. Action-background ambiguity, primarily caused by background noise resulting from aggregation and intra-action variation, is a significant challenge for existing WS-TAL methods. In this paper, we introduce a hybrid multi-head attention (HMHA) module and generalized uncertainty-based evidential fusion (GUEF) module to address the problem. The proposed HMHA effectively enhances RGB and optical flow features by filtering redundant information and adjusting their feature distribution to better align with the WS-TAL task. Additionally, the proposed GUEF adaptively eliminates the interference of background noise by fusing snippet-level evidences to refine uncertainty measurement and select superior foreground feature information, which enables the model to concentrate on integral action instances to achieve better action localization and classification performance. Experimental results conducted on the THUMOS14 dataset demonstrate that our method outperforms state-of-the-art methods. Our code is available in \url{https://github.com/heyuanpengpku/GUEF/tree/main}.
<div id='section'>Paperid: <span id='pid'>100, <a href='https://arxiv.org/pdf/2412.18738.pdf' target='_blank'>https://arxiv.org/pdf/2412.18738.pdf</a></span>   <span><a href='https://github.com/IPMI-NWU/HELPNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Zhang, Shaoxuan Wu, Peilin Zhang, Zhuo Jin, Xiaosong Xiong, Qirong Bu, Jingkun Chen, Jun Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.18738">HELPNet: Hierarchical Perturbations Consistency and Entropy-guided Ensemble for Scribble Supervised Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating fully annotated labels for medical image segmentation is prohibitively time-intensive and costly, emphasizing the necessity for innovative approaches that minimize reliance on detailed annotations. Scribble annotations offer a more cost-effective alternative, significantly reducing the expenses associated with full annotations. However, scribble annotations offer limited and imprecise information, failing to capture the detailed structural and boundary characteristics necessary for accurate organ delineation. To address these challenges, we propose HELPNet, a novel scribble-based weakly supervised segmentation framework, designed to bridge the gap between annotation efficiency and segmentation performance. HELPNet integrates three modules. The Hierarchical perturbations consistency (HPC) module enhances feature learning by employing density-controlled jigsaw perturbations across global, local, and focal views, enabling robust modeling of multi-scale structural representations. Building on this, the Entropy-guided pseudo-label (EGPL) module evaluates the confidence of segmentation predictions using entropy, generating high-quality pseudo-labels. Finally, the structural prior refinement (SPR) module incorporates connectivity and bounded priors to enhance the precision and reliability and pseudo-labels. Experimental results on three public datasets ACDC, MSCMRseg, and CHAOS show that HELPNet significantly outperforms state-of-the-art methods for scribble-based weakly supervised segmentation and achieves performance comparable to fully supervised methods. The code is available at https://github.com/IPMI-NWU/HELPNet.
<div id='section'>Paperid: <span id='pid'>101, <a href='https://arxiv.org/pdf/2412.17601.pdf' target='_blank'>https://arxiv.org/pdf/2412.17601.pdf</a></span>   <span><a href='https://github.com/jarch-ma/AFANet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Ma, Guo-Sen Xie, Fang Zhao, Zechao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.17601">AFANet: Adaptive Frequency-Aware Network for Weakly-Supervised Few-Shot Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Few-shot learning aims to recognize novel concepts by leveraging prior knowledge learned from a few samples. However, for visually intensive tasks such as few-shot semantic segmentation, pixel-level annotations are time-consuming and costly. Therefore, in this paper, we utilize the more challenging image-level annotations and propose an adaptive frequency-aware network (AFANet) for weakly-supervised few-shot semantic segmentation (WFSS). Specifically, we first propose a cross-granularity frequency-aware module (CFM) that decouples RGB images into high-frequency and low-frequency distributions and further optimizes semantic structural information by realigning them. Unlike most existing WFSS methods using the textual information from the multi-modal language-vision model, e.g., CLIP, in an offline learning manner, we further propose a CLIP-guided spatial-adapter module (CSM), which performs spatial domain adaptive transformation on textual information through online learning, thus providing enriched cross-modal semantic information for CFM. Extensive experiments on the Pascal-5\textsuperscript{i} and COCO-20\textsuperscript{i} datasets demonstrate that AFANet has achieved state-of-the-art performance. The code is available at https://github.com/jarch-ma/AFANet.
<div id='section'>Paperid: <span id='pid'>102, <a href='https://arxiv.org/pdf/2412.11076.pdf' target='_blank'>https://arxiv.org/pdf/2412.11076.pdf</a></span>   <span><a href='https://github.com/zwyang6/MoRe' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiwei Yang, Yucong Meng, Kexue Fu, Shuo Wang, Zhijian Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11076">MoRe: Class Patch Attention Needs Regularization for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Semantic Segmentation (WSSS) with image-level labels typically uses Class Activation Maps (CAM) to achieve dense predictions. Recently, Vision Transformer (ViT) has provided an alternative to generate localization maps from class-patch attention. However, due to insufficient constraints on modeling such attention, we observe that the Localization Attention Maps (LAM) often struggle with the artifact issue, i.e., patch regions with minimal semantic relevance are falsely activated by class tokens. In this work, we propose MoRe to address this issue and further explore the potential of LAM. Our findings suggest that imposing additional regularization on class-patch attention is necessary. To this end, we first view the attention as a novel directed graph and propose the Graph Category Representation module to implicitly regularize the interaction among class-patch entities. It ensures that class tokens dynamically condense the related patch information and suppress unrelated artifacts at a graph level. Second, motivated by the observation that CAM from classification weights maintains smooth localization of objects, we devise the Localization-informed Regularization module to explicitly regularize the class-patch attention. It directly mines the token relations from CAM and further supervises the consistency between class and patch tokens in a learnable manner. Extensive experiments are conducted on PASCAL VOC and MS COCO, validating that MoRe effectively addresses the artifact issue and achieves state-of-the-art performance, surpassing recent single-stage and even multi-stage methods. Code is available at https://github.com/zwyang6/MoRe.
<div id='section'>Paperid: <span id='pid'>103, <a href='https://arxiv.org/pdf/2412.06286.pdf' target='_blank'>https://arxiv.org/pdf/2412.06286.pdf</a></span>   <span><a href='https://github.com/patrick-john-ramos/nada' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Patrick Ramos, Nicolas Gonthier, Selina Khan, Yuta Nakashima, Noa Garcia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.06286">No Annotations for Object Detection in Art through Stable Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object detection in art is a valuable tool for the digital humanities, as it allows for faster identification of objects in artistic and historical images compared to humans. However, annotating such images poses significant challenges due to the need for specialized domain expertise. We present NADA (no annotations for detection in art), a pipeline that leverages diffusion models' art-related knowledge for object detection in paintings without the need for full bounding box supervision. Our method, which supports both weakly-supervised and zero-shot scenarios and does not require any fine-tuning of its pretrained components, consists of a class proposer based on large vision-language models and a class-conditioned detector based on Stable Diffusion. NADA is evaluated on two artwork datasets, ArtDL 2.0 and IconArt, outperforming prior work in weakly-supervised detection, while being the first work for zero-shot object detection in art. Code is available at https://github.com/patrick-john-ramos/nada
<div id='section'>Paperid: <span id='pid'>104, <a href='https://arxiv.org/pdf/2412.05876.pdf' target='_blank'>https://arxiv.org/pdf/2412.05876.pdf</a></span>   <span><a href='https://github.com/Xuefeng-Ni/MG-3D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuefeng Ni, Linshan Wu, Jiaxin Zhuang, Qiong Wang, Mingxiang Wu, Varut Vardhanabhuti, Lihai Zhang, Hanyu Gao, Hao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05876">MG-3D: Multi-Grained Knowledge-Enhanced 3D Medical Vision-Language Pre-training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D medical image analysis is pivotal in numerous clinical applications. However, the scarcity of labeled data and limited generalization capabilities hinder the advancement of AI-empowered models. Radiology reports are easily accessible and can serve as weakly-supervised signals. However, large-scale vision-language pre-training (VLP) remains underexplored in 3D medical image analysis. Specifically, the insufficient investigation into multi-grained radiology semantics and their correlations across patients leads to underutilization of large-scale volume-report data.
  Considering intra-patient cross-modal semantic consistency and inter-patient semantic correlations, we propose a multi-task VLP method, MG-3D, pre-trained on large-scale data (47.1K), addressing the challenges by the following two aspects: 1) Establishing the correspondence between volume semantics and multi-grained medical knowledge of each patient with cross-modal global alignment and complementary modality-guided local reconstruction, ensuring intra-patient features of different modalities cohesively represent the same semantic content; 2) Correlating inter-patient visual semantics based on fine-grained report correlations across patients, and keeping sensitivity to global individual differences via contrastive learning, enhancing the discriminative feature representation. Furthermore, we delve into the scaling law to explore potential performance improvements. Comprehensive evaluations across nine uni- and cross-modal clinical tasks are carried out to assess model efficacy. Extensive experiments on both internal and external datasets demonstrate the superior transferability, scalability, and generalization of MG-3D, showcasing its potential in advancing feature representation for 3D medical image analysis. Code will be available: https://github.com/Xuefeng-Ni/MG-3D.
<div id='section'>Paperid: <span id='pid'>105, <a href='https://arxiv.org/pdf/2412.04473.pdf' target='_blank'>https://arxiv.org/pdf/2412.04473.pdf</a></span>   <span><a href='https://github.com/woshixiaobai2019/nids-gpt.gi' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04473">Take Package as Language: Anomaly Detection Using Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Network data packet anomaly detection faces numerous challenges, including exploring new anomaly supervision signals, researching weakly supervised anomaly detection, and improving model interpretability. This paper proposes NIDS-GPT, a GPT-based causal language model for network intrusion detection. Unlike previous work, NIDS-GPT innovatively treats each number in the packet as an independent "word" rather than packet fields, enabling a more fine-grained data representation. We adopt an improved GPT-2 model and design special tokenizers and embedding layers to better capture the structure and semantics of network data. NIDS-GPT has good scalability, supports unsupervised pre-training, and enhances model interpretability through attention weight visualization. Experiments on the CICIDS2017 and car-hacking datasets show that NIDS-GPT achieves 100\% accuracy under extreme imbalance conditions, far surpassing traditional methods; it also achieves over 90\% accuracy in one-shot learning. These results demonstrate NIDS-GPT's excellent performance and potential in handling complex network anomaly detection tasks, especially in data-imbalanced and resource-constrained scenarios. The code is available at \url{https://github.com/woshixiaobai2019/nids-gpt.gi
<div id='section'>Paperid: <span id='pid'>106, <a href='https://arxiv.org/pdf/2412.03968.pdf' target='_blank'>https://arxiv.org/pdf/2412.03968.pdf</a></span>   <span><a href='https://github.com/MiSsU-HH/Exact' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Zhu, Yan Zhu, Jiayu Xiao, Tianxiang Xiao, Yike Ma, Yucheng Zhang, Feng Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.03968">Exact: Exploring Space-Time Perceptive Clues for Weakly Supervised Satellite Image Time Series Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automated crop mapping through Satellite Image Time Series (SITS) has emerged as a crucial avenue for agricultural monitoring and management. However, due to the low resolution and unclear parcel boundaries, annotating pixel-level masks is exceptionally complex and time-consuming in SITS. This paper embraces the weakly supervised paradigm (i.e., only image-level categories available) to liberate the crop mapping task from the exhaustive annotation burden. The unique characteristics of SITS give rise to several challenges in weakly supervised learning: (1) noise perturbation from spatially neighboring regions, and (2) erroneous semantic bias from anomalous temporal periods. To address the above difficulties, we propose a novel method, termed exploring space-time perceptive clues (Exact). First, we introduce a set of spatial clues to explicitly capture the representative patterns of different crops from the most class-relative regions. Besides, we leverage the temporal-to-class interaction of the model to emphasize the contributions of pivotal clips, thereby enhancing the model perception for crop regions. Build upon the space-time perceptive clues, we derive the clue-based CAMs to effectively supervise the SITS segmentation network. Our method demonstrates impressive performance on various SITS benchmarks. Remarkably, the segmentation network trained on Exact-generated masks achieves 95% of its fully supervised performance, showing the bright promise of weakly supervised paradigm in crop mapping scenario. Our code will be publicly available.
<div id='section'>Paperid: <span id='pid'>107, <a href='https://arxiv.org/pdf/2412.02012.pdf' target='_blank'>https://arxiv.org/pdf/2412.02012.pdf</a></span>   <span><a href='https://zhangdylan83.github.io/ewsmia/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenbo Zhang, Junyu Chen, Christopher Kanan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02012">INSIGHT: Explainable Weakly-Supervised Medical Image Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to their large sizes, volumetric scans and whole-slide pathology images (WSIs) are often processed by extracting embeddings from local regions and then an aggregator makes predictions from this set. However, current methods require post-hoc visualization techniques (e.g., Grad-CAM) and often fail to localize small yet clinically crucial details. To address these limitations, we introduce INSIGHT, a novel weakly-supervised aggregator that integrates heatmap generation as an inductive bias. Starting from pre-trained feature maps, INSIGHT employs a detection module with small convolutional kernels to capture fine details and a context module with a broader receptive field to suppress local false positives. The resulting internal heatmap highlights diagnostically relevant regions. On CT and WSI benchmarks, INSIGHT achieves state-of-the-art classification results and high weakly-labeled semantic segmentation performance. Project website and code are available at: https://zhangdylan83.github.io/ewsmia/
<div id='section'>Paperid: <span id='pid'>108, <a href='https://arxiv.org/pdf/2411.19067.pdf' target='_blank'>https://arxiv.org/pdf/2411.19067.pdf</a></span>   <span><a href='https://github.com/naver-ai/maskris' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Minhyun Lee, Seungho Lee, Song Park, Dongyoon Han, Byeongho Heo, Hyunjung Shim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19067">MaskRIS: Semantic Distortion-aware Data Augmentation for Referring Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring Image Segmentation (RIS) is an advanced vision-language task that involves identifying and segmenting objects within an image as described by free-form text descriptions. While previous studies focused on aligning visual and language features, exploring training techniques, such as data augmentation, remains underexplored. In this work, we explore effective data augmentation for RIS and propose a novel training framework called Masked Referring Image Segmentation (MaskRIS). We observe that the conventional image augmentations fall short of RIS, leading to performance degradation, while simple random masking significantly enhances the performance of RIS. MaskRIS uses both image and text masking, followed by Distortion-aware Contextual Learning (DCL) to fully exploit the benefits of the masking strategy. This approach can improve the model's robustness to occlusions, incomplete information, and various linguistic complexities, resulting in a significant performance improvement. Experiments demonstrate that MaskRIS can easily be applied to various RIS models, outperforming existing methods in both fully supervised and weakly supervised settings. Finally, MaskRIS achieves new state-of-the-art performance on RefCOCO, RefCOCO+, and RefCOCOg datasets. Code is available at https://github.com/naver-ai/maskris.
<div id='section'>Paperid: <span id='pid'>109, <a href='https://arxiv.org/pdf/2411.17376.pdf' target='_blank'>https://arxiv.org/pdf/2411.17376.pdf</a></span>   <span><a href='https://fujiry0.github.io/RealTraj-project-page' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryo Fujii, Hideo Saito, Ryo Hachiuma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17376">RealTraj: Towards Real-World Pedestrian Trajectory Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper jointly addresses three key limitations in conventional pedestrian trajectory forecasting: pedestrian perception errors, real-world data collection costs, and person ID annotation costs. We propose a novel framework, RealTraj, that enhances the real-world applicability of trajectory forecasting. Our approach includes two training phases -- self-supervised pretraining on synthetic data and weakly-supervised fine-tuning with limited real-world data -- to minimize data collection efforts. To improve robustness to real-world errors, we focus on both model design and training objectives. Specifically, we present Det2TrajFormer, a trajectory forecasting model that remains invariant to tracking noise by using past detections as inputs. Additionally, we pretrain the model using multiple pretext tasks, which enhance robustness and improve forecasting performance based solely on detection data. Unlike previous trajectory forecasting methods, our approach fine-tunes the model using only ground-truth detections, reducing the need for costly person ID annotations. In the experiments, we comprehensively verify the effectiveness of the proposed method against the limitations, and the method outperforms state-of-the-art trajectory forecasting methods on multiple datasets. The code will be released at https://fujiry0.github.io/RealTraj-project-page.
<div id='section'>Paperid: <span id='pid'>110, <a href='https://arxiv.org/pdf/2411.16219.pdf' target='_blank'>https://arxiv.org/pdf/2411.16219.pdf</a></span>   <span><a href='https://github.com/manuelknott/banana-defect-segmentation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Manuel Knott, Divinefavour Odion, Sameer Sontakke, Anup Karwa, Thijs Defraeye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16219">Weakly Supervised Panoptic Segmentation for Defect-Based Grading of Fresh Produce</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual inspection for defect grading in agricultural supply chains is crucial but traditionally labor-intensive and error-prone. Automated computer vision methods typically require extensively annotated datasets, which are often unavailable in decentralized supply chains. We address this challenge by evaluating the Segment Anything Model (SAM) to generate dense panoptic segmentation masks from sparse annotations. These dense predictions are then used to train a supervised panoptic segmentation model. Focusing on banana surface defects (bruises and scars), we validate our approach using 476 field images annotated with 1440 defects. While SAM-generated masks generally align with human annotations, substantially reducing annotation effort, we explicitly identify failure cases associated with specific defect sizes and shapes. Despite these limitations, our approach offers practical estimates of defect number and relative size from panoptic masks, underscoring the potential and current boundaries of foundation models for defect quantification in low-data agricultural scenarios. GitHub: https://github.com/manuelknott/banana-defect-segmentation
<div id='section'>Paperid: <span id='pid'>111, <a href='https://arxiv.org/pdf/2411.15763.pdf' target='_blank'>https://arxiv.org/pdf/2411.15763.pdf</a></span>   <span><a href='https://github.com/arvindmvepa/al-seg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Arvind Murari Vepa, Zukang Yang, Andrew Choi, Jungseock Joo, Fabien Scalzo, Yizhou Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15763">Integrating Deep Metric Learning with Coreset for Active Learning in 3D Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning has seen remarkable advancements in machine learning, yet it often demands extensive annotated data. Tasks like 3D semantic segmentation impose a substantial annotation burden, especially in domains like medicine, where expert annotations drive up the cost. Active learning (AL) holds great potential to alleviate this annotation burden in 3D medical segmentation. The majority of existing AL methods, however, are not tailored to the medical domain. While weakly-supervised methods have been explored to reduce annotation burden, the fusion of AL with weak supervision remains unexplored, despite its potential to significantly reduce annotation costs. Additionally, there is little focus on slice-based AL for 3D segmentation, which can also significantly reduce costs in comparison to conventional volume-based AL. This paper introduces a novel metric learning method for Coreset to perform slice-based active learning in 3D medical segmentation. By merging contrastive learning with inherent data groupings in medical imaging, we learn a metric that emphasizes the relevant differences in samples for training 3D medical segmentation models. We perform comprehensive evaluations using both weak and full annotations across four datasets (medical and non-medical). Our findings demonstrate that our approach surpasses existing active learning techniques on both weak and full annotations and obtains superior performance with low-annotation budgets which is crucial in medical imaging. Source code for this project is available in the supplementary materials and on GitHub: https://github.com/arvindmvepa/al-seg.
<div id='section'>Paperid: <span id='pid'>112, <a href='https://arxiv.org/pdf/2411.14429.pdf' target='_blank'>https://arxiv.org/pdf/2411.14429.pdf</a></span>   <span><a href='https://github.com/rayleizhu/GLMix' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Zhu, Xinjiang Wang, Wayne Zhang, Rynson W. H. Lau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.14429">Revisiting the Integration of Convolution and Attention for Vision Backbone</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Convolutions (Convs) and multi-head self-attentions (MHSAs) are typically considered alternatives to each other for building vision backbones. Although some works try to integrate both, they apply the two operators simultaneously at the finest pixel granularity. With Convs responsible for per-pixel feature extraction already, the question is whether we still need to include the heavy MHSAs at such a fine-grained level. In fact, this is the root cause of the scalability issue w.r.t. the input resolution for vision transformers. To address this important problem, we propose in this work to use MSHAs and Convs in parallel \textbf{at different granularity levels} instead. Specifically, in each layer, we use two different ways to represent an image: a fine-grained regular grid and a coarse-grained set of semantic slots. We apply different operations to these two representations: Convs to the grid for local features, and MHSAs to the slots for global features. A pair of fully differentiable soft clustering and dispatching modules is introduced to bridge the grid and set representations, thus enabling local-global fusion. Through extensive experiments on various vision tasks, we empirically verify the potential of the proposed integration scheme, named \textit{GLMix}: by offloading the burden of fine-grained features to light-weight Convs, it is sufficient to use MHSAs in a few (e.g., 64) semantic slots to match the performance of recent state-of-the-art backbones, while being more efficient. Our visualization results also demonstrate that the soft clustering module produces a meaningful semantic grouping effect with only IN1k classification supervision, which may induce better interpretability and inspire new weakly-supervised semantic segmentation approaches. Code will be available at \url{https://github.com/rayleizhu/GLMix}.
<div id='section'>Paperid: <span id='pid'>113, <a href='https://arxiv.org/pdf/2411.10364.pdf' target='_blank'>https://arxiv.org/pdf/2411.10364.pdf</a></span>   <span><a href='https://github.com/TianhaoMa5/LLP-AHIL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianhao Ma, Han Chen, Juncheng Hu, Yungang Zhu, Ximing Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10364">Forming Auxiliary High-confident Instance-level Loss to Promote Learning from Label Proportions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning from label proportions (LLP), i.e., a challenging weakly-supervised learning task, aims to train a classifier by using bags of instances and the proportions of classes within bags, rather than annotated labels for each instance. Beyond the traditional bag-level loss, the mainstream methodology of LLP is to incorporate an auxiliary instance-level loss with pseudo-labels formed by predictions. Unfortunately, we empirically observed that the pseudo-labels are are often inaccurate due to over-smoothing, especially for the scenarios with large bag sizes, hurting the classifier induction. To alleviate this problem, we suggest a novel LLP method, namely Learning from Label Proportions with Auxiliary High-confident Instance-level Loss (L^2P-AHIL). Specifically, we propose a dual entropy-based weight (DEW) method to adaptively measure the confidences of pseudo-labels. It simultaneously emphasizes accurate predictions at the bag level and avoids overly smoothed predictions. We then form high-confident instance-level loss with DEW, and jointly optimize it with the bag-level loss in a self-training manner. The experimental results on benchmark datasets show that L^2P-AHIL can surpass the existing baseline methods, and the performance gain can be more significant as the bag size increases. The implementation of our method is available at https://github.com/TianhaoMa5/LLP-AHIL.
<div id='section'>Paperid: <span id='pid'>114, <a href='https://arxiv.org/pdf/2411.06652.pdf' target='_blank'>https://arxiv.org/pdf/2411.06652.pdf</a></span>   <span><a href='https://github.com/liuzywen/LFScribble' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengyi Liu, Longzhen Wang, Xianyong Fang, Zhengzheng Tu, Linbo Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06652">LFSamba: Marry SAM with Mamba for Light Field Salient Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A light field camera can reconstruct 3D scenes using captured multi-focus images that contain rich spatial geometric information, enhancing applications in stereoscopic photography, virtual reality, and robotic vision. In this work, a state-of-the-art salient object detection model for multi-focus light field images, called LFSamba, is introduced to emphasize four main insights: (a) Efficient feature extraction, where SAM is used to extract modality-aware discriminative features; (b) Inter-slice relation modeling, leveraging Mamba to capture long-range dependencies across multiple focal slices, thus extracting implicit depth cues; (c) Inter-modal relation modeling, utilizing Mamba to integrate all-focus and multi-focus images, enabling mutual enhancement; (d) Weakly supervised learning capability, developing a scribble annotation dataset from an existing pixel-level mask dataset, establishing the first scribble-supervised baseline for light field salient object detection.https://github.com/liuzywen/LFScribble
<div id='section'>Paperid: <span id='pid'>115, <a href='https://arxiv.org/pdf/2410.14083.pdf' target='_blank'>https://arxiv.org/pdf/2410.14083.pdf</a></span>   <span><a href='https://github.com/sqhuang0103/SAMReg.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiqi Huang, Tingfa Xu, Ziyi Shen, Shaheer Ullah Saeed, Wen Yan, Dean Barratt, Yipeng Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14083">SAMReg: SAM-enabled Image Registration with ROI-based Correspondence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper describes a new spatial correspondence representation based on paired regions-of-interest (ROIs), for medical image registration. The distinct properties of the proposed ROI-based correspondence are discussed, in the context of potential benefits in clinical applications following image registration, compared with alternative correspondence-representing approaches, such as those based on sampled displacements and spatial transformation functions. These benefits include a clear connection between learning-based image registration and segmentation, which in turn motivates two cases of image registration approaches using (pre-)trained segmentation networks. Based on the segment anything model (SAM), a vision foundation model for segmentation, we develop a new registration algorithm SAMReg, which does not require any training (or training data), gradient-based fine-tuning or prompt engineering. The proposed SAMReg models are evaluated across five real-world applications, including intra-subject registration tasks with cardiac MR and lung CT, challenging inter-subject registration scenarios with prostate MR and retinal imaging, and an additional evaluation with a non-clinical example with aerial image registration. The proposed methods outperform both intensity-based iterative algorithms and DDF-predicting learning-based networks across tested metrics including Dice and target registration errors on anatomical structures, and further demonstrates competitive performance compared to weakly-supervised registration approaches that rely on fully-segmented training data. Open source code and examples are available at: https://github.com/sqhuang0103/SAMReg.git.
<div id='section'>Paperid: <span id='pid'>116, <a href='https://arxiv.org/pdf/2410.13621.pdf' target='_blank'>https://arxiv.org/pdf/2410.13621.pdf</a></span>   <span><a href='https://github.com/QI-NemoSong/EP-SAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Joonhyeon Song, Seohwan Yun, Seongho Yoon, Joohyeok Kim, Sangmin Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13621">EP-SAM: Weakly Supervised Histopathology Segmentation via Enhanced Prompt with Segment Anything</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work proposes a novel approach beyond supervised learning for effective pathological image analysis, addressing the challenge of limited robust labeled data. Pathological diagnosis of diseases like cancer has conventionally relied on the evaluation of morphological features by physicians and pathologists. However, recent advancements in compute-aided diagnosis (CAD) systems are gaining significant attention as diagnostic support tools. Although the advancement of deep learning has improved CAD significantly, segmentation models typically require large pixel-level annotated dataset, and such labeling is expensive. Existing studies not based on supervised approaches still struggle with limited generalization, and no practical approach has emerged yet. To address this issue, we present a weakly supervised semantic segmentation (WSSS) model by combining class activation map and Segment Anything Model (SAM)-based pseudo-labeling. For effective pretraining, we adopt the SAM-a foundation model that is pretrained on large datasets and operates in zero-shot configurations using only coarse prompts. The proposed approach transfer enhanced Attention Dropout Layer's knowledge to SAM, thereby generating pseudo-labels. To demonstrate the superiority of the proposed method, experimental studies are conducted on histopathological breast cancer datasets. The proposed method outperformed other WSSS methods across three datasets, demonstrating its efficiency by achieving this with only 12GB of GPU memory during training. Our code is available at : https://github.com/QI-NemoSong/EP-SAM
<div id='section'>Paperid: <span id='pid'>117, <a href='https://arxiv.org/pdf/2410.08509.pdf' target='_blank'>https://arxiv.org/pdf/2410.08509.pdf</a></span>   <span><a href='https://github.com/MoriLabNU/Bayesian_WSS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhou Zheng, Yuichiro Hayashi, Masahiro Oda, Takayuki Kitasaka, Kensaku Mori
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08509">A Bayesian Approach to Weakly-supervised Laparoscopic Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we study weakly-supervised laparoscopic image segmentation with sparse annotations. We introduce a novel Bayesian deep learning approach designed to enhance both the accuracy and interpretability of the model's segmentation, founded upon a comprehensive Bayesian framework, ensuring a robust and theoretically validated method. Our approach diverges from conventional methods that directly train using observed images and their corresponding weak annotations. Instead, we estimate the joint distribution of both images and labels given the acquired data. This facilitates the sampling of images and their high-quality pseudo-labels, enabling the training of a generalizable segmentation model. Each component of our model is expressed through probabilistic formulations, providing a coherent and interpretable structure. This probabilistic nature benefits accurate and practical learning from sparse annotations and equips our model with the ability to quantify uncertainty. Extensive evaluations with two public laparoscopic datasets demonstrated the efficacy of our method, which consistently outperformed existing methods. Furthermore, our method was adapted for scribble-supervised cardiac multi-structure segmentation, presenting competitive performance compared to previous methods. The code is available at https://github.com/MoriLabNU/Bayesian_WSS.
<div id='section'>Paperid: <span id='pid'>118, <a href='https://arxiv.org/pdf/2410.02212.pdf' target='_blank'>https://arxiv.org/pdf/2410.02212.pdf</a></span>   <span><a href='https://github.com/winston52/HNM-WSI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wentao Huang, Xiaoling Hu, Shahira Abousamra, Prateek Prasanna, Chao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02212">Hard Negative Sample Mining for Whole Slide Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised whole slide image (WSI) classification is challenging due to the lack of patch-level labels and high computational costs. State-of-the-art methods use self-supervised patch-wise feature representations for multiple instance learning (MIL). Recently, methods have been proposed to fine-tune the feature representation on the downstream task using pseudo labeling, but mostly focusing on selecting high-quality positive patches. In this paper, we propose to mine hard negative samples during fine-tuning. This allows us to obtain better feature representations and reduce the training cost. Furthermore, we propose a novel patch-wise ranking loss in MIL to better exploit these hard negative samples. Experiments on two public datasets demonstrate the efficacy of these proposed ideas. Our codes are available at https://github.com/winston52/HNM-WSI
<div id='section'>Paperid: <span id='pid'>119, <a href='https://arxiv.org/pdf/2410.01341.pdf' target='_blank'>https://arxiv.org/pdf/2410.01341.pdf</a></span>   <span><a href='https://github.com/ZhaofengSHI/CTDN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaofeng Shi, Heqian Qiu, Lanxiao Wang, Fanman Meng, Qingbo Wu, Hongliang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01341">Cognition Transferring and Decoupling for Text-supervised Egocentric Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we explore a novel Text-supervised Egocentic Semantic Segmentation (TESS) task that aims to assign pixel-level categories to egocentric images weakly supervised by texts from image-level labels. In this task with prospective potential, the egocentric scenes contain dense wearer-object relations and inter-object interference. However, most recent third-view methods leverage the frozen Contrastive Language-Image Pre-training (CLIP) model, which is pre-trained on the semantic-oriented third-view data and lapses in the egocentric view due to the ``relation insensitive" problem. Hence, we propose a Cognition Transferring and Decoupling Network (CTDN) that first learns the egocentric wearer-object relations via correlating the image and text. Besides, a Cognition Transferring Module (CTM) is developed to distill the cognitive knowledge from the large-scale pre-trained model to our model for recognizing egocentric objects with various semantics. Based on the transferred cognition, the Foreground-background Decoupling Module (FDM) disentangles the visual representations to explicitly discriminate the foreground and background regions to mitigate false activation areas caused by foreground-background interferential objects during egocentric relation learning. Extensive experiments on four TESS benchmarks demonstrate the effectiveness of our approach, which outperforms many recent related methods by a large margin. Code will be available at https://github.com/ZhaofengSHI/CTDN.
<div id='section'>Paperid: <span id='pid'>120, <a href='https://arxiv.org/pdf/2409.19720.pdf' target='_blank'>https://arxiv.org/pdf/2409.19720.pdf</a></span>   <span><a href='https://github.com/fukexue/FAST' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kexue Fu, Xiaoyuan Luo, Linhao Qu, Shuo Wang, Ying Xiong, Ilias Maglogiannis, Longxiang Gao, Manning Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19720">FAST: A Dual-tier Few-Shot Learning Paradigm for Whole Slide Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The expensive fine-grained annotation and data scarcity have become the primary obstacles for the widespread adoption of deep learning-based Whole Slide Images (WSI) classification algorithms in clinical practice. Unlike few-shot learning methods in natural images that can leverage the labels of each image, existing few-shot WSI classification methods only utilize a small number of fine-grained labels or weakly supervised slide labels for training in order to avoid expensive fine-grained annotation. They lack sufficient mining of available WSIs, severely limiting WSI classification performance. To address the above issues, we propose a novel and efficient dual-tier few-shot learning paradigm for WSI classification, named FAST. FAST consists of a dual-level annotation strategy and a dual-branch classification framework. Firstly, to avoid expensive fine-grained annotation, we collect a very small number of WSIs at the slide level, and annotate an extremely small number of patches. Then, to fully mining the available WSIs, we use all the patches and available patch labels to build a cache branch, which utilizes the labeled patches to learn the labels of unlabeled patches and through knowledge retrieval for patch classification. In addition to the cache branch, we also construct a prior branch that includes learnable prompt vectors, using the text encoder of visual-language models for patch classification. Finally, we integrate the results from both branches to achieve WSI classification. Extensive experiments on binary and multi-class datasets demonstrate that our proposed method significantly surpasses existing few-shot classification methods and approaches the accuracy of fully supervised methods with only 0.22$\%$ annotation costs. All codes and models will be publicly available on https://github.com/fukexue/FAST.
<div id='section'>Paperid: <span id='pid'>121, <a href='https://arxiv.org/pdf/2409.19483.pdf' target='_blank'>https://arxiv.org/pdf/2409.19483.pdf</a></span>   <span><a href='https://github.com/HealthX-Lab/MedCLIP-SAMv2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Taha Koleilat, Hojat Asgariandehkordi, Hassan Rivaz, Yiming Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19483">MedCLIP-SAMv2: Towards Universal Text-Driven Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Segmentation of anatomical structures and pathological regions in medical images is essential for modern clinical diagnosis, disease research, and treatment planning. While significant advancements have been made in deep learning-based segmentation techniques, many of these methods still suffer from limitations in data efficiency, generalizability, and interactivity. As a result, developing precise segmentation methods that require fewer labeled datasets remains a critical challenge in medical image analysis. Recently, the introduction of foundation models like CLIP and Segment-Anything-Model (SAM), with robust cross-domain representations, has paved the way for interactive and universal image segmentation. However, further exploration of these models for data-efficient segmentation in medical imaging is still needed and highly relevant. In this paper, we introduce MedCLIP-SAMv2, a novel framework that integrates the CLIP and SAM models to perform segmentation on clinical scans using text prompts, in both zero-shot and weakly supervised settings. Our approach includes fine-tuning the BiomedCLIP model with a new Decoupled Hard Negative Noise Contrastive Estimation (DHN-NCE) loss, and leveraging the Multi-modal Information Bottleneck (M2IB) to create visual prompts for generating segmentation masks from SAM in the zero-shot setting. We also investigate using zero-shot segmentation labels within a weakly supervised paradigm to enhance segmentation quality further. Extensive testing across four diverse segmentation tasks and medical imaging modalities (breast tumor ultrasound, brain tumor MRI, lung X-ray, and lung CT) demonstrates the high accuracy of our proposed framework. Our code is available at https://github.com/HealthX-Lab/MedCLIP-SAMv2.
<div id='section'>Paperid: <span id='pid'>122, <a href='https://arxiv.org/pdf/2409.19370.pdf' target='_blank'>https://arxiv.org/pdf/2409.19370.pdf</a></span>   <span><a href='https://github.com/GtLinyer/MambaEviScrib' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoxiang Han, Xinyu Li, Jiang Shang, Yiman Liu, Keyan Chen, Shugong Xu, Qiaohong Liu, Qi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19370">MambaEviScrib: Mamba and Evidence-Guided Consistency Enhance CNN Robustness for Scribble-Based Weakly Supervised Ultrasound Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Segmenting anatomical structures and lesions from ultrasound images contributes to disease assessment. Weakly supervised learning (WSL) based on sparse annotation has achieved encouraging performance and demonstrated the potential to reduce annotation costs. This study attempts to introduce scribble-based WSL into ultrasound image segmentation tasks. However, ultrasound images often suffer from poor contrast and unclear edges, coupled with insufficient supervison signals for edges, posing challenges to edge prediction. Uncertainty modeling has been proven to facilitate models in dealing with these issues. Nevertheless, existing uncertainty estimation paradigms are not robust enough and often filter out predictions near decision boundaries, resulting in unstable edge predictions. Therefore, we propose leveraging predictions near decision boundaries effectively. Specifically, we introduce Dempster-Shafer Theory (DST) of evidence to design an Evidence-Guided Consistency strategy. This strategy utilizes high-evidence predictions, which are more likely to occur near high-density regions, to guide the optimization of low-evidence predictions that may appear near decision boundaries. Furthermore, the diverse sizes and locations of lesions in ultrasound images pose a challenge for CNNs with local receptive fields, as they struggle to model global information. Therefore, we introduce Visual Mamba based on structured state space sequence models, which achieves long-range dependency with linear computational complexity, and we construct a novel hybrid CNN-Mamba framework. During training, the collaboration between the CNN branch and the Mamba branch in the proposed framework draws inspiration from each other based on the EGC strategy. Experiments demonstrate the competitiveness of the proposed method. Dataset and code will be available on https://github.com/GtLinyer/MambaEviScrib.
<div id='section'>Paperid: <span id='pid'>123, <a href='https://arxiv.org/pdf/2409.16213.pdf' target='_blank'>https://arxiv.org/pdf/2409.16213.pdf</a></span>   <span><a href='https://github.com/Harry-Rogers/PSIE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Harry Rogers, Tahmina Zebin, Grzegorz Cielniak, Beatriz De La Iglesia, Ben Magri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16213">Deep Learning for Precision Agriculture: Post-Spraying Evaluation and Deposition Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Precision spraying evaluation requires automation primarily in post-spraying imagery. In this paper we propose an eXplainable Artificial Intelligence (XAI) computer vision pipeline to evaluate a precision spraying system post-spraying without the need for traditional agricultural methods. The developed system can semantically segment potential targets such as lettuce, chickweed, and meadowgrass and correctly identify if targets have been sprayed. Furthermore, this pipeline evaluates using a domain-specific Weakly Supervised Deposition Estimation task, allowing for class-specific quantification of spray deposit weights in Î¼L. Estimation of coverage rates of spray deposition in a class-wise manner allows for further understanding of effectiveness of precision spraying systems. Our study evaluates different Class Activation Mapping techniques, namely AblationCAM and ScoreCAM, to determine which is more effective and interpretable for these tasks. In the pipeline, inference-only feature fusion is used to allow for further interpretability and to enable the automation of precision spraying evaluation post-spray. Our findings indicate that a Fully Convolutional Network with an EfficientNet-B0 backbone and inference-only feature fusion achieves an average absolute difference in deposition values of 156.8 Î¼L across three classes in our test set. The dataset curated in this paper is publicly available at https://github.com/Harry-Rogers/PSIE
<div id='section'>Paperid: <span id='pid'>124, <a href='https://arxiv.org/pdf/2409.14319.pdf' target='_blank'>https://arxiv.org/pdf/2409.14319.pdf</a></span>   <span><a href='https://github.com/zhousheng97/ViTXT-GQA.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sheng Zhou, Junbin Xiao, Xun Yang, Peipei Song, Dan Guo, Angela Yao, Meng Wang, Tat-Seng Chua
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14319">Scene-Text Grounding for Text-Based Video Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing efforts in text-based video question answering (TextVideoQA) are criticized for their opaque decisionmaking and heavy reliance on scene-text recognition. In this paper, we propose to study Grounded TextVideoQA by forcing models to answer questions and spatio-temporally localize the relevant scene-text regions, thus decoupling QA from scenetext recognition and promoting research towards interpretable QA. The task has three-fold significance. First, it encourages scene-text evidence versus other short-cuts for answer predictions. Second, it directly accepts scene-text regions as visual answers, thus circumventing the problem of ineffective answer evaluation by stringent string matching. Third, it isolates the challenges inherited in VideoQA and scene-text recognition. This enables the diagnosis of the root causes for failure predictions, e.g., wrong QA or wrong scene-text recognition? To achieve Grounded TextVideoQA, we propose the T2S-QA model that highlights a disentangled temporal-to-spatial contrastive learning strategy for weakly-supervised scene-text grounding and grounded TextVideoQA. To facilitate evaluation, we construct a new dataset ViTXT-GQA which features 52K scene-text bounding boxes within 2.2K temporal segments related to 2K questions and 729 videos. With ViTXT-GQA, we perform extensive experiments and demonstrate the severe limitations of existing techniques in Grounded TextVideoQA. While T2S-QA achieves superior results, the large performance gap with human leaves ample space for improvement. Our further analysis of oracle scene-text inputs posits that the major challenge is scene-text recognition. To advance the research of Grounded TextVideoQA, our dataset and code are at https://github.com/zhousheng97/ViTXT-GQA.git
<div id='section'>Paperid: <span id='pid'>125, <a href='https://arxiv.org/pdf/2409.13431.pdf' target='_blank'>https://arxiv.org/pdf/2409.13431.pdf</a></span>   <span><a href='https://github.com/wzx99/TMIM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixiao Wang, Hongtao Xie, YuXin Wang, Yadong Qu, Fengjun Guo, Pengwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.13431">Leveraging Text Localization for Scene Text Removal via Text-aware Masked Image Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing scene text removal (STR) task suffers from insufficient training data due to the expensive pixel-level labeling. In this paper, we aim to address this issue by introducing a Text-aware Masked Image Modeling algorithm (TMIM), which can pretrain STR models with low-cost text detection labels (e.g., text bounding box). Different from previous pretraining methods that use indirect auxiliary tasks only to enhance the implicit feature extraction ability, our TMIM first enables the STR task to be directly trained in a weakly supervised manner, which explores the STR knowledge explicitly and efficiently. In TMIM, first, a Background Modeling stream is built to learn background generation rules by recovering the masked non-text region. Meanwhile, it provides pseudo STR labels on the masked text region. Second, a Text Erasing stream is proposed to learn from the pseudo labels and equip the model with end-to-end STR ability. Benefiting from the two collaborative streams, our STR model can achieve impressive performance only with the public text detection datasets, which greatly alleviates the limitation of the high-cost STR labels. Experiments demonstrate that our method outperforms other pretrain methods and achieves state-of-the-art performance (37.35 PSNR on SCUT-EnsText). Code will be available at https://github.com/wzx99/TMIM.
<div id='section'>Paperid: <span id='pid'>126, <a href='https://arxiv.org/pdf/2409.10291.pdf' target='_blank'>https://arxiv.org/pdf/2409.10291.pdf</a></span>   <span><a href='https://github.com/mishgon/ape' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mikhail Goncharov, Valentin Samokhin, Eugenia Soboleva, Roman Sokolov, Boris Shirokikh, Mikhail Belyaev, Anvar Kurmukov, Ivan Oseledets
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10291">Anatomical Positional Embeddings</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a self-supervised model producing 3D anatomical positional embeddings (APE) of individual medical image voxels. APE encodes voxels' anatomical closeness, i.e., voxels of the same organ or nearby organs always have closer positional embeddings than the voxels of more distant body parts. In contrast to the existing models of anatomical positional embeddings, our method is able to efficiently produce a map of voxel-wise embeddings for a whole volumetric input image, which makes it an optimal choice for different downstream applications. We train our APE model on 8400 publicly available CT images of abdomen and chest regions. We demonstrate its superior performance compared with the existing models on anatomical landmark retrieval and weakly-supervised few-shot localization of 13 abdominal organs. As a practical application, we show how to cheaply train APE to crop raw CT images to different anatomical regions of interest with 0.99 recall, while reducing the image volume by 10-100 times. The code and the pre-trained APE model are available at https://github.com/mishgon/ape .
<div id='section'>Paperid: <span id='pid'>127, <a href='https://arxiv.org/pdf/2409.09369.pdf' target='_blank'>https://arxiv.org/pdf/2409.09369.pdf</a></span>   <span><a href='https://github.com/liupei101/VLSA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pei Liu, Luping Ji, Jiaxiang Gou, Bo Fu, Mao Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09369">Interpretable Vision-Language Survival Analysis with Ordinal Inductive Bias for Computational Pathology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Histopathology Whole-Slide Images (WSIs) provide an important tool to assess cancer prognosis in computational pathology (CPATH). While existing survival analysis (SA) approaches have made exciting progress, they are generally limited to adopting highly-expressive network architectures and only coarse-grained patient-level labels to learn visual prognostic representations from gigapixel WSIs. Such learning paradigm suffers from critical performance bottlenecks, when facing present scarce training data and standard multi-instance learning (MIL) framework in CPATH. To overcome it, this paper, for the first time, proposes a new Vision-Language-based SA (VLSA) paradigm. Concretely, (1) VLSA is driven by pathology VL foundation models. It no longer relies on high-capability networks and shows the advantage of data efficiency. (2) In vision-end, VLSA encodes textual prognostic prior and then employs it as auxiliary signals to guide the aggregating of visual prognostic features at instance level, thereby compensating for the weak supervision in MIL. Moreover, given the characteristics of SA, we propose i) ordinal survival prompt learning to transform continuous survival labels into textual prompts; and ii) ordinal incidence function as prediction target to make SA compatible with VL-based prediction. Notably, VLSA's predictions can be interpreted intuitively by our Shapley values-based method. The extensive experiments on five datasets confirm the effectiveness of our scheme. Our VLSA could pave a new way for SA in CPATH by offering weakly-supervised MIL an effective means to learn valuable prognostic clues from gigapixel WSIs. Our source code is available at https://github.com/liupei101/VLSA.
<div id='section'>Paperid: <span id='pid'>128, <a href='https://arxiv.org/pdf/2409.01691.pdf' target='_blank'>https://arxiv.org/pdf/2409.01691.pdf</a></span>   <span><a href='https://github.com/CUHK-AIM-Group/SAMTooth' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Liu, Wuyang Li, Cheng Wang, Hui Chen, Yixuan Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01691">When 3D Partial Points Meets SAM: Tooth Point Cloud Segmentation with Sparse Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tooth point cloud segmentation is a fundamental task in many orthodontic applications. Current research mainly focuses on fully supervised learning which demands expensive and tedious manual point-wise annotation. Although recent weakly-supervised alternatives are proposed to use weak labels for 3D segmentation and achieve promising results, they tend to fail when the labels are extremely sparse. Inspired by the powerful promptable segmentation capability of the Segment Anything Model (SAM), we propose a framework named SAMTooth that leverages such capacity to complement the extremely sparse supervision. To automatically generate appropriate point prompts for SAM, we propose a novel Confidence-aware Prompt Generation strategy, where coarse category predictions are aggregated with confidence-aware filtering. Furthermore, to fully exploit the structural and shape clues in SAM's outputs for assisting the 3D feature learning, we advance a Mask-guided Representation Learning that re-projects the generated tooth masks of SAM into 3D space and constrains these points of different teeth to possess distinguished representations. To demonstrate the effectiveness of the framework, we conduct experiments on the public dataset and surprisingly find with only 0.1\% annotations (one point per tooth), our method can surpass recent weakly supervised methods by a large margin, and the performance is even comparable to the recent fully-supervised methods, showcasing the significant potential of applying SAM to 3D perception tasks with sparse labels. Code is available at https://github.com/CUHK-AIM-Group/SAMTooth.
<div id='section'>Paperid: <span id='pid'>129, <a href='https://arxiv.org/pdf/2409.01472.pdf' target='_blank'>https://arxiv.org/pdf/2409.01472.pdf</a></span>   <span><a href='https://github.com/xuanrui-work/WSSSByRec' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuanrui Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01472">Semantic Segmentation from Image Labels by Reconstruction from Structured Decomposition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised image segmentation (WSSS) from image tags remains challenging due to its under-constraint nature. Most mainstream work focus on the extraction of class activation map (CAM) and imposing various additional regularization. Contrary to the mainstream, we propose to frame WSSS as a problem of reconstruction from decomposition of the image using its mask, under which most regularization are embedded implicitly within the framework of the new problem. Our approach has demonstrated promising results on initial experiments, and shown robustness against the problem of background ambiguity. Our code is available at \url{https://github.com/xuanrui-work/WSSSByRec}.
<div id='section'>Paperid: <span id='pid'>130, <a href='https://arxiv.org/pdf/2408.17143.pdf' target='_blank'>https://arxiv.org/pdf/2408.17143.pdf</a></span>   <span><a href='https://github.com/n-kubiak/RenDetNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikolina Kubiak, Elliot Wortman, Armin Mustafa, Graeme Phillipson, Stephen Jolly, Simon Hadfield
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.17143">RenDetNet: Weakly-supervised Shadow Detection with Shadow Caster Verification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing shadow detection models struggle to differentiate dark image areas from shadows. In this paper, we tackle this issue by verifying that all detected shadows are real, i.e. they have paired shadow casters. We perform this step in a physically-accurate manner by differentiably re-rendering the scene and observing the changes stemming from carving out estimated shadow casters. Thanks to this approach, the RenDetNet proposed in this paper is the first learning-based shadow detection model whose supervisory signals can be computed in a self-supervised manner. The developed system compares favourably against recent models trained on our data. As part of this publication, we release our code on github.
<div id='section'>Paperid: <span id='pid'>131, <a href='https://arxiv.org/pdf/2408.16661.pdf' target='_blank'>https://arxiv.org/pdf/2408.16661.pdf</a></span>   <span><a href='https://github.com/farnooshar/EigenClusterVIS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Farnoosh Arefi, Amir M. Mansourian, Shohreh Kasaei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.16661">Improving Weakly-supervised Video Instance Segmentation by Leveraging Spatio-temporal Consistency</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The performance of Video Instance Segmentation (VIS) methods has improved significantly with the advent of transformer networks. However, these networks often face challenges in training due to the high annotation cost. To address this, unsupervised and weakly-supervised methods have been developed to reduce the dependency on annotations. This work introduces a novel weakly-supervised method called Eigen-Cluster VIS that, without requiring any mask annotations, achieves competitive accuracy compared to other VIS approaches. This method is based on two key innovations: a Temporal Eigenvalue Loss (TEL) and a clip-level Quality Cluster Coefficient (QCC). The TEL ensures temporal coherence by leveraging the eigenvalues of the Laplacian matrix derived from graph adjacency matrices. By minimizing the mean absolute error between the eigenvalues of adjacent frames, this loss function promotes smooth transitions and stable segmentation boundaries over time, reducing temporal discontinuities and improving overall segmentation quality. The QCC employs the K-means method to ensure the quality of spatio-temporal clusters without relying on ground truth masks. Using the Davies-Bouldin score, the QCC provides an unsupervised measure of feature discrimination, allowing the model to self-evaluate and adapt to varying object distributions, enhancing robustness during the testing phase. These enhancements are computationally efficient and straightforward, offering significant performance gains without additional annotated data. The proposed Eigen-Cluster VIS method is evaluated on the YouTube-Video Instance Segmentation (YouTube-VIS) 2019/2021 and Occluded Video Instance Segmentation (OVIS) datasets, demonstrating that it effectively narrows the performance gap between the fully-supervised and weakly-supervised VIS approaches. The code is available on https://github.com/farnooshar/EigenClusterVIS
<div id='section'>Paperid: <span id='pid'>132, <a href='https://arxiv.org/pdf/2408.16451.pdf' target='_blank'>https://arxiv.org/pdf/2408.16451.pdf</a></span>   <span><a href='https://github.com/yc-zh/WSVM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongcun Zhang, Jiajun Xu, Yina He, Shaozi Li, Zhiming Luo, Huangwei Lei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.16451">Weakly Supervised Object Detection for Automatic Tooth-marked Tongue Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tongue diagnosis in Traditional Chinese Medicine (TCM) is a crucial diagnostic method that can reflect an individual's health status. Traditional methods for identifying tooth-marked tongues are subjective and inconsistent because they rely on practitioner experience. We propose a novel fully automated Weakly Supervised method using Vision transformer and Multiple instance learning WSVM for tongue extraction and tooth-marked tongue recognition. Our approach first accurately detects and extracts the tongue region from clinical images, removing any irrelevant background information. Then, we implement an end-to-end weakly supervised object detection method. We utilize Vision Transformer (ViT) to process tongue images in patches and employ multiple instance loss to identify tooth-marked regions with only image-level annotations. WSVM achieves high accuracy in tooth-marked tongue classification, and visualization experiments demonstrate its effectiveness in pinpointing these regions. This automated approach enhances the objectivity and accuracy of tooth-marked tongue diagnosis. It provides significant clinical value by assisting TCM practitioners in making precise diagnoses and treatment recommendations. Code is available at https://github.com/yc-zh/WSVM.
<div id='section'>Paperid: <span id='pid'>133, <a href='https://arxiv.org/pdf/2408.14130.pdf' target='_blank'>https://arxiv.org/pdf/2408.14130.pdf</a></span>   <span><a href='https://github.com/stainlessnight/LLP-LargeBags' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shunsuke Kubo, Shinnosuke Matsuo, Daiki Suehiro, Kazuhiro Terada, Hiroaki Ito, Akihiko Yoshizawa, Ryoma Bise
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.14130">Theoretical Proportion Label Perturbation for Learning from Label Proportions in Large Bags</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning from label proportions (LLP) is a kind of weakly supervised learning that trains an instance-level classifier from label proportions of bags, which consist of sets of instances without using instance labels. A challenge in LLP arises when the number of instances in a bag (bag size) is numerous, making the traditional LLP methods difficult due to GPU memory limitations. This study aims to develop an LLP method capable of learning from bags with large sizes. In our method, smaller bags (mini-bags) are generated by sampling instances from large-sized bags (original bags), and these mini-bags are used in place of the original bags. However, the proportion of a mini-bag is unknown and differs from that of the original bag, leading to overfitting. To address this issue, we propose a perturbation method for the proportion labels of sampled mini-bags to mitigate overfitting to noisy label proportions. This perturbation is added based on the multivariate hypergeometric distribution, which is statistically modeled. Additionally, loss weighting is implemented to reduce the negative impact of proportions sampled from the tail of the distribution. Experimental results demonstrate that the proportion label perturbation and loss weighting achieve classification accuracy comparable to that obtained without sampling. Our codes are available at https://github.com/stainlessnight/LLP-LargeBags.
<div id='section'>Paperid: <span id='pid'>134, <a href='https://arxiv.org/pdf/2408.12489.pdf' target='_blank'>https://arxiv.org/pdf/2408.12489.pdf</a></span>   <span><a href='https://github.com/wbkit/Scribbles4All' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wolfgang Boettcher, Lukas Hoyer, Ozan Unal, Jan Eric Lenssen, Bernt Schiele
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.12489">Scribbles for All: Benchmarking Scribble Supervised Segmentation Across Datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we introduce Scribbles for All, a label and training data generation algorithm for semantic segmentation trained on scribble labels. Training or fine-tuning semantic segmentation models with weak supervision has become an important topic recently and was subject to significant advances in model quality. In this setting, scribbles are a promising label type to achieve high quality segmentation results while requiring a much lower annotation effort than usual pixel-wise dense semantic segmentation annotations. The main limitation of scribbles as source for weak supervision is the lack of challenging datasets for scribble segmentation, which hinders the development of novel methods and conclusive evaluations. To overcome this limitation, Scribbles for All provides scribble labels for several popular segmentation datasets and provides an algorithm to automatically generate scribble labels for any dataset with dense annotations, paving the way for new insights and model advancements in the field of weakly supervised segmentation. In addition to providing datasets and algorithm, we evaluate state-of-the-art segmentation models on our datasets and show that models trained with our synthetic labels perform competitively with respect to models trained on manual labels. Thus, our datasets enable state-of-the-art research into methods for scribble-labeled semantic segmentation. The datasets, scribble generation algorithm, and baselines are publicly available at https://github.com/wbkit/Scribbles4All
<div id='section'>Paperid: <span id='pid'>135, <a href='https://arxiv.org/pdf/2408.11505.pdf' target='_blank'>https://arxiv.org/pdf/2408.11505.pdf</a></span>   <span><a href='https://github.com/Hanminghao/MSCPT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Minghao Han, Linhao Qu, Dingkang Yang, Xukun Zhang, Xiaoying Wang, Lihua Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.11505">MSCPT: Few-shot Whole Slide Image Classification with Multi-scale and Context-focused Prompt Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple instance learning (MIL) has become a standard paradigm for the weakly supervised classification of whole slide images (WSIs). However, this paradigm relies on using a large number of labeled WSIs for training. The lack of training data and the presence of rare diseases pose significant challenges for these methods. Prompt tuning combined with pre-trained Vision-Language models (VLMs) is an effective solution to the Few-shot Weakly Supervised WSI Classification (FSWC) task. Nevertheless, applying prompt tuning methods designed for natural images to WSIs presents three significant challenges: 1) These methods fail to fully leverage the prior knowledge from the VLM's text modality; 2) They overlook the essential multi-scale and contextual information in WSIs, leading to suboptimal results; and 3) They lack exploration of instance aggregation methods. To address these problems, we propose a Multi-Scale and Context-focused Prompt Tuning (MSCPT) method for FSWC task. Specifically, MSCPT employs the frozen large language model to generate pathological visual language prior knowledge at multiple scales, guiding hierarchical prompt tuning. Additionally, we design a graph prompt tuning module to learn essential contextual information within WSI, and finally, a non-parametric cross-guided instance aggregation module has been introduced to derive the WSI-level features. Extensive experiments, visualizations, and interpretability analyses were conducted on five datasets and three downstream tasks using three VLMs, demonstrating the strong performance of our MSCPT. All codes have been made publicly accessible at https://github.com/Hanminghao/MSCPT.
<div id='section'>Paperid: <span id='pid'>136, <a href='https://arxiv.org/pdf/2408.10031.pdf' target='_blank'>https://arxiv.org/pdf/2408.10031.pdf</a></span>   <span><a href='https://github.com/covisionlab/dynamic-label-injection.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Emanuele Caruso, Francesco Pelosin, Alessandro Simoni, Marco Boschetti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.10031">Dynamic Label Injection for Imbalanced Industrial Defect Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we propose a simple yet effective method to tackle the problem of imbalanced multi-class semantic segmentation in deep learning systems. One of the key properties for a good training set is the balancing among the classes. When the input distribution is heavily imbalanced in the number of instances, the learning process could be hindered or difficult to carry on. To this end, we propose a Dynamic Label Injection (DLI) algorithm to impose a uniform distribution in the input batch. Our algorithm computes the current batch defect distribution and re-balances it by transferring defects using a combination of Poisson-based seamless image cloning and cut-paste techniques. A thorough experimental section on the Magnetic Tiles dataset shows better results of DLI compared to other balancing loss approaches also in the challenging weakly-supervised setup. The code is available at https://github.com/covisionlab/dynamic-label-injection.git
<div id='section'>Paperid: <span id='pid'>137, <a href='https://arxiv.org/pdf/2408.09411.pdf' target='_blank'>https://arxiv.org/pdf/2408.09411.pdf</a></span>   <span><a href='https://github.com/WltyBY/LNQ2023_training_code.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Litingyu Wang, Yijie Qu, Xiangde Luo, Wenjun Liao, Shichuan Zhang, Guotai Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09411">Weakly Supervised Lymph Nodes Segmentation Based on Partial Instance Annotations with Pre-trained Dual-branch Network and Pseudo Label Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Assessing the presence of potentially malignant lymph nodes aids in estimating cancer progression, and identifying surrounding benign lymph nodes can assist in determining potential metastatic pathways for cancer. For quantitative analysis, automatic segmentation of lymph nodes is crucial. However, due to the labor-intensive and time-consuming manual annotation process required for a large number of lymph nodes, it is more practical to annotate only a subset of the lymph node instances to reduce annotation costs. In this study, we propose a pre-trained Dual-Branch network with Dynamically Mixed Pseudo label (DBDMP) to learn from partial instance annotations for lymph nodes segmentation. To obtain reliable pseudo labels for lymph nodes that are not annotated, we employ a dual-decoder network to generate different outputs that are then dynamically mixed. We integrate the original weak partial annotations with the mixed pseudo labels to supervise the network. To further leverage the extensive amount of unannotated voxels, we apply a self-supervised pre-training strategy to enhance the model's feature extraction capability. Experiments on the mediastinal Lymph Node Quantification (LNQ) dataset demonstrate that our method, compared to directly learning from partial instance annotations, significantly improves the Dice Similarity Coefficient (DSC) from 11.04% to 54.10% and reduces the Average Symmetric Surface Distance (ASSD) from 20.83 $mm$ to 8.72 $mm$. The code is available at https://github.com/WltyBY/LNQ2023_training_code.git
<div id='section'>Paperid: <span id='pid'>138, <a href='https://arxiv.org/pdf/2408.09085.pdf' target='_blank'>https://arxiv.org/pdf/2408.09085.pdf</a></span>   <span><a href='https://xiaoaoran.github.io/projects/MM-SAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aoran Xiao, Weihao Xuan, Heli Qi, Yun Xing, Naoto Yokoya, Shijian Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09085">Segment Anything with Multiple Modalities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robust and accurate segmentation of scenes has become one core functionality in various visual recognition and navigation tasks. This has inspired the recent development of Segment Anything Model (SAM), a foundation model for general mask segmentation. However, SAM is largely tailored for single-modal RGB images, limiting its applicability to multi-modal data captured with widely-adopted sensor suites, such as LiDAR plus RGB, depth plus RGB, thermal plus RGB, etc. We develop MM-SAM, an extension and expansion of SAM that supports cross-modal and multi-modal processing for robust and enhanced segmentation with different sensor suites. MM-SAM features two key designs, namely, unsupervised cross-modal transfer and weakly-supervised multi-modal fusion, enabling label-efficient and parameter-efficient adaptation toward various sensor modalities. It addresses three main challenges: 1) adaptation toward diverse non-RGB sensors for single-modal processing, 2) synergistic processing of multi-modal data via sensor fusion, and 3) mask-free training for different downstream tasks. Extensive experiments show that MM-SAM consistently outperforms SAM by large margins, demonstrating its effectiveness and robustness across various sensors and data modalities.
<div id='section'>Paperid: <span id='pid'>139, <a href='https://arxiv.org/pdf/2408.07079.pdf' target='_blank'>https://arxiv.org/pdf/2408.07079.pdf</a></span>   <span><a href='https://github.com/EIDOSLAB/AnatCL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Carlo Alberto Barbano, Matteo Brunello, Benoit Dufumier, Marco Grangetto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.07079">Anatomical Foundation Models for Brain MRIs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep Learning (DL) in neuroimaging has become increasingly relevant for detecting neurological conditions and neurodegenerative disorders. One of the most predominant biomarkers in neuroimaging is represented by brain age, which has been shown to be a good indicator for different conditions, such as Alzheimer's Disease. Using brain age for weakly supervised pre-training of DL models in transfer learning settings has also recently shown promising results, especially when dealing with data scarcity of different conditions. On the other hand, anatomical information of brain MRIs (e.g. cortical thickness) can provide important information for learning good representations that can be transferred to many downstream tasks. In this work, we propose AnatCL, an anatomical foundation model for brain MRIs that i.) leverages anatomical information in a weakly contrastive learning approach, and ii.) achieves state-of-the-art performances across many different downstream tasks. To validate our approach we consider 12 different downstream tasks for the diagnosis of different conditions such as Alzheimer's Disease, autism spectrum disorder, and schizophrenia. Furthermore, we also target the prediction of 10 different clinical assessment scores using structural MRI data. Our findings show that incorporating anatomical information during pre-training leads to more robust and generalizable representations. Pre-trained models can be found at: https://github.com/EIDOSLAB/AnatCL.
<div id='section'>Paperid: <span id='pid'>140, <a href='https://arxiv.org/pdf/2408.05955.pdf' target='_blank'>https://arxiv.org/pdf/2408.05955.pdf</a></span>   <span><a href='https://github.com/sejong-rcv/PVLR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Geuntaek Lim, Hyunwoo Kim, Joonsoo Kim, Yukyung Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.05955">Probabilistic Vision-Language Representation for Weakly Supervised Temporal Action Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised temporal action localization (WTAL) aims to detect action instances in untrimmed videos using only video-level annotations. Since many existing works optimize WTAL models based on action classification labels, they encounter the task discrepancy problem (i.e., localization-by-classification). To tackle this issue, recent studies have attempted to utilize action category names as auxiliary semantic knowledge through vision-language pre-training (VLP). However, there are still areas where existing research falls short. Previous approaches primarily focused on leveraging textual information from language models but overlooked the alignment of dynamic human action and VLP knowledge in a joint space. Furthermore, the deterministic representation employed in previous studies struggles to capture fine-grained human motions. To address these problems, we propose a novel framework that aligns human action knowledge and VLP knowledge in a probabilistic embedding space. Moreover, we propose intra- and inter-distribution contrastive learning to enhance the probabilistic embedding space based on statistical similarities. Extensive experiments and ablation studies reveal that our method significantly outperforms all previous state-of-the-art methods. Code is available at https://github.com/sejong-rcv/PVLR.
<div id='section'>Paperid: <span id='pid'>141, <a href='https://arxiv.org/pdf/2408.05215.pdf' target='_blank'>https://arxiv.org/pdf/2408.05215.pdf</a></span>   <span><a href='https://github.com/nec-research/PICPS-ML4Sci' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Makoto Takamoto, Viktor Zaverkin, Mathias Niepert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.05215">Physics-Informed Weakly Supervised Learning for Interatomic Potentials</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning plays an increasingly important role in computational chemistry and materials science, complementing computationally intensive ab initio and first-principles methods. Despite their utility, machine-learning models often lack generalization capability and robustness during atomistic simulations, yielding unphysical energy and force predictions that hinder their real-world applications. We address this challenge by introducing a physics-informed, weakly supervised approach for training machine-learned interatomic potentials (MLIPs). We introduce two novel loss functions, extrapolating the potential energy via a Taylor expansion and using the concept of conservative forces. Our approach improves the accuracy of MLIPs applied to training tasks with sparse training data sets and reduces the need for pre-training computationally demanding models with large data sets. Particularly, we perform extensive experiments demonstrating reduced energy and force errors -- often lower by a factor of two -- for various baseline models and benchmark data sets. Moreover, we demonstrate improved robustness during MD simulations of the MLIP models trained with the proposed weakly supervised loss. Finally, our approach improves the fine-tuning of foundation models on sparse, highly accurate ab initio data. An implementation of our method and scripts for executing experiments are available at https://github.com/nec-research/PICPS-ML4Sci.
<div id='section'>Paperid: <span id='pid'>142, <a href='https://arxiv.org/pdf/2408.01191.pdf' target='_blank'>https://arxiv.org/pdf/2408.01191.pdf</a></span>   <span><a href='https://github.com/xrt11/tumor-segmentation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruitao Xie, Limai Jiang, Xiaoxi He, Yi Pan, Yunpeng Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.01191">A Weakly Supervised and Globally Explainable Learning Framework for Brain Tumor Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine-based brain tumor segmentation can help doctors make better diagnoses. However, the complex structure of brain tumors and expensive pixel-level annotations present challenges for automatic tumor segmentation. In this paper, we propose a counterfactual generation framework that not only achieves exceptional brain tumor segmentation performance without the need for pixel-level annotations, but also provides explainability. Our framework effectively separates class-related features from class-unrelated features of the samples, and generate new samples that preserve identity features while altering class attributes by embedding different class-related features. We perform topological data analysis on the extracted class-related features and obtain a globally explainable manifold, and for each abnormal sample to be segmented, a meaningful normal sample could be effectively generated with the guidance of the rule-based paths designed within the manifold for comparison for identifying the tumor regions. We evaluate our proposed method on two datasets, which demonstrates superior performance of brain tumor segmentation. The code is available at https://github.com/xrt11/tumor-segmentation.
<div id='section'>Paperid: <span id='pid'>143, <a href='https://arxiv.org/pdf/2408.01162.pdf' target='_blank'>https://arxiv.org/pdf/2408.01162.pdf</a></span>   <span><a href='https://github.com/bryanwong17/PreMix' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bryan Wong, Mun Yong Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.01162">PreMix: Label-Efficient Multiple Instance Learning via Non-Contrastive Pre-training and Feature Mixing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple instance learning (MIL) has emerged as a powerful framework for weakly supervised whole slide image (WSI) classification, enabling slide-level predictions without requiring detailed patch-level annotations. Despite its success, a critical limitation of current MIL methods lies in the underutilization of pre-training for the MIL aggregator. Most existing approaches initialize the aggregator randomly and train it from scratch, making performance highly sensitive to the quantity of labeled WSIs and ignoring the abundance of unlabeled WSIs commonly available in clinical settings. To address this, we propose PreMix, a novel framework that leverages a non-contrastive pre-training method, Barlow Twins, augmented with the Slide Mixing approach to generate additional positive pairs and enhance feature learning, particularly under limited labeled WSI conditions. Fine-tuning with Mixup and Manifold Mixup further enhances robustness by effectively handling the diverse sizes of gigapixel WSIs. Experimental results demonstrate that integrating PreMix as a plug-in module into HIPT yields an average F1 improvement of 4.7% over the baseline HIPT across various WSI training sizes and datasets. These findings underscore its potential to advance WSI classification with limited labeled data and its applicability to real-world histopathology practices. The code is available at https://github.com/bryanwong17/PreMix
<div id='section'>Paperid: <span id='pid'>144, <a href='https://arxiv.org/pdf/2407.21604.pdf' target='_blank'>https://arxiv.org/pdf/2407.21604.pdf</a></span>   <span><a href='https://github.com/kimjongwoo-cell/MicroMIL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jongwoo Kim, Bryan Wong, Huazhu Fu, Willmer Rafell QuiÃ±ones, Youngsin Ko, Mun Yong Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.21604">MicroMIL: Graph-Based Multiple Instance Learning for Context-Aware Diagnosis with Microscopic Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cancer diagnosis has greatly benefited from the integration of whole-slide images (WSIs) with multiple instance learning (MIL), enabling high-resolution analysis of tissue morphology. Graph-based MIL (GNN-MIL) approaches have emerged as powerful solutions for capturing contextual information in WSIs, thereby improving diagnostic accuracy. However, WSIs require significant computational and infrastructural resources, limiting accessibility in resource-constrained settings. Conventional light microscopes offer a cost-effective alternative, but applying GNN-MIL to such data is challenging due to extensive redundant images and missing spatial coordinates, which hinder contextual learning. To address these issues, we introduce MicroMIL, the first weakly-supervised MIL framework specifically designed for images acquired from conventional light microscopes. MicroMIL leverages a representative image extractor (RIE) that employs deep cluster embedding (DCE) and hard Gumbel-Softmax to dynamically reduce redundancy and select representative images. These images serve as graph nodes, with edges computed via cosine similarity, eliminating the need for spatial coordinates while preserving contextual information. Extensive experiments on a real-world colon cancer dataset and the BreakHis dataset demonstrate that MicroMIL achieves state-of-the-art performance, improving both diagnostic accuracy and robustness to redundancy. The code is available at https://github.com/kimjongwoo-cell/MicroMIL
<div id='section'>Paperid: <span id='pid'>145, <a href='https://arxiv.org/pdf/2407.19192.pdf' target='_blank'>https://arxiv.org/pdf/2407.19192.pdf</a></span>   <span><a href='https://github.com/wangbing1416/HAMI-M3D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bing Wang, Shengsheng Wang, Changchun Li, Renchu Guan, Ximing Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.19192">Harmfully Manipulated Images Matter in Multimodal Misinformation Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nowadays, misinformation is widely spreading over various social media platforms and causes extremely negative impacts on society. To combat this issue, automatically identifying misinformation, especially those containing multimodal content, has attracted growing attention from the academic and industrial communities, and induced an active research topic named Multimodal Misinformation Detection (MMD). Typically, existing MMD methods capture the semantic correlation and inconsistency between multiple modalities, but neglect some potential clues in multimodal content. Recent studies suggest that manipulated traces of the images in articles are non-trivial clues for detecting misinformation. Meanwhile, we find that the underlying intentions behind the manipulation, e.g., harmful and harmless, also matter in MMD. Accordingly, in this work, we propose to detect misinformation by learning manipulation features that indicate whether the image has been manipulated, as well as intention features regarding the harmful and harmless intentions of the manipulation. Unfortunately, the manipulation and intention labels that make these features discriminative are unknown. To overcome the problem, we propose two weakly supervised signals as alternatives by introducing additional datasets on image manipulation detection and formulating two classification tasks as positive and unlabeled learning problems. Based on these ideas, we propose a novel MMD method, namely Harmfully Manipulated Images Matter in MMD (HAMI-M3D). Extensive experiments across three benchmark datasets can demonstrate that HAMI-M3D can consistently improve the performance of any MMD baselines.
<div id='section'>Paperid: <span id='pid'>146, <a href='https://arxiv.org/pdf/2407.17197.pdf' target='_blank'>https://arxiv.org/pdf/2407.17197.pdf</a></span>   <span><a href='https://github.com/CEA-LIST/ALPI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Saad Lahlali, Nicolas Granger, HervÃ© Le Borgne, Quoc-Cuong Pham
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.17197">ALPI: Auto-Labeller with Proxy Injection for 3D Object Detection using 2D Labels Only</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D object detection plays a crucial role in various applications such as autonomous vehicles, robotics and augmented reality. However, training 3D detectors requires a costly precise annotation, which is a hindrance to scaling annotation to large datasets. To address this challenge, we propose a weakly supervised 3D annotator that relies solely on 2D bounding box annotations from images, along with size priors. One major problem is that supervising a 3D detection model using only 2D boxes is not reliable due to ambiguities between different 3D poses and their identical 2D projection. We introduce a simple yet effective and generic solution: we build 3D proxy objects with annotations by construction and add them to the training dataset. Our method requires only size priors to adapt to new classes. To better align 2D supervision with 3D detection, our method ensures depth invariance with a novel expression of the 2D losses. Finally, to detect more challenging instances, our annotator follows an offline pseudo-labelling scheme which gradually improves its 3D pseudo-labels. Extensive experiments on the KITTI dataset demonstrate that our method not only performs on-par or above previous works on the Car category, but also achieves performance close to fully supervised methods on more challenging classes. We further demonstrate the effectiveness and robustness of our method by being the first to experiment on the more challenging nuScenes dataset. We additionally propose a setting where weak labels are obtained from a 2D detector pre-trained on MS-COCO instead of human annotations. The code is available at https://github.com/CEA-LIST/ALPI
<div id='section'>Paperid: <span id='pid'>147, <a href='https://arxiv.org/pdf/2407.15036.pdf' target='_blank'>https://arxiv.org/pdf/2407.15036.pdf</a></span>   <span><a href='https://github.com/libeibeics/AsyCo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Beibei Li, Yiyuan Zheng, Beihong Jin, Tao Xiang, Haobo Wang, Lei Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.15036">AsyCo: An Asymmetric Dual-task Co-training Model for Partial-label Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partial-Label Learning (PLL) is a typical problem of weakly supervised learning, where each training instance is annotated with a set of candidate labels. Self-training PLL models achieve state-of-the-art performance but suffer from error accumulation problem caused by mistakenly disambiguated instances. Although co-training can alleviate this issue by training two networks simultaneously and allowing them to interact with each other, most existing co-training methods train two structurally identical networks with the same task, i.e., are symmetric, rendering it insufficient for them to correct each other due to their similar limitations. Therefore, in this paper, we propose an asymmetric dual-task co-training PLL model called AsyCo, which forces its two networks, i.e., a disambiguation network and an auxiliary network, to learn from different views explicitly by optimizing distinct tasks. Specifically, the disambiguation network is trained with self-training PLL task to learn label confidence, while the auxiliary network is trained in a supervised learning paradigm to learn from the noisy pairwise similarity labels that are constructed according to the learned label confidence. Finally, the error accumulation problem is mitigated via information distillation and confidence refinement. Extensive experiments on both uniform and instance-dependent partially labeled datasets demonstrate the effectiveness of AsyCo. The code is available at https://github.com/libeibeics/AsyCo.
<div id='section'>Paperid: <span id='pid'>148, <a href='https://arxiv.org/pdf/2407.13748.pdf' target='_blank'>https://arxiv.org/pdf/2407.13748.pdf</a></span>   <span><a href='https://github.com/gwenzhang/GGA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guowen Zhang, Junsong Fan, Liyi Chen, Zhaoxiang Zhang, Zhen Lei, Lei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.13748">General Geometry-aware Weakly Supervised 3D Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D object detection is an indispensable component for scene understanding. However, the annotation of large-scale 3D datasets requires significant human effort. To tackle this problem, many methods adopt weakly supervised 3D object detection that estimates 3D boxes by leveraging 2D boxes and scene/class-specific priors. However, these approaches generally depend on sophisticated manual priors, which is hard to generalize to novel categories and scenes. In this paper, we are motivated to propose a general approach, which can be easily adapted to new scenes and/or classes. A unified framework is developed for learning 3D object detectors from RGB images and associated 2D boxes. In specific, we propose three general components: prior injection module to obtain general object geometric priors from LLM model, 2D space projection constraint to minimize the discrepancy between the boundaries of projected 3D boxes and their corresponding 2D boxes on the image plane, and 3D space geometry constraint to build a Point-to-Box alignment loss to further refine the pose of estimated 3D boxes. Experiments on KITTI and SUN-RGBD datasets demonstrate that our method yields surprisingly high-quality 3D bounding boxes with only 2D annotation. The source code is available at https://github.com/gwenzhang/GGA.
<div id='section'>Paperid: <span id='pid'>149, <a href='https://arxiv.org/pdf/2407.11486.pdf' target='_blank'>https://arxiv.org/pdf/2407.11486.pdf</a></span>   <span><a href='https://github.com/CVIU-CSU/TCT-InfoNCE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jialong Huang, Gaojie Li, Shichao Kan, Jianfeng Liu, Yixiong Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.11486">An efficient framework based on large foundation model for cervical cytopathology whole slide image screening</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current cervical cytopathology whole slide image (WSI) screening primarily relies on detection-based approaches, which are limited in performance due to the expense and time-consuming annotation process. Multiple Instance Learning (MIL), a weakly supervised approach that relies solely on bag-level labels, can effectively alleviate these challenges. Nonetheless, MIL commonly employs frozen pretrained models or self-supervised learning for feature extraction, which suffers from low efficacy or inefficiency. In this paper, we propose an efficient framework for cervical cytopathology WSI classification using only WSI-level labels through unsupervised and weakly supervised learning. Given the sparse and dispersed nature of abnormal cells within cytopathological WSIs, we propose a strategy that leverages the pretrained foundation model to filter the top$k$ high-risk patches. Subsequently, we suggest parameter-efficient fine-tuning (PEFT) of a large foundation model using contrastive learning on the filtered patches to enhance its representation ability for task-specific signals. By training only the added linear adapters, we enhance the learning of patch-level features with substantially reduced time and memory consumption. Experiments conducted on the CSD and FNAC 2019 datasets demonstrate that the proposed method enhances the performance of various MIL methods and achieves state-of-the-art (SOTA) performance. The code and trained models are publicly available at https://github.com/CVIU-CSU/TCT-InfoNCE.
<div id='section'>Paperid: <span id='pid'>150, <a href='https://arxiv.org/pdf/2407.11216.pdf' target='_blank'>https://arxiv.org/pdf/2407.11216.pdf</a></span>   <span><a href='https://github.com/Chohoonhee/EV-WSSS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hoonhee Cho, Sung-Hoon Yoon, Hyeokjun Kweon, Kuk-Jin Yoon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.11216">Finding Meaning in Points: Weakly Supervised Semantic Segmentation for Event Cameras</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Event cameras excel in capturing high-contrast scenes and dynamic objects, offering a significant advantage over traditional frame-based cameras. Despite active research into leveraging event cameras for semantic segmentation, generating pixel-wise dense semantic maps for such challenging scenarios remains labor-intensive. As a remedy, we present EV-WSSS: a novel weakly supervised approach for event-based semantic segmentation that utilizes sparse point annotations. To fully leverage the temporal characteristics of event data, the proposed framework performs asymmetric dual-student learning between 1) the original forward event data and 2) the longer reversed event data, which contain complementary information from the past and the future, respectively. Besides, to mitigate the challenges posed by sparse supervision, we propose feature-level contrastive learning based on class-wise prototypes, carefully aggregated at both spatial region and sample levels. Additionally, we further excavate the potential of our dual-student learning model by exchanging prototypes between the two learning paths, thereby harnessing their complementary strengths. With extensive experiments on various datasets, including DSEC Night-Point with sparse point annotations newly provided by this paper, the proposed method achieves substantial segmentation results even without relying on pixel-level dense ground truths. The code and dataset are available at https://github.com/Chohoonhee/EV-WSSS.
<div id='section'>Paperid: <span id='pid'>151, <a href='https://arxiv.org/pdf/2407.09786.pdf' target='_blank'>https://arxiv.org/pdf/2407.09786.pdf</a></span>   <span><a href='https://github.com/ltwu6/malspc' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lintai Wu, Xianjing Cheng, Yong Xu, Huanqiang Zeng, Junhui Hou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.09786">Unsupervised 3D Point Cloud Completion via Multi-view Adversarial Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In real-world scenarios, scanned point clouds are often incomplete due to occlusion issues. The tasks of self-supervised and weakly-supervised point cloud completion involve reconstructing missing regions of these incomplete objects without the supervision of complete ground truth. Current methods either rely on multiple views of partial observations for supervision or overlook the intrinsic geometric similarity that can be identified and utilized from the given partial point clouds. In this paper, we propose MAL-UPC, a framework that effectively leverages both region-level and category-specific geometric similarities to complete missing structures. Our MAL-UPC does not require any 3D complete supervision and only necessitates single-view partial observations in the training set. Specifically, we first introduce a Pattern Retrieval Network to retrieve similar position and curvature patterns between the partial input and the predicted shape, then leverage these similarities to densify and refine the reconstructed results. Additionally, we render the reconstructed complete shape into multi-view depth maps and design an adversarial learning module to learn the geometry of the target shape from category-specific single-view depth images of the partial point clouds in the training set. To achieve anisotropic rendering, we design a density-aware radius estimation algorithm to improve the quality of the rendered images. Our MAL-UPC outperforms current state-of-the-art self-supervised methods and even some unpaired approaches. We will make the source code publicly available at https://github.com/ltwu6/malspc
<div id='section'>Paperid: <span id='pid'>152, <a href='https://arxiv.org/pdf/2407.07406.pdf' target='_blank'>https://arxiv.org/pdf/2407.07406.pdf</a></span>   <span><a href='https://github.com/med-air/GazeMedSeg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Zhong, Chenhui Tang, Yumeng Yang, Ruoxi Qi, Kang Zhou, Yuqi Gong, Pheng Ann Heng, Janet H. Hsiao, Qi Dou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.07406">Weakly-supervised Medical Image Segmentation with Gaze Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Eye gaze that reveals human observational patterns has increasingly been incorporated into solutions for vision tasks. Despite recent explorations on leveraging gaze to aid deep networks, few studies exploit gaze as an efficient annotation approach for medical image segmentation which typically entails heavy annotating costs. In this paper, we propose to collect dense weak supervision for medical image segmentation with a gaze annotation scheme. To train with gaze, we propose a multi-level framework that trains multiple networks from discriminative human attention, simulated with a set of pseudo-masks derived by applying hierarchical thresholds on gaze heatmaps. Furthermore, to mitigate gaze noise, a cross-level consistency is exploited to regularize overfitting noisy labels, steering models toward clean patterns learned by peer networks. The proposed method is validated on two public medical datasets of polyp and prostate segmentation tasks. We contribute a high-quality gaze dataset entitled GazeMedSeg as an extension to the popular medical segmentation datasets. To the best of our knowledge, this is the first gaze dataset for medical image segmentation. Our experiments demonstrate that gaze annotation outperforms previous label-efficient annotation schemes in terms of both performance and annotation time. Our collected gaze data and code are available at: https://github.com/med-air/GazeMedSeg.
<div id='section'>Paperid: <span id='pid'>153, <a href='https://arxiv.org/pdf/2407.05607.pdf' target='_blank'>https://arxiv.org/pdf/2407.05607.pdf</a></span>   <span><a href='https://github.com/dzungdoan6/WSTTA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Anh-Dzung Doan, Bach Long Nguyen, Terry Lim, Madhuka Jayawardhana, Surabhi Gupta, Christophe Guettier, Ian Reid, Markus Wagner, Tat-Jun Chin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05607">Weakly Supervised Test-Time Domain Adaptation for Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prior to deployment, an object detector is trained on a dataset compiled from a previous data collection campaign. However, the environment in which the object detector is deployed will invariably evolve, particularly in outdoor settings where changes in lighting, weather and seasons will significantly affect the appearance of the scene and target objects. It is almost impossible for all potential scenarios that the object detector may come across to be present in a finite training dataset. This necessitates continuous updates to the object detector to maintain satisfactory performance. Test-time domain adaptation techniques enable machine learning models to self-adapt based on the distributions of the testing data. However, existing methods mainly focus on fully automated adaptation, which makes sense for applications such as self-driving cars. Despite the prevalence of fully automated approaches, in some applications such as surveillance, there is usually a human operator overseeing the system's operation. We propose to involve the operator in test-time domain adaptation to raise the performance of object detection beyond what is achievable by fully automated adaptation. To reduce manual effort, the proposed method only requires the operator to provide weak labels, which are then used to guide the adaptation process. Furthermore, the proposed method can be performed in a streaming setting, where each online sample is observed only once. We show that the proposed method outperforms existing works, demonstrating a great benefit of human-in-the-loop test-time domain adaptation. Our code is publicly available at https://github.com/dzungdoan6/WSTTA
<div id='section'>Paperid: <span id='pid'>154, <a href='https://arxiv.org/pdf/2407.03575.pdf' target='_blank'>https://arxiv.org/pdf/2407.03575.pdf</a></span>   <span><a href='https://github.com/ChongQingNoSubway/DGR-MIL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhui Zhu, Xiwen Chen, Peijie Qiu, Aristeidis Sotiras, Abolfazl Razi, Yalin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.03575">DGR-MIL: Exploring Diverse Global Representation in Multiple Instance Learning for Whole Slide Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple instance learning (MIL) stands as a powerful approach in weakly supervised learning, regularly employed in histological whole slide image (WSI) classification for detecting tumorous lesions. However, existing mainstream MIL methods focus on modeling correlation between instances while overlooking the inherent diversity among instances. However, few MIL methods have aimed at diversity modeling, which empirically show inferior performance but with a high computational cost. To bridge this gap, we propose a novel MIL aggregation method based on diverse global representation (DGR-MIL), by modeling diversity among instances through a set of global vectors that serve as a summary of all instances. First, we turn the instance correlation into the similarity between instance embeddings and the predefined global vectors through a cross-attention mechanism. This stems from the fact that similar instance embeddings typically would result in a higher correlation with a certain global vector. Second, we propose two mechanisms to enforce the diversity among the global vectors to be more descriptive of the entire bag: (i) positive instance alignment and (ii) a novel, efficient, and theoretically guaranteed diversification learning paradigm. Specifically, the positive instance alignment module encourages the global vectors to align with the center of positive instances (e.g., instances containing tumors in WSI). To further diversify the global representations, we propose a novel diversification learning paradigm leveraging the determinantal point process. The proposed model outperforms the state-of-the-art MIL aggregation models by a substantial margin on the CAMELYON-16 and the TCGA-lung cancer datasets. The code is available at \url{https://github.com/ChongQingNoSubway/DGR-MIL}.
<div id='section'>Paperid: <span id='pid'>155, <a href='https://arxiv.org/pdf/2407.03009.pdf' target='_blank'>https://arxiv.org/pdf/2407.03009.pdf</a></span>   <span><a href='https://github.com/Kainmueller-Lab/TW-autoencoder' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyan Yu, Jannik Franzen, Wojciech Samek, Marina M. -C. HÃ¶hne, Dagmar Kainmueller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.03009">Model Guidance via Explanations Turns Image Classifiers into Segmentation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Heatmaps generated on inputs of image classification networks via explainable AI methods like Grad-CAM and LRP have been observed to resemble segmentations of input images in many cases. Consequently, heatmaps have also been leveraged for achieving weakly supervised segmentation with image-level supervision. On the other hand, losses can be imposed on differentiable heatmaps, which has been shown to serve for (1)~improving heatmaps to be more human-interpretable, (2)~regularization of networks towards better generalization, (3)~training diverse ensembles of networks, and (4)~for explicitly ignoring confounding input features. Due to the latter use case, the paradigm of imposing losses on heatmaps is often referred to as "Right for the right reasons". We unify these two lines of research by investigating semi-supervised segmentation as a novel use case for the Right for the Right Reasons paradigm. First, we show formal parallels between differentiable heatmap architectures and standard encoder-decoder architectures for image segmentation. Second, we show that such differentiable heatmap architectures yield competitive results when trained with standard segmentation losses. Third, we show that such architectures allow for training with weak supervision in the form of image-level labels and small numbers of pixel-level labels, outperforming comparable encoder-decoder models. Code is available at \url{https://github.com/Kainmueller-Lab/TW-autoencoder}.
<div id='section'>Paperid: <span id='pid'>156, <a href='https://arxiv.org/pdf/2407.02768.pdf' target='_blank'>https://arxiv.org/pdf/2407.02768.pdf</a></span>   <span><a href='https://github.com/NUST-Machine-Intelligence-Laboratory/KTSE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Chen, XiRuo Jiang, Gensheng Pei, Zeren Sun, Yucheng Wang, Yazhou Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.02768">Knowledge Transfer with Simulated Inter-Image Erasing for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Though adversarial erasing has prevailed in weakly supervised semantic segmentation to help activate integral object regions, existing approaches still suffer from the dilemma of under-activation and over-expansion due to the difficulty in determining when to stop erasing. In this paper, we propose a \textbf{K}nowledge \textbf{T}ransfer with \textbf{S}imulated Inter-Image \textbf{E}rasing (KTSE) approach for weakly supervised semantic segmentation to alleviate the above problem. In contrast to existing erasing-based methods that remove the discriminative part for more object discovery, we propose a simulated inter-image erasing scenario to weaken the original activation by introducing extra object information. Then, object knowledge is transferred from the anchor image to the consequent less activated localization map to strengthen network localization ability. Considering the adopted bidirectional alignment will also weaken the anchor image activation if appropriate constraints are missing, we propose a self-supervised regularization module to maintain the reliable activation in discriminative regions and improve the inter-class object boundary recognition for complex images with multiple categories of objects. In addition, we resort to intra-image erasing and propose a multi-granularity alignment module to gently enlarge the object activation to boost the object knowledge transfer. Extensive experiments and ablation studies on PASCAL VOC 2012 and COCO datasets demonstrate the superiority of our proposed approach. Source codes and models are available at https://github.com/NUST-Machine-Intelligence-Laboratory/KTSE.
<div id='section'>Paperid: <span id='pid'>157, <a href='https://arxiv.org/pdf/2407.00614.pdf' target='_blank'>https://arxiv.org/pdf/2407.00614.pdf</a></span>   <span><a href='https://github.com/yangfan293/GAAF-DEX' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/yangfan293/GAAF-DEX' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Yang, Wenrui Chen, Kailun Yang, Haoran Lin, Dongsheng Luo, Conghui Tang, Zhiyong Li, Yaonan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.00614">Learning Granularity-Aware Affordances from Human-Object Interaction for Tool-Based Functional Dexterous Grasping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To enable robots to use tools, the initial step is teaching robots to employ dexterous gestures for touching specific areas precisely where tasks are performed. Affordance features of objects serve as a bridge in the functional interaction between agents and objects. However, leveraging these affordance cues to help robots achieve functional tool grasping remains unresolved. To address this, we propose a granularity-aware affordance feature extraction method for locating functional affordance areas and predicting dexterous coarse gestures. We study the intrinsic mechanisms of human tool use. On one hand, we use fine-grained affordance features of object-functional finger contact areas to locate functional affordance regions. On the other hand, we use highly activated coarse-grained affordance features in hand-object interaction regions to predict grasp gestures. Additionally, we introduce a model-based post-processing module that transforms affordance localization and gesture prediction into executable robotic actions. This forms GAAF-Dex, a complete framework that learns Granularity-Aware Affordances from human-object interaction to enable tool-based functional grasping with dexterous hands. Unlike fully-supervised methods that require extensive data annotation, we employ a weakly supervised approach to extract relevant cues from exocentric (Exo) images of hand-object interactions to supervise feature extraction in egocentric (Ego) images. To support this approach, we have constructed a small-scale dataset, Functional Affordance Hand-object Interaction Dataset (FAH), which includes nearly 6K images of functional hand-object interaction Exo images and Ego images. Extensive experiments on the dataset demonstrate that our method outperforms state-of-the-art methods. The source code and the established dataset are available at https://github.com/yangfan293/GAAF-DEX.
<div id='section'>Paperid: <span id='pid'>158, <a href='https://arxiv.org/pdf/2406.19364.pdf' target='_blank'>https://arxiv.org/pdf/2406.19364.pdf</a></span>   <span><a href='https://github.com/xyx1024/SimTxtSeg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxin Xie, Tao Zhou, Yi Zhou, Geng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.19364">SimTxtSeg: Weakly-Supervised Medical Image Segmentation with Simple Text Cues</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised medical image segmentation is a challenging task that aims to reduce the annotation cost while keep the segmentation performance. In this paper, we present a novel framework, SimTxtSeg, that leverages simple text cues to generate high-quality pseudo-labels and study the cross-modal fusion in training segmentation models, simultaneously. Our contribution consists of two key components: an effective Textual-to-Visual Cue Converter that produces visual prompts from text prompts on medical images, and a text-guided segmentation model with Text-Vision Hybrid Attention that fuses text and image features. We evaluate our framework on two medical image segmentation tasks: colonic polyp segmentation and MRI brain tumor segmentation, and achieve consistent state-of-the-art performance. Source code is available at: https://github.com/xyx1024/SimTxtSeg.
<div id='section'>Paperid: <span id='pid'>159, <a href='https://arxiv.org/pdf/2406.18815.pdf' target='_blank'>https://arxiv.org/pdf/2406.18815.pdf</a></span>   <span><a href='https://github.com/c0510gy/MissionGNN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sanggeon Yun, Ryozo Masukawa, Minhyoung Na, Mohsen Imani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.18815">MissionGNN: Hierarchical Multimodal GNN-based Weakly Supervised Video Anomaly Recognition with Mission-Specific Knowledge Graph Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the context of escalating safety concerns across various domains, the tasks of Video Anomaly Detection (VAD) and Video Anomaly Recognition (VAR) have emerged as critically important for applications in intelligent surveillance, evidence investigation, violence alerting, etc. These tasks, aimed at identifying and classifying deviations from normal behavior in video data, face significant challenges due to the rarity of anomalies which leads to extremely imbalanced data and the impracticality of extensive frame-level data annotation for supervised learning. This paper introduces a novel hierarchical graph neural network (GNN) based model MissionGNN that addresses these challenges by leveraging a state-of-the-art large language model and a comprehensive knowledge graph for efficient weakly supervised learning in VAR. Our approach circumvents the limitations of previous methods by avoiding heavy gradient computations on large multimodal models and enabling fully frame-level training without fixed video segmentation. Utilizing automated, mission-specific knowledge graph generation, our model provides a practical and efficient solution for real-time video analysis without the constraints of previous segmentation-based or multimodal approaches. Experimental validation on benchmark datasets demonstrates our model's performance in VAD and VAR, highlighting its potential to redefine the landscape of anomaly detection and recognition in video surveillance systems. The code is available here: https://github.com/c0510gy/MissionGNN.
<div id='section'>Paperid: <span id='pid'>160, <a href='https://arxiv.org/pdf/2406.18558.pdf' target='_blank'>https://arxiv.org/pdf/2406.18558.pdf</a></span>   <span><a href='https://github.com/wsis-seg/BAISeg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tengbo Wang, Yu Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.18558">BAISeg: Boundary Assisted Weakly Supervised Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>How to extract instance-level masks without instance-level supervision is the main challenge of weakly supervised instance segmentation (WSIS). Popular WSIS methods estimate a displacement field (DF) via learning inter-pixel relations and perform clustering to identify instances. However, the resulting instance centroids are inherently unstable and vary significantly across different clustering algorithms. In this paper, we propose Boundary-Assisted Instance Segmentation (BAISeg), which is a novel paradigm for WSIS that realizes instance segmentation with pixel-level annotations. BAISeg comprises an instance-aware boundary detection (IABD) branch and a semantic segmentation branch. The IABD branch identifies instances by predicting class-agnostic instance boundaries rather than instance centroids, therefore, it is different from previous DF-based approaches. In particular, we proposed the Cascade Fusion Module (CFM) and the Deep Mutual Attention (DMA) in the IABD branch to obtain rich contextual information and capture instance boundaries with weak responses. During the training phase, we employed Pixel-to-Pixel Contrast to enhance the discriminative capacity of the IABD branch. This further strengthens the continuity and closedness of the instance boundaries. Extensive experiments on PASCAL VOC 2012 and MS COCO demonstrate the effectiveness of our approach, and we achieve considerable performance with only pixel-level annotations. The code will be available at https://github.com/wsis-seg/BAISeg.
<div id='section'>Paperid: <span id='pid'>161, <a href='https://arxiv.org/pdf/2406.15805.pdf' target='_blank'>https://arxiv.org/pdf/2406.15805.pdf</a></span>   <span><a href='https://github.com/hzx-9894/MMA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoxin Hu, Keyan Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.15805">Smart Feature is What You Need</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Lack of shape guidance and label jitter caused by information deficiency of weak label are the main problems in 3D weakly-supervised object detection. Current weakly-supervised models often use heuristics or assumptions methods to infer information from weak labels without taking advantage of the inherent clues of weakly-supervised and fully-supervised methods, thus it is difficult to explore a method that combines data utilization efficiency and model accuracy. In an attempt to address these issues, we propose a novel plug-and-in point cloud feature representation network called Multi-scale Mixed Attention (MMA). MMA utilizes adjacency attention within neighborhoods and disparity attention at different density scales to build a feature representation network. The smart feature representation obtained from MMA has shape tendency and object existence area inference, which can constrain the region of the detection boxes, thereby alleviating the problems caused by the information default of weak labels. Extensive experiments show that in indoor weak label scenarios, the fully-supervised network can perform close to that of the weakly-supervised network merely through the improvement of point feature by MMA. At the same time, MMA can turn waste into treasure, reversing the label jitter problem that originally interfered with weakly-supervised detection into the source of data enhancement, strengthening the performance of existing weak supervision detection methods. Our code is available at https://github.com/hzx-9894/MMA.
<div id='section'>Paperid: <span id='pid'>162, <a href='https://arxiv.org/pdf/2406.14274.pdf' target='_blank'>https://arxiv.org/pdf/2406.14274.pdf</a></span>   <span><a href='https://github.com/mc-lan/SP-TCL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengcheng Lan, Min Meng, Jun Yu, Jigang Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.14274">Learning to Discover Knowledge: A Weakly-Supervised Partial Domain Adaptation Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain adaptation has shown appealing performance by leveraging knowledge from a source domain with rich annotations. However, for a specific target task, it is cumbersome to collect related and high-quality source domains. In real-world scenarios, large-scale datasets corrupted with noisy labels are easy to collect, stimulating a great demand for automatic recognition in a generalized setting, i.e., weakly-supervised partial domain adaptation (WS-PDA), which transfers a classifier from a large source domain with noises in labels to a small unlabeled target domain. As such, the key issues of WS-PDA are: 1) how to sufficiently discover the knowledge from the noisy labeled source domain and the unlabeled target domain, and 2) how to successfully adapt the knowledge across domains. In this paper, we propose a simple yet effective domain adaptation approach, termed as self-paced transfer classifier learning (SP-TCL), to address the above issues, which could be regarded as a well-performing baseline for several generalized domain adaptation tasks. The proposed model is established upon the self-paced learning scheme, seeking a preferable classifier for the target domain. Specifically, SP-TCL learns to discover faithful knowledge via a carefully designed prudent loss function and simultaneously adapts the learned knowledge to the target domain by iteratively excluding source examples from training under the self-paced fashion. Extensive evaluations on several benchmark datasets demonstrate that SP-TCL significantly outperforms state-of-the-art approaches on several generalized domain adaptation tasks.
<div id='section'>Paperid: <span id='pid'>163, <a href='https://arxiv.org/pdf/2406.13124.pdf' target='_blank'>https://arxiv.org/pdf/2406.13124.pdf</a></span>   <span><a href='https://github.com/amazon-science/learning-to-generate-answers-with-citations' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rami Aly, Zhiqiang Tang, Samson Tan, George Karypis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.13124">Learning to Generate Answers with Citations via Factual Consistency Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) frequently hallucinate, impeding their reliability in mission-critical situations. One approach to address this issue is to provide citations to relevant sources alongside generated content, enhancing the verifiability of generations. However, citing passages accurately in answers remains a substantial challenge. This paper proposes a weakly-supervised fine-tuning method leveraging factual consistency models (FCMs). Our approach alternates between generating texts with citations and supervised fine-tuning with FCM-filtered citation data. Focused learning is integrated into the objective, directing the fine-tuning process to emphasise the factual unit tokens, as measured by an FCM. Results on the ALCE few-shot citation benchmark with various instruction-tuned LLMs demonstrate superior performance compared to in-context learning, vanilla supervised fine-tuning, and state-of-the-art methods, with an average improvement of $34.1$, $15.5$, and $10.5$ citation F$_1$ points, respectively. Moreover, in a domain transfer setting we show that the obtained citation generation ability robustly transfers to unseen datasets. Notably, our citation improvements contribute to the lowest factual error rate across baselines.
<div id='section'>Paperid: <span id='pid'>164, <a href='https://arxiv.org/pdf/2406.11189.pdf' target='_blank'>https://arxiv.org/pdf/2406.11189.pdf</a></span>   <span><a href='https://github.com/zbf1991/WeCLIP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingfeng Zhang, Siyue Yu, Yunchao Wei, Yao Zhao, Jimin Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.11189">Frozen CLIP: A Strong Backbone for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation has witnessed great achievements with image-level labels. Several recent approaches use the CLIP model to generate pseudo labels for training an individual segmentation model, while there is no attempt to apply the CLIP model as the backbone to directly segment objects with image-level labels. In this paper, we propose WeCLIP, a CLIP-based single-stage pipeline, for weakly supervised semantic segmentation. Specifically, the frozen CLIP model is applied as the backbone for semantic feature extraction, and a new decoder is designed to interpret extracted semantic features for final prediction. Meanwhile, we utilize the above frozen backbone to generate pseudo labels for training the decoder. Such labels cannot be optimized during training. We then propose a refinement module (RFM) to rectify them dynamically. Our architecture enforces the proposed decoder and RFM to benefit from each other to boost the final performance. Extensive experiments show that our approach significantly outperforms other approaches with less training cost. Additionally, our WeCLIP also obtains promising results for fully supervised settings. The code is available at https://github.com/zbf1991/WeCLIP.
<div id='section'>Paperid: <span id='pid'>165, <a href='https://arxiv.org/pdf/2406.04280.pdf' target='_blank'>https://arxiv.org/pdf/2406.04280.pdf</a></span>   <span><a href='https://github.com/bifold-pathomics/xMIL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Julius Hense, Mina Jamshidi Idaji, Oliver Eberle, Thomas Schnake, Jonas Dippel, Laure Ciernik, Oliver Buchstab, Andreas Mock, Frederick Klauschen, Klaus-Robert MÃ¼ller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04280">xMIL: Insightful Explanations for Multiple Instance Learning in Histopathology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple instance learning (MIL) is an effective and widely used approach for weakly supervised machine learning. In histopathology, MIL models have achieved remarkable success in tasks like tumor detection, biomarker prediction, and outcome prognostication. However, MIL explanation methods are still lagging behind, as they are limited to small bag sizes or disregard instance interactions. We revisit MIL through the lens of explainable AI (XAI) and introduce xMIL, a refined framework with more general assumptions. We demonstrate how to obtain improved MIL explanations using layer-wise relevance propagation (LRP) and conduct extensive evaluation experiments on three toy settings and four real-world histopathology datasets. Our approach consistently outperforms previous explanation attempts with particularly improved faithfulness scores on challenging biomarker prediction tasks. Finally, we showcase how xMIL explanations enable pathologists to extract insights from MIL models, representing a significant advance for knowledge discovery and model debugging in digital histopathology. Codes are available at: https://github.com/bifold-pathomics/xMIL.
<div id='section'>Paperid: <span id='pid'>166, <a href='https://arxiv.org/pdf/2406.02822.pdf' target='_blank'>https://arxiv.org/pdf/2406.02822.pdf</a></span>   <span><a href='https://github.com/andreschreiber/W-RIZZ' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Andre Schreiber, Arun N. Sivakumar, Peter Du, Mateus V. Gasparino, Girish Chowdhary, Katherine Driggs-Campbell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.02822">W-RIZZ: A Weakly-Supervised Framework for Relative Traversability Estimation in Mobile Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Successful deployment of mobile robots in unstructured domains requires an understanding of the environment and terrain to avoid hazardous areas, getting stuck, and colliding with obstacles. Traversability estimation--which predicts where in the environment a robot can travel--is one prominent approach that tackles this problem. Existing geometric methods may ignore important semantic considerations, while semantic segmentation approaches involve a tedious labeling process. Recent self-supervised methods reduce labeling tedium, but require additional data or models and tend to struggle to explicitly label untraversable areas. To address these limitations, we introduce a weakly-supervised method for relative traversability estimation. Our method involves manually annotating the relative traversability of a small number of point pairs, which significantly reduces labeling effort compared to traditional segmentation-based methods and avoids the limitations of self-supervised methods. We further improve the performance of our method through a novel cross-image labeling strategy and loss function. We demonstrate the viability and performance of our method through deployment on a mobile robot in outdoor environments.
<div id='section'>Paperid: <span id='pid'>167, <a href='https://arxiv.org/pdf/2405.16628.pdf' target='_blank'>https://arxiv.org/pdf/2405.16628.pdf</a></span>   <span><a href='https://github.com/s-sd/spurl/tree/main/wss' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaheer U. Saeed, Shiqi Huang, JoÃ£o Ramalhinho, Iani J. M. B. Gayo, Nina MontaÃ±a-Brown, Ester Bonmati, Stephen P. Pereira, Brian Davidson, Dean C. Barratt, Matthew J. Clarkson, Yipeng Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.16628">Competing for pixels: a self-play algorithm for weakly-supervised segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised segmentation (WSS) methods, reliant on image-level labels indicating object presence, lack explicit correspondence between labels and regions of interest (ROIs), posing a significant challenge. Despite this, WSS methods have attracted attention due to their much lower annotation costs compared to fully-supervised segmentation. Leveraging reinforcement learning (RL) self-play, we propose a novel WSS method that gamifies image segmentation of a ROI. We formulate segmentation as a competition between two agents that compete to select ROI-containing patches until exhaustion of all such patches. The score at each time-step, used to compute the reward for agent training, represents likelihood of object presence within the selection, determined by an object presence detector pre-trained using only image-level binary classification labels of object presence. Additionally, we propose a game termination condition that can be called by either side upon exhaustion of all ROI-containing patches, followed by the selection of a final patch from each. Upon termination, the agent is incentivised if ROI-containing patches are exhausted or disincentivised if an ROI-containing patch is found by the competitor. This competitive setup ensures minimisation of over- or under-segmentation, a common problem with WSS methods. Extensive experimentation across four datasets demonstrates significant performance improvements over recent state-of-the-art methods. Code: https://github.com/s-sd/spurl/tree/main/wss
<div id='section'>Paperid: <span id='pid'>168, <a href='https://arxiv.org/pdf/2405.16038.pdf' target='_blank'>https://arxiv.org/pdf/2405.16038.pdf</a></span>   <span><a href='https://github.com/XueZ-phd/Efficient-RGB-T-Early-Fusion-Detection' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xue Zhang, Si-Yuan Cao, Fang Wang, Runmin Zhang, Zhe Wu, Xiaohan Zhang, Xiaokai Bai, Hui-Liang Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.16038">Rethinking Early-Fusion Strategies for Improved Multispectral Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most recent multispectral object detectors employ a two-branch structure to extract features from RGB and thermal images. While the two-branch structure achieves better performance than a single-branch structure, it overlooks inference efficiency. This conflict is increasingly aggressive, as recent works solely pursue higher performance rather than both performance and efficiency. In this paper, we address this issue by improving the performance of efficient single-branch structures. We revisit the reasons causing the performance gap between these structures. For the first time, we reveal the information interference problem in the naive early-fusion strategy adopted by previous single-branch structures. Besides, we find that the domain gap between multispectral images, and weak feature representation of the single-branch structure are also key obstacles for performance. Focusing on these three problems, we propose corresponding solutions, including a novel shape-priority early-fusion strategy, a weakly supervised learning method, and a core knowledge distillation technique. Experiments demonstrate that single-branch networks equipped with these three contributions achieve significant performance enhancements while retaining high efficiency. Our code will be available at \url{https://github.com/XueZ-phd/Efficient-RGB-T-Early-Fusion-Detection}.
<div id='section'>Paperid: <span id='pid'>169, <a href='https://arxiv.org/pdf/2405.15961.pdf' target='_blank'>https://arxiv.org/pdf/2405.15961.pdf</a></span>   <span><a href='https://github.com/fpsluozi/SMD-SMOS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiran Luo, Joshua Feinglass, Tejas Gokhale, Kuan-Cheng Lee, Chitta Baral, Yezhou Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15961">Grounding Stylistic Domain Generalization with Quantitative Domain Shift Measures and Synthetic Scene Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG) is a challenging task in machine learning that requires a coherent ability to comprehend shifts across various domains through extraction of domain-invariant features. DG performance is typically evaluated by performing image classification in domains of various image styles. However, current methodology lacks quantitative understanding about shifts in stylistic domain, and relies on a vast amount of pre-training data, such as ImageNet1K, which are predominantly in photo-realistic style with weakly supervised class labels. Such a data-driven practice could potentially result in spurious correlation and inflated performance on DG benchmarks. In this paper, we introduce a new DG paradigm to address these risks. We first introduce two new quantitative measures ICV and IDD to describe domain shifts in terms of consistency of classes within one domain and similarity between two stylistic domains. We then present SuperMarioDomains (SMD), a novel synthetic multi-domain dataset sampled from video game scenes with more consistent classes and sufficient dissimilarity compared to ImageNet1K. We demonstrate our DG method SMOS. SMOS first uses SMD to train a precursor model, which is then used to ground the training on a DG benchmark. We observe that SMOS contributes to state-of-the-art performance across five DG benchmarks, gaining large improvements to performances on abstract domains along with on-par or slight improvements to those on photo-realistic domains. Our qualitative analysis suggests that these improvements can be attributed to reduced distributional divergence between originally distant domains. Our data are available at https://github.com/fpsluozi/SMD-SMOS .
<div id='section'>Paperid: <span id='pid'>170, <a href='https://arxiv.org/pdf/2405.15275.pdf' target='_blank'>https://arxiv.org/pdf/2405.15275.pdf</a></span>   <span><a href='https://github.com/Biomedical-Data-Analysis-Laboratory/GradeMIL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Saul Fuster, Umay Kiraz, Trygve EftestÃ¸l, Emiel A. M. Janssen, Kjersti Engan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15275">NMGrad: Advancing Histopathological Bladder Cancer Grading with Weakly Supervised Deep Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The most prevalent form of bladder cancer is urothelial carcinoma, characterized by a high recurrence rate and substantial lifetime treatment costs for patients. Grading is a prime factor for patient risk stratification, although it suffers from inconsistencies and variations among pathologists. Moreover, absence of annotations in medical imaging difficults training deep learning models. To address these challenges, we introduce a pipeline designed for bladder cancer grading using histological slides. First, it extracts urothelium tissue tiles at different magnification levels, employing a convolutional neural network for processing for feature extraction. Then, it engages in the slide-level prediction process. It employs a nested multiple instance learning approach with attention to predict the grade. To distinguish different levels of malignancy within specific regions of the slide, we include the origins of the tiles in our analysis. The attention scores at region level is shown to correlate with verified high-grade regions, giving some explainability to the model. Clinical evaluations demonstrate that our model consistently outperforms previous state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>171, <a href='https://arxiv.org/pdf/2405.15264.pdf' target='_blank'>https://arxiv.org/pdf/2405.15264.pdf</a></span>   <span><a href='https://github.com/Biomedical-Data-Analysis-Laboratory/HistoPrognostics' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Saul Fuster, Farbod Khoraminia, Julio Silva-RodrÃ­guez, Umay Kiraz, Geert J. L. H. van Leenders, Trygve EftestÃ¸l, Valery Naranjo, Emiel A. M. Janssen, Tahlita C. M. Zuiverloon, Kjersti Engan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15264">Self-Contrastive Weakly Supervised Learning Framework for Prognostic Prediction Using Whole Slide Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a pioneering investigation into the application of deep learning techniques to analyze histopathological images for addressing the substantial challenge of automated prognostic prediction. Prognostic prediction poses a unique challenge as the ground truth labels are inherently weak, and the model must anticipate future events that are not directly observable in the image. To address this challenge, we propose a novel three-part framework comprising of a convolutional network based tissue segmentation algorithm for region of interest delineation, a contrastive learning module for feature extraction, and a nested multiple instance learning classification module. Our study explores the significance of various regions of interest within the histopathological slides and exploits diverse learning scenarios. The pipeline is initially validated on artificially generated data and a simpler diagnostic task. Transitioning to prognostic prediction, tasks become more challenging. Employing bladder cancer as use case, our best models yield an AUC of 0.721 and 0.678 for recurrence and treatment outcome prediction respectively.
<div id='section'>Paperid: <span id='pid'>172, <a href='https://arxiv.org/pdf/2405.15228.pdf' target='_blank'>https://arxiv.org/pdf/2405.15228.pdf</a></span>   <span><a href='https://github.com/Tranquilxu/TMP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongnian Li, Jinghao Xu, Peng Ying, Meng Wei, Xinzheng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15228">Learning from True-False Labels via Multi-modal Prompt Retrieving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pre-trained Vision-Language Models (VLMs) exhibit strong zero-shot classification abilities, demonstrating great potential for generating weakly supervised labels. Unfortunately, existing weakly supervised learning methods are short of ability in generating accurate labels via VLMs. In this paper, we propose a novel weakly supervised labeling setting, namely True-False Labels (TFLs) which can achieve high accuracy when generated by VLMs. The TFL indicates whether an instance belongs to the label, which is randomly and uniformly sampled from the candidate label set. Specifically, we theoretically derive a risk-consistent estimator to explore and utilize the conditional probability distribution information of TFLs. Besides, we propose a convolutional-based Multi-modal Prompt Retrieving (MRP) method to bridge the gap between the knowledge of VLMs and target learning tasks. Experimental results demonstrate the effectiveness of the proposed TFL setting and MRP learning method. The code to reproduce the experiments is at https://github.com/Tranquilxu/TMP.
<div id='section'>Paperid: <span id='pid'>173, <a href='https://arxiv.org/pdf/2405.14271.pdf' target='_blank'>https://arxiv.org/pdf/2405.14271.pdf</a></span>   <span><a href='https://github.com/Eaphan/OLIVINE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Zhang, Junhui Hou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14271">Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contrastive image-to-LiDAR knowledge transfer, commonly used for learning 3D representations with synchronized images and point clouds, often faces a self-conflict dilemma. This issue arises as contrastive losses unintentionally dissociate features of unmatched points and pixels that share semantic labels, compromising the integrity of learned representations. To overcome this, we harness Visual Foundation Models (VFMs), which have revolutionized the acquisition of pixel-level semantics, to enhance 3D representation learning. Specifically, we utilize off-the-shelf VFMs to generate semantic labels for weakly-supervised pixel-to-point contrastive distillation. Additionally, we employ von Mises-Fisher distributions to structure the feature space, ensuring semantic embeddings within the same class remain consistent across varying inputs. Furthermore, we adapt sampling probabilities of points to address imbalances in spatial distribution and category frequency, promoting comprehensive and balanced learning. Extensive experiments demonstrate that our approach mitigates the challenges posed by traditional methods and consistently surpasses existing image-to-LiDAR contrastive distillation methods in downstream tasks. The source code is available at https://github.com/Eaphan/OLIVINE.
<div id='section'>Paperid: <span id='pid'>174, <a href='https://arxiv.org/pdf/2405.06288.pdf' target='_blank'>https://arxiv.org/pdf/2405.06288.pdf</a></span>   <span><a href='https://github.com/Torpedo2648/PCLMix' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Lei, Haolun Luo, Lituan Wang, Zhenwei Zhang, Lei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06288">PCLMix: Weakly Supervised Medical Image Segmentation via Pixel-Level Contrastive Learning and Dynamic Mix Augmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In weakly supervised medical image segmentation, the absence of structural priors and the discreteness of class feature distribution present a challenge, i.e., how to accurately propagate supervision signals from local to global regions without excessively spreading them to other irrelevant regions? To address this, we propose a novel weakly supervised medical image segmentation framework named PCLMix, comprising dynamic mix augmentation, pixel-level contrastive learning, and consistency regularization strategies. Specifically, PCLMix is built upon a heterogeneous dual-decoder backbone, addressing the absence of structural priors through a strategy of dynamic mix augmentation during training. To handle the discrete distribution of class features, PCLMix incorporates pixel-level contrastive learning based on prediction uncertainty, effectively enhancing the model's ability to differentiate inter-class pixel differences and intra-class consistency. Furthermore, to reinforce segmentation consistency and robustness, PCLMix employs an auxiliary decoder for dual consistency regularization. In the inference phase, the auxiliary decoder will be dropped and no computation complexity is increased. Extensive experiments on the ACDC dataset demonstrate that PCLMix appropriately propagates local supervision signals to the global scale, further narrowing the gap between weakly supervised and fully supervised segmentation methods. Our code is available at https://github.com/Torpedo2648/PCLMix.
<div id='section'>Paperid: <span id='pid'>175, <a href='https://arxiv.org/pdf/2405.05130.pdf' target='_blank'>https://arxiv.org/pdf/2405.05130.pdf</a></span>   <span><a href='https://github.com/shengyangsun/MSBT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengyang Sun, Xiaojin Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.05130">Multi-scale Bottleneck Transformer for Weakly Supervised Multimodal Violence Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised multimodal violence detection aims to learn a violence detection model by leveraging multiple modalities such as RGB, optical flow, and audio, while only video-level annotations are available. In the pursuit of effective multimodal violence detection (MVD), information redundancy, modality imbalance, and modality asynchrony are identified as three key challenges. In this work, we propose a new weakly supervised MVD method that explicitly addresses these challenges. Specifically, we introduce a multi-scale bottleneck transformer (MSBT) based fusion module that employs a reduced number of bottleneck tokens to gradually condense information and fuse each pair of modalities and utilizes a bottleneck token-based weighting scheme to highlight more important fused features. Furthermore, we propose a temporal consistency contrast loss to semantically align pairwise fused features. Experiments on the largest-scale XD-Violence dataset demonstrate that the proposed method achieves state-of-the-art performance. Code is available at https://github.com/shengyangsun/MSBT.
<div id='section'>Paperid: <span id='pid'>176, <a href='https://arxiv.org/pdf/2405.04405.pdf' target='_blank'>https://arxiv.org/pdf/2405.04405.pdf</a></span>   <span><a href='https://github.com/liupei101/MIREL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pei Liu, Luping Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04405">Weakly-Supervised Residual Evidential Learning for Multi-Instance Uncertainty Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Uncertainty estimation (UE), as an effective means of quantifying predictive uncertainty, is crucial for safe and reliable decision-making, especially in high-risk scenarios. Existing UE schemes usually assume that there are completely-labeled samples to support fully-supervised learning. In practice, however, many UE tasks often have no sufficiently-labeled data to use, such as the Multiple Instance Learning (MIL) with only weak instance annotations. To bridge this gap, this paper, for the first time, addresses the weakly-supervised issue of Multi-Instance UE (MIUE) and proposes a new baseline scheme, Multi-Instance Residual Evidential Learning (MIREL). Particularly, at the fine-grained instance UE with only weak supervision, we derive a multi-instance residual operator through the Fundamental Theorem of Symmetric Functions. On this operator derivation, we further propose MIREL to jointly model the high-order predictive distribution at bag and instance levels for MIUE. Extensive experiments empirically demonstrate that our MIREL not only could often make existing MIL networks perform better in MIUE, but also could surpass representative UE methods by large margins, especially in instance-level UE tasks. Our source code is available at https://github.com/liupei101/MIREL.
<div id='section'>Paperid: <span id='pid'>177, <a href='https://arxiv.org/pdf/2405.03140.pdf' target='_blank'>https://arxiv.org/pdf/2405.03140.pdf</a></span>   <span><a href='https://github.com/xiwenc1/TimeMIL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiwen Chen, Peijie Qiu, Wenhui Zhu, Huayu Li, Hao Wang, Aristeidis Sotiras, Yalin Wang, Abolfazl Razi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.03140">TimeMIL: Advancing Multivariate Time Series Classification via a Time-aware Multiple Instance Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks, including transformers and convolutional neural networks, have significantly improved multivariate time series classification (MTSC). However, these methods often rely on supervised learning, which does not fully account for the sparsity and locality of patterns in time series data (e.g., diseases-related anomalous points in ECG). To address this challenge, we formally reformulate MTSC as a weakly supervised problem, introducing a novel multiple-instance learning (MIL) framework for better localization of patterns of interest and modeling time dependencies within time series. Our novel approach, TimeMIL, formulates the temporal correlation and ordering within a time-aware MIL pooling, leveraging a tokenized transformer with a specialized learnable wavelet positional token. The proposed method surpassed 26 recent state-of-the-art methods, underscoring the effectiveness of the weakly supervised TimeMIL in MTSC. The code will be available at https://github.com/xiwenc1/TimeMIL.
<div id='section'>Paperid: <span id='pid'>178, <a href='https://arxiv.org/pdf/2405.02114.pdf' target='_blank'>https://arxiv.org/pdf/2405.02114.pdf</a></span>   <span><a href='https://github.com/xzhouzeng/PRPose' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xianzhou Zeng, Hao Qin, Ming Kong, Luyuan Chen, Qiang Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.02114">Probablistic Restoration with Adaptive Noise Sampling for 3D Human Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The accuracy and robustness of 3D human pose estimation (HPE) are limited by 2D pose detection errors and 2D to 3D ill-posed challenges, which have drawn great attention to Multi-Hypothesis HPE research. Most existing MH-HPE methods are based on generative models, which are computationally expensive and difficult to train. In this study, we propose a Probabilistic Restoration 3D Human Pose Estimation framework (PRPose) that can be integrated with any lightweight single-hypothesis model. Specifically, PRPose employs a weakly supervised approach to fit the hidden probability distribution of the 2D-to-3D lifting process in the Single-Hypothesis HPE model and then reverse-map the distribution to the 2D pose input through an adaptive noise sampling strategy to generate reasonable multi-hypothesis samples effectively. Extensive experiments on 3D HPE benchmarks (Human3.6M and MPI-INF-3DHP) highlight the effectiveness and efficiency of PRPose. Code is available at: https://github.com/xzhouzeng/PRPose.
<div id='section'>Paperid: <span id='pid'>179, <a href='https://arxiv.org/pdf/2405.00239.pdf' target='_blank'>https://arxiv.org/pdf/2405.00239.pdf</a></span>   <span><a href='https://github.com/ahxmeds/IgCONDA-PET.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shadab Ahamed, Arman Rahmim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.00239">IgCONDA-PET: Weakly-Supervised PET Anomaly Detection using Implicitly-Guided Attention-Conditional Counterfactual Diffusion Modeling -- a Multi-Center, Multi-Cancer, and Multi-Tracer Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Minimizing the need for pixel-level annotated data to train PET lesion detection and segmentation networks is highly desired and can be transformative, given time and cost constraints associated with expert annotations. Current unsupervised or weakly-supervised anomaly detection methods rely on autoencoder or generative adversarial networks (GANs) trained only on healthy data. While these approaches reduce annotation dependency, GAN-based methods are notably more challenging to train than non-GAN alternatives (such as autoencoders) due to issues such as the simultaneous optimization of two competing networks, mode collapse, and training instability. In this paper, we present the weakly-supervised $\textbf{I}$mplicitly-$\textbf{g}$uided $\textbf{CO}$u$\textbf{N}$terfactual diffusion model for $\textbf{D}$etecting $\textbf{A}$nomalies in $\textbf{PET}$ images (IgCONDA-PET). The solution is developed and validated using PET scans from six retrospective cohorts consisting of a total of 2652 cases (multi-cancer, multi-tracer) containing both local and public datasets (spanning multiple centers). The training is conditioned on image class labels (healthy vs. unhealthy) via attention modules, and we employ implicit diffusion guidance. We perform counterfactual generation which facilitates "unhealthy-to-healthy" domain translation by generating a synthetic, healthy version of an unhealthy input image, enabling the detection of anomalies through the calculated differences. The performance of our method was compared against several other deep learning based weakly-supervised or unsupervised methods as well as traditional methods like 41% SUV$_\text{max}$ thresholding. We also highlight the importance of incorporating attention modules in our network for the detection of small anomalies. The code is publicly available at: https://github.com/ahxmeds/IgCONDA-PET.git.
<div id='section'>Paperid: <span id='pid'>180, <a href='https://arxiv.org/pdf/2404.17253.pdf' target='_blank'>https://arxiv.org/pdf/2404.17253.pdf</a></span>   <span><a href='https://github.com/EPITAResearchLab/pouliquen.24.icdar' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Glen Pouliquen, Guillaume Chiron, Joseph Chazalon, Thierry GÃ©raud, Ahmad Montaser Awal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.17253">Weakly Supervised Training for Hologram Verification in Identity Documents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a method to remotely verify the authenticity of Optically Variable Devices (OVDs), often referred to as ``holograms'', in identity documents. Our method processes video clips captured with smartphones under common lighting conditions, and is evaluated on two public datasets: MIDV-HOLO and MIDV-2020. Thanks to a weakly-supervised training, we optimize a feature extraction and decision pipeline which achieves a new leading performance on MIDV-HOLO, while maintaining a high recall on documents from MIDV-2020 used as attack samples. It is also the first method, to date, to effectively address the photo replacement attack task, and can be trained on either genuine samples, attack samples, or both for increased performance. By enabling to verify OVD shapes and dynamics with very little supervision, this work opens the way towards the use of massive amounts of unlabeled data to build robust remote identity document verification systems on commodity smartphones. Code is available at https://github.com/EPITAResearchLab/pouliquen.24.icdar
<div id='section'>Paperid: <span id='pid'>181, <a href='https://arxiv.org/pdf/2404.16536.pdf' target='_blank'>https://arxiv.org/pdf/2404.16536.pdf</a></span>   <span><a href='https://github.com/liguohao96/WSDF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guohao Li, Hongyu Yang, Di Huang, Yunhong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.16536">3D Face Modeling via Weakly-supervised Disentanglement Network joint Identity-consistency Prior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative 3D face models featuring disentangled controlling factors hold immense potential for diverse applications in computer vision and computer graphics. However, previous 3D face modeling methods face a challenge as they demand specific labels to effectively disentangle these factors. This becomes particularly problematic when integrating multiple 3D face datasets to improve the generalization of the model. Addressing this issue, this paper introduces a Weakly-Supervised Disentanglement Framework, denoted as WSDF, to facilitate the training of controllable 3D face models without an overly stringent labeling requirement. Adhering to the paradigm of Variational Autoencoders (VAEs), the proposed model achieves disentanglement of identity and expression controlling factors through a two-branch encoder equipped with dedicated identity-consistency prior. It then faithfully re-entangles these factors via a tensor-based combination mechanism. Notably, the introduction of the Neutral Bank allows precise acquisition of subject-specific information using only identity labels, thereby averting degeneration due to insufficient supervision. Additionally, the framework incorporates a label-free second-order loss function for the expression factor to regulate deformation space and eliminate extraneous information, resulting in enhanced disentanglement. Extensive experiments have been conducted to substantiate the superior performance of WSDF. Our code is available at https://github.com/liguohao96/WSDF.
<div id='section'>Paperid: <span id='pid'>182, <a href='https://arxiv.org/pdf/2404.15653.pdf' target='_blank'>https://arxiv.org/pdf/2404.15653.pdf</a></span>   <span><a href='https://github.com/apple/corenet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sachin Mehta, Maxwell Horton, Fartash Faghri, Mohammad Hossein Sekhavat, Mahyar Najibi, Mehrdad Farajtabar, Oncel Tuzel, Mohammad Rastegari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.15653">CatLIP: CLIP-level Visual Recognition Accuracy with 2.7x Faster Pre-training on Web-scale Image-Text Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contrastive learning has emerged as a transformative method for learning effective visual representations through the alignment of image and text embeddings. However, pairwise similarity computation in contrastive loss between image and text pairs poses computational challenges. This paper presents a novel weakly supervised pre-training of vision models on web-scale image-text data. The proposed method reframes pre-training on image-text data as a classification task. Consequently, it eliminates the need for pairwise similarity computations in contrastive loss, achieving a remarkable $2.7\times$ acceleration in training speed compared to contrastive learning on web-scale data. Through extensive experiments spanning diverse vision tasks, including detection and segmentation, we demonstrate that the proposed method maintains high representation quality. Our source code along with pre-trained model weights and training recipes is available at \url{https://github.com/apple/corenet}.
<div id='section'>Paperid: <span id='pid'>183, <a href='https://arxiv.org/pdf/2404.14956.pdf' target='_blank'>https://arxiv.org/pdf/2404.14956.pdf</a></span>   <span><a href='https://github.com/zhangye-zoe/DAWN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ye Zhang, Yifeng Wang, Zijie Fang, Hao Bian, Linghan Cai, Ziyue Wang, Yongbing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.14956">DAWN: Domain-Adaptive Weakly Supervised Nuclei Segmentation via Cross-Task Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised segmentation methods have gained significant attention due to their ability to reduce the reliance on costly pixel-level annotations during model training. However, the current weakly supervised nuclei segmentation approaches typically follow a two-stage pseudo-label generation and network training process. The performance of the nuclei segmentation heavily relies on the quality of the generated pseudo-labels, thereby limiting its effectiveness. This paper introduces a novel domain-adaptive weakly supervised nuclei segmentation framework using cross-task interaction strategies to overcome the challenge of pseudo-label generation. Specifically, we utilize weakly annotated data to train an auxiliary detection task, which assists the domain adaptation of the segmentation network. To enhance the efficiency of domain adaptation, we design a consistent feature constraint module integrating prior knowledge from the source domain. Furthermore, we develop pseudo-label optimization and interactive training methods to improve the domain transfer capability. To validate the effectiveness of our proposed method, we conduct extensive comparative and ablation experiments on six datasets. The results demonstrate the superiority of our approach over existing weakly supervised approaches. Remarkably, our method achieves comparable or even better performance than fully supervised methods. Our code will be released in https://github.com/zhangye-zoe/DAWN.
<div id='section'>Paperid: <span id='pid'>184, <a href='https://arxiv.org/pdf/2404.07202.pdf' target='_blank'>https://arxiv.org/pdf/2404.07202.pdf</a></span>   <span><a href='https://weihaox.github.io/UMBRAE' target='_blank'>  GitHub</a></span> <span><a href='https://weihaox.github.io/UMBRAE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weihao Xia, Raoul de Charette, Cengiz Ãztireli, Jing-Hao Xue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.07202">UMBRAE: Unified Multimodal Brain Decoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address prevailing challenges of the brain-powered research, departing from the observation that the literature hardly recover accurate spatial information and require subject-specific models. To address these challenges, we propose UMBRAE, a unified multimodal decoding of brain signals. First, to extract instance-level conceptual and spatial details from neural signals, we introduce an efficient universal brain encoder for multimodal-brain alignment and recover object descriptions at multiple levels of granularity from subsequent multimodal large language model (MLLM). Second, we introduce a cross-subject training strategy mapping subject-specific features to a common feature space. This allows a model to be trained on multiple subjects without extra resources, even yielding superior results compared to subject-specific models. Further, we demonstrate this supports weakly-supervised adaptation to new subjects, with only a fraction of the total training data. Experiments demonstrate that UMBRAE not only achieves superior results in the newly introduced tasks but also outperforms methods in well established tasks. To assess our method, we construct and share with the community a comprehensive brain understanding benchmark BrainHub. Our code and benchmark are available at https://weihaox.github.io/UMBRAE.
<div id='section'>Paperid: <span id='pid'>185, <a href='https://arxiv.org/pdf/2404.05362.pdf' target='_blank'>https://arxiv.org/pdf/2404.05362.pdf</a></span>   <span><a href='https://github.com/tueimage/MAD-MIL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hassan Keshvarikhojasteh, Josien Pluim, Mitko Veta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.05362">Multi-head Attention-based Deep Multiple Instance Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces MAD-MIL, a Multi-head Attention-based Deep Multiple Instance Learning model, designed for weakly supervised Whole Slide Images (WSIs) classification in digital pathology. Inspired by the multi-head attention mechanism of the Transformer, MAD-MIL simplifies model complexity while achieving competitive results against advanced models like CLAM and DS-MIL. Evaluated on the MNIST-BAGS and public datasets, including TUPAC16, TCGA BRCA, TCGA LUNG, and TCGA KIDNEY, MAD-MIL consistently outperforms ABMIL. This demonstrates enhanced information diversity, interpretability, and efficiency in slide representation. The model's effectiveness, coupled with fewer trainable parameters and lower computational complexity makes it a promising solution for automated pathology workflows. Our code is available at https://github.com/tueimage/MAD-MIL.
<div id='section'>Paperid: <span id='pid'>186, <a href='https://arxiv.org/pdf/2404.04998.pdf' target='_blank'>https://arxiv.org/pdf/2404.04998.pdf</a></span>   <span><a href='https://github.com/gimpong/AAAI21-WSDHQ' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinpeng Wang, Bin Chen, Qiang Zhang, Zaiqiao Meng, Shangsong Liang, Shu-Tao Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.04998">Weakly Supervised Deep Hyperspherical Quantization for Image Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep quantization methods have shown high efficiency on large-scale image retrieval. However, current models heavily rely on ground-truth information, hindering the application of quantization in label-hungry scenarios. A more realistic demand is to learn from inexhaustible uploaded images that are associated with informal tags provided by amateur users. Though such sketchy tags do not obviously reveal the labels, they actually contain useful semantic information for supervising deep quantization. To this end, we propose Weakly-Supervised Deep Hyperspherical Quantization (WSDHQ), which is the first work to learn deep quantization from weakly tagged images. Specifically, 1) we use word embeddings to represent the tags and enhance their semantic information based on a tag correlation graph. 2) To better preserve semantic information in quantization codes and reduce quantization error, we jointly learn semantics-preserving embeddings and supervised quantizer on hypersphere by employing a well-designed fusion layer and tailor-made loss functions. Extensive experiments show that WSDHQ can achieve state-of-art performance on weakly-supervised compact coding. Code is available at https://github.com/gimpong/AAAI21-WSDHQ.
<div id='section'>Paperid: <span id='pid'>187, <a href='https://arxiv.org/pdf/2404.01751.pdf' target='_blank'>https://arxiv.org/pdf/2404.01751.pdf</a></span>   <span><a href='https://github.com/enyac-group/T-VSL/tree/main' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tanvir Mahmud, Yapeng Tian, Diana Marculescu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01751">T-VSL: Text-Guided Visual Sound Source Localization in Mixtures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual sound source localization poses a significant challenge in identifying the semantic region of each sounding source within a video. Existing self-supervised and weakly supervised source localization methods struggle to accurately distinguish the semantic regions of each sounding object, particularly in multi-source mixtures. These methods often rely on audio-visual correspondence as guidance, which can lead to substantial performance drops in complex multi-source localization scenarios. The lack of access to individual source sounds in multi-source mixtures during training exacerbates the difficulty of learning effective audio-visual correspondence for localization. To address this limitation, in this paper, we propose incorporating the text modality as an intermediate feature guide using tri-modal joint embedding models (e.g., AudioCLIP) to disentangle the semantic audio-visual source correspondence in multi-source mixtures. Our framework, dubbed T-VSL, begins by predicting the class of sounding entities in mixtures. Subsequently, the textual representation of each sounding source is employed as guidance to disentangle fine-grained audio-visual source correspondence from multi-source mixtures, leveraging the tri-modal AudioCLIP embedding. This approach enables our framework to handle a flexible number of sources and exhibits promising zero-shot transferability to unseen classes during test time. Extensive experiments conducted on the MUSIC, VGGSound, and VGGSound-Instruments datasets demonstrate significant performance improvements over state-of-the-art methods. Code is released at https://github.com/enyac-group/T-VSL/tree/main
<div id='section'>Paperid: <span id='pid'>188, <a href='https://arxiv.org/pdf/2404.00380.pdf' target='_blank'>https://arxiv.org/pdf/2404.00380.pdf</a></span>   <span><a href='https://github.com/shjo-april/DHR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sanghyun Jo, Fei Pan, In-Jae Yu, Kyungsu Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00380">DHR: Dual Features-Driven Hierarchical Rebalancing in Inter- and Intra-Class Regions for Weakly-Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised semantic segmentation (WSS) ensures high-quality segmentation with limited data and excels when employed as input seed masks for large-scale vision models such as Segment Anything. However, WSS faces challenges related to minor classes since those are overlooked in images with adjacent multiple classes, a limitation originating from the overfitting of traditional expansion methods like Random Walk. We first address this by employing unsupervised and weakly-supervised feature maps instead of conventional methodologies, allowing for hierarchical mask enhancement. This method distinctly categorizes higher-level classes and subsequently separates their associated lower-level classes, ensuring all classes are correctly restored in the mask without losing minor ones. Our approach, validated through extensive experimentation, significantly improves WSS across five benchmarks (VOC: 79.8\%, COCO: 53.9\%, Context: 49.0\%, ADE: 32.9\%, Stuff: 37.4\%), reducing the gap with fully supervised methods by over 84\% on the VOC validation set. Code is available at https://github.com/shjo-april/DHR.
<div id='section'>Paperid: <span id='pid'>189, <a href='https://arxiv.org/pdf/2404.00149.pdf' target='_blank'>https://arxiv.org/pdf/2404.00149.pdf</a></span>   <span><a href='https://github.com/skmhrk1209/VSRD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihua Liu, Hiroki Sakuma, Masatoshi Okutomi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00149">VSRD: Instance-Aware Volumetric Silhouette Rendering for Weakly Supervised 3D Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monocular 3D object detection poses a significant challenge in 3D scene understanding due to its inherently ill-posed nature in monocular depth estimation. Existing methods heavily rely on supervised learning using abundant 3D labels, typically obtained through expensive and labor-intensive annotation on LiDAR point clouds. To tackle this problem, we propose a novel weakly supervised 3D object detection framework named VSRD (Volumetric Silhouette Rendering for Detection) to train 3D object detectors without any 3D supervision but only weak 2D supervision. VSRD consists of multi-view 3D auto-labeling and subsequent training of monocular 3D object detectors using the pseudo labels generated in the auto-labeling stage. In the auto-labeling stage, we represent the surface of each instance as a signed distance field (SDF) and render its silhouette as an instance mask through our proposed instance-aware volumetric silhouette rendering. To directly optimize the 3D bounding boxes through rendering, we decompose the SDF of each instance into the SDF of a cuboid and the residual distance field (RDF) that represents the residual from the cuboid. This mechanism enables us to optimize the 3D bounding boxes in an end-to-end manner by comparing the rendered instance masks with the ground truth instance masks. The optimized 3D bounding boxes serve as effective training data for 3D object detection. We conduct extensive experiments on the KITTI-360 dataset, demonstrating that our method outperforms the existing weakly supervised 3D object detection methods. The code is available at https://github.com/skmhrk1209/VSRD.
<div id='section'>Paperid: <span id='pid'>190, <a href='https://arxiv.org/pdf/2403.20253.pdf' target='_blank'>https://arxiv.org/pdf/2403.20253.pdf</a></span>   <span><a href='https://github.com/HealthX-Lab/MedCLIP-SAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Taha Koleilat, Hojat Asgariandehkordi, Hassan Rivaz, Yiming Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.20253">MedCLIP-SAM: Bridging Text and Image Towards Universal Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical image segmentation of anatomical structures and pathology is crucial in modern clinical diagnosis, disease study, and treatment planning. To date, great progress has been made in deep learning-based segmentation techniques, but most methods still lack data efficiency, generalizability, and interactability. Consequently, the development of new, precise segmentation methods that demand fewer labeled datasets is of utmost importance in medical image analysis. Recently, the emergence of foundation models, such as CLIP and Segment-Anything-Model (SAM), with comprehensive cross-domain representation opened the door for interactive and universal image segmentation. However, exploration of these models for data-efficient medical image segmentation is still limited, but is highly necessary. In this paper, we propose a novel framework, called MedCLIP-SAM that combines CLIP and SAM models to generate segmentation of clinical scans using text prompts in both zero-shot and weakly supervised settings. To achieve this, we employed a new Decoupled Hard Negative Noise Contrastive Estimation (DHN-NCE) loss to fine-tune the BiomedCLIP model and the recent gScoreCAM to generate prompts to obtain segmentation masks from SAM in a zero-shot setting. Additionally, we explored the use of zero-shot segmentation labels in a weakly supervised paradigm to improve the segmentation quality further. By extensively testing three diverse segmentation tasks and medical image modalities (breast tumor ultrasound, brain tumor MRI, and lung X-ray), our proposed framework has demonstrated excellent accuracy. Code is available at https://github.com/HealthX-Lab/MedCLIP-SAM.
<div id='section'>Paperid: <span id='pid'>191, <a href='https://arxiv.org/pdf/2403.20105.pdf' target='_blank'>https://arxiv.org/pdf/2403.20105.pdf</a></span>   <span><a href='https://bcorrad.github.io/freesegdiff/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Barbara Toniella Corradini, Mustafa Shukor, Paul Couairon, Guillaume Couairon, Franco Scarselli, Matthieu Cord
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.20105">FreeSeg-Diff: Training-Free Open-Vocabulary Segmentation with Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation models have exhibited unprecedented capabilities in tackling many domains and tasks. Models such as CLIP are currently widely used to bridge cross-modal representations, and text-to-image diffusion models are arguably the leading models in terms of realistic image generation. Image generative models are trained on massive datasets that provide them with powerful internal spatial representations. In this work, we explore the potential benefits of such representations, beyond image generation, in particular, for dense visual prediction tasks. We focus on the task of image segmentation, which is traditionally solved by training models on closed-vocabulary datasets, with pixel-level annotations. To avoid the annotation cost or training large diffusion models, we constraint our setup to be zero-shot and training-free. In a nutshell, our pipeline leverages different and relatively small-sized, open-source foundation models for zero-shot open-vocabulary segmentation. The pipeline is as follows: the image is passed to both a captioner model (i.e. BLIP) and a diffusion model (i.e., Stable Diffusion Model) to generate a text description and visual representation, respectively. The features are clustered and binarized to obtain class agnostic masks for each object. These masks are then mapped to a textual class, using the CLIP model to support open-vocabulary. Finally, we add a refinement step that allows to obtain a more precise segmentation mask. Our approach (dubbed FreeSeg-Diff), which does not rely on any training, outperforms many training-based approaches on both Pascal VOC and COCO datasets. In addition, we show very competitive results compared to the recent weakly-supervised segmentation approaches. We provide comprehensive experiments showing the superiority of diffusion model features compared to other pretrained models. Project page: https://bcorrad.github.io/freesegdiff/
<div id='section'>Paperid: <span id='pid'>192, <a href='https://arxiv.org/pdf/2403.15019.pdf' target='_blank'>https://arxiv.org/pdf/2403.15019.pdf</a></span>   <span><a href='https://github.com/peoplelu/BSNet' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/peoplelu/BSNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Lu, Jiacheng Deng, Tianzhu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15019">BSNet: Box-Supervised Simulation-assisted Mean Teacher for 3D Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D instance segmentation (3DIS) is a crucial task, but point-level annotations are tedious in fully supervised settings. Thus, using bounding boxes (bboxes) as annotations has shown great potential. The current mainstream approach is a two-step process, involving the generation of pseudo-labels from box annotations and the training of a 3DIS network with the pseudo-labels. However, due to the presence of intersections among bboxes, not every point has a determined instance label, especially in overlapping areas. To generate higher quality pseudo-labels and achieve more precise weakly supervised 3DIS results, we propose the Box-Supervised Simulation-assisted Mean Teacher for 3D Instance Segmentation (BSNet), which devises a novel pseudo-labeler called Simulation-assisted Transformer. The labeler consists of two main components. The first is Simulation-assisted Mean Teacher, which introduces Mean Teacher for the first time in this task and constructs simulated samples to assist the labeler in acquiring prior knowledge about overlapping areas. To better model local-global structure, we also propose Local-Global Aware Attention as the decoder for teacher and student labelers. Extensive experiments conducted on the ScanNetV2 and S3DIS datasets verify the superiority of our designs. Code is available at \href{https://github.com/peoplelu/BSNet}{https://github.com/peoplelu/BSNet}.
<div id='section'>Paperid: <span id='pid'>193, <a href='https://arxiv.org/pdf/2403.11672.pdf' target='_blank'>https://arxiv.org/pdf/2403.11672.pdf</a></span>   <span><a href='https://github.com/zhaohaoyu376/WI-LD2ND' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Zhao, Yuliang Gu, Zhou Zhao, Bo Du, Yongchao Xu, Rui Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11672">WIA-LD2ND: Wavelet-based Image Alignment for Self-supervised Low-Dose CT Denoising</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In clinical examinations and diagnoses, low-dose computed tomography (LDCT) is crucial for minimizing health risks compared with normal-dose computed tomography (NDCT). However, reducing the radiation dose compromises the signal-to-noise ratio, leading to degraded quality of CT images. To address this, we analyze LDCT denoising task based on experimental results from the frequency perspective, and then introduce a novel self-supervised CT image denoising method called WIA-LD2ND, only using NDCT data. The proposed WIA-LD2ND comprises two modules: Wavelet-based Image Alignment (WIA) and Frequency-Aware Multi-scale Loss (FAM). First, WIA is introduced to align NDCT with LDCT by mainly adding noise to the high-frequency components, which is the main difference between LDCT and NDCT. Second, to better capture high-frequency components and detailed information, Frequency-Aware Multi-scale Loss (FAM) is proposed by effectively utilizing multi-scale feature space. Extensive experiments on two public LDCT denoising datasets demonstrate that our WIA-LD2ND, only uses NDCT, outperforms existing several state-of-the-art weakly-supervised and self-supervised methods. Source code is available at https://github.com/zhaohaoyu376/WI-LD2ND.
<div id='section'>Paperid: <span id='pid'>194, <a href='https://arxiv.org/pdf/2403.11184.pdf' target='_blank'>https://arxiv.org/pdf/2403.11184.pdf</a></span>   <span><a href='https://github.com/Wu0409/DuPL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanchen Wu, Xichen Ye, Kequan Yang, Jide Li, Xiaoqiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11184">DuPL: Dual Student with Trustworthy Progressive Learning for Robust Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, One-stage Weakly Supervised Semantic Segmentation (WSSS) with image-level labels has gained increasing interest due to simplification over its cumbersome multi-stage counterpart. Limited by the inherent ambiguity of Class Activation Map (CAM), we observe that one-stage pipelines often encounter confirmation bias caused by incorrect CAM pseudo-labels, impairing their final segmentation performance. Although recent works discard many unreliable pseudo-labels to implicitly alleviate this issue, they fail to exploit sufficient supervision for their models. To this end, we propose a dual student framework with trustworthy progressive learning (DuPL). Specifically, we propose a dual student network with a discrepancy loss to yield diverse CAMs for each sub-net. The two sub-nets generate supervision for each other, mitigating the confirmation bias caused by learning their own incorrect pseudo-labels. In this process, we progressively introduce more trustworthy pseudo-labels to be involved in the supervision through dynamic threshold adjustment with an adaptive noise filtering strategy. Moreover, we believe that every pixel, even discarded from supervision due to its unreliability, is important for WSSS. Thus, we develop consistency regularization on these discarded regions, providing supervision of every pixel. Experiment results demonstrate the superiority of the proposed DuPL over the recent state-of-the-art alternatives on PASCAL VOC 2012 and MS COCO datasets. Code is available at https://github.com/Wu0409/DuPL.
<div id='section'>Paperid: <span id='pid'>195, <a href='https://arxiv.org/pdf/2403.07630.pdf' target='_blank'>https://arxiv.org/pdf/2403.07630.pdf</a></span>   <span><a href='https://github.com/Barrett-python/CPAL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Feilong Tang, Zhongxing Xu, Zhaojun Qu, Wei Feng, Xingjian Jiang, Zongyuan Ge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07630">Hunting Attributes: Context Prototype-Aware Learning for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent weakly supervised semantic segmentation (WSSS) methods strive to incorporate contextual knowledge to improve the completeness of class activation maps (CAM). In this work, we argue that the knowledge bias between instances and contexts affects the capability of the prototype to sufficiently understand instance semantics. Inspired by prototype learning theory, we propose leveraging prototype awareness to capture diverse and fine-grained feature attributes of instances. The hypothesis is that contextual prototypes might erroneously activate similar and frequently co-occurring object categories due to this knowledge bias. Therefore, we propose to enhance the prototype representation ability by mitigating the bias to better capture spatial coverage in semantic object regions. With this goal, we present a Context Prototype-Aware Learning (CPAL) strategy, which leverages semantic context to enrich instance comprehension. The core of this method is to accurately capture intra-class variations in object features through context-aware prototypes, facilitating the adaptation to the semantic attributes of various instances. We design feature distribution alignment to optimize prototype awareness, aligning instance feature distributions with dense features. In addition, a unified training framework is proposed to combine label-guided classification supervision and prototypes-guided self-supervision. Experimental results on PASCAL VOC 2012 and MS COCO 2014 show that CPAL significantly improves off-the-shelf methods and achieves state-of-the-art performance. The project is available at https://github.com/Barrett-python/CPAL.
<div id='section'>Paperid: <span id='pid'>196, <a href='https://arxiv.org/pdf/2403.06676.pdf' target='_blank'>https://arxiv.org/pdf/2403.06676.pdf</a></span>   <span><a href='https://github.com/snskysk/CAM-Back-Again' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/snskysk/CAM-Back-Again' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shunsuke Yasuki, Masato Taki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.06676">CAM Back Again: Large Kernel CNNs from a Weakly Supervised Object Localization Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, convolutional neural networks (CNNs) with large size kernels have attracted much attention in the computer vision field, following the success of the Vision Transformers. Large kernel CNNs have been reported to perform well in downstream vision tasks as well as in classification performance. The reason for the high-performance of large kernel CNNs in downstream tasks has been attributed to the large effective receptive field (ERF) produced by large size kernels, but this view has not been fully tested. We therefore revisit the performance of large kernel CNNs in downstream task, focusing on the weakly supervised object localization (WSOL) task. WSOL, a difficult downstream task that is not fully supervised, provides a new angle to explore the capabilities of the large kernel CNNs. Our study compares the modern large kernel CNNs ConvNeXt, RepLKNet, and SLaK to test the validity of the naive expectation that ERF size is important for improving downstream task performance. Our analysis of the factors contributing to high performance provides a different perspective, in which the main factor is feature map improvement. Furthermore, we find that modern CNNs are robust to the CAM problems of local regions of objects being activated, which has long been discussed in WSOL. CAM is the most classic WSOL method, but because of the above-mentioned problems, it is often used as a baseline method for comparison. However, experiments on the CUB-200-2011 dataset show that simply combining a large kernel CNN, CAM, and simple data augmentation methods can achieve performance (90.99% MaxBoxAcc) comparable to the latest WSOL method, which is CNN-based and requires special training or complex post-processing. The code is available at https://github.com/snskysk/CAM-Back-Again.
<div id='section'>Paperid: <span id='pid'>197, <a href='https://arxiv.org/pdf/2403.06567.pdf' target='_blank'>https://arxiv.org/pdf/2403.06567.pdf</a></span>   <span><a href='https://github.com/MIC-DKFZ/foundation-models-for-cbmir' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Stefan Denner, David Zimmerer, Dimitrios Bounias, Markus Bujotzek, Shuhan Xiao, Raphael Stock, Lisa Kausch, Philipp Schader, Tobias Penzkofer, Paul F. JÃ¤ger, Klaus Maier-Hein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.06567">Leveraging Foundation Models for Content-Based Image Retrieval in Radiology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Content-based image retrieval (CBIR) has the potential to significantly improve diagnostic aid and medical research in radiology. However, current CBIR systems face limitations due to their specialization to certain pathologies, limiting their utility. On the other hand, several vision foundation models have been shown to produce general-purpose visual features. Therefore, in this work, we propose using vision foundation models as powerful and versatile off-the-shelf feature extractors for content-based image retrieval. Our contributions include: (1) benchmarking a diverse set of vision foundation models on an extensive dataset comprising 1.6 million 2D radiological images across four modalities and 161 pathologies; (2) identifying weakly-supervised models, particularly BiomedCLIP, as highly effective, achieving a achieving a P@1 of up to 0.594 (P@3: 0.590, P@5: 0.588, P@10: 0.583), comparable to specialized CBIR systems but without additional training; (3) conducting an in-depth analysis of the impact of index size on retrieval performance; (4) evaluating the quality of embedding spaces generated by different models; and (5) investigating specific challenges associated with retrieving anatomical versus pathological structures. Despite these challenges, our research underscores the vast potential of foundation models for CBIR in radiology, proposing a shift towards versatile, general-purpose medical image retrieval systems that do not require specific tuning. Our code, dataset splits and embeddings are publicly available under https://github.com/MIC-DKFZ/foundation-models-for-cbmir.
<div id='section'>Paperid: <span id='pid'>198, <a href='https://arxiv.org/pdf/2403.06154.pdf' target='_blank'>https://arxiv.org/pdf/2403.06154.pdf</a></span>   <span><a href='https://github.com/pipixin321/GlanceVAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huaxin Zhang, Xiang Wang, Xiaohao Xu, Xiaonan Huang, Chuchu Han, Yuehuan Wang, Changxin Gao, Shanjun Zhang, Nong Sang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.06154">GlanceVAD: Exploring Glance Supervision for Label-efficient Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, video anomaly detection has been extensively investigated in both unsupervised and weakly supervised settings to alleviate costly temporal labeling. Despite significant progress, these methods still suffer from unsatisfactory results such as numerous false alarms, primarily due to the absence of precise temporal anomaly annotation. In this paper, we present a novel labeling paradigm, termed "glance annotation", to achieve a better balance between anomaly detection accuracy and annotation cost. Specifically, glance annotation is a random frame within each abnormal event, which can be easily accessed and is cost-effective. To assess its effectiveness, we manually annotate the glance annotations for two standard video anomaly detection datasets: UCF-Crime and XD-Violence. Additionally, we propose a customized GlanceVAD method, that leverages gaussian kernels as the basic unit to compose the temporal anomaly distribution, enabling the learning of diverse and robust anomaly representations from the glance annotations. Through comprehensive analysis and experiments, we verify that the proposed labeling paradigm can achieve an excellent trade-off between annotation cost and model performance. Extensive experimental results also demonstrate the effectiveness of our GlanceVAD approach, which significantly outperforms existing advanced unsupervised and weakly supervised methods. Code and annotations will be publicly available at https://github.com/pipixin321/GlanceVAD.
<div id='section'>Paperid: <span id='pid'>199, <a href='https://arxiv.org/pdf/2403.05796.pdf' target='_blank'>https://arxiv.org/pdf/2403.05796.pdf</a></span>   <span><a href='https://github.com/BinghaoLu/KD-MSI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Binghao Lu, Caiwen Ding, Jinbo Bi, Dongjin Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05796">Weakly Supervised Change Detection via Knowledge Distillation and Multiscale Sigmoid Inference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Change detection, which aims to detect spatial changes from a pair of multi-temporal images due to natural or man-made causes, has been widely applied in remote sensing, disaster management, urban management, etc. Most existing change detection approaches, however, are fully supervised and require labor-intensive pixel-level labels. To address this, we develop a novel weakly supervised change detection technique via Knowledge Distillation and Multiscale Sigmoid Inference (KD-MSI) that leverages image-level labels. In our approach, the Class Activation Maps (CAM) are utilized not only to derive a change probability map but also to serve as a foundation for the knowledge distillation process. This is done through a joint training strategy of the teacher and student networks, enabling the student network to highlight potential change areas more accurately than teacher network based on image-level labels. Moreover, we designed a Multiscale Sigmoid Inference (MSI) module as a post processing step to further refine the change probability map from the trained student network. Empirical results on three public datasets, i.e., WHU-CD, DSIFN-CD, and LEVIR-CD, demonstrate that our proposed technique, with its integrated training strategy, significantly outperforms the state-of-the-art.
<div id='section'>Paperid: <span id='pid'>200, <a href='https://arxiv.org/pdf/2403.02818.pdf' target='_blank'>https://arxiv.org/pdf/2403.02818.pdf</a></span>   <span><a href='https://github.com/gaocq/SS3D2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenqiang Gao, Chuandong Liu, Jun Shu, Fangcen Liu, Jiang Liu, Luyu Yang, Xinbo Gao, Deyu Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.02818">Are Dense Labels Always Necessary for 3D Object Detection from Point Cloud?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current state-of-the-art (SOTA) 3D object detection methods often require a large amount of 3D bounding box annotations for training. However, collecting such large-scale densely-supervised datasets is notoriously costly. To reduce the cumbersome data annotation process, we propose a novel sparsely-annotated framework, in which we just annotate one 3D object per scene. Such a sparse annotation strategy could significantly reduce the heavy annotation burden, while inexact and incomplete sparse supervision may severely deteriorate the detection performance. To address this issue, we develop the SS3D++ method that alternatively improves 3D detector training and confident fully-annotated scene generation in a unified learning scheme. Using sparse annotations as seeds, we progressively generate confident fully-annotated scenes based on designing a missing-annotated instance mining module and reliable background mining module. Our proposed method produces competitive results when compared with SOTA weakly-supervised methods using the same or even more annotation costs. Besides, compared with SOTA fully-supervised methods, we achieve on-par or even better performance on the KITTI dataset with about 5x less annotation cost, and 90% of their performance on the Waymo dataset with about 15x less annotation cost. The additional unlabeled training scenes could further boost the performance. The code will be available at https://github.com/gaocq/SS3D2.
<div id='section'>Paperid: <span id='pid'>201, <a href='https://arxiv.org/pdf/2403.02566.pdf' target='_blank'>https://arxiv.org/pdf/2403.02566.pdf</a></span>   <span><a href='https://github.com/runminjiang/PW4MedSeg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Runmin Jiang, Zhaoxin Fan, Junhao Wu, Lenghan Zhu, Xin Huang, Tianyang Wang, Heng Huang, Min Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.02566">Enhancing Weakly Supervised 3D Medical Image Segmentation through Probabilistic-aware Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D medical image segmentation is a challenging task with crucial implications for disease diagnosis and treatment planning. Recent advances in deep learning have significantly enhanced fully supervised medical image segmentation. However, this approach heavily relies on labor-intensive and time-consuming fully annotated ground-truth labels, particularly for 3D volumes. To overcome this limitation, we propose a novel probabilistic-aware weakly supervised learning pipeline, specifically designed for 3D medical imaging. Our pipeline integrates three innovative components: a Probability-based Pseudo Label Generation technique for synthesizing dense segmentation masks from sparse annotations, a Probabilistic Multi-head Self-Attention network for robust feature extraction within our Probabilistic Transformer Network, and a Probability-informed Segmentation Loss Function to enhance training with annotation confidence. Demonstrating significant advances, our approach not only rivals the performance of fully supervised methods but also surpasses existing weakly supervised methods in CT and MRI datasets, achieving up to 18.1% improvement in Dice scores for certain organs. The code is available at https://github.com/runminjiang/PW4MedSeg.
<div id='section'>Paperid: <span id='pid'>202, <a href='https://arxiv.org/pdf/2403.01169.pdf' target='_blank'>https://arxiv.org/pdf/2403.01169.pdf</a></span>   <span><a href='https://github.com/shiwoaz/lap' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenchen Tao, Xiaohao Peng, Chong Wang, Jiafei Wu, Puning Zhao, Jun Wang, Jiangbo Qian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.01169">Learn Suspected Anomalies from Event Prompts for Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most models for weakly supervised video anomaly detection (WS-VAD) rely on multiple instance learning, aiming to distinguish normal and abnormal snippets without specifying the type of anomaly. However, the ambiguous nature of anomaly definitions across contexts may introduce inaccuracy in discriminating abnormal and normal events. To show the model what is anomalous, a novel framework is proposed to guide the learning of suspected anomalies from event prompts. Given a textual prompt dictionary of potential anomaly events and the captions generated from anomaly videos, the semantic anomaly similarity between them could be calculated to identify the suspected events for each video snippet. It enables a new multi-prompt learning process to constrain the visual-semantic features across all videos, as well as provides a new way to label pseudo anomalies for self-training. To demonstrate its effectiveness, comprehensive experiments and detailed ablation studies are conducted on four datasets, namely XD-Violence, UCF-Crime, TAD, and ShanghaiTech. Our proposed model outperforms most state-of-the-art methods in terms of AP or AUC (86.5\%, \hl{90.4}\%, 94.4\%, and 97.4\%). Furthermore, it shows promising performance in open-set and cross-dataset cases. The data, code, and models can be found at: \url{https://github.com/shiwoaz/lap}.
<div id='section'>Paperid: <span id='pid'>203, <a href='https://arxiv.org/pdf/2402.18467.pdf' target='_blank'>https://arxiv.org/pdf/2402.18467.pdf</a></span>   <span><a href='https://github.com/zwyang6/SeCo.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiwei Yang, Kexue Fu, Minghong Duan, Linhao Qu, Shuo Wang, Zhijian Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.18467">Separate and Conquer: Decoupling Co-occurrence via Decomposition and Representation for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation (WSSS) with image-level labels aims to achieve segmentation tasks without dense annotations. However, attributed to the frequent coupling of co-occurring objects and the limited supervision from image-level labels, the challenging co-occurrence problem is widely present and leads to false activation of objects in WSSS. In this work, we devise a 'Separate and Conquer' scheme SeCo to tackle this issue from dimensions of image space and feature space. In the image space, we propose to 'separate' the co-occurring objects with image decomposition by subdividing images into patches. Importantly, we assign each patch a category tag from Class Activation Maps (CAMs), which spatially helps remove the co-context bias and guide the subsequent representation. In the feature space, we propose to 'conquer' the false activation by enhancing semantic representation with multi-granularity knowledge contrast. To this end, a dual-teacher-single-student architecture is designed and tag-guided contrast is conducted, which guarantee the correctness of knowledge and further facilitate the discrepancy among co-contexts. We streamline the multi-staged WSSS pipeline end-to-end and tackle this issue without external supervision. Extensive experiments are conducted, validating the efficiency of our method and the superiority over previous single-staged and even multi-staged competitors on PASCAL VOC and MS COCO. Code is available at https://github.com/zwyang6/SeCo.git.
<div id='section'>Paperid: <span id='pid'>204, <a href='https://arxiv.org/pdf/2402.17891.pdf' target='_blank'>https://arxiv.org/pdf/2402.17891.pdf</a></span>   <span><a href='https://github.com/youshyee/CoSA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Yang, Hossein Rahmani, Sue Black, Bryan M. Williams
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17891">Weakly Supervised Co-training with Swapping Assignments for Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Class activation maps (CAMs) are commonly employed in weakly supervised semantic segmentation (WSSS) to produce pseudo-labels. Due to incomplete or excessive class activation, existing studies often resort to offline CAM refinement, introducing additional stages or proposing offline modules. This can cause optimization difficulties for single-stage methods and limit generalizability. In this study, we aim to reduce the observed CAM inconsistency and error to mitigate reliance on refinement processes. We propose an end-to-end WSSS model incorporating guided CAMs, wherein our segmentation model is trained while concurrently optimizing CAMs online. Our method, Co-training with Swapping Assignments (CoSA), leverages a dual-stream framework, where one sub-network learns from the swapped assignments generated by the other. We introduce three techniques: i) soft perplexity-based regularization to penalize uncertain regions; ii) a threshold-searching approach to dynamically revise the confidence threshold; and iii) contrastive separation to address the coexistence problem. CoSA demonstrates exceptional performance, achieving mIoU of 76.2\% and 51.0\% on VOC and COCO validation datasets, respectively, surpassing existing baselines by a substantial margin. Notably, CoSA is the first single-stage approach to outperform all existing multi-stage methods including those with additional supervision. Code is avilable at \url{https://github.com/youshyee/CoSA}.
<div id='section'>Paperid: <span id='pid'>205, <a href='https://arxiv.org/pdf/2402.17555.pdf' target='_blank'>https://arxiv.org/pdf/2402.17555.pdf</a></span>   <span><a href='https://github.com/Zxl19990529/Class-driven-Scribble-Promotion-Network' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinliang Zhang, Lei Zhu, Hangzhou He, Lujia Jin, Yanye Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17555">Scribble Hides Class: Promoting Scribble-Based Weakly-Supervised Semantic Segmentation with Its Class Label</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scribble-based weakly-supervised semantic segmentation using sparse scribble supervision is gaining traction as it reduces annotation costs when compared to fully annotated alternatives. Existing methods primarily generate pseudo-labels by diffusing labeled pixels to unlabeled ones with local cues for supervision. However, this diffusion process fails to exploit global semantics and class-specific cues, which are important for semantic segmentation. In this study, we propose a class-driven scribble promotion network, which utilizes both scribble annotations and pseudo-labels informed by image-level classes and global semantics for supervision. Directly adopting pseudo-labels might misguide the segmentation model, thus we design a localization rectification module to correct foreground representations in the feature space. To further combine the advantages of both supervisions, we also introduce a distance entropy loss for uncertainty reduction, which adapts per-pixel confidence weights according to the reliable region determined by the scribble and pseudo-label's boundary. Experiments on the ScribbleSup dataset with different qualities of scribble annotations outperform all the previous methods, demonstrating the superiority and robustness of our method.The code is available at https://github.com/Zxl19990529/Class-driven-Scribble-Promotion-Network.
<div id='section'>Paperid: <span id='pid'>206, <a href='https://arxiv.org/pdf/2402.16001.pdf' target='_blank'>https://arxiv.org/pdf/2402.16001.pdf</a></span>   <span><a href='https://github.com/yu-ni1989/ANLC-Former' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huan Ni, Yubin Zhao, Haiyan Guan, Cheng Jiang, Yongshi Jie, Xing Wang, Yiyang Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.16001">Cross-Resolution Land Cover Classification Using Outdated Products and Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale high-resolution land cover classification is a prerequisite for constructing Earth system models and addressing ecological and resource issues. Advancements in satellite sensor technology have led to an improvement in spatial resolution and wider coverage areas. Nevertheless, the lack of high-resolution labeled data is still a challenge, hindering the largescale application of land cover classification methods. In this paper, we propose a Transformerbased weakly supervised method for cross-resolution land cover classification using outdated data. First, to capture long-range dependencies without missing the fine-grained details of objects, we propose a U-Net-like Transformer based on a reverse difference mechanism (RDM) using dynamic sparse attention. Second, we propose an anti-noise loss calculation (ANLC) module based on optimal transport (OT). Anti-noise loss calculation identifies confident areas (CA) and vague areas (VA) based on the OT matrix, which relieves the impact of noises in outdated land cover products. By introducing a weakly supervised loss with weights and employing unsupervised loss, the RDM-based U-Net-like Transformer was trained. Remote sensing images with 1 m resolution and the corresponding ground-truths of six states in the United States were employed to validate the performance of the proposed method. The experiments utilized outdated land cover products with 30 m resolution from 2013 as training labels, and produced land cover maps with 1 m resolution from 2017. The results show the superiority of the proposed method compared to state-of-the-art methods. The code is available at https://github.com/yu-ni1989/ANLC-Former.
<div id='section'>Paperid: <span id='pid'>207, <a href='https://arxiv.org/pdf/2402.14812.pdf' target='_blank'>https://arxiv.org/pdf/2402.14812.pdf</a></span>   <span><a href='https://github.com/hustvl/WeakSAM' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/hustvl/WeakSAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lianghui Zhu, Junwei Zhou, Yan Liu, Xin Hao, Wenyu Liu, Xinggang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.14812">WeakSAM: Segment Anything Meets Weakly-supervised Instance-level Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised visual recognition using inexact supervision is a critical yet challenging learning problem. It significantly reduces human labeling costs and traditionally relies on multi-instance learning and pseudo-labeling. This paper introduces WeakSAM and solves the weakly-supervised object detection (WSOD) and segmentation by utilizing the pre-learned world knowledge contained in a vision foundation model, i.e., the Segment Anything Model (SAM). WeakSAM addresses two critical limitations in traditional WSOD retraining, i.e., pseudo ground truth (PGT) incompleteness and noisy PGT instances, through adaptive PGT generation and Region of Interest (RoI) drop regularization. It also addresses the SAM's problems of requiring prompts and category unawareness for automatic object detection and segmentation. Our results indicate that WeakSAM significantly surpasses previous state-of-the-art methods in WSOD and WSIS benchmarks with large margins, i.e. average improvements of 7.4% and 8.5%, respectively. The code is available at \url{https://github.com/hustvl/WeakSAM}.
<div id='section'>Paperid: <span id='pid'>208, <a href='https://arxiv.org/pdf/2402.12208.pdf' target='_blank'>https://arxiv.org/pdf/2402.12208.pdf</a></span>   <span><a href='https://github.com/jishengpeng/languagecodec' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengpeng Ji, Minghui Fang, Jialong Zuo, Ziyue Jiang, Dingdong Wang, Hanting Wang, Hai Huang, Zhou Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.12208">Language-Codec: Bridging Discrete Codec Representations and Speech Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, large language models have achieved significant success in generative tasks related to speech, audio, music, and other signal domains. A crucial element of these models is the discrete acoustic codecs, which serve as an intermediate representation replacing the mel-spectrogram. However, there exist several gaps between discrete codecs and downstream speech language models. Specifically, 1) Due to the reconstruction paradigm of the Codec model and the structure of residual vector quantization, the initial channel of the codebooks contains excessive information, making it challenging to directly generate acoustic tokens from weakly supervised signals such as text in downstream tasks. 2) numerous codebooks increases the burden on downstream speech language models. Consequently, leveraging the characteristics of speech language models, we propose Language-Codec. In the Language-Codec, we introduce a Masked Channel Residual Vector Quantization (MCRVQ) mechanism along with improved fourier transform structures and attention blocks, refined discriminator design to address the aforementioned gaps. We compare our method with competing audio compression algorithms and observe significant outperformance across extensive evaluations. Furthermore, we also validate the efficiency of the Language-Codec on downstream speech language models. The source code and pre-trained models can be accessed at https://github.com/jishengpeng/languagecodec .
<div id='section'>Paperid: <span id='pid'>209, <a href='https://arxiv.org/pdf/2402.12128.pdf' target='_blank'>https://arxiv.org/pdf/2402.12128.pdf</a></span>   <span><a href='https://github.com/gzq17/Weakly-Supervised-by-MIP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhanqiang Guo, Zimeng Tan, Jianjiang Feng, Jie Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.12128">3D Vascular Segmentation Supervised by 2D Annotation of Maximum Intensity Projection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vascular structure segmentation plays a crucial role in medical analysis and clinical applications. The practical adoption of fully supervised segmentation models is impeded by the intricacy and time-consuming nature of annotating vessels in the 3D space. This has spurred the exploration of weakly-supervised approaches that reduce reliance on expensive segmentation annotations. Despite this, existing weakly supervised methods employed in organ segmentation, which encompass points, bounding boxes, or graffiti, have exhibited suboptimal performance when handling sparse vascular structure. To alleviate this issue, we employ maximum intensity projection (MIP) to decrease the dimensionality of 3D volume to 2D image for efficient annotation, and the 2D labels are utilized to provide guidance and oversight for training 3D vessel segmentation model. Initially, we generate pseudo-labels for 3D blood vessels using the annotations of 2D projections. Subsequently, taking into account the acquisition method of the 2D labels, we introduce a weakly-supervised network that fuses 2D-3D deep features via MIP to further improve segmentation performance. Furthermore, we integrate confidence learning and uncertainty estimation to refine the generated pseudo-labels, followed by fine-tuning the segmentation network. Our method is validated on five datasets (including cerebral vessel, aorta and coronary artery), demonstrating highly competitive performance in segmenting vessels and the potential to significantly reduce the time and effort required for vessel annotation. Our code is available at: https://github.com/gzq17/Weakly-Supervised-by-MIP.
<div id='section'>Paperid: <span id='pid'>210, <a href='https://arxiv.org/pdf/2402.11985.pdf' target='_blank'>https://arxiv.org/pdf/2402.11985.pdf</a></span>   <span><a href='https://github.com/philip-mueller/wsrpn' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Philip MÃ¼ller, Felix Meissen, Georgios Kaissis, Daniel Rueckert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.11985">Weakly Supervised Object Detection in Chest X-Rays with Differentiable ROI Proposal Networks and Soft ROI Pooling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised object detection (WSup-OD) increases the usefulness and interpretability of image classification algorithms without requiring additional supervision. The successes of multiple instance learning in this task for natural images, however, do not translate well to medical images due to the very different characteristics of their objects (i.e. pathologies). In this work, we propose Weakly Supervised ROI Proposal Networks (WSRPN), a new method for generating bounding box proposals on the fly using a specialized region of interest-attention (ROI-attention) module. WSRPN integrates well with classic backbone-head classification algorithms and is end-to-end trainable with only image-label supervision. We experimentally demonstrate that our new method outperforms existing methods in the challenging task of disease localization in chest X-ray images. Code: https://github.com/philip-mueller/wsrpn
<div id='section'>Paperid: <span id='pid'>211, <a href='https://arxiv.org/pdf/2402.09811.pdf' target='_blank'>https://arxiv.org/pdf/2402.09811.pdf</a></span>   <span><a href='https://github.com/IITB-LEAP-OCR/TEXTRON' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dhruv Kudale, Badri Vishal Kasuba, Venkatapathy Subramanian, Parag Chaudhuri, Ganesh Ramakrishnan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.09811">TEXTRON: Weakly Supervised Multilingual Text Detection through Data Programming</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Several recent deep learning (DL) based techniques perform considerably well on image-based multilingual text detection. However, their performance relies heavily on the availability and quality of training data. There are numerous types of page-level document images consisting of information in several modalities, languages, fonts, and layouts. This makes text detection a challenging problem in the field of computer vision (CV), especially for low-resource or handwritten languages. Furthermore, there is a scarcity of word-level labeled data for text detection, especially for multilingual settings and Indian scripts that incorporate both printed and handwritten text. Conventionally, Indian script text detection requires training a DL model on plenty of labeled data, but to the best of our knowledge, no relevant datasets are available. Manual annotation of such data requires a lot of time, effort, and expertise. In order to solve this problem, we propose TEXTRON, a Data Programming-based approach, where users can plug various text detection methods into a weak supervision-based learning framework. One can view this approach to multilingual text detection as an ensemble of different CV-based techniques and DL approaches. TEXTRON can leverage the predictions of DL models pre-trained on a significant amount of language data in conjunction with CV-based methods to improve text detection in other languages. We demonstrate that TEXTRON can improve the detection performance for documents written in Indian languages, despite the absence of corresponding labeled data. Further, through extensive experimentation, we show improvement brought about by our approach over the current State-of-the-art (SOTA) models, especially for handwritten Devanagari text. Code and dataset has been made available at https://github.com/IITB-LEAP-OCR/TEXTRON
<div id='section'>Paperid: <span id='pid'>212, <a href='https://arxiv.org/pdf/2402.07633.pdf' target='_blank'>https://arxiv.org/pdf/2402.07633.pdf</a></span>   <span><a href='https://github.com/ZechengLi19/CIM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zecheng Li, Zening Zeng, Yuqi Liang, Jin-Gang Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.07633">Complete Instances Mining for Weakly Supervised Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised instance segmentation (WSIS) using only image-level labels is a challenging task due to the difficulty of aligning coarse annotations with the finer task. However, with the advancement of deep neural networks (DNNs), WSIS has garnered significant attention. Following a proposal-based paradigm, we encounter a redundant segmentation problem resulting from a single instance being represented by multiple proposals. For example, we feed a picture of a dog and proposals into the network and expect to output only one proposal containing a dog, but the network outputs multiple proposals. To address this problem, we propose a novel approach for WSIS that focuses on the online refinement of complete instances through the use of MaskIoU heads to predict the integrity scores of proposals and a Complete Instances Mining (CIM) strategy to explicitly model the redundant segmentation problem and generate refined pseudo labels. Our approach allows the network to become aware of multiple instances and complete instances, and we further improve its robustness through the incorporation of an Anti-noise strategy. Empirical evaluations on the PASCAL VOC 2012 and MS COCO datasets demonstrate that our method achieves state-of-the-art performance with a notable margin. Our implementation will be made available at https://github.com/ZechengLi19/CIM.
<div id='section'>Paperid: <span id='pid'>213, <a href='https://arxiv.org/pdf/2402.04563.pdf' target='_blank'>https://arxiv.org/pdf/2402.04563.pdf</a></span>   <span><a href='https://github.com/LeemSaebom/Attention-Guided-CAM-Visual-Explanations-of-Vision-Transformer-Guided-by-Self-Attention.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Saebom Leem, Hyunseok Seo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04563">Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision Transformer(ViT) is one of the most widely used models in the computer vision field with its great performance on various tasks. In order to fully utilize the ViT-based architecture in various applications, proper visualization methods with a decent localization performance are necessary, but these methods employed in CNN-based models are still not available in ViT due to its unique structure. In this work, we propose an attention-guided visualization method applied to ViT that provides a high-level semantic explanation for its decision. Our method selectively aggregates the gradients directly propagated from the classification output to each self-attention, collecting the contribution of image features extracted from each location of the input image. These gradients are additionally guided by the normalized self-attention scores, which are the pairwise patch correlation scores. They are used to supplement the gradients on the patch-level context information efficiently detected by the self-attention mechanism. This approach of our method provides elaborate high-level semantic explanations with great localization performance only with the class labels. As a result, our method outperforms the previous leading explainability methods of ViT in the weakly-supervised localization task and presents great capability in capturing the full instances of the target class object. Meanwhile, our method provides a visualization that faithfully explains the model, which is demonstrated in the perturbation comparison test.
<div id='section'>Paperid: <span id='pid'>214, <a href='https://arxiv.org/pdf/2402.01155.pdf' target='_blank'>https://arxiv.org/pdf/2402.01155.pdf</a></span>   <span><a href='https://github.com/Sohanpatnaik106/CABINET_QA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sohan Patnaik, Heril Changwal, Milan Aggarwal, Sumit Bhatia, Yaman Kumar, Balaji Krishnamurthy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.01155">CABINET: Content Relevance based Noise Reduction for Table Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Table understanding capability of Large Language Models (LLMs) has been extensively studied through the task of question-answering (QA) over tables. Typically, only a small part of the whole table is relevant to derive the answer for a given question. The irrelevant parts act as noise and are distracting information, resulting in sub-optimal performance due to the vulnerability of LLMs to noise. To mitigate this, we propose CABINET (Content RelevAnce-Based NoIse ReductioN for TablE QuesTion-Answering) - a framework to enable LLMs to focus on relevant tabular data by suppressing extraneous information. CABINET comprises an Unsupervised Relevance Scorer (URS), trained differentially with the QA LLM, that weighs the table content based on its relevance to the input question before feeding it to the question-answering LLM (QA LLM). To further aid the relevance scorer, CABINET employs a weakly supervised module that generates a parsing statement describing the criteria of rows and columns relevant to the question and highlights the content of corresponding table cells. CABINET significantly outperforms various tabular LLM baselines, as well as GPT3-based in-context learning methods, is more robust to noise, maintains outperformance on tables of varying sizes, and establishes new SoTA performance on WikiTQ, FeTaQA, and WikiSQL datasets. We release our code and datasets at https://github.com/Sohanpatnaik106/CABINET_QA.
<div id='section'>Paperid: <span id='pid'>215, <a href='https://arxiv.org/pdf/2401.17828.pdf' target='_blank'>https://arxiv.org/pdf/2401.17828.pdf</a></span>   <span><a href='https://github.com/RozhanAhmadi/SWTformer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rozhan Ahmadi, Shohreh Kasaei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.17828">Leveraging Swin Transformer for Local-to-Global Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, weakly supervised semantic segmentation using image-level labels as supervision has received significant attention in the field of computer vision. Most existing methods have addressed the challenges arising from the lack of spatial information in these labels by focusing on facilitating supervised learning through the generation of pseudo-labels from class activation maps (CAMs). Due to the localized pattern detection of CNNs, CAMs often emphasize only the most discriminative parts of an object, making it challenging to accurately distinguish foreground objects from each other and the background. Recent studies have shown that Vision Transformer (ViT) features, due to their global view, are more effective in capturing the scene layout than CNNs. However, the use of hierarchical ViTs has not been extensively explored in this field. This work explores the use of Swin Transformer by proposing "SWTformer" to enhance the accuracy of the initial seed CAMs by bringing local and global views together. SWTformer-V1 generates class probabilities and CAMs using only the patch tokens as features. SWTformer-V2 incorporates a multi-scale feature fusion mechanism to extract additional information and utilizes a background-aware mechanism to generate more accurate localization maps with improved cross-object discrimination. Based on experiments on the PascalVOC 2012 dataset, SWTformer-V1 achieves a 0.98% mAP higher localization accuracy, outperforming state-of-the-art models. It also yields comparable performance by 0.82% mIoU on average higher than other methods in generating initial localization maps, depending only on the classification network. SWTformer-V2 further improves the accuracy of the generated seed CAMs by 5.32% mIoU, further proving the effectiveness of the local-to-global view provided by the Swin transformer. Code available at: https://github.com/RozhanAhmadi/SWTformer
<div id='section'>Paperid: <span id='pid'>216, <a href='https://arxiv.org/pdf/2401.14442.pdf' target='_blank'>https://arxiv.org/pdf/2401.14442.pdf</a></span>   <span><a href='https://github.com/AstraZeneca/SelfPAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Talip Ucar, Aubin Ramon, Dino Oglic, Rebecca Croasdale-Wood, Tom Diethe, Pietro Sormanni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.14442">Improving Antibody Humanness Prediction using Patent Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We investigate the potential of patent data for improving the antibody humanness prediction using a multi-stage, multi-loss training process. Humanness serves as a proxy for the immunogenic response to antibody therapeutics, one of the major causes of attrition in drug discovery and a challenging obstacle for their use in clinical settings. We pose the initial learning stage as a weakly-supervised contrastive-learning problem, where each antibody sequence is associated with possibly multiple identifiers of function and the objective is to learn an encoder that groups them according to their patented properties. We then freeze a part of the contrastive encoder and continue training it on the patent data using the cross-entropy loss to predict the humanness score of a given antibody sequence. We illustrate the utility of the patent data and our approach by performing inference on three different immunogenicity datasets, unseen during training. Our empirical results demonstrate that the learned model consistently outperforms the alternative baselines and establishes new state-of-the-art on five out of six inference tasks, irrespective of the used metric.
<div id='section'>Paperid: <span id='pid'>217, <a href='https://arxiv.org/pdf/2401.11791.pdf' target='_blank'>https://arxiv.org/pdf/2401.11791.pdf</a></span>   <span><a href='https://projectdisr.github.io/semples/' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/NVlabs/SemPLeS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ci-Siang Lin, Chien-Yi Wang, Yu-Chiang Frank Wang, Min-Hung Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11791">Semantic Prompt Learning for Weakly-Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-Supervised Semantic Segmentation (WSSS) aims to train segmentation models using image data with only image-level supervision. Since precise pixel-level annotations are not accessible, existing methods typically focus on producing pseudo masks for training segmentation models by refining CAM-like heatmaps. However, the produced heatmaps may capture only the discriminative image regions of object categories or the associated co-occurring backgrounds. To address the issues, we propose a Semantic Prompt Learning for WSSS (SemPLeS) framework, which learns to effectively prompt the CLIP latent space to enhance the semantic alignment between the segmented regions and the target object categories. More specifically, we propose Contrastive Prompt Learning and Prompt-guided Semantic Refinement to learn the prompts that adequately describe and suppress the co-occurring backgrounds associated with each object category. In this way, SemPLeS can perform better semantic alignment between object regions and class labels, resulting in desired pseudo masks for training segmentation models. The proposed SemPLeS framework achieves competitive performance on standard WSSS benchmarks, PASCAL VOC 2012 and MS COCO 2014, and shows compatibility with other WSSS methods. Code: https://github.com/NVlabs/SemPLeS.
<div id='section'>Paperid: <span id='pid'>218, <a href='https://arxiv.org/pdf/2401.11719.pdf' target='_blank'>https://arxiv.org/pdf/2401.11719.pdf</a></span>   <span><a href='https://github.com/Barrett-python/SFC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinqiao Zhao, Feilong Tang, Xiaoyang Wang, Jimin Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11719">SFC: Shared Feature Calibration in Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image-level weakly supervised semantic segmentation has received increasing attention due to its low annotation cost. Existing methods mainly rely on Class Activation Mapping (CAM) to obtain pseudo-labels for training semantic segmentation models. In this work, we are the first to demonstrate that long-tailed distribution in training data can cause the CAM calculated through classifier weights over-activated for head classes and under-activated for tail classes due to the shared features among head- and tail- classes. This degrades pseudo-label quality and further influences final semantic segmentation performance. To address this issue, we propose a Shared Feature Calibration (SFC) method for CAM generation. Specifically, we leverage the class prototypes that carry positive shared features and propose a Multi-Scaled Distribution-Weighted (MSDW) consistency loss for narrowing the gap between the CAMs generated through classifier weights and class prototypes during training. The MSDW loss counterbalances over-activation and under-activation by calibrating the shared features in head-/tail-class classifier weights. Experimental results show that our SFC significantly improves CAM boundaries and achieves new state-of-the-art performances. The project is available at https://github.com/Barrett-python/SFC.
<div id='section'>Paperid: <span id='pid'>219, <a href='https://arxiv.org/pdf/2401.11115.pdf' target='_blank'>https://arxiv.org/pdf/2401.11115.pdf</a></span>   <span><a href='https://nhathoang2002.github.io/MotionMix-page/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nhat M. Hoang, Kehong Gong, Chuan Guo, Michael Bi Mi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11115">MotionMix: Weakly-Supervised Diffusion for Controllable Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Controllable generation of 3D human motions becomes an important topic as the world embraces digital transformation. Existing works, though making promising progress with the advent of diffusion models, heavily rely on meticulously captured and annotated (e.g., text) high-quality motion corpus, a resource-intensive endeavor in the real world. This motivates our proposed MotionMix, a simple yet effective weakly-supervised diffusion model that leverages both noisy and unannotated motion sequences. Specifically, we separate the denoising objectives of a diffusion model into two stages: obtaining conditional rough motion approximations in the initial $T-T^*$ steps by learning the noisy annotated motions, followed by the unconditional refinement of these preliminary motions during the last $T^*$ steps using unannotated motions. Notably, though learning from two sources of imperfect data, our model does not compromise motion generation quality compared to fully supervised approaches that access gold data. Extensive experiments on several benchmarks demonstrate that our MotionMix, as a versatile framework, consistently achieves state-of-the-art performances on text-to-motion, action-to-motion, and music-to-dance tasks. Project page: https://nhathoang2002.github.io/MotionMix-page/
<div id='section'>Paperid: <span id='pid'>220, <a href='https://arxiv.org/pdf/2401.09883.pdf' target='_blank'>https://arxiv.org/pdf/2401.09883.pdf</a></span>   <span><a href='https://github.com/CVI-SZU/QA-CLIMS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Songhe Deng, Wei Zhuo, Jinheng Xie, Linlin Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.09883">Question-Answer Cross Language Image Matching for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Class Activation Map (CAM) has emerged as a popular tool for weakly supervised semantic segmentation (WSSS), allowing the localization of object regions in an image using only image-level labels. However, existing CAM methods suffer from under-activation of target object regions and false-activation of background regions due to the fact that a lack of detailed supervision can hinder the model's ability to understand the image as a whole. In this paper, we propose a novel Question-Answer Cross-Language-Image Matching framework for WSSS (QA-CLIMS), leveraging the vision-language foundation model to maximize the text-based understanding of images and guide the generation of activation maps. First, a series of carefully designed questions are posed to the VQA (Visual Question Answering) model with Question-Answer Prompt Engineering (QAPE) to generate a corpus of both foreground target objects and backgrounds that are adaptive to query images. We then employ contrastive learning in a Region Image Text Contrastive (RITC) network to compare the obtained foreground and background regions with the generated corpus. Our approach exploits the rich textual information from the open vocabulary as additional supervision, enabling the model to generate high-quality CAMs with a more complete object region and reduce false-activation of background regions. We conduct extensive analysis to validate the proposed method and show that our approach performs state-of-the-art on both PASCAL VOC 2012 and MS COCO datasets. Code is available at: https://github.com/CVI-SZU/QA-CLIMS
<div id='section'>Paperid: <span id='pid'>221, <a href='https://arxiv.org/pdf/2401.07437.pdf' target='_blank'>https://arxiv.org/pdf/2401.07437.pdf</a></span>   <span><a href='https://github.com/hust-linyi/bonus' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Lin, Zeyu Wang, Dong Zhang, Kwang-Ting Cheng, Hao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.07437">BoNuS: Boundary Mining for Nuclei Segmentation with Partial Point Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nuclei segmentation is a fundamental prerequisite in the digital pathology workflow. The development of automated methods for nuclei segmentation enables quantitative analysis of the wide existence and large variances in nuclei morphometry in histopathology images. However, manual annotation of tens of thousands of nuclei is tedious and time-consuming, which requires significant amount of human effort and domain-specific expertise. To alleviate this problem, in this paper, we propose a weakly-supervised nuclei segmentation method that only requires partial point labels of nuclei. Specifically, we propose a novel boundary mining framework for nuclei segmentation, named BoNuS, which simultaneously learns nuclei interior and boundary information from the point labels. To achieve this goal, we propose a novel boundary mining loss, which guides the model to learn the boundary information by exploring the pairwise pixel affinity in a multiple-instance learning manner. Then, we consider a more challenging problem, i.e., partial point label, where we propose a nuclei detection module with curriculum learning to detect the missing nuclei with prior morphological knowledge. The proposed method is validated on three public datasets, MoNuSeg, CPM, and CoNIC datasets. Experimental results demonstrate the superior performance of our method to the state-of-the-art weakly-supervised nuclei segmentation methods. Code: https://github.com/hust-linyi/bonus.
<div id='section'>Paperid: <span id='pid'>222, <a href='https://arxiv.org/pdf/2401.02584.pdf' target='_blank'>https://arxiv.org/pdf/2401.02584.pdf</a></span>   <span><a href='https://github.com/wsntxxn/TextToAudioGrounding' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuenan Xu, Ziyang Ma, Mengyue Wu, Kai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.02584">Towards Weakly Supervised Text-to-Audio Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-audio grounding (TAG) task aims to predict the onsets and offsets of sound events described by natural language. This task can facilitate applications such as multimodal information retrieval. This paper focuses on weakly-supervised text-to-audio grounding (WSTAG), where frame-level annotations of sound events are unavailable, and only the caption of a whole audio clip can be utilized for training. WSTAG is superior to strongly-supervised approaches in its scalability to large audio-text datasets. Two WSTAG frameworks are studied in this paper: sentence-level and phrase-level. First, we analyze the limitations of mean pooling used in the previous WSTAG approach and investigate the effects of different pooling strategies. We then propose phrase-level WSTAG to use matching labels between audio clips and phrases for training. Advanced negative sampling strategies and self-supervision are proposed to enhance the accuracy of the weak labels and provide pseudo strong labels. Experimental results show that our system significantly outperforms the previous WSTAG SOTA. Finally, we conduct extensive experiments to analyze the effects of several factors on phrase-level WSTAG. The code and model is available at https://github.com/wsntxxn/TextToAudioGrounding.
<div id='section'>Paperid: <span id='pid'>223, <a href='https://arxiv.org/pdf/2502.12917.pdf' target='_blank'>https://arxiv.org/pdf/2502.12917.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haicheng Wang, Chen Ju, Weixiong Lin, Chaofan Ma, Shuai Xiao, Ya Zhang, Yanfeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.12917">Contrast-Unity for Partially-Supervised Temporal Sentence Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Temporal sentence grounding aims to detect event timestamps described by the natural language query from given untrimmed videos. The existing fully-supervised setting achieves great results but requires expensive annotation costs; while the weakly-supervised setting adopts cheap labels but performs poorly. To pursue high performance with less annotation costs, this paper introduces an intermediate partially-supervised setting, i.e., only short-clip is available during training. To make full use of partial labels, we specially design one contrast-unity framework, with the two-stage goal of implicit-explicit progressive grounding. In the implicit stage, we align event-query representations at fine granularity using comprehensive quadruple contrastive learning: event-query gather, event-background separation, intra-cluster compactness and inter-cluster separability. Then, high-quality representations bring acceptable grounding pseudo-labels. In the explicit stage, to explicitly optimize grounding objectives, we train one fully-supervised model using obtained pseudo-labels for grounding refinement and denoising. Extensive experiments and thoroughly ablations on Charades-STA and ActivityNet Captions demonstrate the significance of partial supervision, as well as our superior performance.
<div id='section'>Paperid: <span id='pid'>224, <a href='https://arxiv.org/pdf/2505.16294.pdf' target='_blank'>https://arxiv.org/pdf/2505.16294.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufei Yin, Lechao Cheng, Wengang Zhou, Jiajun Deng, Zhou Yu, Houqiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16294">Self-Classification Enhancement and Correction for Weakly Supervised Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, weakly supervised object detection (WSOD) has attracted much attention due to its low labeling cost. The success of recent WSOD models is often ascribed to the two-stage multi-class classification (MCC) task, i.e., multiple instance learning and online classification refinement. Despite achieving non-trivial progresses, these methods overlook potential classification ambiguities between these two MCC tasks and fail to leverage their unique strengths. In this work, we introduce a novel WSOD framework to ameliorate these two issues. For one thing, we propose a self-classification enhancement module that integrates intra-class binary classification (ICBC) to bridge the gap between the two distinct MCC tasks. The ICBC task enhances the network's discrimination between positive and mis-located samples in a class-wise manner and forges a mutually reinforcing relationship with the MCC task. For another, we propose a self-classification correction algorithm during inference, which combines the results of both MCC tasks to effectively reduce the mis-classified predictions. Extensive experiments on the prevalent VOC 2007 & 2012 datasets demonstrate the superior performance of our framework.
<div id='section'>Paperid: <span id='pid'>225, <a href='https://arxiv.org/pdf/2505.04905.pdf' target='_blank'>https://arxiv.org/pdf/2505.04905.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xi Yang, Songsong Duan, Nannan Wang, Xinbo Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04905">Pro2SAM: Mask Prompt to SAM with Grid Points for Weakly Supervised Object Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Object Localization (WSOL), which aims to localize objects by only using image-level labels, has attracted much attention because of its low annotation cost in real applications. Current studies focus on the Class Activation Map (CAM) of CNN and the self-attention map of transformer to identify the region of objects. However, both CAM and self-attention maps can not learn pixel-level fine-grained information on the foreground objects, which hinders the further advance of WSOL. To address this problem, we initiatively leverage the capability of zero-shot generalization and fine-grained segmentation in Segment Anything Model (SAM) to boost the activation of integral object regions. Further, to alleviate the semantic ambiguity issue accrued in single point prompt-based SAM, we propose an innovative mask prompt to SAM (Pro2SAM) network with grid points for WSOL task. First, we devise a Global Token Transformer (GTFormer) to generate a coarse-grained foreground map as a flexible mask prompt, where the GTFormer jointly embeds patch tokens and novel global tokens to learn foreground semantics. Secondly, we deliver grid points as dense prompts into SAM to maximize the probability of foreground mask, which avoids the lack of objects caused by a single point/box prompt. Finally, we propose a pixel-level similarity metric to come true the mask matching from mask prompt to SAM, where the mask with the highest score is viewed as the final localization map. Experiments show that the proposed Pro2SAM achieves state-of-the-art performance on both CUB-200-2011 and ILSVRC, with 84.03\% and 66.85\% Top-1 Loc, respectively.
<div id='section'>Paperid: <span id='pid'>226, <a href='https://arxiv.org/pdf/2406.17988.pdf' target='_blank'>https://arxiv.org/pdf/2406.17988.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingxuan Wu, Zhiyang Dou, Sirui Xu, Soshi Shimada, Chen Wang, Zhengming Yu, Yuan Liu, Cheng Lin, Zeyu Cao, Taku Komura, Vladislav Golyanik, Christian Theobalt, Wenping Wang, Lingjie Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.17988">DICE: End-to-end Deformation Capture of Hand-Face Interactions from a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing 3D hand-face interactions with deformations from a single image is a challenging yet crucial task with broad applications in AR, VR, and gaming. The challenges stem from self-occlusions during single-view hand-face interactions, diverse spatial relationships between hands and face, complex deformations, and the ambiguity of the single-view setting. The first and only method for hand-face interaction recovery, Decaf, introduces a global fitting optimization guided by contact and deformation estimation networks trained on studio-collected data with 3D annotations. However, Decaf suffers from a time-consuming optimization process and limited generalization capability due to its reliance on 3D annotations of hand-face interaction data. To address these issues, we present DICE, the first end-to-end method for Deformation-aware hand-face Interaction reCovEry from a single image. DICE estimates the poses of hands and faces, contacts, and deformations simultaneously using a Transformer-based architecture. It features disentangling the regression of local deformation fields and global mesh vertex locations into two network branches, enhancing deformation and contact estimation for precise and robust hand-face mesh recovery. To improve generalizability, we propose a weakly-supervised training approach that augments the training set using in-the-wild images without 3D ground-truth annotations, employing the depths of 2D keypoints estimated by off-the-shelf models and adversarial priors of poses for supervision. Our experiments demonstrate that DICE achieves state-of-the-art performance on a standard benchmark and in-the-wild data in terms of accuracy and physical plausibility. Additionally, our method operates at an interactive rate (20 fps) on an Nvidia 4090 GPU, whereas Decaf requires more than 15 seconds for a single image. Our code will be publicly available upon publication.
<div id='section'>Paperid: <span id='pid'>227, <a href='https://arxiv.org/pdf/2511.12229.pdf' target='_blank'>https://arxiv.org/pdf/2511.12229.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhipeng Xue, Zhipeng Gao, Tongtong Xu, Xing Hu, Xin Xia, Shanping Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.12229">Actionable Warning Is Not Enough: Recommending Valid Actionable Warnings with Weak Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The use of static analysis tools has gained increasing popularity among developers in the last few years. However, the widespread adoption of static analysis tools is hindered by their high false alarm rates. Previous studies have introduced the concept of actionable warnings and built a machine-learning method to distinguish actionable warnings from false alarms. However, according to our empirical observation, the current assumption used for actionable warning(s) collection is rather shaky and inaccurate, leading to a large number of invalid actionable warnings. To address this problem, in this study, we build the first large actionable warning dataset by mining 68,274 reversions from Top-500 GitHub C repositories, we then take one step further by assigning each actionable warning a weak label regarding its likelihood of being a real bug. Following that, we propose a two-stage framework called ACWRecommender to automatically recommend the actionable warnings with high probability to be real bugs (AWHB). Our approach warms up the pre-trained model UniXcoder by identifying actionable warnings task (coarse-grained detection stage) and rerank AWHB to the top by weakly supervised learning (fine-grained reranking stage). Experimental results show that our proposed model outperforms several baselines by a large margin in terms of nDCG and MRR for AWHB recommendation. Moreover, we ran our tool on 6 randomly selected projects and manually checked the top-ranked warnings from 2,197 reported warnings, we reported top-10 recommended warnings to developers, 27 of them were already confirmed by developers as real bugs. Developers can quickly find real bugs among the massive amount of reported warnings, which verifies the practical usage of our tool.
<div id='section'>Paperid: <span id='pid'>228, <a href='https://arxiv.org/pdf/2506.20923.pdf' target='_blank'>https://arxiv.org/pdf/2506.20923.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinping Zhao, Xinshuo Hu, Zifei Shan, Shouzheng Huang, Yao Zhou, Zetian Sun, Zhenyu Liu, Dongfang Li, Xinyuan Wei, Qian Chen, Youcheng Pan, Yang Xiang, Meishan Zhang, Haofen Wang, Jun Yu, Baotian Hu, Min Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20923">KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose KaLM-Embedding-V2, a versatile and compact embedding model, which achieves impressive performance in general-purpose text embedding tasks by leveraging superior training techniques and data. Our key innovations include: (1) To better align the architecture with representation learning, we remove the causal attention mask and adopt a fully bidirectional transformer with simple yet effective mean-pooling to produce fixed-length embeddings; (2) We employ a multi-stage training pipeline: (i) pre-training on large-scale weakly supervised open-source corpora; (ii) fine-tuning on high-quality retrieval and non-retrieval datasets; and (iii) model-soup parameter averaging for robust generalization. Besides, we introduce a focal-style reweighting mechanism that concentrates learning on difficult samples and an online hard-negative mixing strategy to continuously enrich hard negatives without expensive offline mining; (3) We collect over 20 categories of data for pre-training and 100 categories of data for fine-tuning, to boost both the performance and generalization of the embedding model. Extensive evaluations on the Massive Text Embedding Benchmark (MTEB) Chinese and English show that our model significantly outperforms others of comparable size, and competes with 3x, 14x, 18x, and 26x larger embedding models, setting a new standard for a versatile and compact embedding model with less than 1B parameters.
<div id='section'>Paperid: <span id='pid'>229, <a href='https://arxiv.org/pdf/2505.06557.pdf' target='_blank'>https://arxiv.org/pdf/2505.06557.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lu Dong, Haiyu Zhang, Hongjie Zhang, Yifei Huang, Zhen-Hua Ling, Yu Qiao, Limin Wang, Yali Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06557">Weakly Supervised Temporal Sentence Grounding via Positive Sample Mining</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The task of weakly supervised temporal sentence grounding (WSTSG) aims to detect temporal intervals corresponding to a language description from untrimmed videos with only video-level video-language correspondence. For an anchor sample, most existing approaches generate negative samples either from other videos or within the same video for contrastive learning. However, some training samples are highly similar to the anchor sample, directly regarding them as negative samples leads to difficulties for optimization and ignores the correlations between these similar samples and the anchor sample. To address this, we propose Positive Sample Mining (PSM), a novel framework that mines positive samples from the training set to provide more discriminative supervision. Specifically, for a given anchor sample, we partition the remaining training set into semantically similar and dissimilar subsets based on the similarity of their text queries. To effectively leverage these correlations, we introduce a PSM-guided contrastive loss to ensure that the anchor proposal is closer to similar samples and further from dissimilar ones. Additionally, we design a PSM-guided rank loss to ensure that similar samples are closer to the anchor proposal than to the negative intra-video proposal, aiming to distinguish the anchor proposal and the negative intra-video proposal. Experiments on the WSTSG and grounded VideoQA tasks demonstrate the effectiveness and superiority of our method.
<div id='section'>Paperid: <span id='pid'>230, <a href='https://arxiv.org/pdf/2506.15757.pdf' target='_blank'>https://arxiv.org/pdf/2506.15757.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruoyu Wang, Tong Yu, Junda Wu, Yao Liu, Julian McAuley, Lina Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.15757">Weakly-supervised VLM-guided Partial Contrastive Learning for Visual Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Language Navigation (VLN) is a fundamental task within the field of Embodied AI, focusing on the ability of agents to navigate complex environments based on natural language instructions. Despite the progress made by existing methods, these methods often present some common challenges. First, they rely on pre-trained backbone models for visual perception, which struggle with the dynamic viewpoints in VLN scenarios. Second, the performance is limited when using pre-trained LLMs or VLMs without fine-tuning, due to the absence of VLN domain knowledge. Third, while fine-tuning LLMs and VLMs can improve results, their computational costs are higher than those without fine-tuning. To address these limitations, we propose Weakly-supervised Partial Contrastive Learning (WPCL), a method that enhances an agent's ability to identify objects from dynamic viewpoints in VLN scenarios by effectively integrating pre-trained VLM knowledge into the perception process, without requiring VLM fine-tuning. Our method enhances the agent's ability to interpret and respond to environmental cues while ensuring computational efficiency. Experimental results have shown that our method outperforms the baseline methods on multiple benchmarks, which validate the effectiveness, robustness and generalizability of our method.
<div id='section'>Paperid: <span id='pid'>231, <a href='https://arxiv.org/pdf/2501.12632.pdf' target='_blank'>https://arxiv.org/pdf/2501.12632.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shakeeb Murtaza, Soufiane Belharbi, Marco Pedersoli, Eric Granger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.12632">TeD-Loc: Text Distillation for Weakly Supervised Object Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised object localization (WSOL) using classification models trained with only image-class labels remains an important challenge in computer vision. Given their reliance on classification objectives, traditional WSOL methods like class activation mapping focus on the most discriminative object parts, often missing the full spatial extent. In contrast, recent WSOL methods based on vision-language models like CLIP require ground truth classes or external classifiers to produce a localization map, limiting their deployment in downstream tasks. Moreover, methods like GenPromp attempt to address these issues but introduce considerable complexity due to their reliance on conditional denoising processes and intricate prompt learning. This paper introduces Text Distillation for Localization (TeD-Loc), an approach that directly distills knowledge from CLIP text embeddings into the model backbone and produces patch-level localization. Multiple instance learning of these image patches allows for accurate localization and classification using one model without requiring external classifiers. Such integration of textual and visual modalities addresses the longstanding challenge of achieving accurate localization and classification concurrently, as WSOL methods in the literature typically converge at different epochs. Extensive experiments show that leveraging text embeddings and localization cues provides a cost-effective WSOL model. TeD-Loc improves Top-1 LOC accuracy over state-of-the-art models by about 5% on both CUB and ILSVRC datasets, while significantly reducing computational complexity compared to GenPromp.
<div id='section'>Paperid: <span id='pid'>232, <a href='https://arxiv.org/pdf/2407.06018.pdf' target='_blank'>https://arxiv.org/pdf/2407.06018.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shakeeb Murtaza, Marco Pedersoli, Aydin Sarraf, Eric Granger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.06018">Leveraging Transformers for Weakly Supervised Object Localization in Unconstrained Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-Supervised Video Object Localization (WSVOL) involves localizing an object in videos using only video-level labels, also referred to as tags. State-of-the-art WSVOL methods like Temporal CAM (TCAM) rely on class activation mapping (CAM) and typically require a pre-trained CNN classifier. However, their localization accuracy is affected by their tendency to minimize the mutual information between different instances of a class and exploit temporal information during training for downstream tasks, e.g., detection and tracking. In the absence of bounding box annotation, it is challenging to exploit precise information about objects from temporal cues because the model struggles to locate objects over time. To address these issues, a novel method called transformer based CAM for videos (TrCAM-V), is proposed for WSVOL. It consists of a DeiT backbone with two heads for classification and localization. The classification head is trained using standard classification loss (CL), while the localization head is trained using pseudo-labels that are extracted using a pre-trained CLIP model. From these pseudo-labels, the high and low activation values are considered to be foreground and background regions, respectively. Our TrCAM-V method allows training a localization network by sampling pseudo-pixels on the fly from these regions. Additionally, a conditional random field (CRF) loss is employed to align the object boundaries with the foreground map. During inference, the model can process individual frames for real-time localization applications. Extensive experiments on challenging YouTube-Objects unconstrained video datasets show that our TrCAM-V method achieves new state-of-the-art performance in terms of classification and localization accuracy.
<div id='section'>Paperid: <span id='pid'>233, <a href='https://arxiv.org/pdf/2404.10034.pdf' target='_blank'>https://arxiv.org/pdf/2404.10034.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shakeeb Murtaza, Soufiane Belharbi, Marco Pedersoli, Eric Granger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10034">A Realistic Protocol for Evaluation of Weakly Supervised Object Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Object Localization (WSOL) allows training deep learning models for classification and localization (LOC) using only global class-level labels. The absence of bounding box (bbox) supervision during training raises challenges in the literature for hyper-parameter tuning, model selection, and evaluation. WSOL methods rely on a validation set with bbox annotations for model selection, and a test set with bbox annotations for threshold estimation for producing bboxes from localization maps. This approach, however, is not aligned with the WSOL setting as these annotations are typically unavailable in real-world scenarios. Our initial empirical analysis shows a significant decline in LOC performance when model selection and threshold estimation rely solely on class labels and the image itself, respectively, compared to using manual bbox annotations. This highlights the importance of incorporating bbox labels for optimal model performance. In this paper, a new WSOL evaluation protocol is proposed that provides LOC information without the need for manual bbox annotations. In particular, we generated noisy pseudo-boxes from a pretrained off-the-shelf region proposal method such as Selective Search, CLIP, and RPN for model selection. These bboxes are also employed to estimate the threshold from LOC maps, circumventing the need for test-set bbox annotations. Our experiments with several WSOL methods on ILSVRC and CUB datasets show that using the proposed pseudo-bboxes for validation facilitates the model selection and threshold estimation, with LOC performance comparable to those selected using GT bboxes on the validation set and threshold estimation on the test set. It also outperforms models selected using class-level labels, and then dynamically thresholded based solely on LOC maps.
<div id='section'>Paperid: <span id='pid'>234, <a href='https://arxiv.org/pdf/2509.26281.pdf' target='_blank'>https://arxiv.org/pdf/2509.26281.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Teng Zhang, Ziqian Fan, Mingxin Liu, Xin Zhang, Xudong Lu, Wentong Li, Yue Zhou, Yi Yu, Xiang Li, Junchi Yan, Xue Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26281">Point2RBox-v3: Self-Bootstrapping from Point Annotations via Integrated Pseudo-Label Refinement and Utilization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Driven by the growing need for Oriented Object Detection (OOD), learning from point annotations under a weakly-supervised framework has emerged as a promising alternative to costly and laborious manual labeling. In this paper, we discuss two deficiencies in existing point-supervised methods: inefficient utilization and poor quality of pseudo labels. Therefore, we present Point2RBox-v3. At the core are two principles: 1) Progressive Label Assignment (PLA). It dynamically estimates instance sizes in a coarse yet intelligent manner at different stages of the training process, enabling the use of label assignment methods. 2) Prior-Guided Dynamic Mask Loss (PGDM-Loss). It is an enhancement of the Voronoi Watershed Loss from Point2RBox-v2, which overcomes the shortcomings of Watershed in its poor performance in sparse scenes and SAM's poor performance in dense scenes. To our knowledge, Point2RBox-v3 is the first model to employ dynamic pseudo labels for label assignment, and it creatively complements the advantages of SAM model with the watershed algorithm, which achieves excellent performance in both sparse and dense scenes. Our solution gives competitive performance, especially in scenarios with large variations in object size or sparse object occurrences: 66.09%/56.86%/41.28%/46.40%/19.60%/45.96% on DOTA-v1.0/DOTA-v1.5/DOTA-v2.0/DIOR/STAR/RSAR.
<div id='section'>Paperid: <span id='pid'>235, <a href='https://arxiv.org/pdf/2410.13786.pdf' target='_blank'>https://arxiv.org/pdf/2410.13786.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fengqi Liu, Hexiang Wang, Jingyu Gong, Ran Yi, Qianyu Zhou, Xuequan Lu, Jiangbo Lu, Lizhuang Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13786">Emphasizing Semantic Consistency of Salient Posture for Speech-Driven Gesture Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech-driven gesture generation aims at synthesizing a gesture sequence synchronized with the input speech signal. Previous methods leverage neural networks to directly map a compact audio representation to the gesture sequence, ignoring the semantic association of different modalities and failing to deal with salient gestures. In this paper, we propose a novel speech-driven gesture generation method by emphasizing the semantic consistency of salient posture. Specifically, we first learn a joint manifold space for the individual representation of audio and body pose to exploit the inherent semantic association between two modalities, and propose to enforce semantic consistency via a consistency loss. Furthermore, we emphasize the semantic consistency of salient postures by introducing a weakly-supervised detector to identify salient postures, and reweighting the consistency loss to focus more on learning the correspondence between salient postures and the high-level semantics of speech content. In addition, we propose to extract audio features dedicated to facial expression and body gesture separately, and design separate branches for face and body gesture synthesis. Extensive experimental results demonstrate the superiority of our method over the state-of-the-art approaches.
<div id='section'>Paperid: <span id='pid'>236, <a href='https://arxiv.org/pdf/2402.01922.pdf' target='_blank'>https://arxiv.org/pdf/2402.01922.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Chen, Jindong Wang, Lei Feng, Xiang Li, Yidong Wang, Xing Xie, Masashi Sugiyama, Rita Singh, Bhiksha Raj
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.01922">A General Framework for Learning from Weak Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised learning generally faces challenges in applicability to various scenarios with diverse weak supervision and in scalability due to the complexity of existing algorithms, thereby hindering the practical deployment. This paper introduces a general framework for learning from weak supervision (GLWS) with a novel algorithm. Central to GLWS is an Expectation-Maximization (EM) formulation, adeptly accommodating various weak supervision sources, including instance partial labels, aggregate statistics, pairwise observations, and unlabeled data. We further present an advanced algorithm that significantly simplifies the EM computational demands using a Non-deterministic Finite Automaton (NFA) along with a forward-backward algorithm, which effectively reduces time complexity from quadratic or factorial often required in existing solutions to linear scale. The problem of learning from arbitrary weak supervision is therefore converted to the NFA modeling of them. GLWS not only enhances the scalability of machine learning models but also demonstrates superior performance and versatility across 11 weak supervision scenarios. We hope our work paves the way for further advancements and practical deployment in this field.
<div id='section'>Paperid: <span id='pid'>237, <a href='https://arxiv.org/pdf/2406.14958.pdf' target='_blank'>https://arxiv.org/pdf/2406.14958.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei Chen, Dingkang Yang, Yuxuan Lei, Lihua Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.14958">Skip and Skip: Segmenting Medical Images with Prompts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most medical image lesion segmentation methods rely on hand-crafted accurate annotations of the original image for supervised learning. Recently, a series of weakly supervised or unsupervised methods have been proposed to reduce the dependence on pixel-level annotations. However, these methods are essentially based on pixel-level annotation, ignoring the image-level diagnostic results of the current massive medical images. In this paper, we propose a dual U-shaped two-stage framework that utilizes image-level labels to prompt the segmentation. In the first stage, we pre-train a classification network with image-level labels, which is used to obtain the hierarchical pyramid features and guide the learning of downstream branches. In the second stage, we feed the hierarchical features obtained from the classification branch into the downstream branch through short-skip and long-skip and get the lesion masks under the supervised learning of pixel-level labels. Experiments show that our framework achieves better results than networks simply using pixel-level annotations.
<div id='section'>Paperid: <span id='pid'>238, <a href='https://arxiv.org/pdf/2510.04091.pdf' target='_blank'>https://arxiv.org/pdf/2510.04091.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Wang, Tianhao Ma, Ming-Kun Xie, Gang Niu, Masashi Sugiyama
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04091">Rethinking Consistent Multi-Label Classification under Inexact Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partial multi-label learning and complementary multi-label learning are two popular weakly supervised multi-label classification paradigms that aim to alleviate the high annotation costs of collecting precisely annotated multi-label data. In partial multi-label learning, each instance is annotated with a candidate label set, among which only some labels are relevant; in complementary multi-label learning, each instance is annotated with complementary labels indicating the classes to which the instance does not belong. Existing consistent approaches for the two paradigms either require accurate estimation of the generation process of candidate or complementary labels or assume a uniform distribution to eliminate the estimation problem. However, both conditions are usually difficult to satisfy in real-world scenarios. In this paper, we propose consistent approaches that do not rely on the aforementioned conditions to handle both problems in a unified way. Specifically, we propose two unbiased risk estimators based on first- and second-order strategies. Theoretically, we prove consistency w.r.t. two widely used multi-label classification evaluation metrics and derive convergence rates for the estimation errors of the proposed risk estimators. Empirically, extensive experimental results validate the effectiveness of our proposed approaches against state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>239, <a href='https://arxiv.org/pdf/2509.24228.pdf' target='_blank'>https://arxiv.org/pdf/2509.24228.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Wang, Dong-Dong Wu, Ming Li, Jingxiong Zhang, Gang Niu, Masashi Sugiyama
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24228">Accessible, Realistic, and Fair Evaluation of Positive-Unlabeled Learning Algorithms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Positive-unlabeled (PU) learning is a weakly supervised binary classification problem, in which the goal is to learn a binary classifier from only positive and unlabeled data, without access to negative data. In recent years, many PU learning algorithms have been developed to improve model performance. However, experimental settings are highly inconsistent, making it difficult to identify which algorithm performs better. In this paper, we propose the first PU learning benchmark to systematically compare PU learning algorithms. During our implementation, we identify subtle yet critical factors that affect the realistic and fair evaluation of PU learning algorithms. On the one hand, many PU learning algorithms rely on a validation set that includes negative data for model selection. This is unrealistic in traditional PU learning settings, where no negative data are available. To handle this problem, we systematically investigate model selection criteria for PU learning. On the other hand, the problem settings and solutions of PU learning have different families, i.e., the one-sample and two-sample settings. However, existing evaluation protocols are heavily biased towards the one-sample setting and neglect the significant difference between them. We identify the internal label shift problem of unlabeled training data for the one-sample setting and propose a simple yet effective calibration approach to ensure fair comparisons within and across families. We hope our framework will provide an accessible, realistic, and fair environment for evaluating PU learning algorithms in the future.
<div id='section'>Paperid: <span id='pid'>240, <a href='https://arxiv.org/pdf/2502.10184.pdf' target='_blank'>https://arxiv.org/pdf/2502.10184.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Wang, Dong-Dong Wu, Jindong Wang, Gang Niu, Min-Ling Zhang, Masashi Sugiyama
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10184">Realistic Evaluation of Deep Partial-Label Learning Algorithms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partial-label learning (PLL) is a weakly supervised learning problem in which each example is associated with multiple candidate labels and only one is the true label. In recent years, many deep PLL algorithms have been developed to improve model performance. However, we find that some early developed algorithms are often underestimated and can outperform many later algorithms with complicated designs. In this paper, we delve into the empirical perspective of PLL and identify several critical but previously overlooked issues. First, model selection for PLL is non-trivial, but has never been systematically studied. Second, the experimental settings are highly inconsistent, making it difficult to evaluate the effectiveness of the algorithms. Third, there is a lack of real-world image datasets that can be compatible with modern network architectures. Based on these findings, we propose PLENCH, the first Partial-Label learning bENCHmark to systematically compare state-of-the-art deep PLL algorithms. We investigate the model selection problem for PLL for the first time, and propose novel model selection criteria with theoretical guarantees. We also create Partial-Label CIFAR-10 (PLCIFAR10), an image dataset of human-annotated partial labels collected from Amazon Mechanical Turk, to provide a testbed for evaluating the performance of PLL algorithms in more realistic scenarios. Researchers can quickly and conveniently perform a comprehensive and fair evaluation and verify the effectiveness of newly developed algorithms based on PLENCH. We hope that PLENCH will facilitate standardized, fair, and practical evaluation of PLL algorithms in the future.
<div id='section'>Paperid: <span id='pid'>241, <a href='https://arxiv.org/pdf/2406.19394.pdf' target='_blank'>https://arxiv.org/pdf/2406.19394.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liujuan Cao, Jianghang Lin, Zebo Hong, Yunhang Shen, Shaohui Lin, Chao Chen, Rongrong Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.19394">HUWSOD: Holistic Self-training for Unified Weakly Supervised Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most WSOD methods rely on traditional object proposals to generate candidate regions and are confronted with unstable training, which easily gets stuck in a poor local optimum. In this paper, we introduce a unified, high-capacity weakly supervised object detection (WSOD) network called HUWSOD, which utilizes a comprehensive self-training framework without needing external modules or additional supervision. HUWSOD innovatively incorporates a self-supervised proposal generator and an autoencoder proposal generator with a multi-rate resampling pyramid to replace traditional object proposals, enabling end-to-end WSOD training and inference. Additionally, we implement a holistic self-training scheme that refines detection scores and coordinates through step-wise entropy minimization and consistency-constraint regularization, ensuring consistent predictions across stochastic augmentations of the same image. Extensive experiments on PASCAL VOC and MS COCO demonstrate that HUWSOD competes with state-of-the-art WSOD methods, eliminating the need for offline proposals and additional data. The peak performance of HUWSOD approaches that of fully-supervised Faster R-CNN. Our findings also indicate that randomly initialized boxes, although significantly different from well-designed offline object proposals, are effective for WSOD training.
<div id='section'>Paperid: <span id='pid'>242, <a href='https://arxiv.org/pdf/2512.02224.pdf' target='_blank'>https://arxiv.org/pdf/2512.02224.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Feng, Tianhao Peng, Fan Zhang, David Bull
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.02224">Towards Unified Video Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent works in video quality assessment (VQA) typically employ monolithic models that typically predict a single quality score for each test video. These approaches cannot provide diagnostic, interpretable feedback, offering little insight into why the video quality is degraded. Most of them are also specialized, format-specific metrics rather than truly ``generic" solutions, as they are designed to learn a compromised representation from disparate perceptual domains. To address these limitations, this paper proposes Unified-VQA, a framework that provides a single, unified quality model applicable to various distortion types within multiple video formats by recasting generic VQA as a Diagnostic Mixture-of-Experts (MoE) problem. Unified-VQA employs multiple ``perceptual experts'' dedicated to distinct perceptual domains. A novel multi-proxy expert training strategy is designed to optimize each expert using a ranking-inspired loss, guided by the most suitable proxy metric for its domain. We also integrated a diagnostic multi-task head into this framework to generate a global quality score and an interpretable multi-dimensional artifact vector, which is optimized using a weakly-supervised learning strategy, leveraging the known properties of the large-scale training database generated for this work. With static model parameters (without retraining or fine-tuning), Unified-VQA demonstrates consistent and superior performance compared to over 18 benchmark methods for both generic VQA and diagnostic artifact detection tasks across 17 databases containing diverse streaming artifacts in HD, UHD, HDR and HFR formats. This work represents an important step towards practical, actionable, and interpretable video quality assessment.
<div id='section'>Paperid: <span id='pid'>243, <a href='https://arxiv.org/pdf/2505.04339.pdf' target='_blank'>https://arxiv.org/pdf/2505.04339.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Peng, Xiang Huang, Shuo Sun, Ruitong Zhang, Philip S. Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04339">Adaptive and Robust DBSCAN with Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>DBSCAN, a well-known density-based clustering algorithm, has gained widespread popularity and usage due to its effectiveness in identifying clusters of arbitrary shapes and handling noisy data. However, it encounters challenges in producing satisfactory cluster results when confronted with datasets of varying density scales, a common scenario in real-world applications. In this paper, we propose a novel Adaptive and Robust DBSCAN with Multi-agent Reinforcement Learning cluster framework, namely AR-DBSCAN. First, we model the initial dataset as a two-level encoding tree and categorize the data vertices into distinct density partitions according to the information uncertainty determined in the encoding tree. Each partition is then assigned to an agent to find the best clustering parameters without manual assistance. The allocation is density-adaptive, enabling AR-DBSCAN to effectively handle diverse density distributions within the dataset by utilizing distinct agents for different partitions. Second, a multi-agent deep reinforcement learning guided automatic parameter searching process is designed. The process of adjusting the parameter search direction by perceiving the clustering environment is modeled as a Markov decision process. Using a weakly-supervised reward training policy network, each agent adaptively learns the optimal clustering parameters by interacting with the clusters. Third, a recursive search mechanism adaptable to the data's scale is presented, enabling efficient and controlled exploration of large parameter spaces. Extensive experiments are conducted on nine artificial datasets and a real-world dataset. The results of offline and online tasks show that AR-DBSCAN not only improves clustering accuracy by up to 144.1% and 175.3% in the NMI and ARI metrics, respectively, but also is capable of robustly finding dominant parameters.
<div id='section'>Paperid: <span id='pid'>244, <a href='https://arxiv.org/pdf/2504.00844.pdf' target='_blank'>https://arxiv.org/pdf/2504.00844.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdelrahman Elskhawy, Mengze Li, Nassir Navab, Benjamin Busam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.00844">PRISM-0: A Predicate-Rich Scene Graph Generation Framework for Zero-Shot Open-Vocabulary Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In Scene Graph Generation (SGG), structured representations are extracted from visual inputs as object nodes and connecting predicates, enabling image-based reasoning for diverse downstream tasks. While fully supervised SGG has improved steadily, it suffers from training bias due to limited curated data and long-tail predicate distributions, leading to poor predicate diversity and degraded downstream performance. We present PRISM-0, a zero-shot open-vocabulary SGG framework that leverages foundation models in a bottom-up pipeline to capture a broad spectrum of predicates. Detected object pairs are filtered, described via a Vision-Language Model (VLM), and processed by a Large Language Model (LLM) to generate fine- and coarse-grained predicates, which are then validated by a Visual Question Answering (VQA) model. PRISM-0 modular, dataset-independent design enriches existing SGG datasets such as Visual Genome and produces diverse, unbiased graphs. While operating entirely in a zero-shot setting, PRISM-0 achieves performance on par with state-of-the-art weakly-supervised models on SGG benchmarks and even state-of-the-art supervised methods in tasks such as Sentence-to-Graph Retrieval.
<div id='section'>Paperid: <span id='pid'>245, <a href='https://arxiv.org/pdf/2405.11868.pdf' target='_blank'>https://arxiv.org/pdf/2405.11868.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Ju, Yifan Wang, Yifang Qin, Zhengyang Mao, Zhiping Xiao, Junyu Luo, Junwei Yang, Yiyang Gu, Dongjie Wang, Qingqing Long, Siyu Yi, Xiao Luo, Ming Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.11868">Towards Graph Contrastive Learning: A Survey and Beyond</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, deep learning on graphs has achieved remarkable success in various domains. However, the reliance on annotated graph data remains a significant bottleneck due to its prohibitive cost and time-intensive nature. To address this challenge, self-supervised learning (SSL) on graphs has gained increasing attention and has made significant progress. SSL enables machine learning models to produce informative representations from unlabeled graph data, reducing the reliance on expensive labeled data. While SSL on graphs has witnessed widespread adoption, one critical component, Graph Contrastive Learning (GCL), has not been thoroughly investigated in the existing literature. Thus, this survey aims to fill this gap by offering a dedicated survey on GCL. We provide a comprehensive overview of the fundamental principles of GCL, including data augmentation strategies, contrastive modes, and contrastive optimization objectives. Furthermore, we explore the extensions of GCL to other aspects of data-efficient graph learning, such as weakly supervised learning, transfer learning, and related scenarios. We also discuss practical applications spanning domains such as drug discovery, genomics analysis, recommender systems, and finally outline the challenges and potential future directions in this field.
<div id='section'>Paperid: <span id='pid'>246, <a href='https://arxiv.org/pdf/2505.20106.pdf' target='_blank'>https://arxiv.org/pdf/2505.20106.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zuyao Chen, Jinlin Wu, Zhen Lei, Chang Wen Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20106">From Data to Modeling: Fully Open-vocabulary Scene Graph Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present OvSGTR, a novel transformer-based framework for fully open-vocabulary scene graph generation that overcomes the limitations of traditional closed-set models. Conventional methods restrict both object and relationship recognition to a fixed vocabulary, hindering their applicability to real-world scenarios where novel concepts frequently emerge. In contrast, our approach jointly predicts objects (nodes) and their inter-relationships (edges) beyond predefined categories. OvSGTR leverages a DETR-like architecture featuring a frozen image backbone and text encoder to extract high-quality visual and semantic features, which are then fused via a transformer decoder for end-to-end scene graph prediction. To enrich the model's understanding of complex visual relations, we propose a relation-aware pre-training strategy that synthesizes scene graph annotations in a weakly supervised manner. Specifically, we investigate three pipelines--scene parser-based, LLM-based, and multimodal LLM-based--to generate transferable supervision signals with minimal manual annotation. Furthermore, we address the common issue of catastrophic forgetting in open-vocabulary settings by incorporating a visual-concept retention mechanism coupled with a knowledge distillation strategy, ensuring that the model retains rich semantic cues during fine-tuning. Extensive experiments on the VG150 benchmark demonstrate that OvSGTR achieves state-of-the-art performance across multiple settings, including closed-set, open-vocabulary object detection-based, relation-based, and fully open-vocabulary scenarios. Our results highlight the promise of large-scale relation-aware pre-training and transformer architectures for advancing scene graph generation towards more generalized and reliable visual understanding.
<div id='section'>Paperid: <span id='pid'>247, <a href='https://arxiv.org/pdf/2406.00919.pdf' target='_blank'>https://arxiv.org/pdf/2406.00919.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinxing Zhou, Dan Guo, Yiran Zhong, Meng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00919">Advancing Weakly-Supervised Audio-Visual Video Parsing via Segment-wise Pseudo Labeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Audio-Visual Video Parsing task aims to identify and temporally localize the events that occur in either or both the audio and visual streams of audible videos. It often performs in a weakly-supervised manner, where only video event labels are provided, \ie, the modalities and the timestamps of the labels are unknown. Due to the lack of densely annotated labels, recent work attempts to leverage pseudo labels to enrich the supervision. A commonly used strategy is to generate pseudo labels by categorizing the known video event labels for each modality. However, the labels are still confined to the video level, and the temporal boundaries of events remain unlabeled. In this paper, we propose a new pseudo label generation strategy that can explicitly assign labels to each video segment by utilizing prior knowledge learned from the open world. Specifically, we exploit the large-scale pretrained models, namely CLIP and CLAP, to estimate the events in each video segment and generate segment-level visual and audio pseudo labels, respectively. We then propose a new loss function to exploit these pseudo labels by taking into account their category-richness and segment-richness. A label denoising strategy is also adopted to further improve the visual pseudo labels by flipping them whenever abnormally large forward losses occur. We perform extensive experiments on the LLP dataset and demonstrate the effectiveness of each proposed design and we achieve state-of-the-art video parsing performance on all types of event parsing, \ie, audio event, visual event, and audio-visual event. We also examine the proposed pseudo label generation strategy on a relevant weakly-supervised audio-visual event localization task and the experimental results again verify the benefits and generalization of our method.
<div id='section'>Paperid: <span id='pid'>248, <a href='https://arxiv.org/pdf/2411.16803.pdf' target='_blank'>https://arxiv.org/pdf/2411.16803.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marta Ligero, Tim Lenz, Georg WÃ¶lflein, Omar S. M. El Nahhas, Daniel Truhn, Jakob Nikolas Kather
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16803">Abnormality-Driven Representation Learning for Radiology Imaging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To date, the most common approach for radiology deep learning pipelines is the use of end-to-end 3D networks based on models pre-trained on other tasks, followed by fine-tuning on the task at hand. In contrast, adjacent medical fields such as pathology, which focus on 2D images, have effectively adopted task-agnostic foundational models based on self-supervised learning (SSL), combined with weakly-supervised deep learning (DL). However, the field of radiology still lacks task-agnostic representation models due to the computational and data demands of 3D imaging and the anatomical complexity inherent to radiology scans. To address this gap, we propose CLEAR, a framework for radiology images that uses extracted embeddings from 2D slices along with attention-based aggregation for efficiently predicting clinical endpoints. As part of this framework, we introduce lesion-enhanced contrastive learning (LeCL), a novel approach to obtain visual representations driven by abnormalities in 2D axial slices across different locations of the CT scans. Specifically, we trained single-domain contrastive learning approaches using three different architectures: Vision Transformers, Vision State Space Models and Gated Convolutional Neural Networks. We evaluate our approach across three clinical tasks: tumor lesion location, lung disease detection, and patient staging, benchmarking against four state-of-the-art foundation models, including BiomedCLIP. Our findings demonstrate that CLEAR using representations learned through LeCL, outperforms existing foundation models, while being substantially more compute- and data-efficient.
<div id='section'>Paperid: <span id='pid'>249, <a href='https://arxiv.org/pdf/2408.15823.pdf' target='_blank'>https://arxiv.org/pdf/2408.15823.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peter Neidlinger, Omar S. M. El Nahhas, Hannah Sophie Muti, Tim Lenz, Michael Hoffmeister, Hermann Brenner, Marko van Treeck, Rupert Langer, Bastian Dislich, Hans Michael Behrens, Christoph RÃ¶cken, Sebastian Foersch, Daniel Truhn, Antonio Marra, Oliver Lester Saldanha, Jakob Nikolas Kather
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.15823">Benchmarking foundation models as feature extractors for weakly-supervised computational pathology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advancements in artificial intelligence have driven the development of numerous pathology foundation models capable of extracting clinically relevant information. However, there is currently limited literature independently evaluating these foundation models on truly external cohorts and clinically-relevant tasks to uncover adjustments for future improvements. In this study, we benchmarked 19 histopathology foundation models on 13 patient cohorts with 6,818 patients and 9,528 slides from lung, colorectal, gastric, and breast cancers. The models were evaluated on weakly-supervised tasks related to biomarkers, morphological properties, and prognostic outcomes. We show that a vision-language foundation model, CONCH, yielded the highest performance when compared to vision-only foundation models, with Virchow2 as close second. The experiments reveal that foundation models trained on distinct cohorts learn complementary features to predict the same label, and can be fused to outperform the current state of the art. An ensemble combining CONCH and Virchow2 predictions outperformed individual models in 55% of tasks, leveraging their complementary strengths in classification scenarios. Moreover, our findings suggest that data diversity outweighs data volume for foundation models. Our work highlights actionable adjustments to improve pathology foundation models.
<div id='section'>Paperid: <span id='pid'>250, <a href='https://arxiv.org/pdf/2403.03891.pdf' target='_blank'>https://arxiv.org/pdf/2403.03891.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Omar S. M. El Nahhas, Georg WÃ¶lflein, Marta Ligero, Tim Lenz, Marko van Treeck, Firas Khader, Daniel Truhn, Jakob Nikolas Kather
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.03891">Joint multi-task learning improves weakly-supervised biomarker prediction in computational pathology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep Learning (DL) can predict biomarkers directly from digitized cancer histology in a weakly-supervised setting. Recently, the prediction of continuous biomarkers through regression-based DL has seen an increasing interest. Nonetheless, clinical decision making often requires a categorical outcome. Consequently, we developed a weakly-supervised joint multi-task Transformer architecture which has been trained and evaluated on four public patient cohorts for the prediction of two key predictive biomarkers, microsatellite instability (MSI) and homologous recombination deficiency (HRD), trained with auxiliary regression tasks related to the tumor microenvironment. Moreover, we perform a comprehensive benchmark of 16 approaches of task balancing for weakly-supervised joint multi-task learning in computational pathology. Using our novel approach, we improve over the state-of-the-art area under the receiver operating characteristic by +7.7% and +4.1%, as well as yielding better clustering of latent embeddings by +8% and +5% for the prediction of MSI and HRD in external cohorts, respectively.
<div id='section'>Paperid: <span id='pid'>251, <a href='https://arxiv.org/pdf/2503.20685.pdf' target='_blank'>https://arxiv.org/pdf/2503.20685.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhao Huang, Ao Chang, Haoran Dou, Xing Tao, Xinrui Zhou, Yan Cao, Ruobing Huang, Alejandro F Frangi, Lingyun Bao, Xin Yang, Dong Ni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20685">Flip Learning: Weakly Supervised Erase to Segment Nodules in Breast Ultrasound</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate segmentation of nodules in both 2D breast ultrasound (BUS) and 3D automated breast ultrasound (ABUS) is crucial for clinical diagnosis and treatment planning. Therefore, developing an automated system for nodule segmentation can enhance user independence and expedite clinical analysis. Unlike fully-supervised learning, weakly-supervised segmentation (WSS) can streamline the laborious and intricate annotation process. However, current WSS methods face challenges in achieving precise nodule segmentation, as many of them depend on inaccurate activation maps or inefficient pseudo-mask generation algorithms. In this study, we introduce a novel multi-agent reinforcement learning-based WSS framework called Flip Learning, which relies solely on 2D/3D boxes for accurate segmentation. Specifically, multiple agents are employed to erase the target from the box to facilitate classification tag flipping, with the erased region serving as the predicted segmentation mask. The key contributions of this research are as follows: (1) Adoption of a superpixel/supervoxel-based approach to encode the standardized environment, capturing boundary priors and expediting the learning process. (2) Introduction of three meticulously designed rewards, comprising a classification score reward and two intensity distribution rewards, to steer the agents' erasing process precisely, thereby avoiding both under- and over-segmentation. (3) Implementation of a progressive curriculum learning strategy to enable agents to interact with the environment in a progressively challenging manner, thereby enhancing learning efficiency. Extensively validated on the large in-house BUS and ABUS datasets, our Flip Learning method outperforms state-of-the-art WSS methods and foundation models, and achieves comparable performance as fully-supervised learning algorithms.
<div id='section'>Paperid: <span id='pid'>252, <a href='https://arxiv.org/pdf/2409.11223.pdf' target='_blank'>https://arxiv.org/pdf/2409.11223.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuta Kaneko, Abu Saleh Musa Miah, Najmul Hassan, Hyoun-Sup Lee, Si-Woong Jang, Jungpil Shin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11223">Multimodal Attention-Enhanced Feature Fusion-based Weekly Supervised Anomaly Violence Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised video anomaly detection (WS-VAD) is a crucial area in computer vision for developing intelligent surveillance systems. This system uses three feature streams: RGB video, optical flow, and audio signals, where each stream extracts complementary spatial and temporal features using an enhanced attention module to improve detection accuracy and robustness. In the first stream, we employed an attention-based, multi-stage feature enhancement approach to improve spatial and temporal features from the RGB video where the first stage consists of a ViT-based CLIP module, with top-k features concatenated in parallel with I3D and Temporal Contextual Aggregation (TCA) based rich spatiotemporal features. The second stage effectively captures temporal dependencies using the Uncertainty-Regulated Dual Memory Units (UR-DMU) model, which learns representations of normal and abnormal data simultaneously, and the third stage is employed to select the most relevant spatiotemporal features. The second stream extracted enhanced attention-based spatiotemporal features from the flow data modality-based feature by taking advantage of the integration of the deep learning and attention module. The audio stream captures auditory cues using an attention module integrated with the VGGish model, aiming to detect anomalies based on sound patterns. These streams enrich the model by incorporating motion and audio signals often indicative of abnormal events undetectable through visual analysis alone. The concatenation of the multimodal fusion leverages the strengths of each modality, resulting in a comprehensive feature set that significantly improves anomaly detection accuracy and robustness across three datasets. The extensive experiment and high performance with the three benchmark datasets proved the effectiveness of the proposed system over the existing state-of-the-art system.
<div id='section'>Paperid: <span id='pid'>253, <a href='https://arxiv.org/pdf/2510.10111.pdf' target='_blank'>https://arxiv.org/pdf/2510.10111.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Chen, Bin Liu, Changtao Miao, Xinghao Wang, Yi Li, Tao Gong, Qi Chu, Nenghai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10111">Training-Free In-Context Forensic Chain for Image Manipulation Detection and Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advances in image tampering pose serious security threats, underscoring the need for effective image manipulation localization (IML). While supervised IML achieves strong performance, it depends on costly pixel-level annotations. Existing weakly supervised or training-free alternatives often underperform and lack interpretability. We propose the In-Context Forensic Chain (ICFC), a training-free framework that leverages multi-modal large language models (MLLMs) for interpretable IML tasks. ICFC integrates an objectified rule construction with adaptive filtering to build a reliable knowledge base and a multi-step progressive reasoning pipeline that mirrors expert forensic workflows from coarse proposals to fine-grained forensics results. This design enables systematic exploitation of MLLM reasoning for image-level classification, pixel-level localization, and text-level interpretability. Across multiple benchmarks, ICFC not only surpasses state-of-the-art training-free methods but also achieves competitive or superior performance compared to weakly and fully supervised approaches.
<div id='section'>Paperid: <span id='pid'>254, <a href='https://arxiv.org/pdf/2509.26004.pdf' target='_blank'>https://arxiv.org/pdf/2509.26004.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicola Messina, Rosario Leonardi, Luca Ciampi, Fabio Carrara, Giovanni Maria Farinella, Fabrizio Falchi, Antonino Furnari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26004">Learning Egocentric In-Hand Object Segmentation through Weak Supervision from Human Narrations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pixel-level recognition of objects manipulated by the user from egocentric images enables key applications spanning assistive technologies, industrial safety, and activity monitoring. However, progress in this area is currently hindered by the scarcity of annotated datasets, as existing approaches rely on costly manual labels. In this paper, we propose to learn human-object interaction detection leveraging narrations -- natural language descriptions of the actions performed by the camera wearer which contain clues about manipulated objects (e.g., "I am pouring vegetables from the chopping board to the pan"). Narrations provide a form of weak supervision that is cheap to acquire and readily available in state-of-the-art egocentric datasets. We introduce Narration-Supervised in-Hand Object Segmentation (NS-iHOS), a novel task where models have to learn to segment in-hand objects by learning from natural-language narrations. Narrations are then not employed at inference time. We showcase the potential of the task by proposing Weakly-Supervised In-hand Object Segmentation from Human Narrations (WISH), an end-to-end model distilling knowledge from narrations to learn plausible hand-object associations and enable in-hand object segmentation without using narrations at test time. We benchmark WISH against different baselines based on open-vocabulary object detectors and vision-language models, showing the superiority of its design. Experiments on EPIC-Kitchens and Ego4D show that WISH surpasses all baselines, recovering more than 50% of the performance of fully supervised methods, without employing fine-grained pixel-wise annotations.
<div id='section'>Paperid: <span id='pid'>255, <a href='https://arxiv.org/pdf/2503.20294.pdf' target='_blank'>https://arxiv.org/pdf/2503.20294.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinghao Wang, Tao Gong, Qi Chu, Bin Liu, Nenghai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20294">Context-Aware Weakly Supervised Image Manipulation Localization with SAM Refinement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Malicious image manipulation poses societal risks, increasing the importance of effective image manipulation detection methods. Recent approaches in image manipulation detection have largely been driven by fully supervised approaches, which require labor-intensive pixel-level annotations. Thus, it is essential to explore weakly supervised image manipulation localization methods that only require image-level binary labels for training. However, existing weakly supervised image manipulation methods overlook the importance of edge information for accurate localization, leading to suboptimal localization performance. To address this, we propose a Context-Aware Boundary Localization (CABL) module to aggregate boundary features and learn context-inconsistency for localizing manipulated areas. Furthermore, by leveraging Class Activation Mapping (CAM) and Segment Anything Model (SAM), we introduce the CAM-Guided SAM Refinement (CGSR) module to generate more accurate manipulation localization maps. By integrating two modules, we present a novel weakly supervised framework based on a dual-branch Transformer-CNN architecture. Our method achieves outstanding localization performance across multiple datasets.
<div id='section'>Paperid: <span id='pid'>256, <a href='https://arxiv.org/pdf/2502.09967.pdf' target='_blank'>https://arxiv.org/pdf/2502.09967.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuming Wang, Yihao Zheng, Jiarui Li, Yaofei Wu, Yan Huang, Zun Li, Lifang Wu, Liang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09967">VicKAM: Visual Conceptual Knowledge Guided Action Map for Weakly Supervised Group Activity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing weakly supervised group activity recognition methods rely on object detectors or attention mechanisms to capture key areas automatically. However, they overlook the semantic information associated with captured areas, which may adversely affect the recognition performance. In this paper, we propose a novel framework named Visual Conceptual Knowledge Guided Action Map (VicKAM) which effectively captures the locations of individual actions and integrates them with action semantics for weakly supervised group activity recognition.It generates individual action prototypes from training set as visual conceptual knowledge to bridge action semantics and visual representations. Guided by this knowledge, VicKAM produces action maps that indicate the likelihood of each action occurring at various locations, based on image correlation theorem. It further augments individual action maps using group activity related statistical information, representing individual action distribution under different group activities, to establish connections between action maps and specific group activities. The augmented action map is incorporated with action semantic representations for group activity recognition.Extensive experiments on two public benchmarks, the Volleyball and the NBA datasets, demonstrate the effectiveness of our proposed method, even in cases of limited training data. The code will be released later.
<div id='section'>Paperid: <span id='pid'>257, <a href='https://arxiv.org/pdf/2401.07854.pdf' target='_blank'>https://arxiv.org/pdf/2401.07854.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Quan Liu, Jiawen Yao, Lisha Yao, Xin Chen, Jingren Zhou, Le Lu, Ling Zhang, Zaiyi Liu, Yuankai Huo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.07854">$M^{2}$Fusion: Bayesian-based Multimodal Multi-level Fusion on Colorectal Cancer Microsatellite Instability Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Colorectal cancer (CRC) micro-satellite instability (MSI) prediction on histopathology images is a challenging weakly supervised learning task that involves multi-instance learning on gigapixel images. To date, radiology images have proven to have CRC MSI information and efficient patient imaging techniques. Different data modalities integration offers the opportunity to increase the accuracy and robustness of MSI prediction. Despite the progress in representation learning from the whole slide images (WSI) and exploring the potential of making use of radiology data, CRC MSI prediction remains a challenge to fuse the information from multiple data modalities (e.g., pathology WSI and radiology CT image). In this paper, we propose $M^{2}$Fusion: a Bayesian-based multimodal multi-level fusion pipeline for CRC MSI. The proposed fusion model $M^{2}$Fusion is capable of discovering more novel patterns within and across modalities that are beneficial for predicting MSI than using a single modality alone, as well as other fusion methods. The contribution of the paper is three-fold: (1) $M^{2}$Fusion is the first pipeline of multi-level fusion on pathology WSI and 3D radiology CT image for MSI prediction; (2) CT images are the first time integrated into multimodal fusion for CRC MSI prediction; (3) feature-level fusion strategy is evaluated on both Transformer-based and CNN-based method. Our approach is validated on cross-validation of 352 cases and outperforms either feature-level (0.8177 vs. 0.7908) or decision-level fusion strategy (0.8177 vs. 0.7289) on AUC score.
<div id='section'>Paperid: <span id='pid'>258, <a href='https://arxiv.org/pdf/2407.16182.pdf' target='_blank'>https://arxiv.org/pdf/2407.16182.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Chen, Fanman Meng, Chenhao Wu, Haoran Wei, Runtong Zhang, Qingbo Wu, Linfeng Xu, Hongliang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.16182">No Re-Train, More Gain: Upgrading Backbones with Diffusion model for Pixel-Wise and Weakly-Supervised Few-Shot Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Few-Shot Segmentation (FSS) aims to segment novel classes using only a few annotated images. Despite considerable progress under pixel-wise support annotation, current FSS methods still face three issues: the inflexibility of backbone upgrade without re-training, the inability to uniformly handle various types of annotations (e.g., scribble, bounding box, mask, and text), and the difficulty in accommodating different annotation quantity. To address these issues simultaneously, we propose DiffUp, a novel framework that conceptualizes the FSS task as a conditional generative problem using a diffusion process. For the first issue, we introduce a backbone-agnostic feature transformation module that converts different segmentation cues into unified coarse priors, facilitating seamless backbone upgrade without re-training. For the second issue, due to the varying granularity of transformed priors from diverse annotation types (scribble, bounding box, mask, and text), we conceptualize these multi-granular transformed priors as analogous to noisy intermediates at different steps of a diffusion model. This is implemented via a self-conditioned modulation block coupled with a dual-level quality modulation branch. For the third issue, we incorporate an uncertainty-aware information fusion module to harmonize the variability across zero-shot, one-shot, and many-shot scenarios. Evaluated through rigorous benchmarks, DiffUp significantly outperforms existing FSS models in terms of flexibility and accuracy.
<div id='section'>Paperid: <span id='pid'>259, <a href='https://arxiv.org/pdf/2401.00368.pdf' target='_blank'>https://arxiv.org/pdf/2401.00368.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.00368">Improving Text Embeddings with Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce a novel and simple method for obtaining high-quality text embeddings using only synthetic data and less than 1k training steps. Unlike existing methods that often depend on multi-stage intermediate pre-training with billions of weakly-supervised text pairs, followed by fine-tuning with a few labeled datasets, our method does not require building complex training pipelines or relying on manually collected datasets that are often constrained by task diversity and language coverage. We leverage proprietary LLMs to generate diverse synthetic data for hundreds of thousands of text embedding tasks across 93 languages. We then fine-tune open-source decoder-only LLMs on the synthetic data using standard contrastive loss. Experiments demonstrate that our method achieves strong performance on highly competitive text embedding benchmarks without using any labeled data. Furthermore, when fine-tuned with a mixture of synthetic and labeled data, our model sets new state-of-the-art results on the BEIR and MTEB benchmarks.
<div id='section'>Paperid: <span id='pid'>260, <a href='https://arxiv.org/pdf/2510.14532.pdf' target='_blank'>https://arxiv.org/pdf/2510.14532.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinrui Huang, Fan Xiao, Dongming He, Anqi Gao, Dandan Li, Xiaofan Zhang, Shaoting Zhang, Xudong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.14532">Towards Generalist Intelligence in Dentistry: Vision Foundation Models for Oral and Maxillofacial Radiology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Oral and maxillofacial radiology plays a vital role in dental healthcare, but radiographic image interpretation is limited by a shortage of trained professionals. While AI approaches have shown promise, existing dental AI systems are restricted by their single-modality focus, task-specific design, and reliance on costly labeled data, hindering their generalization across diverse clinical scenarios. To address these challenges, we introduce DentVFM, the first family of vision foundation models (VFMs) designed for dentistry. DentVFM generates task-agnostic visual representations for a wide range of dental applications and uses self-supervised learning on DentVista, a large curated dental imaging dataset with approximately 1.6 million multi-modal radiographic images from various medical centers. DentVFM includes 2D and 3D variants based on the Vision Transformer (ViT) architecture. To address gaps in dental intelligence assessment and benchmarks, we introduce DentBench, a comprehensive benchmark covering eight dental subspecialties, more diseases, imaging modalities, and a wide geographical distribution. DentVFM shows impressive generalist intelligence, demonstrating robust generalization to diverse dental tasks, such as disease diagnosis, treatment analysis, biomarker identification, and anatomical landmark detection and segmentation. Experimental results indicate DentVFM significantly outperforms supervised, self-supervised, and weakly supervised baselines, offering superior generalization, label efficiency, and scalability. Additionally, DentVFM enables cross-modality diagnostics, providing more reliable results than experienced dentists in situations where conventional imaging is unavailable. DentVFM sets a new paradigm for dental AI, offering a scalable, adaptable, and label-efficient model to improve intelligent dental healthcare and address critical gaps in global oral healthcare.
<div id='section'>Paperid: <span id='pid'>261, <a href='https://arxiv.org/pdf/2405.14230.pdf' target='_blank'>https://arxiv.org/pdf/2405.14230.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangyu Guo, Jiawen Yao, Yingda Xia, Tony C. W. Mok, Zhilin Zheng, Junwei Han, Le Lu, Dingwen Zhang, Jian Zhou, Ling Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14230">Boosting Medical Image-based Cancer Detection via Text-guided Supervision from Reports</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The absence of adequately sufficient expert-level tumor annotations hinders the effectiveness of supervised learning based opportunistic cancer screening on medical imaging. Clinical reports (that are rich in descriptive textual details) can offer a "free lunch'' supervision information and provide tumor location as a type of weak label to cope with screening tasks, thus saving human labeling workloads, if properly leveraged. However, predicting cancer only using such weak labels can be very changeling since tumors are usually presented in small anatomical regions compared to the whole 3D medical scans. Weakly semi-supervised learning (WSSL) utilizes a limited set of voxel-level tumor annotations and incorporates alongside a substantial number of medical images that have only off-the-shelf clinical reports, which may strike a good balance between minimizing expert annotation workload and optimizing screening efficacy. In this paper, we propose a novel text-guided learning method to achieve highly accurate cancer detection results. Through integrating diagnostic and tumor location text prompts into the text encoder of a vision-language model (VLM), optimization of weakly supervised learning can be effectively performed in the latent space of VLM, thereby enhancing the stability of training. Our approach can leverage clinical knowledge by large-scale pre-trained VLM to enhance generalization ability, and produce reliable pseudo tumor masks to improve cancer detection. Our extensive quantitative experimental results on a large-scale cancer dataset, including 1,651 unique patients, validate that our approach can reduce human annotation efforts by at least 70% while maintaining comparable cancer detection accuracy to competing fully supervised methods (AUC value 0.961 versus 0.966).
<div id='section'>Paperid: <span id='pid'>262, <a href='https://arxiv.org/pdf/2501.13584.pdf' target='_blank'>https://arxiv.org/pdf/2501.13584.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Wang, Mingxuan Xia, Chang Yao, Lei Feng, Junbo Zhao, Gang Chen, Haobo Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13584">Towards Robust Incremental Learning under Ambiguous Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional Incremental Learning (IL) targets to handle sequential fully-supervised learning problems where novel classes emerge from time to time. However, due to inherent annotation uncertainty and ambiguity, collecting high-quality annotated data in a dynamic learning system can be extremely expensive. To mitigate this problem, we propose a novel weakly-supervised learning paradigm called Incremental Partial Label Learning (IPLL), where the sequentially arrived data relate to a set of candidate labels rather than the ground truth. Technically, we develop the Prototype-Guided Disambiguation and Replay Algorithm (PGDR) which leverages the class prototypes as a proxy to mitigate two intertwined challenges in IPLL, i.e., label ambiguity and catastrophic forgetting. To handle the former, PGDR encapsulates a momentum-based pseudo-labeling algorithm along with prototype-guided initialization, resulting in a balanced perception of classes. To alleviate forgetting, we develop a memory replay technique that collects well-disambiguated samples while maintaining representativeness and diversity. By jointly distilling knowledge from curated memory data, our framework exhibits a great disambiguation ability for samples of new tasks and achieves less forgetting of knowledge. Extensive experiments demonstrate that PGDR achieves superior
<div id='section'>Paperid: <span id='pid'>263, <a href='https://arxiv.org/pdf/2511.19527.pdf' target='_blank'>https://arxiv.org/pdf/2511.19527.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyu Lyu, Thomas Monninger, Julie Stephany Berrio Perez, Mao Shan, Zhenxing Ming, Stewart Worrall
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19527">MapRF: Weakly Supervised Online HD Map Construction via NeRF-Guided Self-Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous driving systems benefit from high-definition (HD) maps that provide critical information about road infrastructure. The online construction of HD maps offers a scalable approach to generate local maps from on-board sensors. However, existing methods typically rely on costly 3D map annotations for training, which limits their generalization and scalability across diverse driving environments. In this work, we propose MapRF, a weakly supervised framework that learns to construct 3D maps using only 2D image labels. To generate high-quality pseudo labels, we introduce a novel Neural Radiance Fields (NeRF) module conditioned on map predictions, which reconstructs view-consistent 3D geometry and semantics. These pseudo labels are then iteratively used to refine the map network in a self-training manner, enabling progressive improvement without additional supervision. Furthermore, to mitigate error accumulation during self-training, we propose a Map-to-Ray Matching strategy that aligns map predictions with camera rays derived from 2D labels. Extensive experiments on the Argoverse 2 and nuScenes datasets demonstrate that MapRF achieves performance comparable to fully supervised methods, attaining around 75% of the baseline while surpassing several approaches using only 2D labels. This highlights the potential of MapRF to enable scalable and cost-effective online HD map construction for autonomous driving.
<div id='section'>Paperid: <span id='pid'>264, <a href='https://arxiv.org/pdf/2509.11312.pdf' target='_blank'>https://arxiv.org/pdf/2509.11312.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenchao Gu, Yupan Chen, Yanlin Wang, Hongyu Zhang, Cuiyun Gao, Michael R. Lyu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11312">Weakly Supervised Vulnerability Localization via Multiple Instance Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Software vulnerability detection has emerged as a significant concern in the field of software security recently, capturing the attention of numerous researchers and developers. Most previous approaches focus on coarse-grained vulnerability detection, such as at the function or file level. However, the developers would still encounter the challenge of manually inspecting a large volume of code inside the vulnerable function to identify the specific vulnerable statements for modification, indicating the importance of vulnerability localization. Training the model for vulnerability localization usually requires ground-truth labels at the statement-level, and labeling vulnerable statements demands expert knowledge, which incurs high costs. Hence, the demand for an approach that eliminates the need for additional labeling at the statement-level is on the rise. To tackle this problem, we propose a novel approach called WAVES for WeAkly supervised Vulnerability Localization via multiplE inStance learning, which does not need the additional statement-level labels during the training. WAVES has the capability to determine whether a function is vulnerable (i.e., vulnerability detection) and pinpoint the vulnerable statements (i.e., vulnerability localization). Specifically, inspired by the concept of multiple instance learning, WAVES converts the ground-truth label at the function-level into pseudo labels for individual statements, eliminating the need for additional statement-level labeling. These pseudo labels are utilized to train the classifiers for the function-level representation vectors. Extensive experimentation on three popular benchmark datasets demonstrates that, in comparison to previous baselines, our approach achieves comparable performance in vulnerability detection and state-of-the-art performance in statement-level vulnerability localization.
<div id='section'>Paperid: <span id='pid'>265, <a href='https://arxiv.org/pdf/2412.19650.pdf' target='_blank'>https://arxiv.org/pdf/2412.19650.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongxing Xu, Feilong Tang, Zhe Chen, Yingxue Su, Zhiyi Zhao, Ge Zhang, Jionglong Su, Zongyuan Ge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19650">Toward Modality Gap: Vision Prototype Learning for Weakly-supervised Semantic Segmentation with CLIP</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The application of Contrastive Language-Image Pre-training (CLIP) in Weakly Supervised Semantic Segmentation (WSSS) research powerful cross-modal semantic understanding capabilities. Existing methods attempt to optimize input text prompts for improved alignment of images and text, by finely adjusting text prototypes to facilitate semantic matching. Nevertheless, given the modality gap between text and vision spaces, the text prototypes employed by these methods have not effectively established a close correspondence with pixel-level vision features. In this work, our theoretical analysis indicates that the inherent modality gap results in misalignment of text and region features, and that this gap cannot be sufficiently reduced by minimizing contrast loss in CLIP. To mitigate the impact of the modality gap, we propose a Vision Prototype Learning (VPL) framework, by introducing more representative vision prototypes. The core of this framework is to learn class-specific vision prototypes in vision space with the help of text prototypes, for capturing high-quality localization maps. Moreover, we propose a regional semantic contrast module that contrasts regions embedding with corresponding prototypes, leading to more comprehensive and robust feature learning. Experimental results show that our proposed framework achieves state-of-the-art performance on two benchmark datasets.
<div id='section'>Paperid: <span id='pid'>266, <a href='https://arxiv.org/pdf/2404.11981.pdf' target='_blank'>https://arxiv.org/pdf/2404.11981.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chongjie Si, Xuehui Wang, Xiaokang Yang, Wei Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.11981">Tendency-driven Mutual Exclusivity for Weakly Supervised Incremental Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Incremental Learning for Semantic Segmentation (WILSS) leverages a pre-trained segmentation model to segment new classes using cost-effective and readily available image-level labels. A prevailing way to solve WILSS is the generation of seed areas for each new class, serving as a form of pixel-level supervision. However, a scenario usually arises where a pixel is concurrently predicted as an old class by the pre-trained segmentation model and a new class by the seed areas. Such a scenario becomes particularly problematic in WILSS, as the lack of pixel-level annotations on new classes makes it intractable to ascertain whether the pixel pertains to the new class or not. To surmount this issue, we propose an innovative, tendency-driven relationship of mutual exclusivity, meticulously tailored to govern the behavior of the seed areas and the predictions generated by the pre-trained segmentation model. This relationship stipulates that predictions for the new and old classes must not conflict whilst prioritizing the preservation of predictions for the old classes, which not only addresses the conflicting prediction issue but also effectively mitigates the inherent challenge of incremental learning - catastrophic forgetting. Furthermore, under the auspices of this tendency-driven mutual exclusivity relationship, we generate pseudo masks for the new classes, allowing for concurrent execution with model parameter updating via the resolution of a bi-level optimization problem. Extensive experiments substantiate the effectiveness of our framework, resulting in the establishment of new benchmarks and paving the way for further research in this field.
<div id='section'>Paperid: <span id='pid'>267, <a href='https://arxiv.org/pdf/2505.22063.pdf' target='_blank'>https://arxiv.org/pdf/2505.22063.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingchen Shao, Xinfa Zhu, Chengyou Wang, Bingshen Mu, Hai Li, Ying Yan, Junhui Liu, Danming Xie, Lei Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22063">Weakly Supervised Data Refinement and Flexible Sequence Compression for Efficient Thai LLM-based ASR</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite remarkable achievements, automatic speech recognition (ASR) in low-resource scenarios still faces two challenges: high-quality data scarcity and high computational demands. This paper proposes EThai-ASR, the first to apply large language models (LLMs) to Thai ASR and create an efficient LLM-based ASR system. EThai-ASR comprises a speech encoder, a connection module and a Thai LLM decoder. To address the data scarcity and obtain a powerful speech encoder, EThai-ASR introduces a self-evolving data refinement strategy to refine weak labels, yielding an enhanced speech encoder. Moreover, we propose a pluggable sequence compression module used in the connection module with three modes designed to reduce the sequence length, thus decreasing computational demands while maintaining decent performance. Extensive experiments demonstrate that EThai-ASR has achieved state-of-the-art accuracy in multiple datasets. We release our refined text transcripts to promote further research.
<div id='section'>Paperid: <span id='pid'>268, <a href='https://arxiv.org/pdf/2504.04495.pdf' target='_blank'>https://arxiv.org/pdf/2504.04495.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Wu, Wanshun Su, Guansong Pang, Yujia Sun, Qingsen Yan, Peng Wang, Yanning Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04495">AVadCLIP: Audio-Visual Collaboration for Robust Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the increasing adoption of video anomaly detection in intelligent surveillance domains, conventional visual-based detection approaches often struggle with information insufficiency and high false-positive rates in complex environments. To address these limitations, we present a novel weakly supervised framework that leverages audio-visual collaboration for robust video anomaly detection. Capitalizing on the exceptional cross-modal representation learning capabilities of Contrastive Language-Image Pretraining (CLIP) across visual, audio, and textual domains, our framework introduces two major innovations: an efficient audio-visual fusion that enables adaptive cross-modal integration through lightweight parametric adaptation while maintaining the frozen CLIP backbone, and a novel audio-visual prompt that dynamically enhances text embeddings with key multimodal information based on the semantic correlation between audio-visual features and textual labels, significantly improving CLIP's generalization for the video anomaly detection task. Moreover, to enhance robustness against modality deficiency during inference, we further develop an uncertainty-driven feature distillation module that synthesizes audio-visual representations from visual-only inputs. This module employs uncertainty modeling based on the diversity of audio-visual features to dynamically emphasize challenging features during the distillation process. Our framework demonstrates superior performance across multiple benchmarks, with audio integration significantly boosting anomaly detection accuracy in various scenarios. Notably, with unimodal data enhanced by uncertainty-driven distillation, our approach consistently outperforms current unimodal VAD methods.
<div id='section'>Paperid: <span id='pid'>269, <a href='https://arxiv.org/pdf/2409.05383.pdf' target='_blank'>https://arxiv.org/pdf/2409.05383.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Wu, Chengyu Pan, Yuting Yan, Guansong Pang, Peng Wang, Yanning Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.05383">Deep Learning for Video Anomaly Detection: A Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video anomaly detection (VAD) aims to discover behaviors or events deviating from the normality in videos. As a long-standing task in the field of computer vision, VAD has witnessed much good progress. In the era of deep learning, with the explosion of architectures of continuously growing capability and capacity, a great variety of deep learning based methods are constantly emerging for the VAD task, greatly improving the generalization ability of detection algorithms and broadening the application scenarios. Therefore, such a multitude of methods and a large body of literature make a comprehensive survey a pressing necessity. In this paper, we present an extensive and comprehensive research review, covering the spectrum of five different categories, namely, semi-supervised, weakly supervised, fully supervised, unsupervised and open-set supervised VAD, and we also delve into the latest VAD works based on pre-trained large models, remedying the limitations of past reviews in terms of only focusing on semi-supervised VAD and small model based methods. For the VAD task with different levels of supervision, we construct a well-organized taxonomy, profoundly discuss the characteristics of different types of methods, and show their performance comparisons. In addition, this review involves the public datasets, open-source codes, and evaluation metrics covering all the aforementioned VAD tasks. Finally, we provide several important research directions for the VAD community.
<div id='section'>Paperid: <span id='pid'>270, <a href='https://arxiv.org/pdf/2408.05905.pdf' target='_blank'>https://arxiv.org/pdf/2408.05905.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Wu, Xuerong Zhou, Guansong Pang, Zhiwei Yang, Qingsen Yan, Peng Wang, Yanning Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.05905">Weakly Supervised Video Anomaly Detection and Localization with Spatio-Temporal Prompts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current weakly supervised video anomaly detection (WSVAD) task aims to achieve frame-level anomalous event detection with only coarse video-level annotations available. Existing works typically involve extracting global features from full-resolution video frames and training frame-level classifiers to detect anomalies in the temporal dimension. However, most anomalous events tend to occur in localized spatial regions rather than the entire video frames, which implies existing frame-level feature based works may be misled by the dominant background information and lack the interpretation of the detected anomalies. To address this dilemma, this paper introduces a novel method called STPrompt that learns spatio-temporal prompt embeddings for weakly supervised video anomaly detection and localization (WSVADL) based on pre-trained vision-language models (VLMs). Our proposed method employs a two-stream network structure, with one stream focusing on the temporal dimension and the other primarily on the spatial dimension. By leveraging the learned knowledge from pre-trained VLMs and incorporating natural motion priors from raw videos, our model learns prompt embeddings that are aligned with spatio-temporal regions of videos (e.g., patches of individual frames) for identify specific local regions of anomalies, enabling accurate video anomaly detection while mitigating the influence of background information. Without relying on detailed spatio-temporal annotations or auxiliary object detection/tracking, our method achieves state-of-the-art performance on three public benchmarks for the WSVADL task.
<div id='section'>Paperid: <span id='pid'>271, <a href='https://arxiv.org/pdf/2406.14183.pdf' target='_blank'>https://arxiv.org/pdf/2406.14183.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marco Fumero, Marco Pegoraro, Valentino Maiorca, Francesco Locatello, Emanuele RodolÃ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.14183">Latent Functional Maps: a spectral framework for representation alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural models learn data representations that lie on low-dimensional manifolds, yet modeling the relation between these representational spaces is an ongoing challenge. By integrating spectral geometry principles into neural modeling, we show that this problem can be better addressed in the functional domain, mitigating complexity, while enhancing interpretability and performances on downstream tasks. To this end, we introduce a multi-purpose framework to the representation learning community, which allows to: (i) compare different spaces in an interpretable way and measure their intrinsic similarity; (ii) find correspondences between them, both in unsupervised and weakly supervised settings, and (iii) to effectively transfer representations between distinct spaces. We validate our framework on various applications, ranging from stitching to retrieval tasks, and on multiple modalities, demonstrating that Latent Functional Maps can serve as a swiss-army knife for representation alignment.
<div id='section'>Paperid: <span id='pid'>272, <a href='https://arxiv.org/pdf/2509.13629.pdf' target='_blank'>https://arxiv.org/pdf/2509.13629.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue He, Min Liu, Qinghao Liu, Jiazheng Wang, Yaonan Wang, Hang Zhang, Xiang Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13629">SAMIR, an efficient registration framework via robust feature learning from SAM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image registration is a fundamental task in medical image analysis. Deformations are often closely related to the morphological characteristics of tissues, making accurate feature extraction crucial. Recent weakly supervised methods improve registration by incorporating anatomical priors such as segmentation masks or landmarks, either as inputs or in the loss function. However, such weak labels are often not readily available, limiting their practical use. Motivated by the strong representation learning ability of visual foundation models, this paper introduces SAMIR, an efficient medical image registration framework that utilizes the Segment Anything Model (SAM) to enhance feature extraction. SAM is pretrained on large-scale natural image datasets and can learn robust, general-purpose visual representations. Rather than using raw input images, we design a task-specific adaptation pipeline using SAM's image encoder to extract structure-aware feature embeddings, enabling more accurate modeling of anatomical consistency and deformation patterns. We further design a lightweight 3D head to refine features within the embedding space, adapting to local deformations in medical images. Additionally, we introduce a Hierarchical Feature Consistency Loss to guide coarse-to-fine feature matching and improve anatomical alignment. Extensive experiments demonstrate that SAMIR significantly outperforms state-of-the-art methods on benchmark datasets for both intra-subject cardiac image registration and inter-subject abdomen CT image registration, achieving performance improvements of 2.68% on ACDC and 6.44% on the abdomen dataset. The source code will be publicly available on GitHub following the acceptance of this paper.
<div id='section'>Paperid: <span id='pid'>273, <a href='https://arxiv.org/pdf/2412.10410.pdf' target='_blank'>https://arxiv.org/pdf/2412.10410.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaofei Cai, Bowei Zhang, Zihao Wang, Haowei Lin, Xiaojian Ma, Anji Liu, Yitao Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10410">GROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing agents that can follow multimodal instructions remains a fundamental challenge in robotics and AI. Although large-scale pre-training on unlabeled datasets (no language instruction) has enabled agents to learn diverse behaviors, these agents often struggle with following instructions. While augmenting the dataset with instruction labels can mitigate this issue, acquiring such high-quality annotations at scale is impractical. To address this issue, we frame the problem as a semi-supervised learning task and introduce GROOT-2, a multimodal instructable agent trained using a novel approach that combines weak supervision with latent variable models. Our method consists of two key components: constrained self-imitating, which utilizes large amounts of unlabeled demonstrations to enable the policy to learn diverse behaviors, and human intention alignment, which uses a smaller set of labeled demonstrations to ensure the latent space reflects human intentions. GROOT-2's effectiveness is validated across four diverse environments, ranging from video games to robotic manipulation, demonstrating its robust multimodal instruction-following capabilities.
<div id='section'>Paperid: <span id='pid'>274, <a href='https://arxiv.org/pdf/2502.15370.pdf' target='_blank'>https://arxiv.org/pdf/2502.15370.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kibum Kim, Kanghoon Yoon, Yeonjun In, Jaehyeong Jeon, Jinyoung Moon, Donghyun Kim, Chanyoung Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.15370">Weakly Supervised Video Scene Graph Generation via Natural Language Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing Video Scene Graph Generation (VidSGG) studies are trained in a fully supervised manner, which requires all frames in a video to be annotated, thereby incurring high annotation cost compared to Image Scene Graph Generation (ImgSGG). Although the annotation cost of VidSGG can be alleviated by adopting a weakly supervised approach commonly used for ImgSGG (WS-ImgSGG) that uses image captions, there are two key reasons that hinder such a naive adoption: 1) Temporality within video captions, i.e., unlike image captions, video captions include temporal markers (e.g., before, while, then, after) that indicate time related details, and 2) Variability in action duration, i.e., unlike human actions in image captions, human actions in video captions unfold over varying duration. To address these issues, we propose a Natural Language-based Video Scene Graph Generation (NL-VSGG) framework that only utilizes the readily available video captions for training a VidSGG model. NL-VSGG consists of two key modules: Temporality-aware Caption Segmentation (TCS) module and Action Duration Variability-aware caption-frame alignment (ADV) module. Specifically, TCS segments the video captions into multiple sentences in a temporal order based on a Large Language Model (LLM), and ADV aligns each segmented sentence with appropriate frames considering the variability in action duration. Our approach leads to a significant enhancement in performance compared to simply applying the WS-ImgSGG pipeline to VidSGG on the Action Genome dataset. As a further benefit of utilizing the video captions as weak supervision, we show that the VidSGG model trained by NL-VSGG is able to predict a broader range of action classes that are not included in the training data, which makes our framework practical in reality.
<div id='section'>Paperid: <span id='pid'>275, <a href='https://arxiv.org/pdf/2502.08888.pdf' target='_blank'>https://arxiv.org/pdf/2502.08888.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruichao Yang, Jing Ma, Wei Gao, Hongzhan Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.08888">LLM-Enhanced Multiple Instance Learning for Joint Rumor and Stance Detection with Social Context Information</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The proliferation of misinformation, such as rumors on social media, has drawn significant attention, prompting various expressions of stance among users. Although rumor detection and stance detection are distinct tasks, they can complement each other. Rumors can be identified by cross-referencing stances in related posts, and stances are influenced by the nature of the rumor. However, existing stance detection methods often require post-level stance annotations, which are costly to obtain. We propose a novel LLM-enhanced MIL approach to jointly predict post stance and claim class labels, supervised solely by claim labels, using an undirected microblog propagation model. Our weakly supervised approach relies only on bag-level labels of claim veracity, aligning with multi-instance learning (MIL) principles. To achieve this, we transform the multi-class problem into multiple MIL-based binary classification problems. We then employ a discriminative attention layer to aggregate the outputs from these classifiers into finer-grained classes. Experiments conducted on three rumor datasets and two stance datasets demonstrate the effectiveness of our approach, highlighting strong connections between rumor veracity and expressed stances in responding posts. Our method shows promising performance in joint rumor and stance detection compared to the state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>276, <a href='https://arxiv.org/pdf/2402.08960.pdf' target='_blank'>https://arxiv.org/pdf/2402.08960.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoqing Wang, Xiaobo Xia, Ziye Chen, Xiao He, Yandong Guo, Mingming Gong, Tongliang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.08960">Open-Vocabulary Segmentation with Unpaired Mask-Text Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current state-of-the-art open-vocabulary segmentation methods typically rely on image-mask-text triplet annotations for supervision. However, acquiring such detailed annotations is labour-intensive and poses scalability challenges in complex real-world scenarios. While existing weakly-supervised approaches leverage image-text pairs to reduce the expansive annotation cost, the lack of mask supervision makes it difficult for the model to locate multiple instances and accurately group pixels with similar semantics, significantly hampering versatility and performance. In this paper, we introduce Unpair-Seg, a novel weakly-supervised open-vocabulary segmentation framework that learns from unpaired image-mask and image-text pairs, which can be independently and efficiently collected. Unpair-Seg initially predicts a set of binary masks and generates pseudo labels by identifying confident pairs of masks and text entities. We then train a feature adapter to align region embeddings with text embeddings based on these pseudo labels, achieving open-vocabulary segmentation. However, the inherent noise in the mask-entity correspondence poses a challenge to obtaining reliable pairs. To address this, we employ a vision-language large model to re-caption the input images and extract precise entities, and we design a multi-scale matching strategy to reduce noisy mask-entity pairs. Our Unpair-Seg framework demonstrates impressive performance, achieving 14.6\% and 19.5\% mIoU on the ADE-847 and PASCAL Context-459 datasets, significantly narrowing the gap between fully-supervised and weakly-supervised methods.
<div id='section'>Paperid: <span id='pid'>277, <a href='https://arxiv.org/pdf/2510.14668.pdf' target='_blank'>https://arxiv.org/pdf/2510.14668.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md. Abdur Rahman, Mohaimenul Azam Khan Raiaan, Sami Azam, Asif Karim, Jemima Beissbarth, Amanda Leach
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.14668">WeCKD: Weakly-supervised Chained Distillation Network for Efficient Multimodal Medical Imaging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Knowledge distillation (KD) has traditionally relied on a static teacher-student framework, where a large, well-trained teacher transfers knowledge to a single student model. However, these approaches often suffer from knowledge degradation, inefficient supervision, and reliance on either a very strong teacher model or large labeled datasets, which limits their effectiveness in real-world, limited-data scenarios. To address these, we present the first-ever Weakly-supervised Chain-based KD network (WeCKD) that redefines knowledge transfer through a structured sequence of interconnected models. Unlike conventional KD, it forms a progressive distillation chain, where each model not only learns from its predecessor but also refines the knowledge before passing it forward. This structured knowledge transfer further enhances feature learning, reduces data dependency, and mitigates the limitations of one-step KD. Each model in the distillation chain is trained on only a fraction of the dataset and demonstrates that effective learning can be achieved with minimal supervision. Extensive evaluations across four otoscopic imaging datasets demonstrate that it not only matches but in many cases surpasses the performance of existing supervised methods. Experimental results on two other datasets further underscore its generalization across diverse medical imaging modalities, including microscopic and magnetic resonance imaging. Furthermore, our evaluations resulted in cumulative accuracy gains of up to +23% over a single backbone trained on the same limited data, which highlights its potential for real-world adoption.
<div id='section'>Paperid: <span id='pid'>278, <a href='https://arxiv.org/pdf/2509.03893.pdf' target='_blank'>https://arxiv.org/pdf/2509.03893.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stefan Stojanov, Linan Zhao, Yunzhi Zhang, Daniel L. K. Yamins, Jiajun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03893">Weakly-Supervised Learning of Dense Functional Correspondences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Establishing dense correspondences across image pairs is essential for tasks such as shape reconstruction and robot manipulation. In the challenging setting of matching across different categories, the function of an object, i.e., the effect that an object can cause on other objects, can guide how correspondences should be established. This is because object parts that enable specific functions often share similarities in shape and appearance. We derive the definition of dense functional correspondence based on this observation and propose a weakly-supervised learning paradigm to tackle the prediction task. The main insight behind our approach is that we can leverage vision-language models to pseudo-label multi-view images to obtain functional parts. We then integrate this with dense contrastive learning from pixel correspondences to distill both functional and spatial knowledge into a new model that can establish dense functional correspondence. Further, we curate synthetic and real evaluation datasets as task benchmarks. Our results demonstrate the advantages of our approach over baseline solutions consisting of off-the-shelf self-supervised image representations and grounded vision language models.
<div id='section'>Paperid: <span id='pid'>279, <a href='https://arxiv.org/pdf/2503.20190.pdf' target='_blank'>https://arxiv.org/pdf/2503.20190.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Chen, Jiawen Li, Jiali Hu, Xitong Ling, Tian Guan, Anjia Han, Yonghong He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20190">Cross-Modal Prototype Allocation: Unsupervised Slide Representation Learning via Patch-Text Contrast in Computational Pathology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancement of pathology foundation models (FMs), the representation learning of whole slide images (WSIs) attracts increasing attention. Existing studies develop high-quality patch feature extractors and employ carefully designed aggregation schemes to derive slide-level representations. However, mainstream weakly supervised slide representation learning methods, primarily based on multiple instance learning (MIL), are tailored to specific downstream tasks, which limits their generalizability. To address this issue, some studies explore unsupervised slide representation learning. However, these approaches focus solely on the visual modality of patches, neglecting the rich semantic information embedded in textual data. In this work, we propose ProAlign, a cross-modal unsupervised slide representation learning framework. Specifically, we leverage a large language model (LLM) to generate descriptive text for the prototype types present in a WSI, introducing patch-text contrast to construct initial prototype embeddings. Furthermore, we propose a parameter-free attention aggregation strategy that utilizes the similarity between patches and these prototypes to form unsupervised slide embeddings applicable to a wide range of downstream tasks. Extensive experiments on four public datasets show that ProAlign outperforms existing unsupervised frameworks and achieves performance comparable to some weakly supervised models.
<div id='section'>Paperid: <span id='pid'>280, <a href='https://arxiv.org/pdf/2503.00915.pdf' target='_blank'>https://arxiv.org/pdf/2503.00915.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xitong Ling, Yifeng Ping, Jiawen Li, Jing Peng, Yuxuan Chen, Minxi Ouyang, Yizhi Wang, Yonghong He, Tian Guan, Xiaoping Liu, Lianghui Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00915">Multimodal Distillation-Driven Ensemble Learning for Long-Tailed Histopathology Whole Slide Images Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple Instance Learning (MIL) plays a significant role in computational pathology, enabling weakly supervised analysis of Whole Slide Image (WSI) datasets. The field of WSI analysis is confronted with a severe long-tailed distribution problem, which significantly impacts the performance of classifiers. Long-tailed distributions lead to class imbalance, where some classes have sparse samples while others are abundant, making it difficult for classifiers to accurately identify minority class samples. To address this issue, we propose an ensemble learning method based on MIL, which employs expert decoders with shared aggregators and consistency constraints to learn diverse distributions and reduce the impact of class imbalance on classifier performance. Moreover, we introduce a multimodal distillation framework that leverages text encoders pre-trained on pathology-text pairs to distill knowledge and guide the MIL aggregator in capturing stronger semantic features relevant to class information. To ensure flexibility, we use learnable prompts to guide the distillation process of the pre-trained text encoder, avoiding limitations imposed by specific prompts. Our method, MDE-MIL, integrates multiple expert branches focusing on specific data distributions to address long-tailed issues. Consistency control ensures generalization across classes. Multimodal distillation enhances feature extraction. Experiments on Camelyon+-LT and PANDA-LT datasets show it outperforms state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>281, <a href='https://arxiv.org/pdf/2502.20823.pdf' target='_blank'>https://arxiv.org/pdf/2502.20823.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawen Li, Jiali Hu, Qiehe Sun, Renao Yan, Minxi Ouyang, Tian Guan, Anjia Han, Chao He, Yonghong He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20823">Can We Simplify Slide-level Fine-tuning of Pathology Foundation Models?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emergence of foundation models in computational pathology has transformed histopathological image analysis, with whole slide imaging (WSI) diagnosis being a core application. Traditionally, weakly supervised fine-tuning via multiple instance learning (MIL) has been the primary method for adapting foundation models to WSIs. However, in this work we present a key experimental finding: a simple nonlinear mapping strategy combining mean pooling and a multilayer perceptron, called SiMLP, can effectively adapt patch-level foundation models to slide-level tasks without complex MIL-based learning. Through extensive experiments across diverse downstream tasks, we demonstrate the superior performance of SiMLP with state-of-the-art methods. For instance, on a large-scale pan-cancer classification task, SiMLP surpasses popular MIL-based methods by 3.52%. Furthermore, SiMLP shows strong learning ability in few-shot classification and remaining highly competitive with slide-level foundation models pretrained on tens of thousands of slides. Finally, SiMLP exhibits remarkable robustness and transferability in lung cancer subtyping. Overall, our findings challenge the conventional MIL-based fine-tuning paradigm, demonstrating that a task-agnostic representation strategy alone can effectively adapt foundation models to WSI analysis. These insights offer a unique and meaningful perspective for future research in digital pathology, paving the way for more efficient and broadly applicable methodologies.
<div id='section'>Paperid: <span id='pid'>282, <a href='https://arxiv.org/pdf/2412.19847.pdf' target='_blank'>https://arxiv.org/pdf/2412.19847.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexandr Korchemnyi, Alexey K. Kovalev, Aleksandr I. Panov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19847">Symbolic Disentangled Representations for Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The idea of disentangled representations is to reduce the data to a set of generative factors that produce it. Typically, such representations are vectors in latent space, where each coordinate corresponds to one of the generative factors. The object can then be modified by changing the value of a particular coordinate, but it is necessary to determine which coordinate corresponds to the desired generative factor -- a difficult task if the vector representation has a high dimension. In this article, we propose ArSyD (Architecture for Symbolic Disentanglement), which represents each generative factor as a vector of the same dimension as the resulting representation. In ArSyD, the object representation is obtained as a superposition of the generative factor vector representations. We call such a representation a \textit{symbolic disentangled representation}. We use the principles of Hyperdimensional Computing (also known as Vector Symbolic Architectures), where symbols are represented as hypervectors, allowing vector operations on them. Disentanglement is achieved by construction, no additional assumptions about the underlying distributions are made during training, and the model is only trained to reconstruct images in a weakly supervised manner. We study ArSyD on the dSprites and CLEVR datasets and provide a comprehensive analysis of the learned symbolic disentangled representations. We also propose new disentanglement metrics that allow comparison of methods using latent representations of different dimensions. ArSyD allows to edit the object properties in a controlled and interpretable way, and the dimensionality of the object property representation coincides with the dimensionality of the object representation itself.
<div id='section'>Paperid: <span id='pid'>283, <a href='https://arxiv.org/pdf/2411.06702.pdf' target='_blank'>https://arxiv.org/pdf/2411.06702.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jia Syuen Lim, Yadan Luo, Zhi Chen, Tianqi Wei, Scott Chapman, Zi Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06702">Track Any Peppers: Weakly Supervised Sweet Pepper Tracking Using VLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the Detection and Multi-Object Tracking of Sweet Peppers Challenge, we present Track Any Peppers (TAP) - a weakly supervised ensemble technique for sweet peppers tracking. TAP leverages the zero-shot detection capabilities of vision-language foundation models like Grounding DINO to automatically generate pseudo-labels for sweet peppers in video sequences with minimal human intervention. These pseudo-labels, refined when necessary, are used to train a YOLOv8 segmentation network. To enhance detection accuracy under challenging conditions, we incorporate pre-processing techniques such as relighting adjustments and apply depth-based filtering during post-inference. For object tracking, we integrate the Matching by Segment Anything (MASA) adapter with the BoT-SORT algorithm. Our approach achieves a HOTA score of 80.4%, MOTA of 66.1%, Recall of 74.0%, and Precision of 90.7%, demonstrating effective tracking of sweet peppers without extensive manual effort. This work highlights the potential of foundation models for efficient and accurate object detection and tracking in agricultural settings.
<div id='section'>Paperid: <span id='pid'>284, <a href='https://arxiv.org/pdf/2409.11664.pdf' target='_blank'>https://arxiv.org/pdf/2409.11664.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xitong Ling, Minxi Ouyang, Yizhi Wang, Xinrui Chen, Renao Yan, Hongbo Chu, Junru Cheng, Tian Guan, Sufang Tian, Xiaoping Liu, Yonghong He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11664">Agent Aggregator with Mask Denoise Mechanism for Histopathology Whole Slide Image Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Histopathology analysis is the gold standard for medical diagnosis. Accurate classification of whole slide images (WSIs) and region-of-interests (ROIs) localization can assist pathologists in diagnosis. The gigapixel resolution of WSI and the absence of fine-grained annotations make direct classification and analysis challenging. In weakly supervised learning, multiple instance learning (MIL) presents a promising approach for WSI classification. The prevailing strategy is to use attention mechanisms to measure instance importance for classification. However, attention mechanisms fail to capture inter-instance information, and self-attention causes quadratic computational complexity. To address these challenges, we propose AMD-MIL, an agent aggregator with a mask denoise mechanism. The agent token acts as an intermediate variable between the query and key for computing instance importance. Mask and denoising matrices, mapped from agents-aggregated value, dynamically mask low-contribution representations and eliminate noise. AMD-MIL achieves better attention allocation by adjusting feature representations, capturing micro-metastases in cancer, and improving interpretability. Extensive experiments on CAMELYON-16, CAMELYON-17, TCGA-KIDNEY, and TCGA-LUNG show AMD-MIL's superiority over state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>285, <a href='https://arxiv.org/pdf/2408.12825.pdf' target='_blank'>https://arxiv.org/pdf/2408.12825.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingxi Ouyang, Yuqiu Fu, Renao Yan, ShanShan Shi, Xitong Ling, Lianghui Zhu, Yonghong He, Tian Guan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.12825">MergeUp-augmented Semi-Weakly Supervised Learning for WSI Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in computational pathology and artificial intelligence have significantly improved whole slide image (WSI) classification. However, the gigapixel resolution of WSIs and the scarcity of manual annotations present substantial challenges. Multiple instance learning (MIL) is a promising weakly supervised learning approach for WSI classification. Recently research revealed employing pseudo bag augmentation can encourage models to learn various data, thus bolstering models' performance. While directly inheriting the parents' labels can introduce more noise by mislabeling in training. To address this issue, we translate the WSI classification task from weakly supervised learning to semi-weakly supervised learning, termed SWS-MIL, where adaptive pseudo bag augmentation (AdaPse) is employed to assign labeled and unlabeled data based on a threshold strategy. Using the "student-teacher" pattern, we introduce a feature augmentation technique, MergeUp, which merges bags with low-priority bags to enhance inter-category information, increasing training data diversity. Experimental results on the CAMELYON-16, BRACS, and TCGA-LUNG datasets demonstrate the superiority of our method over existing state-of-the-art approaches, affirming its efficacy in WSI classification.
<div id='section'>Paperid: <span id='pid'>286, <a href='https://arxiv.org/pdf/2403.15272.pdf' target='_blank'>https://arxiv.org/pdf/2403.15272.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jialu Wang, Kaichen Zhou, Andrew Markham, Niki Trigoni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15272">WSCLoc: Weakly-Supervised Sparse-View Camera Relocalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the advancements in deep learning for camera relocalization tasks, obtaining ground truth pose labels required for the training process remains a costly endeavor. While current weakly supervised methods excel in lightweight label generation, their performance notably declines in scenarios with sparse views. In response to this challenge, we introduce WSCLoc, a system capable of being customized to various deep learning-based relocalization models to enhance their performance under weakly-supervised and sparse view conditions. This is realized with two stages. In the initial stage, WSCLoc employs a multilayer perceptron-based structure called WFT-NeRF to co-optimize image reconstruction quality and initial pose information. To ensure a stable learning process, we incorporate temporal information as input. Furthermore, instead of optimizing SE(3), we opt for $\mathfrak{sim}(3)$ optimization to explicitly enforce a scale constraint. In the second stage, we co-optimize the pre-trained WFT-NeRF and WFT-Pose. This optimization is enhanced by Time-Encoding based Random View Synthesis and supervised by inter-frame geometric constraints that consider pose, depth, and RGB information. We validate our approaches on two publicly available datasets, one outdoor and one indoor. Our experimental results demonstrate that our weakly-supervised relocalization solutions achieve superior pose estimation accuracy in sparse-view scenarios, comparable to state-of-the-art camera relocalization methods. We will make our code publicly available.
<div id='section'>Paperid: <span id='pid'>287, <a href='https://arxiv.org/pdf/2401.11847.pdf' target='_blank'>https://arxiv.org/pdf/2401.11847.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Chen, Jiaze Wang, Ziyu Guo, Jinpeng Li, Donghao Zhou, Bian Wu, Chenyong Guan, Guangyong Chen, Pheng-Ann Heng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11847">SignVTCL: Multi-Modal Continuous Sign Language Recognition Enhanced by Visual-Textual Contrastive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sign language recognition (SLR) plays a vital role in facilitating communication for the hearing-impaired community. SLR is a weakly supervised task where entire videos are annotated with glosses, making it challenging to identify the corresponding gloss within a video segment. Recent studies indicate that the main bottleneck in SLR is the insufficient training caused by the limited availability of large-scale datasets. To address this challenge, we present SignVTCL, a multi-modal continuous sign language recognition framework enhanced by visual-textual contrastive learning, which leverages the full potential of multi-modal data and the generalization ability of language model. SignVTCL integrates multi-modal data (video, keypoints, and optical flow) simultaneously to train a unified visual backbone, thereby yielding more robust visual representations. Furthermore, SignVTCL contains a visual-textual alignment approach incorporating gloss-level and sentence-level alignment to ensure precise correspondence between visual features and glosses at the level of individual glosses and sentence. Experimental results conducted on three datasets, Phoenix-2014, Phoenix-2014T, and CSL-Daily, demonstrate that SignVTCL achieves state-of-the-art results compared with previous methods.
<div id='section'>Paperid: <span id='pid'>288, <a href='https://arxiv.org/pdf/2509.12090.pdf' target='_blank'>https://arxiv.org/pdf/2509.12090.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihong Chen, Jiancheng Yang, Deniz Sayin Mercadier, Hieu Le, Juerg Schwitter, Pascal Fua
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12090">End-to-End 4D Heart Mesh Recovery Across Full-Stack and Sparse Cardiac MRI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing cardiac motion from cine CMR sequences is critical for diagnosis, prediction, and intervention. Existing methods rely on complete CMR stacks to infer full heart motion, limiting their utility in intra-procedural scenarios where only sparse observations are available. We present TetHeart, the first end-to-end framework that unifies full 4D multi-structure heart mesh recovery from both offline full-stack acquisitions and intra-procedural sparse-slice observations. Our method leverages deep deformable tetrahedra, an explicit-implicit hybrid representation, to capture shape and motion in a coherent space shared across cardiac structures. It is initialized from high-quality pre-procedural or offline-acquired full stacks to build detailed, patient-specific heart meshes, which can then be updated using whatever slices are available, from full stacks down to a single slice. We further incorporate several key innovations: (i) an attentive mechanism for slice-adaptive 2D-3D feature assembly that dynamically integrates information from arbitrary numbers of slices at any position, combined with a distillation strategy from full-slice to sparse-slice settings to ensure accurate reconstruction under extreme sparsity; and (ii) a two-stage weakly supervised motion learning scheme requiring only keyframe (e.g., ED and ES) annotations. Trained and validated on three large public datasets and externally evaluated zero-shot on additional private interventional and public CMR datasets, TetHeart achieves state-of-the-art accuracy and strong generalization in both pre- and intra-procedural settings.
<div id='section'>Paperid: <span id='pid'>289, <a href='https://arxiv.org/pdf/2411.10356.pdf' target='_blank'>https://arxiv.org/pdf/2411.10356.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrea Agostini, DaphnÃ© Chopard, Yang Meng, Norbert Fortin, Babak Shahbaba, Stephan Mandt, Thomas M. Sutter, Julia E. Vogt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10356">Weakly-Supervised Multimodal Learning on MIMIC-CXR</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal data integration and label scarcity pose significant challenges for machine learning in medical settings. To address these issues, we conduct an in-depth evaluation of the newly proposed Multimodal Variational Mixture-of-Experts (MMVM) VAE on the challenging MIMIC-CXR dataset. Our analysis demonstrates that the MMVM VAE consistently outperforms other multimodal VAEs and fully supervised approaches, highlighting its strong potential for real-world medical applications.
<div id='section'>Paperid: <span id='pid'>290, <a href='https://arxiv.org/pdf/2510.11073.pdf' target='_blank'>https://arxiv.org/pdf/2510.11073.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Tian, Min Zhou, Yitong Chen, Fang Li, Lingzi Qi, Shuo Wang, Xieyang Xu, Yu Yu, Shiqiong Xu, Chaoyu Lei, Yankai Jiang, Rongzhao Zhang, Jia Tan, Li Wu, Hong Chen, Xiaowei Liu, Wei Lu, Lin Li, Huifang Zhou, Xuefei Song, Guangtao Zhai, Xianqun Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.11073">ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Patient face images provide a convenient mean for evaluating eye diseases, while also raising privacy concerns. Here, we introduce ROFI, a deep learning-based privacy protection framework for ophthalmology. Using weakly supervised learning and neural identity translation, ROFI anonymizes facial features while retaining disease features (over 98\% accuracy, $κ> 0.90$). It achieves 100\% diagnostic sensitivity and high agreement ($κ> 0.90$) across eleven eye diseases in three cohorts, anonymizing over 95\% of images. ROFI works with AI systems, maintaining original diagnoses ($κ> 0.80$), and supports secure image reversal (over 98\% similarity), enabling audits and long-term care. These results show ROFI's effectiveness of protecting patient privacy in the digital medicine era.
<div id='section'>Paperid: <span id='pid'>291, <a href='https://arxiv.org/pdf/2403.11449.pdf' target='_blank'>https://arxiv.org/pdf/2403.11449.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hang Gao, Jiaguo Yuan, Jiangmeng Li, Peng Qiao, Fengge Wu, Changwen Zheng, Huaping Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11449">Graph Partial Label Learning with Potential Cause Discovering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph Neural Networks (GNNs) have garnered widespread attention for their potential to address the challenges posed by graph representation learning, which face complex graph-structured data across various domains. However, due to the inherent complexity and interconnectedness of graphs, accurately annotating graph data for training GNNs is extremely challenging. To address this issue, we have introduced Partial Label Learning (PLL) into graph representation learning. PLL is a critical weakly supervised learning problem where each training instance is associated with a set of candidate labels, including the ground-truth label and the additional interfering labels. PLL allows annotators to make errors, which reduces the difficulty of data labeling. Subsequently, we propose a novel graph representation learning method that enables GNN models to effectively learn discriminative information within the context of PLL. Our approach utilizes potential cause extraction to obtain graph data that holds causal relationships with the labels. By conducting auxiliary training based on the extracted graph data, our model can effectively eliminate the interfering information in the PLL scenario. We support the rationale behind our method with a series of theoretical analyses. Moreover, we conduct extensive evaluations and ablation studies on multiple datasets, demonstrating the superiority of our proposed method.
<div id='section'>Paperid: <span id='pid'>292, <a href='https://arxiv.org/pdf/2510.16536.pdf' target='_blank'>https://arxiv.org/pdf/2510.16536.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Niranjana Arun Menon, Yulong Li, Iqra Farooq, Sara Ahmed, Muhammad Awais, Imran Razzak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.16536">Few-Label Multimodal Modeling of SNP Variants and ECG Phenotypes Using Large Language Models for Cardiovascular Risk Stratification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cardiovascular disease (CVD) risk stratification remains a major challenge due to its multifactorial nature and limited availability of high-quality labeled datasets. While genomic and electrophysiological data such as SNP variants and ECG phenotypes are increasingly accessible, effectively integrating these modalities in low-label settings is non-trivial. This challenge arises from the scarcity of well-annotated multimodal datasets and the high dimensionality of biological signals, which limit the effectiveness of conventional supervised models. To address this, we present a few-label multimodal framework that leverages large language models (LLMs) to combine genetic and electrophysiological information for cardiovascular risk stratification. Our approach incorporates a pseudo-label refinement strategy to adaptively distill high-confidence labels from weakly supervised predictions, enabling robust model fine-tuning with only a small set of ground-truth annotations. To enhance the interpretability, we frame the task as a Chain of Thought (CoT) reasoning problem, prompting the model to produce clinically relevant rationales alongside predictions. Experimental results demonstrate that the integration of multimodal inputs, few-label supervision, and CoT reasoning improves robustness and generalizability across diverse patient profiles. Experimental results using multimodal SNP variants and ECG-derived features demonstrated comparable performance to models trained on the full dataset, underscoring the promise of LLM-based few-label multimodal modeling for advancing personalized cardiovascular care.
<div id='section'>Paperid: <span id='pid'>293, <a href='https://arxiv.org/pdf/2510.03016.pdf' target='_blank'>https://arxiv.org/pdf/2510.03016.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dong-Dong Wu, Jiacheng Cui, Wei Wang, Zhiqiang She, Masashi Sugiyama
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03016">Learning Robust Diffusion Models from Imprecise Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conditional diffusion models have achieved remarkable success in various generative tasks recently, but their training typically relies on large-scale datasets that inevitably contain imprecise information in conditional inputs. Such supervision, often stemming from noisy, ambiguous, or incomplete labels, will cause condition mismatch and degrade generation quality. To address this challenge, we propose DMIS, a unified framework for training robust Diffusion Models from Imprecise Supervision, which is the first systematic study within diffusion models. Our framework is derived from likelihood maximization and decomposes the objective into generative and classification components: the generative component models imprecise-label distributions, while the classification component leverages a diffusion classifier to infer class-posterior probabilities, with its efficiency further improved by an optimized timestep sampling strategy. Extensive experiments on diverse forms of imprecise supervision, covering tasks of image generation, weakly supervised learning, and noisy dataset condensation demonstrate that DMIS consistently produces high-quality and class-discriminative samples.
<div id='section'>Paperid: <span id='pid'>294, <a href='https://arxiv.org/pdf/2505.18989.pdf' target='_blank'>https://arxiv.org/pdf/2505.18989.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Catalina Tan, Yipeng Hu, Shaheer U. Saeed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18989">SPARS: Self-Play Adversarial Reinforcement Learning for Segmentation of Liver Tumours</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate tumour segmentation is vital for various targeted diagnostic and therapeutic procedures for cancer, e.g., planning biopsies or tumour ablations. Manual delineation is extremely labour-intensive, requiring substantial expert time. Fully-supervised machine learning models aim to automate such localisation tasks, but require a large number of costly and often subjective 3D voxel-level labels for training. The high-variance and subjectivity in such labels impacts model generalisability, even when large datasets are available. Histopathology labels may offer more objective labels but the infeasibility of acquiring pixel-level annotations to develop tumour localisation methods based on histology remains challenging in-vivo. In this work, we propose a novel weakly-supervised semantic segmentation framework called SPARS (Self-Play Adversarial Reinforcement Learning for Segmentation), which utilises an object presence classifier, trained on a small number of image-level binary cancer presence labels, to localise cancerous regions on CT scans. Such binary labels of patient-level cancer presence can be sourced more feasibly from biopsies and histopathology reports, enabling a more objective cancer localisation on medical images. Evaluating with real patient data, we observed that SPARS yielded a mean dice score of $77.3 \pm 9.4$, which outperformed other weakly-supervised methods by large margins. This performance was comparable with recent fully-supervised methods that require voxel-level annotations. Our results demonstrate the potential of using SPARS to reduce the need for extensive human-annotated labels to detect cancer in real-world healthcare settings.
<div id='section'>Paperid: <span id='pid'>295, <a href='https://arxiv.org/pdf/2505.17915.pdf' target='_blank'>https://arxiv.org/pdf/2505.17915.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lynn Karam, Yipei Wang, Veeru Kasivisvanathan, Mirabela Rusu, Yipeng Hu, Shaheer U. Saeed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17915">Promptable cancer segmentation using minimal expert-curated data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automated segmentation of cancer on medical images can aid targeted diagnostic and therapeutic procedures. However, its adoption is limited by the high cost of expert annotations required for training and inter-observer variability in datasets. While weakly-supervised methods mitigate some challenges, using binary histology labels for training as opposed to requiring full segmentation, they require large paired datasets of histology and images, which are difficult to curate. Similarly, promptable segmentation aims to allow segmentation with no re-training for new tasks at inference, however, existing models perform poorly on pathological regions, again necessitating large datasets for training. In this work we propose a novel approach for promptable segmentation requiring only 24 fully-segmented images, supplemented by 8 weakly-labelled images, for training. Curating this minimal data to a high standard is relatively feasible and thus issues with the cost and variability of obtaining labels can be mitigated. By leveraging two classifiers, one weakly-supervised and one fully-supervised, our method refines segmentation through a guided search process initiated by a single-point prompt. Our approach outperforms existing promptable segmentation methods, and performs comparably with fully-supervised methods, for the task of prostate cancer segmentation, while using substantially less annotated data (up to 100X less). This enables promptable segmentation with very minimal labelled data, such that the labels can be curated to a very high standard.
<div id='section'>Paperid: <span id='pid'>296, <a href='https://arxiv.org/pdf/2402.13778.pdf' target='_blank'>https://arxiv.org/pdf/2402.13778.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Martynas Pocius, Wen Yan, Dean C. Barratt, Mark Emberton, Matthew J. Clarkson, Yipeng Hu, Shaheer U. Saeed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.13778">Weakly supervised localisation of prostate cancer using reinforcement learning for bi-parametric MR images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper we propose a reinforcement learning based weakly supervised system for localisation. We train a controller function to localise regions of interest within an image by introducing a novel reward definition that utilises non-binarised classification probability, generated by a pre-trained binary classifier which classifies object presence in images or image crops. The object-presence classifier may then inform the controller of its localisation quality by quantifying the likelihood of the image containing an object. Such an approach allows us to minimize any potential labelling or human bias propagated via human labelling for fully supervised localisation. We evaluate our proposed approach for a task of cancerous lesion localisation on a large dataset of real clinical bi-parametric MR images of the prostate. Comparisons to the commonly used multiple-instance learning weakly supervised localisation and to a fully supervised baseline show that our proposed method outperforms the multi-instance learning and performs comparably to fully-supervised learning, using only image-level classification labels for training.
<div id='section'>Paperid: <span id='pid'>297, <a href='https://arxiv.org/pdf/2402.10728.pdf' target='_blank'>https://arxiv.org/pdf/2402.10728.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiwen Li, Yunguan Fu, Iani J. M. B. Gayo, Qianye Yang, Zhe Min, Shaheer U. Saeed, Wen Yan, Yipei Wang, J. Alison Noble, Mark Emberton, Matthew J. Clarkson, Dean C. Barratt, Victor A. Prisacariu, Yipeng Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.10728">Semi-weakly-supervised neural network training for medical image registration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For training registration networks, weak supervision from segmented corresponding regions-of-interest (ROIs) have been proven effective for (a) supplementing unsupervised methods, and (b) being used independently in registration tasks in which unsupervised losses are unavailable or ineffective. This correspondence-informing supervision entails cost in annotation that requires significant specialised effort. This paper describes a semi-weakly-supervised registration pipeline that improves the model performance, when only a small corresponding-ROI-labelled dataset is available, by exploiting unlabelled image pairs. We examine two types of augmentation methods by perturbation on network weights and image resampling, such that consistency-based unsupervised losses can be applied on unlabelled data. The novel WarpDDF and RegCut approaches are proposed to allow commutative perturbation between an image pair and the predicted spatial transformation (i.e. respective input and output of registration networks), distinct from existing perturbation methods for classification or segmentation. Experiments using 589 male pelvic MR images, labelled with eight anatomical ROIs, show the improvement in registration performance and the ablated contributions from the individual strategies. Furthermore, this study attempts to construct one of the first computational atlases for pelvic structures, enabled by registering inter-subject MRs, and quantifies the significant differences due to the proposed semi-weak supervision with a discussion on the potential clinical use of example atlas-derived statistics.
<div id='section'>Paperid: <span id='pid'>298, <a href='https://arxiv.org/pdf/2407.09826.pdf' target='_blank'>https://arxiv.org/pdf/2407.09826.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoxu Xu, Yitian Yuan, Jinlong Li, Qiudan Zhang, Zequn Jie, Lin Ma, Hao Tang, Nicu Sebe, Xu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.09826">3D Weakly Supervised Semantic Segmentation with 2D Vision-Language Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose 3DSS-VLG, a weakly supervised approach for 3D Semantic Segmentation with 2D Vision-Language Guidance, an alternative approach that a 3D model predicts dense-embedding for each point which is co-embedded with both the aligned image and text spaces from the 2D vision-language model. Specifically, our method exploits the superior generalization ability of the 2D vision-language models and proposes the Embeddings Soft-Guidance Stage to utilize it to implicitly align 3D embeddings and text embeddings. Moreover, we introduce the Embeddings Specialization Stage to purify the feature representation with the help of a given scene-level label, specifying a better feature supervised by the corresponding text embedding. Thus, the 3D model is able to gain informative supervisions both from the image embedding and text embedding, leading to competitive segmentation performances. To the best of our knowledge, this is the first work to investigate 3D weakly supervised semantic segmentation by using the textual semantic information of text category labels. Moreover, with extensive quantitative and qualitative experiments, we present that our 3DSS-VLG is able not only to achieve the state-of-the-art performance on both S3DIS and ScanNet datasets, but also to maintain strong generalization capability.
<div id='section'>Paperid: <span id='pid'>299, <a href='https://arxiv.org/pdf/2403.01156.pdf' target='_blank'>https://arxiv.org/pdf/2403.01156.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lian Xu, Mohammed Bennamoun, Farid Boussaid, Wanli Ouyang, Ferdous Sohel, Dan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.01156">Auxiliary Tasks Enhanced Dual-affinity Learning for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most existing weakly supervised semantic segmentation (WSSS) methods rely on Class Activation Mapping (CAM) to extract coarse class-specific localization maps using image-level labels. Prior works have commonly used an off-line heuristic thresholding process that combines the CAM maps with off-the-shelf saliency maps produced by a general pre-trained saliency model to produce more accurate pseudo-segmentation labels. We propose AuxSegNet+, a weakly supervised auxiliary learning framework to explore the rich information from these saliency maps and the significant inter-task correlation between saliency detection and semantic segmentation. In the proposed AuxSegNet+, saliency detection and multi-label image classification are used as auxiliary tasks to improve the primary task of semantic segmentation with only image-level ground-truth labels. We also propose a cross-task affinity learning mechanism to learn pixel-level affinities from the saliency and segmentation feature maps. In particular, we propose a cross-task dual-affinity learning module to learn both pairwise and unary affinities, which are used to enhance the task-specific features and predictions by aggregating both query-dependent and query-independent global context for both saliency detection and semantic segmentation. The learned cross-task pairwise affinity can also be used to refine and propagate CAM maps to provide better pseudo labels for both tasks. Iterative improvement of segmentation performance is enabled by cross-task affinity learning and pseudo-label updating. Extensive experiments demonstrate the effectiveness of the proposed approach with new state-of-the-art WSSS results on the challenging PASCAL VOC and MS COCO benchmarks.
<div id='section'>Paperid: <span id='pid'>300, <a href='https://arxiv.org/pdf/2512.15310.pdf' target='_blank'>https://arxiv.org/pdf/2512.15310.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wangyu Wu, Zhenhong Chen, Xiaowei Huang, Fei Ma, Jimin Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.15310">SynthSeg-Agents: Multi-Agent Synthetic Data Generation for Zero-Shot Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Semantic Segmentation (WSSS) with image level labels aims to produce pixel level predictions without requiring dense annotations. While recent approaches have leveraged generative models to augment existing data, they remain dependent on real world training samples. In this paper, we introduce a novel direction, Zero Shot Weakly Supervised Semantic Segmentation (ZSWSSS), and propose SynthSeg Agents, a multi agent framework driven by Large Language Models (LLMs) to generate synthetic training data entirely without real images. SynthSeg Agents comprises two key modules, a Self Refine Prompt Agent and an Image Generation Agent. The Self Refine Prompt Agent autonomously crafts diverse and semantically rich image prompts via iterative refinement, memory mechanisms, and prompt space exploration, guided by CLIP based similarity and nearest neighbor diversity filtering. These prompts are then passed to the Image Generation Agent, which leverages Vision Language Models (VLMs) to synthesize candidate images. A frozen CLIP scoring model is employed to select high quality samples, and a ViT based classifier is further trained to relabel the entire synthetic dataset with improved semantic precision. Our framework produces high quality training data without any real image supervision. Experiments on PASCAL VOC 2012 and COCO 2014 show that SynthSeg Agents achieves competitive performance without using real training images. This highlights the potential of LLM driven agents in enabling cost efficient and scalable semantic segmentation.
<div id='section'>Paperid: <span id='pid'>301, <a href='https://arxiv.org/pdf/2508.17009.pdf' target='_blank'>https://arxiv.org/pdf/2508.17009.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wangyu Wu, Zhenhong Chen, Xiaowen Ma, Wenqiao Zhang, Xianglin Qiu, Siqi Song, Xiaowei Huang, Fei Ma, Jimin Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17009">Contrastive Prompt Clustering for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Semantic Segmentation (WSSS) with image-level labels has gained attention for its cost-effectiveness. Most existing methods emphasize inter-class separation, often neglecting the shared semantics among related categories and lacking fine-grained discrimination. To address this, we propose Contrastive Prompt Clustering (CPC), a novel WSSS framework. CPC exploits Large Language Models (LLMs) to derive category clusters that encode intrinsic inter-class relationships, and further introduces a class-aware patch-level contrastive loss to enforce intra-class consistency and inter-class separation. This hierarchical design leverages clusters as coarse-grained semantic priors while preserving fine-grained boundaries, thereby reducing confusion among visually similar categories. Experiments on PASCAL VOC 2012 and MS COCO 2014 demonstrate that CPC surpasses existing state-of-the-art methods in WSSS.
<div id='section'>Paperid: <span id='pid'>302, <a href='https://arxiv.org/pdf/2412.20439.pdf' target='_blank'>https://arxiv.org/pdf/2412.20439.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wangyu Wu, Xianglin Qiu, Siqi Song, Zhenhong Chen, Xiaowei Huang, Fei Ma, Jimin Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20439">Image Augmentation Agent for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised semantic segmentation (WSSS) has achieved remarkable progress using only image-level labels. However, most existing WSSS methods focus on designing new network structures and loss functions to generate more accurate dense labels, overlooking the limitations imposed by fixed datasets, which can constrain performance improvements. We argue that more diverse trainable images provides WSSS richer information and help model understand more comprehensive semantic pattern. Therefore in this paper, we introduce a novel approach called Image Augmentation Agent (IAA) which shows that it is possible to enhance WSSS from data generation perspective. IAA mainly design an augmentation agent that leverages large language models (LLMs) and diffusion models to automatically generate additional images for WSSS. In practice, to address the instability in prompt generation by LLMs, we develop a prompt self-refinement mechanism. It allow LLMs to re-evaluate the rationality of generated prompts to produce more coherent prompts. Additionally, we insert an online filter into diffusion generation process to dynamically ensure the quality and balance of generated images. Experimental results show that our method significantly surpasses state-of-the-art WSSS approaches on the PASCAL VOC 2012 and MS COCO 2014 datasets.
<div id='section'>Paperid: <span id='pid'>303, <a href='https://arxiv.org/pdf/2412.13823.pdf' target='_blank'>https://arxiv.org/pdf/2412.13823.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wangyu Wu, Xianglin Qiu, Siqi Song, Xiaowei Huang, Fei Ma, Jimin Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13823">Prompt Categories Cluster for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Semantic Segmentation (WSSS), which leverages image-level labels, has garnered significant attention due to its cost-effectiveness. The previous methods mainly strengthen the inter-class differences to avoid class semantic ambiguity which may lead to erroneous activation. However, they overlook the positive function of some shared information between similar classes. Categories within the same cluster share some similar features. Allowing the model to recognize these features can further relieve the semantic ambiguity between these classes. To effectively identify and utilize this shared information, in this paper, we introduce a novel WSSS framework called Prompt Categories Clustering (PCC). Specifically, we explore the ability of Large Language Models (LLMs) to derive category clusters through prompts. These clusters effectively represent the intrinsic relationships between categories. By integrating this relational information into the training network, our model is able to better learn the hidden connections between categories. Experimental results demonstrate the effectiveness of our approach, showing its ability to enhance performance on the PASCAL VOC 2012 dataset and surpass existing state-of-the-art methods in WSSS.
<div id='section'>Paperid: <span id='pid'>304, <a href='https://arxiv.org/pdf/2407.10649.pdf' target='_blank'>https://arxiv.org/pdf/2407.10649.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wangyu Wu, Tianhong Dai, Zhenhong Chen, Xiaowei Huang, Jimin Xiao, Fei Ma, Renrong Ouyang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10649">Adaptive Patch Contrast for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Semantic Segmentation (WSSS) using only image-level labels has gained significant attention due to its cost-effectiveness. The typical framework involves using image-level labels as training data to generate pixel-level pseudo-labels with refinements. Recently, methods based on Vision Transformers (ViT) have demonstrated superior capabilities in generating reliable pseudo-labels, particularly in recognizing complete object regions, compared to CNN methods. However, current ViT-based approaches have some limitations in the use of patch embeddings, being prone to being dominated by certain abnormal patches, as well as many multi-stage methods being time-consuming and lengthy in training, thus lacking efficiency. Therefore, in this paper, we introduce a novel ViT-based WSSS method named \textit{Adaptive Patch Contrast} (APC) that significantly enhances patch embedding learning for improved segmentation effectiveness. APC utilizes an Adaptive-K Pooling (AKP) layer to address the limitations of previous max pooling selection methods. Additionally, we propose a Patch Contrastive Learning (PCL) to enhance patch embeddings, thereby further improving the final results. Furthermore, we improve upon the existing multi-stage training framework without CAM by transforming it into an end-to-end single-stage training approach, thereby enhancing training efficiency. The experimental results show that our approach is effective and efficient, outperforming other state-of-the-art WSSS methods on the PASCAL VOC 2012 and MS COCO 2014 dataset within a shorter training duration.
<div id='section'>Paperid: <span id='pid'>305, <a href='https://arxiv.org/pdf/2405.08911.pdf' target='_blank'>https://arxiv.org/pdf/2405.08911.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pavan Kumar Anasosalu Vasu, Hadi Pouransari, Fartash Faghri, Oncel Tuzel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.08911">CLIP with Quality Captions: A Strong Pretraining for Vision Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>CLIP models perform remarkably well on zero-shot classification and retrieval tasks. But recent studies have shown that learnt representations in CLIP are not well suited for dense prediction tasks like object detection, semantic segmentation or depth estimation. More recently, multi-stage training methods for CLIP models was introduced to mitigate the weak performance of CLIP on downstream tasks. In this work, we find that simply improving the quality of captions in image-text datasets improves the quality of CLIP's visual representations, resulting in significant improvement on downstream dense prediction vision tasks. In fact, we find that CLIP pretraining with good quality captions can surpass recent supervised, self-supervised and weakly supervised pretraining methods. We show that when CLIP model with ViT-B/16 as image encoder is trained on well aligned image-text pairs it obtains 12.1% higher mIoU and 11.5% lower RMSE on semantic segmentation and depth estimation tasks over recent state-of-the-art Masked Image Modeling (MIM) pretraining methods like Masked Autoencoder (MAE). We find that mobile architectures also benefit significantly from CLIP pretraining. A recent mobile vision architecture, MCi2, with CLIP pretraining obtains similar performance as Swin-L, pretrained on ImageNet-22k for semantic segmentation task while being 6.1$\times$ smaller. Moreover, we show that improving caption quality results in $10\times$ data efficiency when finetuning for dense prediction tasks.
<div id='section'>Paperid: <span id='pid'>306, <a href='https://arxiv.org/pdf/2509.18711.pdf' target='_blank'>https://arxiv.org/pdf/2509.18711.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ke Li, Di Wang, Ting Wang, Fuyu Dong, Yiming Zhang, Luyao Zhang, Xiangyu Wang, Shaofeng Li, Quan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18711">RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot Open-Vocabulary Visual Grounding in Remote Sensing Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Remote sensing visual grounding (RSVG) aims to localize objects in remote sensing images based on free-form natural language expressions. Existing approaches are typically constrained to closed-set vocabularies, limiting their applicability in open-world scenarios. While recent attempts to leverage generic foundation models for open-vocabulary RSVG, they overly rely on expensive high-quality datasets and time-consuming fine-tuning. To address these limitations, we propose \textbf{RSVG-ZeroOV}, a training-free framework that aims to explore the potential of frozen generic foundation models for zero-shot open-vocabulary RSVG. Specifically, RSVG-ZeroOV comprises three key stages: (i) Overview: We utilize a vision-language model (VLM) to obtain cross-attention\footnote[1]{In this paper, although decoder-only VLMs use self-attention over all tokens, we refer to the image-text interaction part as cross-attention to distinguish it from pure visual self-attention.}maps that capture semantic correlations between text queries and visual regions. (ii) Focus: By leveraging the fine-grained modeling priors of a diffusion model (DM), we fill in gaps in structural and shape information of objects, which are often overlooked by VLM. (iii) Evolve: A simple yet effective attention evolution module is introduced to suppress irrelevant activations, yielding purified segmentation masks over the referred objects. Without cumbersome task-specific training, RSVG-ZeroOV offers an efficient and scalable solution. Extensive experiments demonstrate that the proposed framework consistently outperforms existing weakly-supervised and zero-shot methods.
<div id='section'>Paperid: <span id='pid'>307, <a href='https://arxiv.org/pdf/2509.08126.pdf' target='_blank'>https://arxiv.org/pdf/2509.08126.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Houjian Yu, Zheming Zhou, Min Sun, Omid Ghasemalizadeh, Yuyin Sun, Cheng-Hao Kuo, Arnie Sen, Changhyun Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08126">Attribute-based Object Grounding and Robot Grasp Detection with Spatial Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enabling robots to grasp objects specified through natural language is essential for effective human-robot interaction, yet it remains a significant challenge. Existing approaches often struggle with open-form language expressions and typically assume unambiguous target objects without duplicates. Moreover, they frequently rely on costly, dense pixel-wise annotations for both object grounding and grasp configuration. We present Attribute-based Object Grounding and Robotic Grasping (OGRG), a novel framework that interprets open-form language expressions and performs spatial reasoning to ground target objects and predict planar grasp poses, even in scenes containing duplicated object instances. We investigate OGRG in two settings: (1) Referring Grasp Synthesis (RGS) under pixel-wise full supervision, and (2) Referring Grasp Affordance (RGA) using weakly supervised learning with only single-pixel grasp annotations. Key contributions include a bi-directional vision-language fusion module and the integration of depth information to enhance geometric reasoning, improving both grounding and grasping performance. Experiment results show that OGRG outperforms strong baselines in tabletop scenes with diverse spatial language instructions. In RGS, it operates at 17.59 FPS on a single NVIDIA RTX 2080 Ti GPU, enabling potential use in closed-loop or multi-object sequential grasping, while delivering superior grounding and grasp prediction accuracy compared to all the baselines considered. Under the weakly supervised RGA setting, OGRG also surpasses baseline grasp-success rates in both simulation and real-robot trials, underscoring the effectiveness of its spatial reasoning design. Project page: https://z.umn.edu/ogrg
<div id='section'>Paperid: <span id='pid'>308, <a href='https://arxiv.org/pdf/2508.00235.pdf' target='_blank'>https://arxiv.org/pdf/2508.00235.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Erin Rainville, Amirhossein Rasoulian, Hassan Rivaz, Yiming Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00235">Weakly Supervised Intracranial Aneurysm Detection and Segmentation in MR angiography via Multi-task UNet with Vesselness Prior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intracranial aneurysms (IAs) are abnormal dilations of cerebral blood vessels that, if ruptured, can lead to life-threatening consequences. However, their small size and soft contrast in radiological scans often make it difficult to perform accurate and efficient detection and morphological analyses, which are critical in the clinical care of the disorder. Furthermore, the lack of large public datasets with voxel-wise expert annotations pose challenges for developing deep learning algorithms to address the issues. Therefore, we proposed a novel weakly supervised 3D multi-task UNet that integrates vesselness priors to jointly perform aneurysm detection and segmentation in time-of-flight MR angiography (TOF-MRA). Specifically, to robustly guide IA detection and segmentation, we employ the popular Frangi's vesselness filter to derive soft cerebrovascular priors for both network input and an attention block to conduct segmentation from the decoder and detection from an auxiliary branch. We train our model on the Lausanne dataset with coarse ground truth segmentation, and evaluate it on the test set with refined labels from the same database. To further assess our model's generalizability, we also validate it externally on the ADAM dataset. Our results demonstrate the superior performance of the proposed technique over the SOTA techniques for aneurysm segmentation (Dice = 0.614, 95%HD =1.38mm) and detection (false positive rate = 1.47, sensitivity = 92.9%).
<div id='section'>Paperid: <span id='pid'>309, <a href='https://arxiv.org/pdf/2506.23519.pdf' target='_blank'>https://arxiv.org/pdf/2506.23519.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Qin, Runmin Cong, Gen Zhan, Yiting Liao, Sam Kwong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23519">From Sight to Insight: Unleashing Eye-Tracking in Weakly Supervised Video Salient Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The eye-tracking video saliency prediction (VSP) task and video salient object detection (VSOD) task both focus on the most attractive objects in video and show the result in the form of predictive heatmaps and pixel-level saliency masks, respectively. In practical applications, eye tracker annotations are more readily obtainable and align closely with the authentic visual patterns of human eyes. Therefore, this paper aims to introduce fixation information to assist the detection of video salient objects under weak supervision. On the one hand, we ponder how to better explore and utilize the information provided by fixation, and then propose a Position and Semantic Embedding (PSE) module to provide location and semantic guidance during the feature learning process. On the other hand, we achieve spatiotemporal feature modeling under weak supervision from the aspects of feature selection and feature contrast. A Semantics and Locality Query (SLQ) Competitor with semantic and locality constraints is designed to effectively select the most matching and accurate object query for spatiotemporal modeling. In addition, an Intra-Inter Mixed Contrastive (IIMC) model improves the spatiotemporal modeling capabilities under weak supervision by forming an intra-video and inter-video contrastive learning paradigm. Experimental results on five popular VSOD benchmarks indicate that our model outperforms other competitors on various evaluation metrics.
<div id='section'>Paperid: <span id='pid'>310, <a href='https://arxiv.org/pdf/2411.12615.pdf' target='_blank'>https://arxiv.org/pdf/2411.12615.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Yang, Nitish Mehta, Xiaoling Hu, Chao Chen, Chia-Ling Tsai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.12615">A Multimodal Approach Combining Structural and Cross-domain Textual Guidance for Weakly Supervised OCT Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate segmentation of Optical Coherence Tomography (OCT) images is crucial for diagnosing and monitoring retinal diseases. However, the labor-intensive nature of pixel-level annotation limits the scalability of supervised learning with large datasets. Weakly Supervised Semantic Segmentation (WSSS) provides a promising alternative by leveraging image-level labels. In this study, we propose a novel WSSS approach that integrates structural guidance with text-driven strategies to generate high-quality pseudo labels, significantly improving segmentation performance. In terms of visual information, our method employs two processing modules that exchange raw image features and structural features from OCT images, guiding the model to identify where lesions are likely to occur. In terms of textual information, we utilize large-scale pretrained models from cross-domain sources to implement label-informed textual guidance and synthetic descriptive integration with two textual processing modules that combine local semantic features with consistent synthetic descriptions. By fusing these visual and textual components within a multimodal framework, our approach enhances lesion localization accuracy. Experimental results on three OCT datasets demonstrate that our method achieves state-of-the-art performance, highlighting its potential to improve diagnostic accuracy and efficiency in medical imaging.
<div id='section'>Paperid: <span id='pid'>311, <a href='https://arxiv.org/pdf/2410.23834.pdf' target='_blank'>https://arxiv.org/pdf/2410.23834.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cosmin I. Bercea, Philippe C. Cattin, Julia A. Schnabel, Julia Wolleb
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.23834">Denoising Diffusion Models for Anomaly Localization in Medical Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This chapter explores anomaly localization in medical images using denoising diffusion models. After providing a brief methodological background of these models, including their application to image reconstruction and their conditioning using guidance mechanisms, we provide an overview of available datasets and evaluation metrics suitable for their application to anomaly localization in medical images. In this context, we discuss supervision schemes ranging from fully supervised segmentation to semi-supervised, weakly supervised, self-supervised, and unsupervised methods, and provide insights into the effectiveness and limitations of these approaches. Furthermore, we highlight open challenges in anomaly localization, including detection bias, domain shift, computational cost, and model interpretability. Our goal is to provide an overview of the current state of the art in the field, outline research gaps, and highlight the potential of diffusion models for robust anomaly localization in medical images.
<div id='section'>Paperid: <span id='pid'>312, <a href='https://arxiv.org/pdf/2404.11036.pdf' target='_blank'>https://arxiv.org/pdf/2404.11036.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Paras Sheth, Tharindu Kumarage, Raha Moraffah, Aman Chadha, Huan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.11036">Cross-Platform Hate Speech Detection with Weakly Supervised Causal Disentanglement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Content moderation faces a challenging task as social media's ability to spread hate speech contrasts with its role in promoting global connectivity. With rapidly evolving slang and hate speech, the adaptability of conventional deep learning to the fluid landscape of online dialogue remains limited. In response, causality inspired disentanglement has shown promise by segregating platform specific peculiarities from universal hate indicators. However, its dependency on available ground truth target labels for discerning these nuances faces practical hurdles with the incessant evolution of platforms and the mutable nature of hate speech. Using confidence based reweighting and contrastive regularization, this study presents HATE WATCH, a novel framework of weakly supervised causal disentanglement that circumvents the need for explicit target labeling and effectively disentangles input features into invariant representations of hate. Empirical validation across platforms two with target labels and two without positions HATE WATCH as a novel method in cross platform hate speech detection with superior performance. HATE WATCH advances scalable content moderation techniques towards developing safer online communities.
<div id='section'>Paperid: <span id='pid'>313, <a href='https://arxiv.org/pdf/2402.19144.pdf' target='_blank'>https://arxiv.org/pdf/2402.19144.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xueying Jiang, Sheng Jin, Lewei Lu, Xiaoqin Zhang, Shijian Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.19144">Weakly Supervised Monocular 3D Detection with a Single-View Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monocular 3D detection (M3D) aims for precise 3D object localization from a single-view image which usually involves labor-intensive annotation of 3D detection boxes. Weakly supervised M3D has recently been studied to obviate the 3D annotation process by leveraging many existing 2D annotations, but it often requires extra training data such as LiDAR point clouds or multi-view images which greatly degrades its applicability and usability in various applications. We propose SKD-WM3D, a weakly supervised monocular 3D detection framework that exploits depth information to achieve M3D with a single-view image exclusively without any 3D annotations or other training data. One key design in SKD-WM3D is a self-knowledge distillation framework, which transforms image features into 3D-like representations by fusing depth information and effectively mitigates the inherent depth ambiguity in monocular scenarios with little computational overhead in inference. In addition, we design an uncertainty-aware distillation loss and a gradient-targeted transfer modulation strategy which facilitate knowledge acquisition and knowledge transfer, respectively. Extensive experiments show that SKD-WM3D surpasses the state-of-the-art clearly and is even on par with many fully supervised methods.
<div id='section'>Paperid: <span id='pid'>314, <a href='https://arxiv.org/pdf/2402.08697.pdf' target='_blank'>https://arxiv.org/pdf/2402.08697.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David C. Oluigboa, Bikash Santra, Tejas Sudharshan Mathai, Pritam Mukherjee, Jianfei Liu, Abhishek Jha, Mayank Patel, Karel Pacak, Ronald M. Summers
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.08697">Weakly Supervised Detection of Pheochromocytomas and Paragangliomas in CT</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pheochromocytomas and Paragangliomas (PPGLs) are rare adrenal and extra-adrenal tumors which have the potential to metastasize. For the management of patients with PPGLs, CT is the preferred modality of choice for precise localization and estimation of their progression. However, due to the myriad variations in size, morphology, and appearance of the tumors in different anatomical regions, radiologists are posed with the challenge of accurate detection of PPGLs. Since clinicians also need to routinely measure their size and track their changes over time across patient visits, manual demarcation of PPGLs is quite a time-consuming and cumbersome process. To ameliorate the manual effort spent for this task, we propose an automated method to detect PPGLs in CT studies via a proxy segmentation task. As only weak annotations for PPGLs in the form of prospectively marked 2D bounding boxes on an axial slice were available, we extended these 2D boxes into weak 3D annotations and trained a 3D full-resolution nnUNet model to directly segment PPGLs. We evaluated our approach on a dataset consisting of chest-abdomen-pelvis CTs of 255 patients with confirmed PPGLs. We obtained a precision of 70% and sensitivity of 64.1% with our proposed approach when tested on 53 CT studies. Our findings highlight the promising nature of detecting PPGLs via segmentation, and furthers the state-of-the-art in this exciting yet challenging area of rare cancer management.
<div id='section'>Paperid: <span id='pid'>315, <a href='https://arxiv.org/pdf/2402.00175.pdf' target='_blank'>https://arxiv.org/pdf/2402.00175.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Sheng, Tejas Sudharshan Mathai, Alexander Shieh, Ronald M. Summers
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.00175">Weakly-Supervised Detection of Bone Lesions in CT</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The skeletal region is one of the common sites of metastatic spread of cancer in the breast and prostate. CT is routinely used to measure the size of lesions in the bones. However, they can be difficult to spot due to the wide variations in their sizes, shapes, and appearances. Precise localization of such lesions would enable reliable tracking of interval changes (growth, shrinkage, or unchanged status). To that end, an automated technique to detect bone lesions is highly desirable. In this pilot work, we developed a pipeline to detect bone lesions (lytic, blastic, and mixed) in CT volumes via a proxy segmentation task. First, we used the bone lesions that were prospectively marked by radiologists in a few 2D slices of CT volumes and converted them into weak 3D segmentation masks. Then, we trained a 3D full-resolution nnUNet model using these weak 3D annotations to segment the lesions and thereby detected them. Our automated method detected bone lesions in CT with a precision of 96.7% and recall of 47.3% despite the use of incomplete and partial training data. To the best of our knowledge, we are the first to attempt the direct detection of bone lesions in CT via a proxy segmentation task.
<div id='section'>Paperid: <span id='pid'>316, <a href='https://arxiv.org/pdf/2506.18042.pdf' target='_blank'>https://arxiv.org/pdf/2506.18042.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongdong Meng, Sheng Li, Hao Wu, Suqing Tian, Wenjun Ma, Guoping Wang, Xueqing Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18042">CmFNet: Cross-modal Fusion Network for Weakly-supervised Segmentation of Medical Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate automatic medical image segmentation relies on high-quality, dense annotations, which are costly and time-consuming. Weakly supervised learning provides a more efficient alternative by leveraging sparse and coarse annotations instead of dense, precise ones. However, segmentation performance degradation and overfitting caused by sparse annotations remain key challenges. To address these issues, we propose CmFNet, a novel 3D weakly supervised cross-modal medical image segmentation approach. CmFNet consists of three main components: a modality-specific feature learning network, a cross-modal feature learning network, and a hybrid-supervised learning strategy. Specifically, the modality-specific feature learning network and the cross-modal feature learning network effectively integrate complementary information from multi-modal images, enhancing shared features across modalities to improve segmentation performance. Additionally, the hybrid-supervised learning strategy guides segmentation through scribble supervision, intra-modal regularization, and inter-modal consistency, modeling spatial and contextual relationships while promoting feature alignment. Our approach effectively mitigates overfitting, delivering robust segmentation results. It excels in segmenting both challenging small tumor regions and common anatomical structures. Extensive experiments on a clinical cross-modal nasopharyngeal carcinoma (NPC) dataset (including CT and MR imaging) and the publicly available CT Whole Abdominal Organ dataset (WORD) show that our approach outperforms state-of-the-art weakly supervised methods. In addition, our approach also outperforms fully supervised methods when full annotation is used. Our approach can facilitate clinical therapy and benefit various specialists, including physicists, radiologists, pathologists, and oncologists.
<div id='section'>Paperid: <span id='pid'>317, <a href='https://arxiv.org/pdf/2506.14912.pdf' target='_blank'>https://arxiv.org/pdf/2506.14912.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dyah Adila, Shuai Zhang, Boran Han, Bonan Min, Yuyang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14912">CrEst: Credibility Estimation for Contexts in LLMs via Weak Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of contextual information has significantly enhanced the performance of large language models (LLMs) on knowledge-intensive tasks. However, existing methods often overlook a critical challenge: the credibility of context documents can vary widely, potentially leading to the propagation of unreliable information. In this paper, we introduce CrEst, a novel weakly supervised framework for assessing the credibility of context documents during LLM inference--without requiring manual annotations. Our approach is grounded in the insight that credible documents tend to exhibit higher semantic coherence with other credible documents, enabling automated credibility estimation through inter-document agreement. To incorporate credibility into LLM inference, we propose two integration strategies: a black-box approach for models without access to internal weights or activations, and a white-box method that directly modifies attention mechanisms. Extensive experiments across three model architectures and five datasets demonstrate that CrEst consistently outperforms strong baselines, achieving up to a 26.86% improvement in accuracy and a 3.49% increase in F1 score. Further analysis shows that CrEst maintains robust performance even under high-noise conditions.
<div id='section'>Paperid: <span id='pid'>318, <a href='https://arxiv.org/pdf/2505.01809.pdf' target='_blank'>https://arxiv.org/pdf/2505.01809.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoqi Li, Jiaming Liu, Nuowei Han, Liang Heng, Yandong Guo, Hao Dong, Yang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01809">3DWG: 3D Weakly Supervised Visual Grounding via Category and Instance-Level Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The 3D weakly-supervised visual grounding task aims to localize oriented 3D boxes in point clouds based on natural language descriptions without requiring annotations to guide model learning. This setting presents two primary challenges: category-level ambiguity and instance-level complexity. Category-level ambiguity arises from representing objects of fine-grained categories in a highly sparse point cloud format, making category distinction challenging. Instance-level complexity stems from multiple instances of the same category coexisting in a scene, leading to distractions during grounding. To address these challenges, we propose a novel weakly-supervised grounding approach that explicitly differentiates between categories and instances. In the category-level branch, we utilize extensive category knowledge from a pre-trained external detector to align object proposal features with sentence-level category features, thereby enhancing category awareness. In the instance-level branch, we utilize spatial relationship descriptions from language queries to refine object proposal features, ensuring clear differentiation among objects. These designs enable our model to accurately identify target-category objects while distinguishing instances within the same category. Compared to previous methods, our approach achieves state-of-the-art performance on three widely used benchmarks: Nr3D, Sr3D, and ScanRef.
<div id='section'>Paperid: <span id='pid'>319, <a href='https://arxiv.org/pdf/2402.03025.pdf' target='_blank'>https://arxiv.org/pdf/2402.03025.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanyi Wang, Wei Tang, Haifeng Sun, Zirui Zhuang, Xiaoyuan Fu, Jingyu Wang, Qi Qi, Jianxin Liao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.03025">Understanding and Guiding Weakly Supervised Entity Alignment with Potential Isomorphism Propagation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Entity Alignment (EA) is the task of identifying equivalent entities across diverse knowledge graphs (KGs) using only a limited number of seed alignments. Despite substantial advances in aggregation-based weakly supervised EA, the underlying mechanisms in this setting remain unexplored. In this paper, we present a propagation perspective to analyze weakly supervised EA and explain the existing aggregation-based EA models. Our theoretical analysis reveals that these models essentially seek propagation operators for pairwise entity similarities. We further prove that, despite the structural heterogeneity of different KGs, the potentially aligned entities within aggregation-based EA models have isomorphic subgraphs, which is the core premise of EA but has not been investigated. Leveraging this insight, we introduce a potential isomorphism propagation operator to enhance the propagation of neighborhood information across KGs. We develop a general EA framework, PipEA, incorporating this operator to improve the accuracy of every type of aggregation-based model without altering the learning process. Extensive experiments substantiate our theoretical findings and demonstrate PipEA's significant performance gains over state-of-the-art weakly supervised EA methods. Our work not only advances the field but also enhances our comprehension of aggregation-based weakly supervised EA.
<div id='section'>Paperid: <span id='pid'>320, <a href='https://arxiv.org/pdf/2511.12020.pdf' target='_blank'>https://arxiv.org/pdf/2511.12020.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xianglong Shi, Silin Cheng, Sirui Zhao, Yunhan Jiang, Enhong Chen, Yang Liu, Sebastien Ourselin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.12020">LIHE: Linguistic Instance-Split Hyperbolic-Euclidean Framework for Generalized Weakly-Supervised Referring Expression Comprehension</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing Weakly-Supervised Referring Expression Comprehension (WREC) methods, while effective, are fundamentally limited by a one-to-one mapping assumption, hindering their ability to handle expressions corresponding to zero or multiple targets in realistic scenarios. To bridge this gap, we introduce the Weakly-Supervised Generalized Referring Expression Comprehension task (WGREC), a more practical paradigm that handles expressions with variable numbers of referents. However, extending WREC to WGREC presents two fundamental challenges: supervisory signal ambiguity, where weak image-level supervision is insufficient for training a model to infer the correct number and identity of referents, and semantic representation collapse, where standard Euclidean similarity forces hierarchically-related concepts into non-discriminative clusters, blurring categorical boundaries. To tackle these challenges, we propose a novel WGREC framework named Linguistic Instance-Split Hyperbolic-Euclidean (LIHE), which operates in two stages. The first stage, Referential Decoupling, predicts the number of target objects and decomposes the complex expression into simpler sub-expressions. The second stage, Referent Grounding, then localizes these sub-expressions using HEMix, our innovative hybrid similarity module that synergistically combines the precise alignment capabilities of Euclidean proximity with the hierarchical modeling strengths of hyperbolic geometry. This hybrid approach effectively prevents semantic collapse while preserving fine-grained distinctions between related concepts. Extensive experiments demonstrate LIHE establishes the first effective weakly supervised WGREC baseline on gRefCOCO and Ref-ZOM, while HEMix achieves consistent improvements on standard REC benchmarks, improving IoU@0.5 by up to 2.5\%. The code is available at https://anonymous.4open.science/r/LIHE.
<div id='section'>Paperid: <span id='pid'>321, <a href='https://arxiv.org/pdf/2505.22230.pdf' target='_blank'>https://arxiv.org/pdf/2505.22230.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhisong Wang, Yiwen Ye, Ziyang Chen, Yong Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22230">Enjoying Information Dividend: Gaze Track-based Medical Weakly Supervised Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation (WSSS) in medical imaging struggles with effectively using sparse annotations. One promising direction for WSSS leverages gaze annotations, captured via eye trackers that record regions of interest during diagnostic procedures. However, existing gaze-based methods, such as GazeMedSeg, do not fully exploit the rich information embedded in gaze data. In this paper, we propose GradTrack, a framework that utilizes physicians' gaze track, including fixation points, durations, and temporal order, to enhance WSSS performance. GradTrack comprises two key components: Gaze Track Map Generation and Track Attention, which collaboratively enable progressive feature refinement through multi-level gaze supervision during the decoding process. Experiments on the Kvasir-SEG and NCI-ISBI datasets demonstrate that GradTrack consistently outperforms existing gaze-based methods, achieving Dice score improvements of 3.21\% and 2.61\%, respectively. Moreover, GradTrack significantly narrows the performance gap with fully supervised models such as nnUNet.
<div id='section'>Paperid: <span id='pid'>322, <a href='https://arxiv.org/pdf/2408.12814.pdf' target='_blank'>https://arxiv.org/pdf/2408.12814.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhisong Wang, Yiwen Ye, Ziyang Chen, Minglei Shu, Yanning Zhang, Yong Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.12814">From Few to More: Scribble-based Medical Image Segmentation via Masked Context Modeling and Continuous Pseudo Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scribble-based weakly supervised segmentation methods have shown promising results in medical image segmentation, significantly reducing annotation costs. However, existing approaches often rely on auxiliary tasks to enforce semantic consistency and use hard pseudo labels for supervision, overlooking the unique challenges faced by models trained with sparse annotations. These models must predict pixel-wise segmentation maps from limited data, making it crucial to handle varying levels of annotation richness effectively. In this paper, we propose MaCo, a weakly supervised model designed for medical image segmentation, based on the principle of "from few to more." MaCo leverages Masked Context Modeling (MCM) and Continuous Pseudo Labels (CPL). MCM employs an attention-based masking strategy to perturb the input image, ensuring that the model's predictions align with those of the original image. CPL converts scribble annotations into continuous pixel-wise labels by applying an exponential decay function to distance maps, producing confidence maps that represent the likelihood of each pixel belonging to a specific category, rather than relying on hard pseudo labels. We evaluate MaCo on three public datasets, comparing it with other weakly supervised methods. Our results show that MaCo outperforms competing methods across all datasets, establishing a new record in weakly supervised medical image segmentation.
<div id='section'>Paperid: <span id='pid'>323, <a href='https://arxiv.org/pdf/2406.11431.pdf' target='_blank'>https://arxiv.org/pdf/2406.11431.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenkai Yang, Shiqi Shen, Guangyao Shen, Wei Yao, Yong Liu, Zhi Gong, Yankai Lin, Ji-Rong Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.11431">Super(ficial)-alignment: Strong Models May Deceive Weak Models in Weak-to-Strong Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Superalignment, where humans act as weak supervisors for superhuman models, has become a crucial problem with the rapid development of Large Language Models (LLMs). Recent work has preliminarily studied this problem by using weak models to supervise strong models, and discovered that weakly supervised strong students can consistently outperform weak teachers towards the alignment target, leading to a weak-to-strong generalization phenomenon. However, we are concerned that behind such a promising phenomenon, whether there exists an issue of weak-to-strong deception, where strong models deceive weak models by exhibiting well-aligned in areas known to weak models but producing misaligned behaviors in cases weak models do not know. We take an initial step towards exploring this security issue in a specific but realistic multi-objective alignment case, where there may be some alignment targets conflicting with each other (e.g., helpfulness v.s. harmlessness). We aim to explore whether, in such cases, strong models might deliberately make mistakes in areas known to them but unknown to weak models within one alignment dimension, in exchange for a higher reward in another dimension. Through extensive experiments in both the reward modeling and preference optimization scenarios, we find: (1) The weak-to-strong deception phenomenon exists across all settings. (2) The deception intensifies as the capability gap between weak and strong models increases. (3) Bootstrapping with an intermediate model can mitigate the deception to some extent, though its effectiveness remains limited. Our work highlights the urgent need to pay more attention to the true reliability of superalignment.
<div id='section'>Paperid: <span id='pid'>324, <a href='https://arxiv.org/pdf/2404.05997.pdf' target='_blank'>https://arxiv.org/pdf/2404.05997.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junlin Hou, Jilan Xu, Hao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.05997">Concept-Attention Whitening for Interpretable Skin Lesion Diagnosis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The black-box nature of deep learning models has raised concerns about their interpretability for successful deployment in real-world clinical applications. To address the concerns, eXplainable Artificial Intelligence (XAI) aims to provide clear and understandable explanations of the decision-making process. In the medical domain, concepts such as attributes of lesions or abnormalities serve as key evidence for deriving diagnostic results. Existing concept-based models mainly depend on concepts that appear independently and require fine-grained concept annotations such as bounding boxes. However, a medical image usually contains multiple concepts, and the fine-grained concept annotations are difficult to acquire. In this paper, we aim to interpret representations in deep neural networks by aligning the axes of the latent space with known concepts of interest. We propose a novel Concept-Attention Whitening (CAW) framework for interpretable skin lesion diagnosis. CAW is comprised of a disease diagnosis branch and a concept alignment branch. In the former branch, we train a convolutional neural network (CNN) with an inserted CAW layer to perform skin lesion diagnosis. The CAW layer decorrelates features and aligns image features to conceptual meanings via an orthogonal matrix. In the latter branch, the orthogonal matrix is calculated under the guidance of the concept attention mask. We particularly introduce a weakly-supervised concept mask generator that only leverages coarse concept labels for filtering local regions that are relevant to certain concepts, improving the optimization of the orthogonal matrix. Extensive experiments on two public skin lesion diagnosis datasets demonstrated that CAW not only enhanced interpretability but also maintained a state-of-the-art diagnostic performance.
<div id='section'>Paperid: <span id='pid'>325, <a href='https://arxiv.org/pdf/2511.11407.pdf' target='_blank'>https://arxiv.org/pdf/2511.11407.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Manyu Li, Ruian He, Chenxi Ma, Weimin Tan, Bo Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.11407">MicroVQA++: High-Quality Microscopy Reasoning Dataset with Weakly Supervised Graphs for Multimodal Large Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal Large Language Models are increasingly applied to biomedical imaging, yet scientific reasoning for microscopy remains limited by the scarcity of large-scale, high-quality training data. We introduce MicroVQA++, a three-stage, large-scale and high-quality microscopy VQA corpus derived from the BIOMEDICA archive. Stage one bootstraps supervision from expert-validated figure-caption pairs sourced from peer-reviewed articles. Stage two applies HiCQA-Graph, a novel heterogeneous graph over images, captions, and QAs that fuses NLI-based textual entailment, CLIP-based vision-language alignment, and agent signals to identify and filter inconsistent samples. Stage three uses a MultiModal Large Language Model (MLLM) agent to generate multiple-choice questions (MCQ) followed by human screening. The resulting release comprises a large training split and a human-checked test split whose Bloom's level hard-sample distribution exceeds the MicroVQA benchmark. Our work delivers (i) a quality-controlled dataset that couples expert literature with graph-based filtering and human refinement; (ii) HiCQA-Graph, the first graph that jointly models (image, caption, QA) for cross-modal consistency filtering; (iii) evidence that careful data construction enables 4B-scale MLLMs to reach competitive microscopy reasoning performance (e.g., GPT-5) and achieve state-of-the-art performance among open-source MLLMs. Code and dataset will be released after the review process concludes.
<div id='section'>Paperid: <span id='pid'>326, <a href='https://arxiv.org/pdf/2511.04035.pdf' target='_blank'>https://arxiv.org/pdf/2511.04035.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongji Gao, Chenda Liao, Changliang Liu, Matthew Wiesner, Leibny Paola Garcia, Daniel Povey, Sanjeev Khudanpur, Jian Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.04035">WST: Weakly Supervised Transducer for Automatic Speech Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Recurrent Neural Network-Transducer (RNN-T) is widely adopted in end-to-end (E2E) automatic speech recognition (ASR) tasks but depends heavily on large-scale, high-quality annotated data, which are often costly and difficult to obtain. To mitigate this reliance, we propose a Weakly Supervised Transducer (WST), which integrates a flexible training graph designed to robustly handle errors in the transcripts without requiring additional confidence estimation or auxiliary pre-trained models. Empirical evaluations on synthetic and industrial datasets reveal that WST effectively maintains performance even with transcription error rates of up to 70%, consistently outperforming existing Connectionist Temporal Classification (CTC)-based weakly supervised approaches, such as Bypass Temporal Classification (BTC) and Omni-Temporal Classification (OTC). These results demonstrate the practical utility and robustness of WST in realistic ASR settings. The implementation will be publicly available.
<div id='section'>Paperid: <span id='pid'>327, <a href='https://arxiv.org/pdf/2510.05196.pdf' target='_blank'>https://arxiv.org/pdf/2510.05196.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daqian Shi, Xiaolei Diao, Jinge Wu, Honghan Wu, Xiongfeng Tang, Felix Naughton, Paulina Bondaronek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05196">Graph-based LLM over Semi-Structured Population Data for Dynamic Policy Response</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Timely and accurate analysis of population-level data is crucial for effective decision-making during public health emergencies such as the COVID-19 pandemic. However, the massive input of semi-structured data, including structured demographic information and unstructured human feedback, poses significant challenges to conventional analysis methods. Manual expert-driven assessments, though accurate, are inefficient, while standard NLP pipelines often require large task-specific labeled datasets and struggle with generalization across diverse domains. To address these challenges, we propose a novel graph-based reasoning framework that integrates large language models with structured demographic attributes and unstructured public feedback in a weakly supervised pipeline. The proposed approach dynamically models evolving citizen needs into a need-aware graph, enabling population-specific analyses based on key features such as age, gender, and the Index of Multiple Deprivation. It generates interpretable insights to inform responsive health policy decision-making. We test our method using a real-world dataset, and preliminary experimental results demonstrate its feasibility. This approach offers a scalable solution for intelligent population health monitoring in resource-constrained clinical and governmental settings.
<div id='section'>Paperid: <span id='pid'>328, <a href='https://arxiv.org/pdf/2501.06038.pdf' target='_blank'>https://arxiv.org/pdf/2501.06038.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tsui Qin Mok, Shuyong Gao, Haozhe Xing, Miaoyang He, Yan Wang, Wenqiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.06038">A Holistically Point-guided Text Framework for Weakly-Supervised Camouflaged Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-Supervised Camouflaged Object Detection (WSCOD) has gained popularity for its promise to train models with weak labels to segment objects that visually blend into their surroundings. Recently, some methods using sparsely-annotated supervision shown promising results through scribbling in WSCOD, while point-text supervision remains underexplored. Hence, this paper introduces a novel holistically point-guided text framework for WSCOD by decomposing into three phases: segment, choose, train. Specifically, we propose Point-guided Candidate Generation (PCG), where the point's foreground serves as a correction for the text path to explicitly correct and rejuvenate the loss detection object during the mask generation process (SEGMENT). We also introduce a Qualified Candidate Discriminator (QCD) to choose the optimal mask from a given text prompt using CLIP (CHOOSE), and employ the chosen pseudo mask for training with a self-supervised Vision Transformer (TRAIN). Additionally, we developed a new point-supervised dataset (P2C-COD) and a text-supervised dataset (T-COD). Comprehensive experiments on four benchmark datasets demonstrate our method outperforms state-of-the-art methods by a large margin, and also outperforms some existing fully-supervised camouflaged object detection methods.
<div id='section'>Paperid: <span id='pid'>329, <a href='https://arxiv.org/pdf/2408.08092.pdf' target='_blank'>https://arxiv.org/pdf/2408.08092.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiming Xia, Hongwei Lin, Wei Ye, Hai Wu, Yadan Luo, Cheng Wang, Chenglu Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.08092">SC3D: Label-Efficient Outdoor 3D Object Detection via Single Click Annotation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LiDAR-based outdoor 3D object detection has received widespread attention. However, training 3D detectors from the LiDAR point cloud typically relies on expensive bounding box annotations. This paper presents SC3D, an innovative label-efficient method requiring only a single coarse click on the bird's eye view of the 3D point cloud for each frame. A key challenge here is the absence of complete geometric descriptions of the target objects from such simple click annotations. To address this issue, our proposed SC3D adopts a progressive pipeline. Initially, we design a mixed pseudo-label generation module that expands limited click annotations into a mixture of bounding box and semantic mask supervision. Next, we propose a mix-supervised teacher model, enabling the detector to learn mixed supervision information. Finally, we introduce a mixed-supervised student network that leverages the teacher model's generalization ability to learn unclicked instances.Experimental results on the widely used nuScenes and KITTI datasets demonstrate that our SC3D with only coarse clicks, which requires only 0.2% annotation cost, achieves state-of-the-art performance compared to weakly-supervised 3D detection methods.The code will be made publicly available.
<div id='section'>Paperid: <span id='pid'>330, <a href='https://arxiv.org/pdf/2403.11463.pdf' target='_blank'>https://arxiv.org/pdf/2403.11463.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaolei Tan, Jianhuang Lai, Wei-Shi Zheng, Jian-Fang Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11463">Siamese Learning with Joint Alignment and Regression for Weakly-Supervised Video Paragraph Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Paragraph Grounding (VPG) is an emerging task in video-language understanding, which aims at localizing multiple sentences with semantic relations and temporal order from an untrimmed video. However, existing VPG approaches are heavily reliant on a considerable number of temporal labels that are laborious and time-consuming to acquire. In this work, we introduce and explore Weakly-Supervised Video Paragraph Grounding (WSVPG) to eliminate the need of temporal annotations. Different from previous weakly-supervised grounding frameworks based on multiple instance learning or reconstruction learning for two-stage candidate ranking, we propose a novel siamese learning framework that jointly learns the cross-modal feature alignment and temporal coordinate regression without timestamp labels to achieve concise one-stage localization for WSVPG. Specifically, we devise a Siamese Grounding TRansformer (SiamGTR) consisting of two weight-sharing branches for learning complementary supervision. An Augmentation Branch is utilized for directly regressing the temporal boundaries of a complete paragraph within a pseudo video, and an Inference Branch is designed to capture the order-guided feature correspondence for localizing multiple sentences in a normal video. We demonstrate by extensive experiments that our paradigm has superior practicability and flexibility to achieve efficient weakly-supervised or semi-supervised learning, outperforming state-of-the-art methods trained with the same or stronger supervision.
<div id='section'>Paperid: <span id='pid'>331, <a href='https://arxiv.org/pdf/2510.00072.pdf' target='_blank'>https://arxiv.org/pdf/2510.00072.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenhui Xu, Fuxun Yu, Michael J. Bianco, Jacob Kovarskiy, Raphael Tang, Qi Zhang, Zirui Xu, Will LeVine, Brandon Dubbs, Heming Liao, Cassandra Burgess, Suvam Bag, Jay Patravali, Rupanjali Kukal, Mikael Figueroa, Rishi Madhok, Nikolaos Karianakis, Jinjun Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00072">Geo-R1: Unlocking VLM Geospatial Reasoning with Cross-View Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Geo-R1, a reasoning-centric post-training framework that unlocks geospatial reasoning in vision-language models by combining thinking scaffolding and elevating. In the scaffolding stage, Geo-R1 instills a ``geospatial thinking paradigm" via supervised fine-tuning on synthetic chain-of-thought exemplars, enabling models to connect visual cues with geographic priors without costly human reasoning annotations. In the elevating stage, it uses GRPO-based reinforcement learning on a weakly-supervised cross-view pairing proxy. This design supplies a verifiable and scalable reward signal: teaching models to capture and reconcile features across modalities, and harnessing reasoning for accurate prediction. Geo-R1 extends geospatial modeling from domain pretraining / supervised finetuning to reasoning-first post-training, and achieves state-of-the-art performance across various geospatial reasoning benchmarks. Our model is available at https://huggingface.co/miniHui/Geo-R1.
<div id='section'>Paperid: <span id='pid'>332, <a href='https://arxiv.org/pdf/2509.01878.pdf' target='_blank'>https://arxiv.org/pdf/2509.01878.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Scarlett Raine, Tobias Fischer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01878">AI-Driven Marine Robotics: Emerging Trends in Underwater Perception and Ecosystem Monitoring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Marine ecosystems face increasing pressure due to climate change, driving the need for scalable, AI-powered monitoring solutions. This paper examines the rapid emergence of underwater AI as a major research frontier and analyzes the factors that have transformed marine perception from a niche application into a catalyst for AI innovation. We identify three convergent drivers: environmental necessity for ecosystem-scale monitoring, democratization of underwater datasets through citizen science platforms, and researcher migration from saturated terrestrial computer vision domains. Our analysis reveals how unique underwater challenges - turbidity, cryptic species detection, expert annotation bottlenecks, and cross-ecosystem generalization - are driving fundamental advances in weakly supervised learning, open-set recognition, and robust perception under degraded conditions. We survey emerging trends in datasets, scene understanding and 3D reconstruction, highlighting the paradigm shift from passive observation toward AI-driven, targeted intervention capabilities. The paper demonstrates how underwater constraints are pushing the boundaries of foundation models, self-supervised learning, and perception, with methodological innovations that extend far beyond marine applications to benefit general computer vision, robotics, and environmental monitoring.
<div id='section'>Paperid: <span id='pid'>333, <a href='https://arxiv.org/pdf/2506.10233.pdf' target='_blank'>https://arxiv.org/pdf/2506.10233.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ana Lawry Aguila, Peirong Liu, Oula Puonti, Juan Eugenio Iglesias
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10233">Conditional diffusion models for guided anomaly detection in brain images using fluid-driven anomaly randomization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Supervised machine learning has enabled accurate pathology detection in brain MRI, but requires training data from diseased subjects that may not be readily available in some scenarios, for example, in the case of rare diseases. Reconstruction-based unsupervised anomaly detection, in particular using diffusion models, has gained popularity in the medical field as it allows for training on healthy images alone, eliminating the need for large disease-specific cohorts. These methods assume that a model trained on normal data cannot accurately represent or reconstruct anomalies. However, this assumption often fails with models failing to reconstruct healthy tissue or accurately reconstruct abnormal regions i.e., failing to remove anomalies. In this work, we introduce a novel conditional diffusion model framework for anomaly detection and healthy image reconstruction in brain MRI. Our weakly supervised approach integrates synthetically generated pseudo-pathology images into the modeling process to better guide the reconstruction of healthy images. To generate these pseudo-pathologies, we apply fluid-driven anomaly randomization to augment real pathology segmentation maps from an auxiliary dataset, ensuring that the synthetic anomalies are both realistic and anatomically coherent. We evaluate our model's ability to detect pathology, using both synthetic anomaly datasets and real pathology from the ATLAS dataset. In our extensive experiments, our model: (i) consistently outperforms variational autoencoders, and conditional and unconditional latent diffusion; and (ii) surpasses on most datasets, the performance of supervised inpainting methods with access to paired diseased/healthy images.
<div id='section'>Paperid: <span id='pid'>334, <a href='https://arxiv.org/pdf/2503.12068.pdf' target='_blank'>https://arxiv.org/pdf/2503.12068.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingchen Tang, Lei Fan, Maurice Pagnucco, Yang Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12068">Prototype-Based Image Prompting for Weakly Supervised Histopathological Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised image segmentation with image-level labels has drawn attention due to the high cost of pixel-level annotations. Traditional methods using Class Activation Maps (CAMs) often highlight only the most discriminative regions, leading to incomplete masks. Recent approaches that introduce textual information struggle with histopathological images due to inter-class homogeneity and intra-class heterogeneity. In this paper, we propose a prototype-based image prompting framework for histopathological image segmentation. It constructs an image bank from the training set using clustering, extracting multiple prototype features per class to capture intra-class heterogeneity. By designing a matching loss between input features and class-specific prototypes using contrastive learning, our method addresses inter-class homogeneity and guides the model to generate more accurate CAMs. Experiments on four datasets (LUAD-HistoSeg, BCSS-WSSS, GCSS, and BCSS) show that our method outperforms existing weakly supervised segmentation approaches, setting new benchmarks in histopathological image segmentation.
<div id='section'>Paperid: <span id='pid'>335, <a href='https://arxiv.org/pdf/2412.12791.pdf' target='_blank'>https://arxiv.org/pdf/2412.12791.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiping Ge, Qiang Chen, Zhiwei Jiang, Yafeng Yin, Liu Qin, Ziyao Chen, Qing Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12791">Implicit Location-Caption Alignment via Complementary Masking for Weakly-Supervised Dense Video Captioning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-Supervised Dense Video Captioning (WSDVC) aims to localize and describe all events of interest in a video without requiring annotations of event boundaries. This setting poses a great challenge in accurately locating the temporal location of event, as the relevant supervision is unavailable. Existing methods rely on explicit alignment constraints between event locations and captions, which involve complex event proposal procedures during both training and inference. To tackle this problem, we propose a novel implicit location-caption alignment paradigm by complementary masking, which simplifies the complex event proposal and localization process while maintaining effectiveness. Specifically, our model comprises two components: a dual-mode video captioning module and a mask generation module. The dual-mode video captioning module captures global event information and generates descriptive captions, while the mask generation module generates differentiable positive and negative masks for localizing the events. These masks enable the implicit alignment of event locations and captions by ensuring that captions generated from positively and negatively masked videos are complementary, thereby forming a complete video description. In this way, even under weak supervision, the event location and event caption can be aligned implicitly. Extensive experiments on the public datasets demonstrate that our method outperforms existing weakly-supervised methods and achieves competitive results compared to fully-supervised methods.
<div id='section'>Paperid: <span id='pid'>336, <a href='https://arxiv.org/pdf/2411.11287.pdf' target='_blank'>https://arxiv.org/pdf/2411.11287.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Scarlett Raine, Frederic Maire, Niko Suenderhauf, Tobias Fischer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11287">Reducing Label Dependency for Underwater Scene Understanding: A Survey of Datasets, Techniques and Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Underwater surveys provide long-term data for informing management strategies, monitoring coral reef health, and estimating blue carbon stocks. Advances in broad-scale survey methods, such as robotic underwater vehicles, have increased the range of marine surveys but generate large volumes of imagery requiring analysis. Computer vision methods such as semantic segmentation aid automated image analysis, but typically rely on fully supervised training with extensive labelled data. While ground truth label masks for tasks like street scene segmentation can be quickly and affordably generated by non-experts through crowdsourcing services like Amazon Mechanical Turk, ecology presents greater challenges. The complexity of underwater images, coupled with the specialist expertise needed to accurately identify species at the pixel level, makes this process costly, time-consuming, and heavily dependent on domain experts. In recent years, some works have performed automated analysis of underwater imagery, and a smaller number of studies have focused on weakly supervised approaches which aim to reduce the expert-provided labelled data required. This survey focuses on approaches which reduce dependency on human expert input, while reviewing the prior and related approaches to position these works in the wider field of underwater perception. Further, we offer an overview of coastal ecosystems and the challenges of underwater imagery. We provide background on weakly and self-supervised deep learning and integrate these elements into a taxonomy that centres on the intersection of underwater monitoring, computer vision, and deep learning, while motivating approaches for weakly supervised deep learning with reduced dependency on domain expert data annotations. Lastly, the survey examines available datasets and platforms, and identifies gaps, barriers, and opportunities for automating underwater surveys.
<div id='section'>Paperid: <span id='pid'>337, <a href='https://arxiv.org/pdf/2410.20371.pdf' target='_blank'>https://arxiv.org/pdf/2410.20371.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxing Huang, Jingyi Zhang, Kai Jiang, Shijian Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20371">Open-Vocabulary Object Detection via Language Hierarchy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent studies on generalizable object detection have attracted increasing attention with additional weak supervision from large-scale datasets with image-level labels. However, weakly-supervised detection learning often suffers from image-to-box label mismatch, i.e., image-level labels do not convey precise object information. We design Language Hierarchical Self-training (LHST) that introduces language hierarchy into weakly-supervised detector training for learning more generalizable detectors. LHST expands the image-level labels with language hierarchy and enables co-regularization between the expanded labels and self-training. Specifically, the expanded labels regularize self-training by providing richer supervision and mitigating the image-to-box label mismatch, while self-training allows assessing and selecting the expanded labels according to the predicted reliability. In addition, we design language hierarchical prompt generation that introduces language hierarchy into prompt generation which helps bridge the vocabulary gaps between training and testing. Extensive experiments show that the proposed techniques achieve superior generalization performance consistently across 14 widely studied object detection datasets.
<div id='section'>Paperid: <span id='pid'>338, <a href='https://arxiv.org/pdf/2405.01217.pdf' target='_blank'>https://arxiv.org/pdf/2405.01217.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenying Liu, Conrad Albrecht, Yi Wang, Xiao Xiang Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.01217">CromSS: Cross-modal pre-training with noisy labels for remote sensing image segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We explore the potential of large-scale noisily labeled data to enhance feature learning by pretraining semantic segmentation models within a multi-modal framework for geospatial applications. We propose a novel Cross-modal Sample Selection (CromSS) method, a weakly supervised pretraining strategy designed to improve feature representations through cross-modal consistency and noise mitigation techniques. Unlike conventional pretraining approaches, CromSS exploits massive amounts of noisy and easy-to-come-by labels for improved feature learning beneficial to semantic segmentation tasks. We investigate middle and late fusion strategies to optimize the multi-modal pretraining architecture design. We also introduce a cross-modal sample selection module to mitigate the adverse effects of label noise, which employs a cross-modal entangling strategy to refine the estimated confidence masks within each modality to guide the sampling process. Additionally, we introduce a spatial-temporal label smoothing technique to counteract overconfidence for enhanced robustness against noisy labels. To validate our approach, we assembled the multi-modal dataset, NoLDO-S12, which consists of a large-scale noisy label subset from Google's Dynamic World (DW) dataset for pretraining and two downstream subsets with high-quality labels from Google DW and OpenStreetMap (OSM) for transfer learning. Experimental results on two downstream tasks and the publicly available DFC2020 dataset demonstrate that when effectively utilized, the low-cost noisy labels can significantly enhance feature learning for segmentation tasks. All data, code, and pretrained weights will be made publicly available.
<div id='section'>Paperid: <span id='pid'>339, <a href='https://arxiv.org/pdf/2403.18504.pdf' target='_blank'>https://arxiv.org/pdf/2403.18504.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Felix Virgo, Fei Cheng, Lis Kanashiro Pereira, Masayuki Asahara, Ichiro Kobayashi, Sadao Kurohashi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.18504">AcTED: Automatic Acquisition of Typical Event Duration for Semi-supervised Temporal Commonsense QA</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a voting-driven semi-supervised approach to automatically acquire the typical duration of an event and use it as pseudo-labeled data. The human evaluation demonstrates that our pseudo labels exhibit surprisingly high accuracy and balanced coverage. In the temporal commonsense QA task, experimental results show that using only pseudo examples of 400 events, we achieve performance comparable to the existing BERT-based weakly supervised approaches that require a significant amount of training examples. When compared to the RoBERTa baselines, our best approach establishes state-of-the-art performance with a 7% improvement in Exact Match.
<div id='section'>Paperid: <span id='pid'>340, <a href='https://arxiv.org/pdf/2401.01823.pdf' target='_blank'>https://arxiv.org/pdf/2401.01823.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kumar Ashutosh, Zihui Xue, Tushar Nagarajan, Kristen Grauman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.01823">Detours for Navigating Instructional Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce the video detours problem for navigating instructional videos. Given a source video and a natural language query asking to alter the how-to video's current path of execution in a certain way, the goal is to find a related ''detour video'' that satisfies the requested alteration. To address this challenge, we propose VidDetours, a novel video-language approach that learns to retrieve the targeted temporal segments from a large repository of how-to's using video-and-text conditioned queries. Furthermore, we devise a language-based pipeline that exploits how-to video narration text to create weakly supervised training data. We demonstrate our idea applied to the domain of how-to cooking videos, where a user can detour from their current recipe to find steps with alternate ingredients, tools, and techniques. Validating on a ground truth annotated dataset of 16K samples, we show our model's significant improvements over best available methods for video retrieval and question answering, with recall rates exceeding the state of the art by 35%.
<div id='section'>Paperid: <span id='pid'>341, <a href='https://arxiv.org/pdf/2511.17735.pdf' target='_blank'>https://arxiv.org/pdf/2511.17735.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Samuel Stevens, Jacob Beattie, Tanya Berger-Wolf, Yu Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17735">Towards Open-Ended Visual Scientific Discovery with Sparse Autoencoders</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scientific archives now contain hundreds of petabytes of data across genomics, ecology, climate, and molecular biology that could reveal undiscovered patterns if systematically analyzed at scale. Large-scale, weakly-supervised datasets in language and vision have driven the development of foundation models whose internal representations encode structure (patterns, co-occurrences and statistical regularities) beyond their training objectives. Most existing methods extract structure only for pre-specified targets; they excel at confirmation but do not support open-ended discovery of unknown patterns. We ask whether sparse autoencoders (SAEs) can enable open-ended feature discovery from foundation model representations. We evaluate this question in controlled rediscovery studies, where the learned SAE features are tested for alignment with semantic concepts on a standard segmentation benchmark and compared against strong label-free alternatives on concept-alignment metrics. Applied to ecological imagery, the same procedure surfaces fine-grained anatomical structure without access to segmentation or part labels, providing a scientific case study with ground-truth validation. While our experiments focus on vision with an ecology case study, the method is domain-agnostic and applicable to models in other sciences (e.g., proteins, genomics, weather). Our results indicate that sparse decomposition provides a practical instrument for exploring what scientific foundation models have learned, an important prerequisite for moving from confirmation to genuine discovery.
<div id='section'>Paperid: <span id='pid'>342, <a href='https://arxiv.org/pdf/2507.11344.pdf' target='_blank'>https://arxiv.org/pdf/2507.11344.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zara Hall, Melanie Subbiah, Thomas P Zollo, Kathleen McKeown, Richard Zemel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11344">Guiding LLM Decision-Making with Fairness Reward Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models are increasingly used to support high-stakes decisions, potentially influencing who is granted bail or receives a loan. Naive chain-of-thought sampling can improve average decision accuracy, but has also been shown to amplify unfair bias. To address this challenge and enable the trustworthy use of reasoning models in high-stakes decision-making, we propose a framework for training a generalizable Fairness Reward Model (FRM). Our model assigns a fairness score to LLM reasoning, enabling the system to down-weight biased trajectories and favor equitable ones when aggregating decisions across reasoning chains. We show that a single Fairness Reward Model, trained on weakly supervised, LLM-annotated examples of biased versus unbiased reasoning, transfers across tasks, domains, and model families without additional fine-tuning. Applied to real-world decision-making tasks including recidivism prediction and social media moderation, we show that our approach consistently improves fairness while matching, or even surpassing, baseline accuracy.
<div id='section'>Paperid: <span id='pid'>343, <a href='https://arxiv.org/pdf/2506.09445.pdf' target='_blank'>https://arxiv.org/pdf/2506.09445.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ayush Gupta, Anirban Roy, Rama Chellappa, Nathaniel D. Bastian, Alvaro Velasquez, Susmit Jha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09445">TOGA: Temporally Grounded Open-Ended Video QA with Weak Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address the problem of video question answering (video QA) with temporal grounding in a weakly supervised setup, without any temporal annotations. Given a video and a question, we generate an open-ended answer grounded with the start and end time. For this task, we propose TOGA: a vision-language model for Temporally Grounded Open-Ended Video QA with Weak Supervision. We instruct-tune TOGA to jointly generate the answer and the temporal grounding. We operate in a weakly supervised setup where the temporal grounding annotations are not available. We generate pseudo labels for temporal grounding and ensure the validity of these labels by imposing a consistency constraint between the question of a grounding response and the response generated by a question referring to the same temporal segment. We notice that jointly generating the answers with the grounding improves performance on question answering as well as grounding. We evaluate TOGA on grounded QA and open-ended QA tasks. For grounded QA, we consider the NExT-GQA benchmark which is designed to evaluate weakly supervised grounded question answering. For open-ended QA, we consider the MSVD-QA and ActivityNet-QA benchmarks. We achieve state-of-the-art performance for both tasks on these benchmarks.
<div id='section'>Paperid: <span id='pid'>344, <a href='https://arxiv.org/pdf/2503.13895.pdf' target='_blank'>https://arxiv.org/pdf/2503.13895.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinliang Zhang, Lei Zhu, Shuang Zeng, Hangzhou He, Ourui Fu, Zhengjian Yao, Zhaoheng Xie, Yanye Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13895">Exploiting Inherent Class Label: Towards Robust Scribble Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scribble-based weakly supervised semantic segmentation leverages only a few annotated pixels as labels to train a segmentation model, presenting significant potential for reducing the human labor involved in the annotation process. This approach faces two primary challenges: first, the sparsity of scribble annotations can lead to inconsistent predictions due to limited supervision; second, the variability in scribble annotations, reflecting differing human annotator preferences, can prevent the model from consistently capturing the discriminative regions of objects, potentially leading to unstable predictions. To address these issues, we propose a holistic framework, the class-driven scribble promotion network, for robust scribble-supervised semantic segmentation. This framework not only utilizes the provided scribble annotations but also leverages their associated class labels to generate reliable pseudo-labels. Within the network, we introduce a localization rectification module to mitigate noisy labels and a distance perception module to identify reliable regions surrounding scribble annotations and pseudo-labels. In addition, we introduce new large-scale benchmarks, ScribbleCOCO and ScribbleCityscapes, accompanied by a scribble simulation algorithm that enables evaluation across varying scribble styles. Our method demonstrates competitive performance in both accuracy and robustness, underscoring its superiority over existing approaches. The datasets and the codes will be made publicly available.
<div id='section'>Paperid: <span id='pid'>345, <a href='https://arxiv.org/pdf/2502.03856.pdf' target='_blank'>https://arxiv.org/pdf/2502.03856.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lin Li, Chuhan Zhang, Dong Zhang, Chong Sun, Chen Li, Long Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03856">Taking A Closer Look at Interacting Objects: Interaction-Aware Open Vocabulary Scene Graph Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Today's open vocabulary scene graph generation (OVSGG) extends traditional SGG by recognizing novel objects and relationships beyond predefined categories, leveraging the knowledge from pre-trained large-scale models. Most existing methods adopt a two-stage pipeline: weakly supervised pre-training with image captions and supervised fine-tuning (SFT) on fully annotated scene graphs. Nonetheless, they omit explicit modeling of interacting objects and treat all objects equally, resulting in mismatched relation pairs. To this end, we propose an interaction-aware OVSGG framework INOVA. During pre-training, INOVA employs an interaction-aware target generation strategy to distinguish interacting objects from non-interacting ones. In SFT, INOVA devises an interaction-guided query selection tactic to prioritize interacting objects during bipartite graph matching. Besides, INOVA is equipped with an interaction-consistent knowledge distillation to enhance the robustness by pushing interacting object pairs away from the background. Extensive experiments on two benchmarks (VG and GQA) show that INOVA achieves state-of-the-art performance, demonstrating the potential of interaction-aware mechanisms for real-world applications.
<div id='section'>Paperid: <span id='pid'>346, <a href='https://arxiv.org/pdf/2408.12800.pdf' target='_blank'>https://arxiv.org/pdf/2408.12800.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cairong Zhao, Chutian Wang, Zifan Song, Guosheng Hu, Haonan Chen, Xiaofan Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.12800">Cap2Sum: Learning to Summarize Videos by Generating Captions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid growth of video data on the internet, video summarization is becoming a very important AI technology. However, due to the high labelling cost of video summarization, existing studies have to be conducted on small-scale datasets, leading to limited performance and generalization capacity. In this work, we introduce the use of dense video captions as a supervision signal to train video summarization models. Motivated by this, we propose Cap2Sum, a model that learns to summarize videos by generating captions, to exploit dense video caption annotations. This weakly-supervised approach allows us to train the models on large-scale dense video caption datasets to achieve better performance and generalization capacity. To further improve the generalization capacity, we introduce a CLIP (a strong vision-language model) Prior mechanism to enhance the learning of important objects that captions may ignore in the videos. In practice, Cap2Sum can perform zero-shot video summarization or be fine-tuned by the ground-truth summary or video caption of the target dataset. To examine the performance of Cap2Sum after weakly-supervised fine-tuning by the video captions, we propose two new datasets, TVSum-Caption and SumMe-Caption, which are derived from two common video summarization datasets and will be publicly released. We conduct extensive experiments and the results demonstrate that our method achieves significant improvements in performance and generalization capacity compared with previous methods.
<div id='section'>Paperid: <span id='pid'>347, <a href='https://arxiv.org/pdf/2510.17384.pdf' target='_blank'>https://arxiv.org/pdf/2510.17384.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiajin Tang, Zhengxuan Wei, Ge Zheng, Sibei Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17384">Closed-Loop Transfer for Weakly-supervised Affordance Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans can perform previously unexperienced interactions with novel objects simply by observing others engage with them. Weakly-supervised affordance grounding mimics this process by learning to locate object regions that enable actions on egocentric images, using exocentric interaction images with image-level annotations. However, extracting affordance knowledge solely from exocentric images and transferring it one-way to egocentric images limits the applicability of previous works in complex interaction scenarios. Instead, this study introduces LoopTrans, a novel closed-loop framework that not only transfers knowledge from exocentric to egocentric but also transfers back to enhance exocentric knowledge extraction. Within LoopTrans, several innovative mechanisms are introduced, including unified cross-modal localization and denoising knowledge distillation, to bridge domain gaps between object-centered egocentric and interaction-centered exocentric images while enhancing knowledge transfer. Experiments show that LoopTrans achieves consistent improvements across all metrics on image and video benchmarks, even handling challenging scenarios where object interaction regions are fully occluded by the human body.
<div id='section'>Paperid: <span id='pid'>348, <a href='https://arxiv.org/pdf/2502.01458.pdf' target='_blank'>https://arxiv.org/pdf/2502.01458.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Yao, Wenkai Yang, Gengze Xu, Ziqiao Wang, Yankai Lin, Yong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01458">The Capabilities and Limitations of Weak-to-Strong Generalization: Generalization and Calibration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weak-to-strong generalization, where weakly supervised strong models outperform their weaker teachers, offers a promising approach to aligning superhuman models with human values. To deepen the understanding of this approach, we provide theoretical insights into its capabilities and limitations. First, in the classification setting, we establish upper and lower generalization error bounds for the strong model, identifying the primary limitations as stemming from the weak model's generalization error and the optimization objective itself. Additionally, we derive lower and upper bounds on the calibration error of the strong model. These theoretical bounds reveal two critical insights: (1) the weak model should demonstrate strong generalization performance and maintain well-calibrated predictions, and (2) the strong model's training process must strike a careful balance, as excessive optimization could undermine its generalization capability by over-relying on the weak supervision signals. Finally, in the regression setting, we extend the work of Charikar et al. (2024) to a loss function based on Kullback-Leibler (KL) divergence, offering guarantees that the strong student can outperform its weak teacher by at least the magnitude of their disagreement. We conduct sufficient experiments to validate our theory.
<div id='section'>Paperid: <span id='pid'>349, <a href='https://arxiv.org/pdf/2409.04758.pdf' target='_blank'>https://arxiv.org/pdf/2409.04758.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuchang Ye, Mingyuan Meng, Mingjian Li, Dagan Feng, Jinman Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.04758">SGSeg: Enabling Text-free Inference in Language-guided Segmentation of Chest X-rays via Self-guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Segmentation of infected areas in chest X-rays is pivotal for facilitating the accurate delineation of pulmonary structures and pathological anomalies. Recently, multi-modal language-guided image segmentation methods have emerged as a promising solution for chest X-rays where the clinical text reports, depicting the assessment of the images, are used as guidance. Nevertheless, existing language-guided methods require clinical reports alongside the images, and hence, they are not applicable for use in image segmentation in a decision support context, but rather limited to retrospective image analysis after clinical reporting has been completed. In this study, we propose a self-guided segmentation framework (SGSeg) that leverages language guidance for training (multi-modal) while enabling text-free inference (uni-modal), which is the first that enables text-free inference in language-guided segmentation. We exploit the critical location information of both pulmonary and pathological structures depicted in the text reports and introduce a novel localization-enhanced report generation (LERG) module to generate clinical reports for self-guidance. Our LERG integrates an object detector and a location-based attention aggregator, weakly-supervised by a location-aware pseudo-label extraction module. Extensive experiments on a well-benchmarked QaTa-COV19 dataset demonstrate that our SGSeg achieved superior performance than existing uni-modal segmentation methods and closely matched the state-of-the-art performance of multi-modal language-guided segmentation methods.
<div id='section'>Paperid: <span id='pid'>350, <a href='https://arxiv.org/pdf/2403.17541.pdf' target='_blank'>https://arxiv.org/pdf/2403.17541.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Astitva Srivastava, Pranav Manu, Amit Raj, Varun Jampani, Avinash Sharma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17541">WordRobe: Text-Guided Generation of Textured 3D Garments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we tackle a new and challenging problem of text-driven generation of 3D garments with high-quality textures. We propose "WordRobe", a novel framework for the generation of unposed & textured 3D garment meshes from user-friendly text prompts. We achieve this by first learning a latent representation of 3D garments using a novel coarse-to-fine training strategy and a loss for latent disentanglement, promoting better latent interpolation. Subsequently, we align the garment latent space to the CLIP embedding space in a weakly supervised manner, enabling text-driven 3D garment generation and editing. For appearance modeling, we leverage the zero-shot generation capability of ControlNet to synthesize view-consistent texture maps in a single feed-forward inference step, thereby drastically decreasing the generation time as compared to existing methods. We demonstrate superior performance over current SOTAs for learning 3D garment latent space, garment interpolation, and text-driven texture synthesis, supported by quantitative evaluation and qualitative user study. The unposed 3D garment meshes generated using WordRobe can be directly fed to standard cloth simulation & animation pipelines without any post-processing.
<div id='section'>Paperid: <span id='pid'>351, <a href='https://arxiv.org/pdf/2403.07603.pdf' target='_blank'>https://arxiv.org/pdf/2403.07603.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Åukasz Struski, Adam Pardyl, Jacek Tabor, Bartosz ZieliÅski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07603">ProPML: Probability Partial Multi-label Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partial Multi-label Learning (PML) is a type of weakly supervised learning where each training instance corresponds to a set of candidate labels, among which only some are true. In this paper, we introduce \our{}, a novel probabilistic approach to this problem that extends the binary cross entropy to the PML setup. In contrast to existing methods, it does not require suboptimal disambiguation and, as such, can be applied to any deep architecture. Furthermore, experiments conducted on artificial and real-world datasets indicate that \our{} outperforms existing approaches, especially for high noise in a candidate set.
<div id='section'>Paperid: <span id='pid'>352, <a href='https://arxiv.org/pdf/2512.06849.pdf' target='_blank'>https://arxiv.org/pdf/2512.06849.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matan Atad, Alexander W. Marka, Lisa Steinhelfer, Anna Curto-Vilalta, Yannik Leonhardt, Sarah C. Foreman, Anna-Sophia Walburga Dietrich, Robert Graf, Alexandra S. Gersing, Bjoern Menze, Daniel Rueckert, Jan S. Kirschke, Hendrik Möller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.06849">Hide-and-Seek Attribution: Weakly Supervised Segmentation of Vertebral Metastases in CT</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate segmentation of vertebral metastasis in CT is clinically important yet difficult to scale, as voxel-level annotations are scarce and both lytic and blastic lesions often resemble benign degenerative changes. We introduce a weakly supervised method trained solely on vertebra-level healthy/malignant labels, without any lesion masks. The method combines a Diffusion Autoencoder (DAE) that produces a classifier-guided healthy edit of each vertebra with pixel-wise difference maps that propose candidate lesion regions. To determine which regions truly reflect malignancy, we introduce Hide-and-Seek Attribution: each candidate is revealed in turn while all others are hidden, the edited image is projected back to the data manifold by the DAE, and a latent-space classifier quantifies the isolated malignant contribution of that component. High-scoring regions form the final lytic or blastic segmentation. On held-out radiologist annotations, we achieve strong blastic/lytic performance despite no mask supervision (F1: 0.91/0.85; Dice: 0.87/0.78), exceeding baselines (F1: 0.79/0.67; Dice: 0.74/0.55). These results show that vertebra-level labels can be transformed into reliable lesion masks, demonstrating that generative editing combined with selective occlusion supports accurate weakly supervised segmentation in CT.
<div id='section'>Paperid: <span id='pid'>353, <a href='https://arxiv.org/pdf/2511.20359.pdf' target='_blank'>https://arxiv.org/pdf/2511.20359.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiqing Guo, Dongdong Xi, Songlin Li, Gaobo Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.20359">From Passive Perception to Active Memory: A Weakly Supervised Image Manipulation Localization Framework Driven by Coarse-Grained Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image manipulation localization (IML) faces a fundamental trade-off between minimizing annotation cost and achieving fine-grained localization accuracy. Existing fully-supervised IML methods depend heavily on dense pixel-level mask annotations, which limits scalability to large datasets or real-world deployment.In contrast, the majority of existing weakly-supervised IML approaches are based on image-level labels, which greatly reduce annotation effort but typically lack precise spatial localization. To address this dilemma, we propose BoxPromptIML, a novel weakly-supervised IML framework that effectively balances annotation cost and localization performance. Specifically, we propose a coarse region annotation strategy, which can generate relatively accurate manipulation masks at lower cost. To improve model efficiency and facilitate deployment, we further design an efficient lightweight student model, which learns to perform fine-grained localization through knowledge distillation from a fixed teacher model based on the Segment Anything Model (SAM). Moreover, inspired by the human subconscious memory mechanism, our feature fusion module employs a dual-guidance strategy that actively contextualizes recalled prototypical patterns with real-time observational cues derived from the input. Instead of passive feature extraction, this strategy enables a dynamic process of knowledge recollection, where long-term memory is adapted to the specific context of the current image, significantly enhancing localization accuracy and robustness. Extensive experiments across both in-distribution and out-of-distribution datasets show that BoxPromptIML outperforms or rivals fully-supervised models, while maintaining strong generalization, low annotation cost, and efficient deployment characteristics.
<div id='section'>Paperid: <span id='pid'>354, <a href='https://arxiv.org/pdf/2511.14238.pdf' target='_blank'>https://arxiv.org/pdf/2511.14238.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Huang, Yongyi Su, Xin Lin, Le Zhang, Xun Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.14238">Enhancing Generalization of Depth Estimation Foundation Model via Weakly-Supervised Adaptation with Regularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emergence of foundation models has substantially advanced zero-shot generalization in monocular depth estimation (MDE), as exemplified by the Depth Anything series. However, given access to some data from downstream tasks, a natural question arises: can the performance of these models be further improved? To this end, we propose WeSTAR, a parameter-efficient framework that performs Weakly supervised Self-Training Adaptation with Regularization, designed to enhance the robustness of MDE foundation models in unseen and diverse domains. We first adopt a dense self-training objective as the primary source of structural self-supervision. To further improve robustness, we introduce semantically-aware hierarchical normalization, which exploits instance-level segmentation maps to perform more stable and multi-scale structural normalization. Beyond dense supervision, we introduce a cost-efficient weak supervision in the form of pairwise ordinal depth annotations to further guide the adaptation process, which enforces informative ordinal constraints to mitigate local topological errors. Finally, a weight regularization loss is employed to anchor the LoRA updates, ensuring training stability and preserving the model's generalizable knowledge. Extensive experiments on both realistic and corrupted out-of-distribution datasets under diverse and challenging scenarios demonstrate that WeSTAR consistently improves generalization and achieves state-of-the-art performance across a wide range of benchmarks.
<div id='section'>Paperid: <span id='pid'>355, <a href='https://arxiv.org/pdf/2508.05585.pdf' target='_blank'>https://arxiv.org/pdf/2508.05585.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haijing Liu, Tao Pu, Hefeng Wu, Keze Wang, Liang Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05585">DART: Dual Adaptive Refinement Transfer for Open-Vocabulary Multi-Label Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-Vocabulary Multi-Label Recognition (OV-MLR) aims to identify multiple seen and unseen object categories within an image, requiring both precise intra-class localization to pinpoint objects and effective inter-class reasoning to model complex category dependencies. While Vision-Language Pre-training (VLP) models offer a strong open-vocabulary foundation, they often struggle with fine-grained localization under weak supervision and typically fail to explicitly leverage structured relational knowledge beyond basic semantics, limiting performance especially for unseen classes. To overcome these limitations, we propose the Dual Adaptive Refinement Transfer (DART) framework. DART enhances a frozen VLP backbone via two synergistic adaptive modules. For intra-class refinement, an Adaptive Refinement Module (ARM) refines patch features adaptively, coupled with a novel Weakly Supervised Patch Selecting (WPS) loss that enables discriminative localization using only image-level labels. Concurrently, for inter-class transfer, an Adaptive Transfer Module (ATM) leverages a Class Relationship Graph (CRG), constructed using structured knowledge mined from a Large Language Model (LLM), and employs graph attention network to adaptively transfer relational information between class representations. DART is the first framework, to our knowledge, to explicitly integrate external LLM-derived relational knowledge for adaptive inter-class transfer while simultaneously performing adaptive intra-class refinement under weak supervision for OV-MLR. Extensive experiments on challenging benchmarks demonstrate that our DART achieves new state-of-the-art performance, validating its effectiveness.
<div id='section'>Paperid: <span id='pid'>356, <a href='https://arxiv.org/pdf/2507.13018.pdf' target='_blank'>https://arxiv.org/pdf/2507.13018.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Songlin Li, Guofeng Yu, Zhiqing Guo, Yunfeng Diao, Dan Ma, Gaobo Yang, Liejun Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13018">Beyond Fully Supervised Pixel Annotations: Scribble-Driven Weakly-Supervised Framework for Image Manipulation Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning-based image manipulation localization (IML) methods have achieved remarkable performance in recent years, but typically rely on large-scale pixel-level annotated datasets. To address the challenge of acquiring high-quality annotations, some recent weakly supervised methods utilize image-level labels to segment manipulated regions. However, the performance is still limited due to insufficient supervision signals. In this study, we explore a form of weak supervision that improves the annotation efficiency and detection performance, namely scribble annotation supervision. We re-annotated mainstream IML datasets with scribble labels and propose the first scribble-based IML (Sc-IML) dataset. Additionally, we propose the first scribble-based weakly supervised IML framework. Specifically, we employ self-supervised training with a structural consistency loss to encourage the model to produce consistent predictions under multi-scale and augmented inputs. In addition, we propose a prior-aware feature modulation module (PFMM) that adaptively integrates prior information from both manipulated and authentic regions for dynamic feature adjustment, further enhancing feature discriminability and prediction consistency in complex scenes. We also propose a gated adaptive fusion module (GAFM) that utilizes gating mechanisms to regulate information flow during feature fusion, guiding the model toward emphasizing potential tampered regions. Finally, we propose a confidence-aware entropy minimization loss (${\mathcal{L}}_{ {CEM }}$). This loss dynamically regularizes predictions in weakly annotated or unlabeled regions based on model uncertainty, effectively suppressing unreliable predictions. Experimental results show that our method outperforms existing fully supervised approaches in terms of average performance both in-distribution and out-of-distribution.
<div id='section'>Paperid: <span id='pid'>357, <a href='https://arxiv.org/pdf/2503.24135.pdf' target='_blank'>https://arxiv.org/pdf/2503.24135.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexis Guichemerre, Soufiane Belharbi, Mohammadhadi Shateri, Luke McCaffrey, Eric Granger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.24135">PixelCAM: Pixel Class Activation Mapping for Histology Image Classification and ROI Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised object localization (WSOL) methods allow training models to classify images and localize ROIs. WSOL only requires low-cost image-class annotations yet provides a visually interpretable classifier. Standard WSOL methods rely on class activation mapping (CAM) methods to produce spatial localization maps according to a single- or two-step strategy. While both strategies have made significant progress, they still face several limitations with histology images. Single-step methods can easily result in under- or over-activation due to the limited visual ROI saliency in histology images and scarce localization cues. They also face the well-known issue of asynchronous convergence between classification and localization tasks. The two-step approach is sub-optimal because it is constrained to a frozen classifier, limiting the capacity for localization. Moreover, these methods also struggle when applied to out-of-distribution (OOD) datasets. In this paper, a multi-task approach for WSOL is introduced for simultaneous training of both tasks to address the asynchronous convergence problem. In particular, localization is performed in the pixel-feature space of an image encoder that is shared with classification. This allows learning discriminant features and accurate delineation of foreground/background regions to support ROI localization and image classification. We propose PixelCAM, a cost-effective foreground/background pixel-wise classifier in the pixel-feature space that allows for spatial object localization. Using partial-cross entropy, PixelCAM is trained using pixel pseudo-labels collected from a pretrained WSOL model. Both image and pixel-wise classifiers are trained simultaneously using standard gradient descent. In addition, our pixel classifier can easily be integrated into CNN- and transformer-based architectures without any modifications.
<div id='section'>Paperid: <span id='pid'>358, <a href='https://arxiv.org/pdf/2502.19698.pdf' target='_blank'>https://arxiv.org/pdf/2502.19698.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangfeng Jiang, Jun Liu, Yongxuan Lv, Yuzhi Wu, Xianfei Li, Wenlong Liao, Tao He, Pai Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19698">You Only Click Once: Single Point Weakly Supervised 3D Instance Segmentation for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Outdoor LiDAR point cloud 3D instance segmentation is a crucial task in autonomous driving. However, it requires laborious human efforts to annotate the point cloud for training a segmentation model. To address this challenge, we propose a YoCo framework, which generates 3D pseudo labels using minimal coarse click annotations in the bird's eye view plane. It is a significant challenge to produce high-quality pseudo labels from sparse annotations. Our YoCo framework first leverages vision foundation models combined with geometric constraints from point clouds to enhance pseudo label generation. Second, a temporal and spatial-based label updating module is designed to generate reliable updated labels. It leverages predictions from adjacent frames and utilizes the inherent density variation of point clouds (dense near, sparse far). Finally, to further improve label quality, an IoU-guided enhancement module is proposed, replacing pseudo labels with high-confidence and high-IoU predictions. Experiments on the Waymo dataset demonstrate YoCo's effectiveness and generality, achieving state-of-the-art performance among weakly supervised methods and surpassing fully supervised Cylinder3D. Additionally, the YoCo is suitable for various networks, achieving performance comparable to fully supervised methods with minimal fine-tuning using only 0.8% of the fully labeled data, significantly reducing annotation costs.
<div id='section'>Paperid: <span id='pid'>359, <a href='https://arxiv.org/pdf/2501.17148.pdf' target='_blank'>https://arxiv.org/pdf/2501.17148.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengxuan Wu, Aryaman Arora, Atticus Geiger, Zheng Wang, Jing Huang, Dan Jurafsky, Christopher D. Manning, Christopher Potts
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.17148">AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse Autoencoders</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fine-grained steering of language model outputs is essential for safety and reliability. Prompting and finetuning are widely used to achieve these goals, but interpretability researchers have proposed a variety of representation-based techniques as well, including sparse autoencoders (SAEs), linear artificial tomography, supervised steering vectors, linear probes, and representation finetuning. At present, there is no benchmark for making direct comparisons between these proposals. Therefore, we introduce AxBench, a large-scale benchmark for steering and concept detection, and report experiments on Gemma-2-2B and 9B. For steering, we find that prompting outperforms all existing methods, followed by finetuning. For concept detection, representation-based methods such as difference-in-means, perform the best. On both evaluations, SAEs are not competitive. We introduce a novel weakly-supervised representational method (Rank-1 Representation Finetuning; ReFT-r1), which is competitive on both tasks while providing the interpretability advantages that prompting lacks. Along with AxBench, we train and publicly release SAE-scale feature dictionaries for ReFT-r1 and DiffMean.
<div id='section'>Paperid: <span id='pid'>360, <a href='https://arxiv.org/pdf/2411.04607.pdf' target='_blank'>https://arxiv.org/pdf/2411.04607.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chong Wang, Fengbei Liu, Yuanhong Chen, Helen Frazer, Gustavo Carneiro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04607">Cross- and Intra-image Prototypical Learning for Multi-label Disease Diagnosis and Interpretation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in prototypical learning have shown remarkable potential to provide useful decision interpretations associating activation maps and predictions with class-specific training prototypes. Such prototypical learning has been well-studied for various single-label diseases, but for quite relevant and more challenging multi-label diagnosis, where multiple diseases are often concurrent within an image, existing prototypical learning models struggle to obtain meaningful activation maps and effective class prototypes due to the entanglement of the multiple diseases. In this paper, we present a novel Cross- and Intra-image Prototypical Learning (CIPL) framework, for accurate multi-label disease diagnosis and interpretation from medical images. CIPL takes advantage of common cross-image semantics to disentangle the multiple diseases when learning the prototypes, allowing a comprehensive understanding of complicated pathological lesions. Furthermore, we propose a new two-level alignment-based regularisation strategy that effectively leverages consistent intra-image information to enhance interpretation robustness and predictive performance. Extensive experiments show that our CIPL attains the state-of-the-art (SOTA) classification accuracy in two public multi-label benchmarks of disease diagnosis: thoracic radiography and fundus images. Quantitative interpretability results show that CIPL also has superiority in weakly-supervised thoracic disease localisation over other leading saliency- and prototype-based explanation methods.
<div id='section'>Paperid: <span id='pid'>361, <a href='https://arxiv.org/pdf/2408.10069.pdf' target='_blank'>https://arxiv.org/pdf/2408.10069.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Reuben Dorent, Roya Khajavi, Tagwa Idris, Erik Ziegler, Bhanusupriya Somarouthu, Heather Jacene, Ann LaCasce, Jonathan Deissler, Jan Ehrhardt, Sofija Engelson, Stefan M. Fischer, Yun Gu, Heinz Handels, Satoshi Kasai, Satoshi Kondo, Klaus Maier-Hein, Julia A. Schnabel, Guotai Wang, Litingyu Wang, Tassilo Wald, Guang-Zhong Yang, Hanxiao Zhang, Minghui Zhang, Steve Pieper, Gordon Harris, Ron Kikinis, Tina Kapur
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.10069">LNQ 2023 challenge: Benchmark of weakly-supervised techniques for mediastinal lymph node quantification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate assessment of lymph node size in 3D CT scans is crucial for cancer staging, therapeutic management, and monitoring treatment response. Existing state-of-the-art segmentation frameworks in medical imaging often rely on fully annotated datasets. However, for lymph node segmentation, these datasets are typically small due to the extensive time and expertise required to annotate the numerous lymph nodes in 3D CT scans. Weakly-supervised learning, which leverages incomplete or noisy annotations, has recently gained interest in the medical imaging community as a potential solution. Despite the variety of weakly-supervised techniques proposed, most have been validated only on private datasets or small publicly available datasets. To address this limitation, the Mediastinal Lymph Node Quantification (LNQ) challenge was organized in conjunction with the 26th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2023). This challenge aimed to advance weakly-supervised segmentation methods by providing a new, partially annotated dataset and a robust evaluation framework. A total of 16 teams from 5 countries submitted predictions to the validation leaderboard, and 6 teams from 3 countries participated in the evaluation phase. The results highlighted both the potential and the current limitations of weakly-supervised approaches. On one hand, weakly-supervised approaches obtained relatively good performance with a median Dice score of $61.0\%$. On the other hand, top-ranked teams, with a median Dice score exceeding $70\%$, boosted their performance by leveraging smaller but fully annotated datasets to combine weak supervision and full supervision. This highlights both the promise of weakly-supervised methods and the ongoing need for high-quality, fully annotated data to achieve higher segmentation performance.
<div id='section'>Paperid: <span id='pid'>362, <a href='https://arxiv.org/pdf/2407.19507.pdf' target='_blank'>https://arxiv.org/pdf/2407.19507.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingjing Wu, Zhengyao Fang, Pengyuan Lyu, Chengquan Zhang, Fanglin Chen, Guangming Lu, Wenjie Pei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.19507">WeCromCL: Weakly Supervised Cross-Modality Contrastive Learning for Transcription-only Supervised Text Spotting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transcription-only Supervised Text Spotting aims to learn text spotters relying only on transcriptions but no text boundaries for supervision, thus eliminating expensive boundary annotation. The crux of this task lies in locating each transcription in scene text images without location annotations. In this work, we formulate this challenging problem as a Weakly Supervised Cross-modality Contrastive Learning problem, and design a simple yet effective model dubbed WeCromCL that is able to detect each transcription in a scene image in a weakly supervised manner. Unlike typical methods for cross-modality contrastive learning that focus on modeling the holistic semantic correlation between an entire image and a text description, our WeCromCL conducts atomistic contrastive learning to model the character-wise appearance consistency between a text transcription and its correlated region in a scene image to detect an anchor point for the transcription in a weakly supervised manner. The detected anchor points by WeCromCL are further used as pseudo location labels to guide the learning of text spotting. Extensive experiments on four challenging benchmarks demonstrate the superior performance of our model over other methods. Code will be released.
<div id='section'>Paperid: <span id='pid'>363, <a href='https://arxiv.org/pdf/2404.19113.pdf' target='_blank'>https://arxiv.org/pdf/2404.19113.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexis Guichemerre, Soufiane Belharbi, Tsiry Mayet, Shakeeb Murtaza, Pourya Shamsolmoali, Luke McCaffrey, Eric Granger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.19113">Source-Free Domain Adaptation of Weakly-Supervised Object Localization Models for Histology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Given the emergence of deep learning, digital pathology has gained popularity for cancer diagnosis based on histology images. Deep weakly supervised object localization (WSOL) models can be trained to classify histology images according to cancer grade and identify regions of interest (ROIs) for interpretation, using inexpensive global image-class annotations. A WSOL model initially trained on some labeled source image data can be adapted using unlabeled target data in cases of significant domain shifts caused by variations in staining, scanners, and cancer type. In this paper, we focus on source-free (unsupervised) domain adaptation (SFDA), a challenging problem where a pre-trained source model is adapted to a new target domain without using any source domain data for privacy and efficiency reasons. SFDA of WSOL models raises several challenges in histology, most notably because they are not intended to adapt for both classification and localization tasks. In this paper, 4 state-of-the-art SFDA methods, each one representative of a main SFDA family, are compared for WSOL in terms of classification and localization accuracy. They are the SFDA-Distribution Estimation, Source HypOthesis Transfer, Cross-Domain Contrastive Learning, and Adaptively Domain Statistics Alignment. Experimental results on the challenging Glas (smaller, breast cancer) and Camelyon16 (larger, colon cancer) histology datasets indicate that these SFDA methods typically perform poorly for localization after adaptation when optimized for classification.
<div id='section'>Paperid: <span id='pid'>364, <a href='https://arxiv.org/pdf/2403.00165.pdf' target='_blank'>https://arxiv.org/pdf/2403.00165.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunyi Zhang, Ruozhen Yang, Xueqiang Xu, Rui Li, Jinfeng Xiao, Jiaming Shen, Jiawei Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.00165">TELEClass: Taxonomy Enrichment and LLM-Enhanced Hierarchical Text Classification with Minimal Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hierarchical text classification aims to categorize each document into a set of classes in a label taxonomy, which is a fundamental web text mining task with broad applications such as web content analysis and semantic indexing. Most earlier works focus on fully or semi-supervised methods that require a large amount of human annotated data which is costly and time-consuming to acquire. To alleviate human efforts, in this paper, we work on hierarchical text classification with a minimal amount of supervision: using the sole class name of each node as the only supervision. Recently, large language models (LLM) have shown competitive performance on various tasks through zero-shot prompting, but this method performs poorly in the hierarchical setting because it is ineffective to include the large and structured label space in a prompt. On the other hand, previous weakly-supervised hierarchical text classification methods only utilize the raw taxonomy skeleton and ignore the rich information hidden in the text corpus that can serve as additional class-indicative features. To tackle the above challenges, we propose TELEClass, Taxonomy Enrichment and LLM-Enhanced weakly-supervised hierarchical text Classification, which combines the general knowledge of LLMs and task-specific features mined from an unlabeled corpus. TELEClass automatically enriches the raw taxonomy with class-indicative features for better label space understanding and utilizes novel LLM-based data annotation and generation methods specifically tailored for the hierarchical setting. Experiments show that TELEClass can significantly outperform previous baselines while achieving comparable performance to zero-shot prompting of LLMs with drastically less inference cost.
<div id='section'>Paperid: <span id='pid'>365, <a href='https://arxiv.org/pdf/2509.11984.pdf' target='_blank'>https://arxiv.org/pdf/2509.11984.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meng Wei, Zhongnian Li, Peng Ying, Xinzheng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11984">Learning from Uncertain Similarity and Unlabeled Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing similarity-based weakly supervised learning approaches often rely on precise similarity annotations between data pairs, which may inadvertently expose sensitive label information and raise privacy risks. To mitigate this issue, we propose Uncertain Similarity and Unlabeled Learning (USimUL), a novel framework where each similarity pair is embedded with an uncertainty component to reduce label leakage. In this paper, we propose an unbiased risk estimator that learns from uncertain similarity and unlabeled data. Additionally, we theoretically prove that the estimator achieves statistically optimal parametric convergence rates. Extensive experiments on both benchmark and real-world datasets show that our method achieves superior classification performance compared to conventional similarity-based approaches.
<div id='section'>Paperid: <span id='pid'>366, <a href='https://arxiv.org/pdf/2508.04566.pdf' target='_blank'>https://arxiv.org/pdf/2508.04566.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinxing Zhou, Ziheng Zhou, Yanghao Zhou, Yuxin Mao, Zhangling Duan, Dan Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04566">CLASP: Cross-modal Salient Anchor-based Semantic Propagation for Weakly-supervised Dense Audio-Visual Event Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Dense Audio-Visual Event Localization (DAVEL) task aims to temporally localize events in untrimmed videos that occur simultaneously in both the audio and visual modalities. This paper explores DAVEL under a new and more challenging weakly-supervised setting (W-DAVEL task), where only video-level event labels are provided and the temporal boundaries of each event are unknown. We address W-DAVEL by exploiting \textit{cross-modal salient anchors}, which are defined as reliable timestamps that are well predicted under weak supervision and exhibit highly consistent event semantics across audio and visual modalities. Specifically, we propose a \textit{Mutual Event Agreement Evaluation} module, which generates an agreement score by measuring the discrepancy between the predicted audio and visual event classes. Then, the agreement score is utilized in a \textit{Cross-modal Salient Anchor Identification} module, which identifies the audio and visual anchor features through global-video and local temporal window identification mechanisms. The anchor features after multimodal integration are fed into an \textit{Anchor-based Temporal Propagation} module to enhance event semantic encoding in the original temporal audio and visual features, facilitating better temporal localization under weak supervision. We establish benchmarks for W-DAVEL on both the UnAV-100 and ActivityNet1.3 datasets. Extensive experiments demonstrate that our method achieves state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>367, <a href='https://arxiv.org/pdf/2507.12942.pdf' target='_blank'>https://arxiv.org/pdf/2507.12942.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yafei Zhang, Lingqi Kong, Huafeng Li, Jie Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12942">Weakly Supervised Visible-Infrared Person Re-Identification via Heterogeneous Expert Collaborative Consistency Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To reduce the reliance of visible-infrared person re-identification (ReID) models on labeled cross-modal samples, this paper explores a weakly supervised cross-modal person ReID method that uses only single-modal sample identity labels, addressing scenarios where cross-modal identity labels are unavailable. To mitigate the impact of missing cross-modal labels on model performance, we propose a heterogeneous expert collaborative consistency learning framework, designed to establish robust cross-modal identity correspondences in a weakly supervised manner. This framework leverages labeled data from each modality to independently train dedicated classification experts. To associate cross-modal samples, these classification experts act as heterogeneous predictors, predicting the identities of samples from the other modality. To improve prediction accuracy, we design a cross-modal relationship fusion mechanism that effectively integrates predictions from different experts. Under the implicit supervision provided by cross-modal identity correspondences, collaborative and consistent learning among the experts is encouraged, significantly enhancing the model's ability to extract modality-invariant features and improve cross-modal identity recognition. Experimental results on two challenging datasets validate the effectiveness of the proposed method.
<div id='section'>Paperid: <span id='pid'>368, <a href='https://arxiv.org/pdf/2507.12015.pdf' target='_blank'>https://arxiv.org/pdf/2507.12015.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoxun Li, Leyuan Qu, Jiaxi Hu, Taihao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12015">EME-TTS: Unlocking the Emphasis and Emotion Link in Speech Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, emotional Text-to-Speech (TTS) synthesis and emphasis-controllable speech synthesis have advanced significantly. However, their interaction remains underexplored. We propose Emphasis Meets Emotion TTS (EME-TTS), a novel framework designed to address two key research questions: (1) how to effectively utilize emphasis to enhance the expressiveness of emotional speech, and (2) how to maintain the perceptual clarity and stability of target emphasis across different emotions. EME-TTS employs weakly supervised learning with emphasis pseudo-labels and variance-based emphasis features. Additionally, the proposed Emphasis Perception Enhancement (EPE) block enhances the interaction between emotional signals and emphasis positions. Experimental results show that EME-TTS, when combined with large language models for emphasis position prediction, enables more natural emotional speech synthesis while preserving stable and distinguishable target emphasis across emotions. Synthesized samples are available on-line.
<div id='section'>Paperid: <span id='pid'>369, <a href='https://arxiv.org/pdf/2507.06744.pdf' target='_blank'>https://arxiv.org/pdf/2507.06744.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yafei Zhang, Yongle Shang, Huafeng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06744">Dual-Granularity Cross-Modal Identity Association for Weakly-Supervised Text-to-Person Image Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised text-to-person image matching, as a crucial approach to reducing models' reliance on large-scale manually labeled samples, holds significant research value. However, existing methods struggle to predict complex one-to-many identity relationships, severely limiting performance improvements. To address this challenge, we propose a local-and-global dual-granularity identity association mechanism. Specifically, at the local level, we explicitly establish cross-modal identity relationships within a batch, reinforcing identity constraints across different modalities and enabling the model to better capture subtle differences and correlations. At the global level, we construct a dynamic cross-modal identity association network with the visual modality as the anchor and introduce a confidence-based dynamic adjustment mechanism, effectively enhancing the model's ability to identify weakly associated samples while improving overall sensitivity. Additionally, we propose an information-asymmetric sample pair construction method combined with consistency learning to tackle hard sample mining and enhance model robustness. Experimental results demonstrate that the proposed method substantially boosts cross-modal matching accuracy, providing an efficient and practical solution for text-to-person image matching.
<div id='section'>Paperid: <span id='pid'>370, <a href='https://arxiv.org/pdf/2505.15123.pdf' target='_blank'>https://arxiv.org/pdf/2505.15123.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ta Duc Huy, Duy Anh Huynh, Yutong Xie, Yuankai Qi, Qi Chen, Phi Le Nguyen, Sen Kim Tran, Son Lam Phung, Anton van den Hengel, Zhibin Liao, Minh-Son To, Johan W. Verjans, Vu Minh Hieu Phan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.15123">Seeing the Trees for the Forest: Rethinking Weakly-Supervised Medical Visual Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual grounding (VG) is the capability to identify the specific regions in an image associated with a particular text description. In medical imaging, VG enhances interpretability by highlighting relevant pathological features corresponding to textual descriptions, improving model transparency and trustworthiness for wider adoption of deep learning models in clinical practice. Current models struggle to associate textual descriptions with disease regions due to inefficient attention mechanisms and a lack of fine-grained token representations. In this paper, we empirically demonstrate two key observations. First, current VLMs assign high norms to background tokens, diverting the model's attention from regions of disease. Second, the global tokens used for cross-modal learning are not representative of local disease tokens. This hampers identifying correlations between the text and disease tokens. To address this, we introduce simple, yet effective Disease-Aware Prompting (DAP) process, which uses the explainability map of a VLM to identify the appropriate image features. This simple strategy amplifies disease-relevant regions while suppressing background interference. Without any additional pixel-level annotations, DAP improves visual grounding accuracy by 20.74% compared to state-of-the-art methods across three major chest X-ray datasets.
<div id='section'>Paperid: <span id='pid'>371, <a href='https://arxiv.org/pdf/2406.14365.pdf' target='_blank'>https://arxiv.org/pdf/2406.14365.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stefan M. Fischer, Johannes Kiechle, Daniel M. Lang, Jan C. Peeken, Julia A. Schnabel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.14365">Mask the Unknown: Assessing Different Strategies to Handle Weak Annotations in the MICCAI2023 Mediastinal Lymph Node Quantification Challenge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pathological lymph node delineation is crucial in cancer diagnosis, progression assessment, and treatment planning. The MICCAI 2023 Lymph Node Quantification Challenge published the first public dataset for pathological lymph node segmentation in the mediastinum. As lymph node annotations are expensive, the challenge was formed as a weakly supervised learning task, where only a subset of all lymph nodes in the training set have been annotated. For the challenge submission, multiple methods for training on these weakly supervised data were explored, including noisy label training, loss masking of unlabeled data, and an approach that integrated the TotalSegmentator toolbox as a form of pseudo labeling in order to reduce the number of unknown voxels. Furthermore, multiple public TCIA datasets were incorporated into the training to improve the performance of the deep learning model. Our submitted model achieved a Dice score of 0.628 and an average symmetric surface distance of 5.8~mm on the challenge test set. With our submitted model, we accomplished third rank in the MICCAI2023 LNQ challenge. A finding of our analysis was that the integration of all visible, including non-pathological, lymph nodes improved the overall segmentation performance on pathological lymph nodes of the test set. Furthermore, segmentation models trained only on clinically enlarged lymph nodes, as given in the challenge scenario, could not generalize to smaller pathological lymph nodes. The code and model for the challenge submission are available at \url{https://gitlab.lrz.de/compai/MediastinalLymphNodeSegmentation}.
<div id='section'>Paperid: <span id='pid'>372, <a href='https://arxiv.org/pdf/2405.20044.pdf' target='_blank'>https://arxiv.org/pdf/2405.20044.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengyu Jie, Wanquan Liu, Chenqiang Gao, Yihui Wen, Rui He, Weiping Wen, Pengcheng Li, Jintao Zhang, Deyu Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.20044">A Point-Neighborhood Learning Framework for Nasal Endoscope Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Lesion segmentation on nasal endoscopic images is challenging due to its complex lesion features. Fully-supervised deep learning methods achieve promising performance with pixel-level annotations but impose a significant annotation burden on experts. Although weakly supervised or semi-supervised methods can reduce the labelling burden, their performance is still limited. Some weakly semi-supervised methods employ a novel annotation strategy that labels weak single-point annotations for the entire training set while providing pixel-level annotations for a small subset of the data. However, the relevant weakly semi-supervised methods only mine the limited information of the point itself, while ignoring its label property and surrounding reliable information. This paper proposes a simple yet efficient weakly semi-supervised method called the Point-Neighborhood Learning (PNL) framework. PNL incorporates the surrounding area of the point, referred to as the point-neighborhood, into the learning process. In PNL, we propose a point-neighborhood supervision loss and a pseudo-label scoring mechanism to explicitly guide the model's training. Meanwhile, we proposed a more reliable data augmentation scheme. The proposed method significantly improves performance without increasing the parameters of the segmentation neural network. Extensive experiments on the NPC-LES dataset demonstrate that PNL outperforms existing methods by a significant margin. Additional validation on colonoscopic polyp segmentation datasets confirms the generalizability of the proposed PNL.
<div id='section'>Paperid: <span id='pid'>373, <a href='https://arxiv.org/pdf/2403.16469.pdf' target='_blank'>https://arxiv.org/pdf/2403.16469.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meng Wei, Zhongnian Li, Yong Zhou, Xinzheng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.16469">Learning from Reduced Labels for Long-Tailed Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Long-tailed data is prevalent in real-world classification tasks and heavily relies on supervised information, which makes the annotation process exceptionally labor-intensive and time-consuming. Unfortunately, despite being a common approach to mitigate labeling costs, existing weakly supervised learning methods struggle to adequately preserve supervised information for tail samples, resulting in a decline in accuracy for the tail classes. To alleviate this problem, we introduce a novel weakly supervised labeling setting called Reduced Label. The proposed labeling setting not only avoids the decline of supervised information for the tail samples, but also decreases the labeling costs associated with long-tailed data. Additionally, we propose an straightforward and highly efficient unbiased framework with strong theoretical guarantees to learn from these Reduced Labels. Extensive experiments conducted on benchmark datasets including ImageNet validate the effectiveness of our approach, surpassing the performance of state-of-the-art weakly supervised methods.
<div id='section'>Paperid: <span id='pid'>374, <a href='https://arxiv.org/pdf/2402.17502.pdf' target='_blank'>https://arxiv.org/pdf/2402.17502.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Li Lin, Yixiang Liu, Jiewei Wu, Pujin Cheng, Zhiyuan Cai, Kenneth K. Y. Wong, Xiaoying Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17502">FedLPPA: Learning Personalized Prompt and Aggregation for Federated Weakly-supervised Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated learning (FL) effectively mitigates the data silo challenge brought about by policies and privacy concerns, implicitly harnessing more data for deep model training. However, traditional centralized FL models grapple with diverse multi-center data, especially in the face of significant data heterogeneity, notably in medical contexts. In the realm of medical image segmentation, the growing imperative to curtail annotation costs has amplified the importance of weakly-supervised techniques which utilize sparse annotations such as points, scribbles, etc. A pragmatic FL paradigm shall accommodate diverse annotation formats across different sites, which research topic remains under-investigated. In such context, we propose a novel personalized FL framework with learnable prompt and aggregation (FedLPPA) to uniformly leverage heterogeneous weak supervision for medical image segmentation. In FedLPPA, a learnable universal knowledge prompt is maintained, complemented by multiple learnable personalized data distribution prompts and prompts representing the supervision sparsity. Integrated with sample features through a dual-attention mechanism, those prompts empower each local task decoder to adeptly adjust to both the local distribution and the supervision form. Concurrently, a dual-decoder strategy, predicated on prompt similarity, is introduced for enhancing the generation of pseudo-labels in weakly-supervised learning, alleviating overfitting and noise accumulation inherent to local data, while an adaptable aggregation method is employed to customize the task decoder on a parameter-wise basis. Extensive experiments on four distinct medical image segmentation tasks involving different modalities underscore the superiority of FedLPPA, with its efficacy closely parallels that of fully supervised centralized training. Our code and data will be available.
<div id='section'>Paperid: <span id='pid'>375, <a href='https://arxiv.org/pdf/2402.03785.pdf' target='_blank'>https://arxiv.org/pdf/2402.03785.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haihong Zhao, Chenyi Zi, Yang Liu, Chen Zhang, Yan Zhou, Jia Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.03785">Weakly Supervised Anomaly Detection via Knowledge-Data Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anomaly detection (AD) plays a pivotal role in numerous web-based applications, including malware detection, anti-money laundering, device failure detection, and network fault analysis. Most methods, which rely on unsupervised learning, are hard to reach satisfactory detection accuracy due to the lack of labels. Weakly Supervised Anomaly Detection (WSAD) has been introduced with a limited number of labeled anomaly samples to enhance model performance. Nevertheless, it is still challenging for models, trained on an inadequate amount of labeled data, to generalize to unseen anomalies. In this paper, we introduce a novel framework Knowledge-Data Alignment (KDAlign) to integrate rule knowledge, typically summarized by human experts, to supplement the limited labeled data. Specifically, we transpose these rules into the knowledge space and subsequently recast the incorporation of knowledge as the alignment of knowledge and data. To facilitate this alignment, we employ the Optimal Transport (OT) technique. We then incorporate the OT distance as an additional loss term to the original objective function of WSAD methodologies. Comprehensive experimental results on five real-world datasets demonstrate that our proposed KDAlign framework markedly surpasses its state-of-the-art counterparts, achieving superior performance across various anomaly types.
<div id='section'>Paperid: <span id='pid'>376, <a href='https://arxiv.org/pdf/2511.22823.pdf' target='_blank'>https://arxiv.org/pdf/2511.22823.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Miao Zhang, Junpeng Li, Changchun Hua, Yana Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.22823">A Unified and Stable Risk Minimization Framework for Weakly Supervised Learning with Theoretical Guarantees</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised learning has emerged as a practical alternative to fully supervised learning when complete and accurate labels are costly or infeasible to acquire. However, many existing methods are tailored to specific supervision patterns -- such as positive-unlabeled (PU), unlabeled-unlabeled (UU), complementary-label (CLL), partial-label (PLL), or similarity-unlabeled annotations -- and rely on post-hoc corrections to mitigate instability induced by indirect supervision. We propose a principled, unified framework that bypasses such post-hoc adjustments by directly formulating a stable surrogate risk grounded in the structure of weakly supervised data. The formulation naturally subsumes diverse settings -- including PU, UU, CLL, PLL, multi-class unlabeled, and tuple-based learning -- under a single optimization objective. We further establish a non-asymptotic generalization bound via Rademacher complexity that clarifies how supervision structure, model capacity, and sample size jointly govern performance. Beyond this, we analyze the effect of class-prior misspecification on the bound, deriving explicit terms that quantify its impact, and we study identifiability, giving sufficient conditions -- most notably via supervision stratification across groups -- under which the target risk is recoverable. Extensive experiments show consistent gains across class priors, dataset scales, and class counts -- without heuristic stabilization -- while exhibiting robustness to overfitting.
<div id='section'>Paperid: <span id='pid'>377, <a href='https://arxiv.org/pdf/2510.18406.pdf' target='_blank'>https://arxiv.org/pdf/2510.18406.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Miao Zhang, Junpeng Li, ChangChun HUa, Yana Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.18406">Learning from N-Tuple Data with M Positive Instances: Unbiased Risk Estimation and Theoretical Guarantees</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised learning often operates with coarse aggregate signals rather than instance labels. We study a setting where each training example is an $n$-tuple containing exactly m positives, while only the count m per tuple is observed. This NTMP (N-tuple with M positives) supervision arises in, e.g., image classification with region proposals and multi-instance measurements. We show that tuple counts admit a trainable unbiased risk estimator (URE) by linking the tuple-generation process to latent instance marginals. Starting from fixed (n,m), we derive a closed-form URE and extend it to variable tuple sizes, variable counts, and their combination. Identification holds whenever the effective mixing rate is separated from the class prior. We establish generalization bounds via Rademacher complexity and prove statistical consistency with standard rates under mild regularity assumptions. To improve finite-sample stability, we introduce simple ReLU corrections to the URE that preserve asymptotic correctness. Across benchmarks converted to NTMP tasks, the approach consistently outperforms representative weak-supervision baselines and yields favorable precision-recall and F1 trade-offs. It remains robust under class-prior imbalance and across diverse tuple configurations, demonstrating that count-only supervision can be exploited effectively through a theoretically grounded and practically stable objective.
<div id='section'>Paperid: <span id='pid'>378, <a href='https://arxiv.org/pdf/2509.14097.pdf' target='_blank'>https://arxiv.org/pdf/2509.14097.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaru Chen, Ruohao Guo, Liting Gao, Yang Xiang, Qingyu Luo, Zhenbo Li, Wenwu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14097">Teacher-Guided Pseudo Supervision and Cross-Modal Alignment for Audio-Visual Video Parsing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised audio-visual video parsing (AVVP) seeks to detect audible, visible, and audio-visual events without temporal annotations. Previous work has emphasized refining global predictions through contrastive or collaborative learning, but neglected stable segment-level supervision and class-aware cross-modal alignment. To address this, we propose two strategies: (1) an exponential moving average (EMA)-guided pseudo supervision framework that generates reliable segment-level masks via adaptive thresholds or top-k selection, offering stable temporal guidance beyond video-level labels; and (2) a class-aware cross-modal agreement (CMA) loss that aligns audio and visual embeddings at reliable segment-class pairs, ensuring consistency across modalities while preserving temporal structure. Evaluations on LLP and UnAV-100 datasets shows that our method achieves state-of-the-art (SOTA) performance across multiple metrics.
<div id='section'>Paperid: <span id='pid'>379, <a href='https://arxiv.org/pdf/2509.04086.pdf' target='_blank'>https://arxiv.org/pdf/2509.04086.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaru Chen, Faegheh Sardari, Peiliang Zhang, Ruohao Guo, Yang Xiang, Zhenbo Li, Wenwu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04086">TEn-CATS: Text-Enriched Audio-Visual Video Parsing with Multi-Scale Category-Aware Temporal Graph</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-Visual Video Parsing (AVVP) task aims to identify event categories and their occurrence times in a given video with weakly supervised labels. Existing methods typically fall into two categories: (i) designing enhanced architectures based on attention mechanism for better temporal modeling, and (ii) generating richer pseudo-labels to compensate for the absence of frame-level annotations. However, the first type methods treat noisy segment-level pseudo labels as reliable supervision and the second type methods let indiscriminate attention spread them across all frames, the initial errors are repeatedly amplified during training. To address this issue, we propose a method that combines the Bi-Directional Text Fusion (BiT) module and Category-Aware Temporal Graph (CATS) module. Specifically, we integrate the strengths and complementarity of the two previous research directions. We first perform semantic injection and dynamic calibration on audio and visual modality features through the BiT module, to locate and purify cleaner and richer semantic cues. Then, we leverage the CATS module for semantic propagation and connection to enable precise semantic information dissemination across time. Experimental results demonstrate that our proposed method achieves state-of-the-art (SOTA) performance in multiple key indicators on two benchmark datasets, LLP and UnAV-100.
<div id='section'>Paperid: <span id='pid'>380, <a href='https://arxiv.org/pdf/2508.02179.pdf' target='_blank'>https://arxiv.org/pdf/2508.02179.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenbo Xu, Wei Lu, Xiangyang Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02179">Weakly Supervised Multimodal Temporal Forgery Localization via Multitask Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The spread of Deepfake videos has caused a trust crisis and impaired social stability. Although numerous approaches have been proposed to address the challenges of Deepfake detection and localization, there is still a lack of systematic research on the weakly supervised multimodal fine-grained temporal forgery localization (WS-MTFL). In this paper, we propose a novel weakly supervised multimodal temporal forgery localization via multitask learning (WMMT), which addresses the WS-MTFL under the multitask learning paradigm. WMMT achieves multimodal fine-grained Deepfake detection and temporal partial forgery localization using merely video-level annotations. Specifically, visual and audio modality detection are formulated as two binary classification tasks. The multitask learning paradigm is introduced to integrate these tasks into a multimodal task. Furthermore, WMMT utilizes a Mixture-of-Experts structure to adaptively select appropriate features and localization head, achieving excellent flexibility and localization precision in WS-MTFL. A feature enhancement module with temporal property preserving attention mechanism is proposed to identify the intra- and inter-modality feature deviation and construct comprehensive video features. To further explore the temporal information for weakly supervised learning, an extensible deviation perceiving loss has been proposed, which aims to enlarge the deviation of adjacent segments of the forged samples and reduce the deviation of genuine samples. Extensive experiments demonstrate the effectiveness of multitask learning for WS-MTFL, and the WMMT achieves comparable results to fully supervised approaches in several evaluation metrics.
<div id='section'>Paperid: <span id='pid'>381, <a href='https://arxiv.org/pdf/2507.16596.pdf' target='_blank'>https://arxiv.org/pdf/2507.16596.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenbo Xu, Junyan Wu, Wei Lu, Xiangyang Luo, Qian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16596">A Multimodal Deviation Perceiving Framework for Weakly-Supervised Temporal Forgery Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current researches on Deepfake forensics often treat detection as a classification task or temporal forgery localization problem, which are usually restrictive, time-consuming, and challenging to scale for large datasets. To resolve these issues, we present a multimodal deviation perceiving framework for weakly-supervised temporal forgery localization (MDP), which aims to identify temporal partial forged segments using only video-level annotations. The MDP proposes a novel multimodal interaction mechanism (MI) and an extensible deviation perceiving loss to perceive multimodal deviation, which achieves the refined start and end timestamps localization of forged segments. Specifically, MI introduces a temporal property preserving cross-modal attention to measure the relevance between the visual and audio modalities in the probabilistic embedding space. It could identify the inter-modality deviation and construct comprehensive video features for temporal forgery localization. To explore further temporal deviation for weakly-supervised learning, an extensible deviation perceiving loss has been proposed, aiming at enlarging the deviation of adjacent segments of the forged samples and reducing that of genuine samples. Extensive experiments demonstrate the effectiveness of the proposed framework and achieve comparable results to fully-supervised approaches in several evaluation metrics.
<div id='section'>Paperid: <span id='pid'>382, <a href='https://arxiv.org/pdf/2507.07771.pdf' target='_blank'>https://arxiv.org/pdf/2507.07771.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuying Huang, Junpeng Li, Changchun Hua, Yana Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07771">A Unified Empirical Risk Minimization Framework for Flexible N-Tuples Weak Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To alleviate the annotation burden in supervised learning, N-tuples learning has recently emerged as a powerful weakly-supervised method. While existing N-tuples learning approaches extend pairwise learning to higher-order comparisons and accommodate various real-world scenarios, they often rely on task-specific designs and lack a unified theoretical foundation. In this paper, we propose a general N-tuples learning framework based on empirical risk minimization, which systematically integrates pointwise unlabeled data to enhance learning performance. This paper first unifies the data generation processes of N-tuples and pointwise unlabeled data under a shared probabilistic formulation. Based on this unified view, we derive an unbiased empirical risk estimator that generalizes a broad class of existing N-tuples models. We further establish a generalization error bound for theoretical support. To demonstrate the flexibility of the framework, we instantiate it in four representative weakly supervised scenarios, each recoverable as a special case of our general model. Additionally, to address overfitting issues arising from negative risk terms, we adopt correction functions to adjust the empirical risk. Extensive experiments on benchmark datasets validate the effectiveness of the proposed framework and demonstrate that leveraging pointwise unlabeled data consistently improves generalization across various N-tuples learning tasks.
<div id='section'>Paperid: <span id='pid'>383, <a href='https://arxiv.org/pdf/2506.22301.pdf' target='_blank'>https://arxiv.org/pdf/2506.22301.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Takumi Okuo, Shinnosuke Matsuo, Shota Harada, Kiyohito Tanaka, Ryoma Bise
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22301">Weakly-Supervised Domain Adaptation with Proportion-Constrained Pseudo-Labeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain shift is a significant challenge in machine learning, particularly in medical applications where data distributions differ across institutions due to variations in data collection practices, equipment, and procedures. This can degrade performance when models trained on source domain data are applied to the target domain. Domain adaptation methods have been widely studied to address this issue, but most struggle when class proportions between the source and target domains differ. In this paper, we propose a weakly-supervised domain adaptation method that leverages class proportion information from the target domain, which is often accessible in medical datasets through prior knowledge or statistical reports. Our method assigns pseudo-labels to the unlabeled target data based on class proportion (called proportion-constrained pseudo-labeling), improving performance without the need for additional annotations. Experiments on two endoscopic datasets demonstrate that our method outperforms semi-supervised domain adaptation techniques, even when 5% of the target domain is labeled. Additionally, the experimental results with noisy proportion labels highlight the robustness of our method, further demonstrating its effectiveness in real-world application scenarios.
<div id='section'>Paperid: <span id='pid'>384, <a href='https://arxiv.org/pdf/2505.22490.pdf' target='_blank'>https://arxiv.org/pdf/2505.22490.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ke Zhang, Tianyu Ding, Jiachen Jiang, Tianyi Chen, Ilya Zharkov, Vishal M. Patel, Luming Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22490">ProCrop: Learning Aesthetic Image Cropping from Professional Compositions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image cropping is crucial for enhancing the visual appeal and narrative impact of photographs, yet existing rule-based and data-driven approaches often lack diversity or require annotated training data. We introduce ProCrop, a retrieval-based method that leverages professional photography to guide cropping decisions. By fusing features from professional photographs with those of the query image, ProCrop learns from professional compositions, significantly boosting performance. Additionally, we present a large-scale dataset of 242K weakly-annotated images, generated by out-painting professional images and iteratively refining diverse crop proposals. This composition-aware dataset generation offers diverse high-quality crop proposals guided by aesthetic principles and becomes the largest publicly available dataset for image cropping. Extensive experiments show that ProCrop significantly outperforms existing methods in both supervised and weakly-supervised settings. Notably, when trained on the new dataset, our ProCrop surpasses previous weakly-supervised methods and even matches fully supervised approaches. Both the code and dataset will be made publicly available to advance research in image aesthetics and composition analysis.
<div id='section'>Paperid: <span id='pid'>385, <a href='https://arxiv.org/pdf/2505.01880.pdf' target='_blank'>https://arxiv.org/pdf/2505.01880.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyan Wu, Wenbo Xu, Wei Lu, Xiangyang Luo, Rui Yang, Shize Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01880">Weakly-supervised Audio Temporal Forgery Localization via Progressive Audio-language Co-learning Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio temporal forgery localization (ATFL) aims to find the precise forgery regions of the partial spoof audio that is purposefully modified. Existing ATFL methods rely on training efficient networks using fine-grained annotations, which are obtained costly and challenging in real-world scenarios. To meet this challenge, in this paper, we propose a progressive audio-language co-learning network (LOCO) that adopts co-learning and self-supervision manners to prompt localization performance under weak supervision scenarios. Specifically, an audio-language co-learning module is first designed to capture forgery consensus features by aligning semantics from temporal and global perspectives. In this module, forgery-aware prompts are constructed by using utterance-level annotations together with learnable prompts, which can incorporate semantic priors into temporal content features dynamically. In addition, a forgery localization module is applied to produce forgery proposals based on fused forgery-class activation sequences. Finally, a progressive refinement strategy is introduced to generate pseudo frame-level labels and leverage supervised semantic contrastive learning to amplify the semantic distinction between real and fake content, thereby continuously optimizing forgery-aware features. Extensive experiments show that the proposed LOCO achieves SOTA performance on three public benchmarks.
<div id='section'>Paperid: <span id='pid'>386, <a href='https://arxiv.org/pdf/2412.04383.pdf' target='_blank'>https://arxiv.org/pdf/2412.04383.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rong Li, Shijie Li, Lingdong Kong, Xulei Yang, Junwei Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04383">SeeGround: See and Ground for Zero-Shot Open-Vocabulary 3D Visual Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Visual Grounding (3DVG) aims to locate objects in 3D scenes based on textual descriptions, essential for applications like augmented reality and robotics. Traditional 3DVG approaches rely on annotated 3D datasets and predefined object categories, limiting scalability and adaptability. To overcome these limitations, we introduce SeeGround, a zero-shot 3DVG framework leveraging 2D Vision-Language Models (VLMs) trained on large-scale 2D data. SeeGround represents 3D scenes as a hybrid of query-aligned rendered images and spatially enriched text descriptions, bridging the gap between 3D data and 2D-VLMs input formats. We propose two modules: the Perspective Adaptation Module, which dynamically selects viewpoints for query-relevant image rendering, and the Fusion Alignment Module, which integrates 2D images with 3D spatial descriptions to enhance object localization. Extensive experiments on ScanRefer and Nr3D demonstrate that our approach outperforms existing zero-shot methods by large margins. Notably, we exceed weakly supervised methods and rival some fully supervised ones, outperforming previous SOTA by 7.7% on ScanRefer and 7.1% on Nr3D, showcasing its effectiveness in complex 3DVG tasks.
<div id='section'>Paperid: <span id='pid'>387, <a href='https://arxiv.org/pdf/2411.18225.pdf' target='_blank'>https://arxiv.org/pdf/2411.18225.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zak Buzzard, Konstantin Hemker, Nikola Simidjievski, Mateja Jamnik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18225">PATHS: A Hierarchical Transformer for Efficient Whole Slide Image Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computational analysis of whole slide images (WSIs) has seen significant research progress in recent years, with applications ranging across important diagnostic and prognostic tasks such as survival or cancer subtype prediction. Many state-of-the-art models process the entire slide - which may be as large as $150,000 \times 150,000$ pixels - as a bag of many patches, the size of which necessitates computationally cheap feature aggregation methods. However, a large proportion of these patches are uninformative, such as those containing only healthy or adipose tissue, adding significant noise and size to the bag. We propose Pathology Transformer with Hierarchical Selection (PATHS), a novel top-down method for hierarchical weakly supervised representation learning on slide-level tasks in computational pathology. PATHS is inspired by the cross-magnification manner in which a human pathologist examines a slide, recursively filtering patches at each magnification level to a small subset relevant to the diagnosis. Our method overcomes the complications of processing the entire slide, enabling quadratic self-attention and providing a simple interpretable measure of region importance. We apply PATHS to five datasets of The Cancer Genome Atlas (TCGA), and achieve superior performance on slide-level prediction tasks when compared to previous methods, despite processing only a small proportion of the slide.
<div id='section'>Paperid: <span id='pid'>388, <a href='https://arxiv.org/pdf/2410.01544.pdf' target='_blank'>https://arxiv.org/pdf/2410.01544.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zaiquan Yang, Yuhao Liu, Jiaying Lin, Gerhard Hancke, Rynson W. H. Lau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01544">Boosting Weakly-Supervised Referring Image Segmentation via Progressive Comprehension</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper explores the weakly-supervised referring image segmentation (WRIS) problem, and focuses on a challenging setup where target localization is learned directly from image-text pairs. We note that the input text description typically already contains detailed information on how to localize the target object, and we also observe that humans often follow a step-by-step comprehension process (\ie, progressively utilizing target-related attributes and relations as cues) to identify the target object. Hence, we propose a novel Progressive Comprehension Network (PCNet) to leverage target-related textual cues from the input description for progressively localizing the target object. Specifically, we first use a Large Language Model (LLM) to decompose the input text description into short phrases. These short phrases are taken as target-related cues and fed into a Conditional Referring Module (CRM) in multiple stages, to allow updating the referring text embedding and enhance the response map for target localization in a multi-stage manner. Based on the CRM, we then propose a Region-aware Shrinking (RaS) loss to constrain the visual localization to be conducted progressively in a coarse-to-fine manner across different stages. Finally, we introduce an Instance-aware Disambiguation (IaD) loss to suppress instance localization ambiguity by differentiating overlapping response maps generated by different referring texts on the same image. Extensive experiments show that our method outperforms SOTA methods on three common benchmarks.
<div id='section'>Paperid: <span id='pid'>389, <a href='https://arxiv.org/pdf/2409.01030.pdf' target='_blank'>https://arxiv.org/pdf/2409.01030.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahe Tian, Peng Chen, Cai Yu, Xiaomeng Fu, Xi Wang, Jiao Dai, Jizhong Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01030">Learning to Discover Forgery Cues for Face Forgery Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Locating manipulation maps, i.e., pixel-level annotation of forgery cues, is crucial for providing interpretable detection results in face forgery detection. Related learning objects have also been widely adopted as auxiliary tasks to improve the classification performance of detectors whereas they require comparisons between paired real and forged faces to obtain manipulation maps as supervision. This requirement restricts their applicability to unpaired faces and contradicts real-world scenarios. Moreover, the used comparison methods annotate all changed pixels, including noise introduced by compression and upsampling. Using such maps as supervision hinders the learning of exploitable cues and makes models prone to overfitting. To address these issues, we introduce a weakly supervised model in this paper, named Forgery Cue Discovery (FoCus), to locate forgery cues in unpaired faces. Unlike some detectors that claim to locate forged regions in attention maps, FoCus is designed to sidestep their shortcomings of capturing partial and inaccurate forgery cues. Specifically, we propose a classification attentive regions proposal module to locate forgery cues during classification and a complementary learning module to facilitate the learning of richer cues. The produced manipulation maps can serve as better supervision to enhance face forgery detectors. Visualization of the manipulation maps of the proposed FoCus exhibits superior interpretability and robustness compared to existing methods. Experiments on five datasets and four multi-task models demonstrate the effectiveness of FoCus in both in-dataset and cross-dataset evaluations.
<div id='section'>Paperid: <span id='pid'>390, <a href='https://arxiv.org/pdf/2408.09613.pdf' target='_blank'>https://arxiv.org/pdf/2408.09613.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Herun Wan, Minnan Luo, Zihan Ma, Guang Dai, Xiang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09613">How Do Social Bots Participate in Misinformation Spread? A Comprehensive Dataset and Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Social media platforms provide an ideal environment to spread misinformation, where social bots can accelerate the spread. This paper explores the interplay between social bots and misinformation on the Sina Weibo platform. We construct a large-scale dataset that includes annotations for both misinformation and social bots. From the misinformation perspective, the dataset is multimodal, containing 11,393 pieces of misinformation and 16,416 pieces of verified information. From the social bot perspective, this dataset contains 65,749 social bots and 345,886 genuine accounts, annotated using a weakly supervised annotator. Extensive experiments demonstrate the comprehensiveness of the dataset, the clear distinction between misinformation and real information, and the high quality of social bot annotations. Further analysis illustrates that: (i) social bots are deeply involved in information spread; (ii) misinformation with the same topics has similar content, providing the basis of echo chambers, and social bots would amplify this phenomenon; and (iii) social bots generate similar content aiming to manipulate public opinions.
<div id='section'>Paperid: <span id='pid'>391, <a href='https://arxiv.org/pdf/2407.05180.pdf' target='_blank'>https://arxiv.org/pdf/2407.05180.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julien Quarez, Marc Modat, Sebastien Ourselin, Jonathan Shapey, Alejandro Granados
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05180">ReCAP: Recursive Cross Attention Network for Pseudo-Label Generation in Robotic Surgical Skill Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In surgical skill assessment, the Objective Structured Assessments of Technical Skills (OSATS) and Global Rating Scale (GRS) are well-established tools for evaluating surgeons during training. These metrics, along with performance feedback, help surgeons improve and reach practice standards. Recent research on the open-source JIGSAWS dataset, which includes both GRS and OSATS labels, has focused on regressing GRS scores from kinematic data, video, or their combination. However, we argue that regressing GRS alone is limiting, as it aggregates OSATS scores and overlooks clinically meaningful variations during a surgical trial. To address this, we developed a weakly-supervised recurrent transformer model that tracks a surgeon's performance throughout a session by mapping hidden states to six OSATS, derived from kinematic data. These OSATS scores are averaged to predict GRS, allowing us to compare our model's performance against state-of-the-art (SOTA) methods. We report Spearman's Correlation Coefficients (SCC) demonstrating that our model outperforms SOTA using kinematic data (SCC 0.83-0.88), and matches performance with video-based models. Our model also surpasses SOTA in most tasks for average OSATS predictions (SCC 0.46-0.70) and specific OSATS (SCC 0.56-0.95). The generation of pseudo-labels at the segment level translates quantitative predictions into qualitative feedback, vital for automated surgical skill assessment pipelines. A senior surgeon validated our model's outputs, agreeing with 77\% of the weakly-supervised predictions \(p=0.006\).
<div id='section'>Paperid: <span id='pid'>392, <a href='https://arxiv.org/pdf/2407.03331.pdf' target='_blank'>https://arxiv.org/pdf/2407.03331.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunzhe Li, Hongzi Zhu, Zhuohong Deng, Yunlong Cheng, Liang Zhang, Shan Chang, Minyi Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.03331">Anole: Adapting Diverse Compressed Models For Cross-Scene Prediction On Mobile Devices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Emerging Artificial Intelligence of Things (AIoT) applications desire online prediction using deep neural network (DNN) models on mobile devices. However, due to the movement of devices, unfamiliar test samples constantly appear, significantly affecting the prediction accuracy of a pre-trained DNN. In addition, unstable network connection calls for local model inference. In this paper, we propose a light-weight scheme, called Anole, to cope with the local DNN model inference on mobile devices. The core idea of Anole is to first establish an army of compact DNN models, and then adaptively select the model fitting the current test sample best for online inference. The key is to automatically identify model-friendly scenes for training scene-specific DNN models. To this end, we design a weakly-supervised scene representation learning algorithm by combining both human heuristics and feature similarity in separating scenes. Moreover, we further train a model classifier to predict the best-fit scene-specific DNN model for each test sample. We implement Anole on different types of mobile devices and conduct extensive trace-driven and real-world experiments based on unmanned aerial vehicles (UAVs). The results demonstrate that Anole outwits the method of using a versatile large DNN in terms of prediction accuracy (4.5% higher), response time (33.1% faster) and power consumption (45.1% lower).
<div id='section'>Paperid: <span id='pid'>393, <a href='https://arxiv.org/pdf/2405.09041.pdf' target='_blank'>https://arxiv.org/pdf/2405.09041.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shinnosuke Matsuo, Daiki Suehiro, Seiichi Uchida, Hiroaki Ito, Kazuhiro Terada, Akihiko Yoshizawa, Ryoma Bise
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.09041">Learning from Partial Label Proportions for Whole Slide Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we address the segmentation of tumor subtypes in whole slide images (WSI) by utilizing incomplete label proportions. Specifically, we utilize `partial' label proportions, which give the proportions among tumor subtypes but do not give the proportion between tumor and non-tumor. Partial label proportions are recorded as the standard diagnostic information by pathologists, and we, therefore, want to use them for realizing the segmentation model that can classify each WSI patch into one of the tumor subtypes or non-tumor. We call this problem ``learning from partial label proportions (LPLP)'' and formulate the problem as a weakly supervised learning problem. Then, we propose an efficient algorithm for this challenging problem by decomposing it into two weakly supervised learning subproblems: multiple instance learning (MIL) and learning from label proportions (LLP). These subproblems are optimized efficiently in the end-to-end manner. The effectiveness of our algorithm is demonstrated through experiments conducted on two WSI datasets.
<div id='section'>Paperid: <span id='pid'>394, <a href='https://arxiv.org/pdf/2511.18136.pdf' target='_blank'>https://arxiv.org/pdf/2511.18136.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunming He, Rihan Zhang, Longxiang Tang, Ziyun Yang, Kai Li, Deng-Ping Fan, Sina Farsiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.18136">SCALER: SAM-Enhanced Collaborative Learning for Label-Deficient Concealed Object Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing methods for label-deficient concealed object segmentation (LDCOS) either rely on consistency constraints or Segment Anything Model (SAM)-based pseudo-labeling. However, their performance remains limited due to the intrinsic concealment of targets and the scarcity of annotations. This study investigates two key questions: (1) Can consistency constraints and SAM-based supervision be jointly integrated to better exploit complementary information and enhance the segmenter? and (2) beyond that, can the segmenter in turn guide SAM through reciprocal supervision, enabling mutual improvement? To answer these questions, we present SCALER, a unified collaborative framework toward LDCOS that jointly optimizes a mean-teacher segmenter and a learnable SAM. SCALER operates in two alternating phases. In \textbf{Phase \uppercase\expandafter{\romannumeral1}}, the segmenter is optimized under fixed SAM supervision using entropy-based image-level and uncertainty-based pixel-level weighting to select reliable pseudo-label regions and emphasize harder examples. In \textbf{Phase \uppercase\expandafter{\romannumeral2}}, SAM is updated via augmentation invariance and noise resistance losses, leveraging its inherent robustness to perturbations. Experiments demonstrate that SCALER yields consistent performance gains across eight semi- and weakly-supervised COS tasks. The results further suggest that SCALER can serve as a general training paradigm to enhance both lightweight segmenters and large foundation models under label-scarce conditions. Code will be released.
<div id='section'>Paperid: <span id='pid'>395, <a href='https://arxiv.org/pdf/2510.23217.pdf' target='_blank'>https://arxiv.org/pdf/2510.23217.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alois Thomas, Maya Varma, Jean-Benoit Delbrouck, Curtis P. Langlotz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.23217">Process Reward Models for Sentence-Level Verification of LVLM Radiology Reports</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automating radiology report generation with Large Vision-Language Models (LVLMs) holds great potential, yet these models often produce clinically critical hallucinations, posing serious risks. Existing hallucination detection methods frequently lack the necessary sentence-level granularity or robust generalization across different LVLM generators. We introduce a novel approach: a sentence-level Process Reward Model (PRM) adapted for this vision-language task. Our PRM predicts the factual correctness of each generated sentence, conditioned on clinical context and preceding text. When fine-tuned on MIMIC-CXR with weakly-supervised labels, a lightweight 0.5B-parameter PRM outperforms existing verification techniques, demonstrating, for instance, relative improvements of 7.5% in Matthews Correlation Coefficient and 1.8% in AUROC over strong white-box baselines on outputs from one LVLM. Unlike methods reliant on internal model states, our PRM demonstrates strong generalization to an unseen LVLM. We further show its practical utility: PRM scores effectively filter low-quality reports, improving F1-CheXbert scores by 4.5% (when discarding the worst 10% of reports). Moreover, when guiding a novel weighted best-of-N selection process on the MIMIC-CXR test set, our PRM show relative improvements in clinical metrics of 7.4% for F1-CheXbert and 0.6% for BERTScore. These results demonstrate that a lightweight, context-aware PRM provides a model-agnostic safety layer for clinical LVLMs without access to internal activations
<div id='section'>Paperid: <span id='pid'>396, <a href='https://arxiv.org/pdf/2509.04737.pdf' target='_blank'>https://arxiv.org/pdf/2509.04737.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryoga Oishi, Sho Sakaino, Toshiaki Tsuji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04737">Imitation Learning Based on Disentangled Representation Learning of Behavioral Characteristics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of robot learning, coordinating robot actions through language instructions is becoming increasingly feasible. However, adapting actions to human instructions remains challenging, as such instructions are often qualitative and require exploring behaviors that satisfy varying conditions. This paper proposes a motion generation model that adapts robot actions in response to modifier directives human instructions imposing behavioral conditions during task execution. The proposed method learns a mapping from modifier directives to actions by segmenting demonstrations into short sequences, assigning weakly supervised labels corresponding to specific modifier types. We evaluated our method in wiping and pick and place tasks. Results show that it can adjust motions online in response to modifier directives, unlike conventional batch-based methods that cannot adapt during execution.
<div id='section'>Paperid: <span id='pid'>397, <a href='https://arxiv.org/pdf/2507.19592.pdf' target='_blank'>https://arxiv.org/pdf/2507.19592.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meng Wei, Charlie Budd, Oluwatosin Alabi, Miaojing Shi, Tom Vercauteren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19592">SurgPIS: Surgical-instrument-level Instances and Part-level Semantics for Weakly-supervised Part-aware Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Consistent surgical instrument segmentation is critical for automation in robot-assisted surgery. Yet, existing methods only treat instrument-level instance segmentation (IIS) or part-level semantic segmentation (PSS) separately, without interaction between these tasks. In this work, we formulate a surgical tool segmentation as a unified part-aware instance segmentation (PIS) problem and introduce SurgPIS, the first PIS model for surgical instruments. Our method adopts a transformer-based mask classification approach and introduces part-specific queries derived from instrument-level object queries, explicitly linking parts to their parent instrument instances. In order to address the lack of large-scale datasets with both instance- and part-level labels, we propose a weakly-supervised learning strategy for SurgPIS to learn from disjoint datasets labelled for either IIS or PSS purposes. During training, we aggregate our PIS predictions into IIS or PSS masks, thereby allowing us to compute a loss against partially labelled datasets. A student-teacher approach is developed to maintain prediction consistency for missing PIS information in the partially labelled data, e.g., parts of the IIS labelled data. Extensive experiments across multiple datasets validate the effectiveness of SurgPIS, achieving state-of-the-art performance in PIS as well as IIS, PSS, and instrument-level semantic segmentation.
<div id='section'>Paperid: <span id='pid'>398, <a href='https://arxiv.org/pdf/2507.10300.pdf' target='_blank'>https://arxiv.org/pdf/2507.10300.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hatef Otroshi Shahreza, SÃ©bastien Marcel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10300">FaceLLM: A Multimodal Large Language Model for Face Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal large language models (MLLMs) have shown remarkable performance in vision-language tasks. However, existing MLLMs are primarily trained on generic datasets, limiting their ability to reason on domain-specific visual cues such as those in facial images. In particular, tasks that require detailed understanding of facial structure, expression, emotion, and demographic features remain underexplored by MLLMs due to the lack of large-scale annotated face image-text datasets. In this work, we introduce FaceLLM, a multimodal large language model trained specifically for facial image understanding. To construct the training data, we propose a novel weakly supervised pipeline that uses ChatGPT with attribute-aware prompts to generate high-quality question-answer pairs based on images from the FairFace dataset. The resulting corpus, called FairFaceGPT, covers a diverse set of attributes including expression, pose, skin texture, and forensic information. Our experiments demonstrate that FaceLLM improves the performance of MLLMs on various face-centric tasks and achieves state-of-the-art performance. This work highlights the potential of synthetic supervision via language models for building domain-specialized MLLMs, and sets a precedent for trustworthy, human-centric multimodal AI systems. FairFaceGPT dataset and pretrained FaceLLM models are publicly available in the project page.
<div id='section'>Paperid: <span id='pid'>399, <a href='https://arxiv.org/pdf/2505.24656.pdf' target='_blank'>https://arxiv.org/pdf/2505.24656.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dimitrios Damianos, Georgios Paraskevopoulos, Alexandros Potamianos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24656">MSDA: Combining Pseudo-labeling and Self-Supervision for Unsupervised Domain Adaptation in ASR</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we investigate the Meta PL unsupervised domain adaptation framework for Automatic Speech Recognition (ASR). We introduce a Multi-Stage Domain Adaptation pipeline (MSDA), a sample-efficient, two-stage adaptation approach that integrates self-supervised learning with semi-supervised techniques. MSDA is designed to enhance the robustness and generalization of ASR models, making them more adaptable to diverse conditions. It is particularly effective for low-resource languages like Greek and in weakly supervised scenarios where labeled data is scarce or noisy. Through extensive experiments, we demonstrate that Meta PL can be applied effectively to ASR tasks, achieving state-of-the-art results, significantly outperforming state-of-the-art methods, and providing more robust solutions for unsupervised domain adaptation in ASR. Our ablations highlight the necessity of utilizing a cascading approach when combining self-supervision with self-training.
<div id='section'>Paperid: <span id='pid'>400, <a href='https://arxiv.org/pdf/2503.12905.pdf' target='_blank'>https://arxiv.org/pdf/2503.12905.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanbin Qian, Shuhan Ye, Chong Wang, Xiaojie Cai, Jiangbo Qian, Jiafei Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12905">UCF-Crime-DVS: A Novel Event-Based Dataset for Video Anomaly Detection with Spiking Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video anomaly detection plays a significant role in intelligent surveillance systems. To enhance model's anomaly recognition ability, previous works have typically involved RGB, optical flow, and text features. Recently, dynamic vision sensors (DVS) have emerged as a promising technology, which capture visual information as discrete events with a very high dynamic range and temporal resolution. It reduces data redundancy and enhances the capture capacity of moving objects compared to conventional camera. To introduce this rich dynamic information into the surveillance field, we created the first DVS video anomaly detection benchmark, namely UCF-Crime-DVS. To fully utilize this new data modality, a multi-scale spiking fusion network (MSF) is designed based on spiking neural networks (SNNs). This work explores the potential application of dynamic information from event data in video anomaly detection. Our experiments demonstrate the effectiveness of our framework on UCF-Crime-DVS and its superior performance compared to other models, establishing a new baseline for SNN-based weakly supervised video anomaly detection.
<div id='section'>Paperid: <span id='pid'>401, <a href='https://arxiv.org/pdf/2503.04154.pdf' target='_blank'>https://arxiv.org/pdf/2503.04154.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chupeng Liu, Runkai Zhao, Weidong Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04154">CA-W3D: Leveraging Context-Aware Knowledge for Weakly Supervised Monocular 3D Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised monocular 3D detection, while less annotation-intensive, often struggles to capture the global context required for reliable 3D reasoning. Conventional label-efficient methods focus on object-centric features, neglecting contextual semantic relationships that are critical in complex scenes. In this work, we propose a Context-Aware Weak Supervision for Monocular 3D object detection, namely CA-W3D, to address this limitation in a two-stage training paradigm. Specifically, we first introduce a pre-training stage employing Region-wise Object Contrastive Matching (ROCM), which aligns regional object embeddings derived from a trainable monocular 3D encoder and a frozen open-vocabulary 2D visual grounding model. This alignment encourages the monocular encoder to discriminate scene-specific attributes and acquire richer contextual knowledge. In the second stage, we incorporate a pseudo-label training process with a Dual-to-One Distillation (D2OD) mechanism, which effectively transfers contextual priors into the monocular encoder while preserving spatial fidelity and maintaining computational efficiency during inference. Extensive experiments conducted on the public KITTI benchmark demonstrate the effectiveness of our approach, surpassing the SoTA method over all metrics, highlighting the importance of contextual-aware knowledge in weakly-supervised monocular 3D detection.
<div id='section'>Paperid: <span id='pid'>402, <a href='https://arxiv.org/pdf/2412.14579.pdf' target='_blank'>https://arxiv.org/pdf/2412.14579.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qianpu Sun, Changyong Shu, Sifan Zhou, Zichen Yu, Yan Chen, Dawei Yang, Yuan Chun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14579">GSRender: Deduplicated Occupancy Prediction via Weakly Supervised 3D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D occupancy perception is gaining increasing attention due to its capability to offer detailed and precise environment representations. Previous weakly-supervised NeRF methods balance efficiency and accuracy, with mIoU varying by 5-10 points due to sampling count along camera rays. Recently, real-time Gaussian splatting has gained widespread popularity in 3D reconstruction, and the occupancy prediction task can also be viewed as a reconstruction task. Consequently, we propose GSRender, which naturally employs 3D Gaussian Splatting for occupancy prediction, simplifying the sampling process. In addition, the limitations of 2D supervision result in duplicate predictions along the same camera ray. We implemented the Ray Compensation (RC) module, which mitigates this issue by compensating for features from adjacent frames. Finally, we redesigned the loss to eliminate the impact of dynamic objects from adjacent frames. Extensive experiments demonstrate that our approach achieves SOTA (state-of-the-art) results in RayIoU (+6.0), while narrowing the gap with 3D supervision methods. Our code will be released soon.
<div id='section'>Paperid: <span id='pid'>403, <a href='https://arxiv.org/pdf/2411.08530.pdf' target='_blank'>https://arxiv.org/pdf/2411.08530.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ravi Kant Gupta, Dadi Dharani, Shambhavi Shanker, Amit Sethi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08530">Efficient Whole Slide Image Classification through Fisher Vector Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advancement of digital pathology, particularly through computational analysis of whole slide images (WSI), is poised to significantly enhance diagnostic precision and efficiency. However, the large size and complexity of WSIs make it difficult to analyze and classify them using computers. This study introduces a novel method for WSI classification by automating the identification and examination of the most informative patches, thus eliminating the need to process the entire slide. Our method involves two-stages: firstly, it extracts only a few patches from the WSIs based on their pathological significance; and secondly, it employs Fisher vectors (FVs) for representing features extracted from these patches, which is known for its robustness in capturing fine-grained details. This approach not only accentuates key pathological features within the WSI representation but also significantly reduces computational overhead, thus making the process more efficient and scalable. We have rigorously evaluated the proposed method across multiple datasets to benchmark its performance against comprehensive WSI analysis and contemporary weakly-supervised learning methodologies. The empirical results indicate that our focused analysis of select patches, combined with Fisher vector representation, not only aligns with, but at times surpasses, the classification accuracy of standard practices. Moreover, this strategy notably diminishes computational load and resource expenditure, thereby establishing an efficient and precise framework for WSI analysis in the realm of digital pathology.
<div id='section'>Paperid: <span id='pid'>404, <a href='https://arxiv.org/pdf/2411.03551.pdf' target='_blank'>https://arxiv.org/pdf/2411.03551.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiling Yue, Yingying Fang, Liutao Yang, Nikhil Baid, Simon Walsh, Guang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03551">Enhancing Weakly Supervised Semantic Segmentation for Fibrosis via Controllable Image Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fibrotic Lung Disease (FLD) is a severe condition marked by lung stiffening and scarring, leading to respiratory decline. High-resolution computed tomography (HRCT) is critical for diagnosing and monitoring FLD; however, fibrosis appears as irregular, diffuse patterns with unclear boundaries, leading to high inter-observer variability and time-intensive manual annotation. To tackle this challenge, we propose DiffSeg, a novel weakly supervised semantic segmentation (WSSS) method that uses image-level annotations to generate pixel-level fibrosis segmentation, reducing the need for fine-grained manual labeling. Additionally, our DiffSeg incorporates a diffusion-based generative model to synthesize HRCT images with different levels of fibrosis from healthy slices, enabling the generation of the fibrosis-injected slices and their paired fibrosis location. Experiments indicate that our method significantly improves the accuracy of pseudo masks generated by existing WSSS methods, greatly reducing the complexity of manual labeling and enhancing the consistency of the generated masks.
<div id='section'>Paperid: <span id='pid'>405, <a href='https://arxiv.org/pdf/2406.00487.pdf' target='_blank'>https://arxiv.org/pdf/2406.00487.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gene Li, Lin Chen, Adel Javanmard, Vahab Mirrokni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00487">Optimistic Rates for Learning from Label Proportions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider a weakly supervised learning problem called Learning from Label Proportions (LLP), where examples are grouped into ``bags'' and only the average label within each bag is revealed to the learner. We study various learning rules for LLP that achieve PAC learning guarantees for classification loss. We establish that the classical Empirical Proportional Risk Minimization (EPRM) learning rule (Yu et al., 2014) achieves fast rates under realizability, but EPRM and similar proportion matching learning rules can fail in the agnostic setting. We also show that (1) a debiased proportional square loss, as well as (2) a recently proposed EasyLLP learning rule (Busa-Fekete et al., 2023) both achieve ``optimistic rates'' (Panchenko, 2002); in both the realizable and agnostic settings, their sample complexity is optimal (up to log factors) in terms of $Îµ, Î´$, and VC dimension.
<div id='section'>Paperid: <span id='pid'>406, <a href='https://arxiv.org/pdf/2402.10575.pdf' target='_blank'>https://arxiv.org/pdf/2402.10575.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Hossein Amani, Nicolas Mario Baldwin, Amin Mansouri, Martin Josifoski, Maxime Peyrard, Robert West
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.10575">Symbolic Autoencoding for Self-Supervised Sequence Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional language models, adept at next-token prediction in text sequences, often struggle with transduction tasks between distinct symbolic systems, particularly when parallel data is scarce. Addressing this issue, we introduce \textit{symbolic autoencoding} ($Î£$AE), a self-supervised framework that harnesses the power of abundant unparallel data alongside limited parallel data. $Î£$AE connects two generative models via a discrete bottleneck layer and is optimized end-to-end by minimizing reconstruction loss (simultaneously with supervised loss for the parallel data), such that the sequence generated by the discrete bottleneck can be read out as the transduced input sequence. We also develop gradient-based methods allowing for efficient self-supervised sequence learning despite the discreteness of the bottleneck. Our results demonstrate that $Î£$AE significantly enhances performance on transduction tasks, even with minimal parallel data, offering a promising solution for weakly supervised learning scenarios.
<div id='section'>Paperid: <span id='pid'>407, <a href='https://arxiv.org/pdf/2401.11122.pdf' target='_blank'>https://arxiv.org/pdf/2401.11122.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Chen, Yazhou Yao, Xingguo Huang, Zechao Li, Liqiang Nie, Jinhui Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11122">Spatial Structure Constraints for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The image-level label has prevailed in weakly supervised semantic segmentation tasks due to its easy availability. Since image-level labels can only indicate the existence or absence of specific categories of objects, visualization-based techniques have been widely adopted to provide object location clues. Considering class activation maps (CAMs) can only locate the most discriminative part of objects, recent approaches usually adopt an expansion strategy to enlarge the activation area for more integral object localization. However, without proper constraints, the expanded activation will easily intrude into the background region. In this paper, we propose spatial structure constraints (SSC) for weakly supervised semantic segmentation to alleviate the unwanted object over-activation of attention expansion. Specifically, we propose a CAM-driven reconstruction module to directly reconstruct the input image from deep CAM features, which constrains the diffusion of last-layer object attention by preserving the coarse spatial structure of the image content. Moreover, we propose an activation self-modulation module to refine CAMs with finer spatial structure details by enhancing regional consistency. Without external saliency models to provide background clues, our approach achieves 72.7\% and 47.0\% mIoU on the PASCAL VOC 2012 and COCO datasets, respectively, demonstrating the superiority of our proposed approach.
<div id='section'>Paperid: <span id='pid'>408, <a href='https://arxiv.org/pdf/2511.10241.pdf' target='_blank'>https://arxiv.org/pdf/2511.10241.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinxuan Li, Yi Zhang, Jian-Fang Hu, Chaolei Tan, Tianming Liang, Beihao Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.10241">TubeRMC: Tube-conditioned Reconstruction with Mutual Constraints for Weakly-supervised Spatio-Temporal Video Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spatio-Temporal Video Grounding (STVG) aims to localize a spatio-temporal tube that corresponds to a given language query in an untrimmed video. This is a challenging task since it involves complex vision-language understanding and spatiotemporal reasoning. Recent works have explored weakly-supervised setting in STVG to eliminate reliance on fine-grained annotations like bounding boxes or temporal stamps. However, they typically follow a simple late-fusion manner, which generates tubes independent of the text description, often resulting in failed target identification and inconsistent target tracking. To address this limitation, we propose a Tube-conditioned Reconstruction with Mutual Constraints (\textbf{TubeRMC}) framework that generates text-conditioned candidate tubes with pre-trained visual grounding models and further refine them via tube-conditioned reconstruction with spatio-temporal constraints. Specifically, we design three reconstruction strategies from temporal, spatial, and spatio-temporal perspectives to comprehensively capture rich tube-text correspondences. Each strategy is equipped with a Tube-conditioned Reconstructor, utilizing spatio-temporal tubes as condition to reconstruct the key clues in the query. We further introduce mutual constraints between spatial and temporal proposals to enhance their quality for reconstruction. TubeRMC outperforms existing methods on two public benchmarks VidSTG and HCSTVG. Further visualization shows that TubeRMC effectively mitigates both target identification errors and inconsistent tracking.
<div id='section'>Paperid: <span id='pid'>409, <a href='https://arxiv.org/pdf/2510.25413.pdf' target='_blank'>https://arxiv.org/pdf/2510.25413.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shakib Yazdani, Yasser Hamidullah, Cristina España-Bonet, Josef van Genabith
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.25413">Seeing, Signing, and Saying: A Vision-Language Model-Assisted Pipeline for Sign Language Data Acquisition and Curation from Social Media</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most existing sign language translation (SLT) datasets are limited in scale, lack multilingual coverage, and are costly to curate due to their reliance on expert annotation and controlled recording setup. Recently, Vision Language Models (VLMs) have demonstrated strong capabilities as evaluators and real-time assistants. Despite these advancements, their potential remains untapped in the context of sign language dataset acquisition. To bridge this gap, we introduce the first automated annotation and filtering framework that utilizes VLMs to reduce reliance on manual effort while preserving data quality. Our method is applied to TikTok videos across eight sign languages and to the already curated YouTube-SL-25 dataset in German Sign Language for the purpose of additional evaluation. Our VLM-based pipeline includes a face visibility detection, a sign activity recognition, a text extraction from video content, and a judgment step to validate alignment between video and text, implementing generic filtering, annotation and validation steps. Using the resulting corpus, TikTok-SL-8, we assess the performance of two off-the-shelf SLT models on our filtered dataset for German and American Sign Languages, with the goal of establishing baselines and evaluating the robustness of recent models on automatically extracted, slightly noisy data. Our work enables scalable, weakly supervised pretraining for SLT and facilitates data acquisition from social media.
<div id='section'>Paperid: <span id='pid'>410, <a href='https://arxiv.org/pdf/2508.10104.pdf' target='_blank'>https://arxiv.org/pdf/2508.10104.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Oriane SimÃ©oni, Huy V. Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, MichaÃ«l Ramamonjisoa, Francisco Massa, Daniel Haziza, Luca Wehrstedt, Jianyuan Wang, TimothÃ©e Darcet, ThÃ©o Moutakanni, Leonel Sentana, Claire Roberts, Andrea Vedaldi, Jamie Tolan, John Brandt, Camille Couprie, Julien Mairal, HervÃ© JÃ©gou, Patrick Labatut, Piotr Bojanowski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10104">DINOv3</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-supervised learning holds the promise of eliminating the need for manual data annotation, enabling models to scale effortlessly to massive datasets and larger architectures. By not being tailored to specific tasks or domains, this training paradigm has the potential to learn visual representations from diverse sources, ranging from natural to aerial images -- using a single algorithm. This technical report introduces DINOv3, a major milestone toward realizing this vision by leveraging simple yet effective strategies. First, we leverage the benefit of scaling both dataset and model size by careful data preparation, design, and optimization. Second, we introduce a new method called Gram anchoring, which effectively addresses the known yet unsolved issue of dense feature maps degrading during long training schedules. Finally, we apply post-hoc strategies that further enhance our models' flexibility with respect to resolution, model size, and alignment with text. As a result, we present a versatile vision foundation model that outperforms the specialized state of the art across a broad range of settings, without fine-tuning. DINOv3 produces high-quality dense features that achieve outstanding performance on various vision tasks, significantly surpassing previous self- and weakly-supervised foundation models. We also share the DINOv3 suite of vision models, designed to advance the state of the art on a wide spectrum of tasks and data by providing scalable solutions for diverse resource constraints and deployment scenarios.
<div id='section'>Paperid: <span id='pid'>411, <a href='https://arxiv.org/pdf/2508.03745.pdf' target='_blank'>https://arxiv.org/pdf/2508.03745.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenwen Li, Chia-Yu Hsu, Maosheng Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03745">Tobler's First Law in GeoAI: A Spatially Explicit Deep Learning Model for Terrain Feature Detection Under Weak Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent interest in geospatial artificial intelligence (GeoAI) has fostered a wide range of applications using artificial intelligence (AI), especially deep learning, for geospatial problem solving. However, major challenges such as a lack of training data and the neglect of spatial principles and spatial effects in AI model design remain, significantly hindering the in-depth integration of AI with geospatial research. This paper reports our work in developing a deep learning model that enables object detection, particularly of natural features, in a weakly supervised manner. Our work makes three contributions: First, we present a method of object detection using only weak labels. This is achieved by developing a spatially explicit model based on Tobler's first law of geography. Second, we incorporate attention maps into the object detection pipeline and develop a multistage training strategy to improve performance. Third, we apply this model to detect impact craters on Mars, a task that previously required extensive manual effort. The model generalizes to both natural and human-made features on the surfaces of Earth and other planets. This research advances the theoretical and methodological foundations of GeoAI.
<div id='section'>Paperid: <span id='pid'>412, <a href='https://arxiv.org/pdf/2507.01792.pdf' target='_blank'>https://arxiv.org/pdf/2507.01792.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Zheng, Ye Wang, Rui Ma, Zuxuan Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01792">FreeLoRA: Enabling Training-Free LoRA Fusion for Autoregressive Multi-Subject Personalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Subject-driven image generation plays a crucial role in applications such as virtual try-on and poster design. Existing approaches typically fine-tune pretrained generative models or apply LoRA-based adaptations for individual subjects. However, these methods struggle with multi-subject personalization, as combining independently adapted modules often requires complex re-tuning or joint optimization. We present FreeLoRA, a simple and generalizable framework that enables training-free fusion of subject-specific LoRA modules for multi-subject personalization. Each LoRA module is adapted on a few images of a specific subject using a Full Token Tuning strategy, where it is applied across all tokens in the prompt to encourage weakly supervised token-content alignment. At inference, we adopt Subject-Aware Inference, activating each module only on its corresponding subject tokens. This enables training-free fusion of multiple personalized subjects within a single image, while mitigating overfitting and mutual interference between subjects. Extensive experiments show that FreeLoRA achieves strong performance in both subject fidelity and prompt consistency.
<div id='section'>Paperid: <span id='pid'>413, <a href='https://arxiv.org/pdf/2503.13821.pdf' target='_blank'>https://arxiv.org/pdf/2503.13821.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chi Hsuan Wu, Kumar Ashutosh, Kristen Grauman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13821">Stitch-a-Recipe: Video Demonstration from Multistep Descriptions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>When obtaining visual illustrations from text descriptions, today's methods take a description with-a single text context caption, or an action description-and retrieve or generate the matching visual context. However, prior work does not permit visual illustration of multistep descriptions, e.g. a cooking recipe composed of multiple steps. Furthermore, simply handling each step description in isolation would result in an incoherent demonstration. We propose Stitch-a-Recipe, a novel retrieval-based method to assemble a video demonstration from a multistep description. The resulting video contains clips, possibly from different sources, that accurately reflect all the step descriptions, while being visually coherent. We formulate a training pipeline that creates large-scale weakly supervised data containing diverse and novel recipes and injects hard negatives that promote both correctness and coherence. Validated on in-the-wild instructional videos, Stitch-a-Recipe achieves state-of-the-art performance, with quantitative gains up to 24% as well as dramatic wins in a human preference study.
<div id='section'>Paperid: <span id='pid'>414, <a href='https://arxiv.org/pdf/2501.14148.pdf' target='_blank'>https://arxiv.org/pdf/2501.14148.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuvendu Roy, Ali Etemad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.14148">SelfPrompt: Confidence-Aware Semi-Supervised Tuning for Robust Vision-Language Model Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present SelfPrompt, a novel prompt-tuning approach for vision-language models (VLMs) in a semi-supervised learning setup. Existing methods for tuning VLMs in semi-supervised setups struggle with the negative impact of the miscalibrated VLMs on pseudo-labelling, and the accumulation of noisy pseudo-labels. SelfPrompt addresses these challenges by introducing a cluster-guided pseudo-labelling method that improves pseudo-label accuracy, and a confidence-aware semi-supervised learning module that maximizes the utilization of unlabelled data by combining supervised learning and weakly-supervised learning. Additionally, we investigate our method in an active semi-supervised learning setup, where the labelled set is strategically selected to ensure the best utilization of a limited labelling budget. To this end, we propose a weakly-supervised sampling technique that selects a diverse and representative labelled set, which can be seamlessly integrated into existing methods to enhance their performance. We conduct extensive evaluations across 13 datasets, significantly surpassing state-of-the-art performances with average improvements of 6.23% in standard semi-supervised learning, 6.25% in active semi-supervised learning, and 4.9% in base-to-novel generalization, using a 2-shot setup. Furthermore, SelfPrompt shows excellent generalization in single-shot settings, achieving an average improvement of 11.78%.
<div id='section'>Paperid: <span id='pid'>415, <a href='https://arxiv.org/pdf/2501.13497.pdf' target='_blank'>https://arxiv.org/pdf/2501.13497.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qijie Shao, Linhao Dong, Kun Wei, Sining Sun, Lei Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13497">DQ-Data2vec: Decoupling Quantization for Multilingual Speech Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data2vec is a self-supervised learning (SSL) approach that employs a teacher-student architecture for contextual representation learning via masked prediction, demonstrating remarkable performance in monolingual ASR. Previous studies have revealed that data2vec's shallow layers capture speaker and language information, middle layers encode phoneme and word features, while deep layers are responsible for reconstruction. Language and phoneme features are crucial for multilingual ASR. However, data2vec's masked representation generation relies on multi-layer averaging, inevitably coupling these features. To address this limitation, we propose a decoupling quantization based data2vec (DQ-Data2vec) for multilingual ASR, which includes a data2vec backbone and two improved online K-means quantizers. Our core idea is using the K-means quantizer with specified cluster numbers to decouple language and phoneme information for masked prediction. Specifically, in the language quantization, considering that the number of languages is significantly different from other irrelevant features (e.g., speakers), we assign the cluster number to match the number of languages, explicitly decoupling shallow layers' language-related information from irrelevant features. This strategy is also applied to decoupling middle layers' phoneme and word features. In a self-supervised scenario, experiments on the CommonVoice dataset demonstrate that DQ-Data2vec achieves a relative reduction of 9.51% in phoneme error rate (PER) and 11.58% in word error rate (WER) compared to data2vec and UniData2vec. Moreover, in a weakly-supervised scenario incorporating language labels and high-resource language text labels, the relative reduction is 18.09% and 1.55%, respectively.
<div id='section'>Paperid: <span id='pid'>416, <a href='https://arxiv.org/pdf/2408.02773.pdf' target='_blank'>https://arxiv.org/pdf/2408.02773.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinmiao Zhao, Zelin Shi, Chuang Yu, Yunpeng Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.02773">Refined Infrared Small Target Detection Scheme with Single-Point Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, infrared small target detection with single-point supervision has attracted extensive attention. However, the detection accuracy of existing methods has difficulty meeting actual needs. Therefore, we propose an innovative refined infrared small target detection scheme with single-point supervision, which has excellent segmentation accuracy and detection rate. Specifically, we introduce label evolution with single point supervision (LESPS) framework and explore the performance of various excellent infrared small target detection networks based on this framework. Meanwhile, to improve the comprehensive performance, we construct a complete post-processing strategy. On the one hand, to improve the segmentation accuracy, we use a combination of test-time augmentation (TTA) and conditional random field (CRF) for post-processing. On the other hand, to improve the detection rate, we introduce an adjustable sensitivity (AS) strategy for post-processing, which fully considers the advantages of multiple detection results and reasonably adds some areas with low confidence to the fine segmentation image in the form of centroid points. In addition, to further improve the performance and explore the characteristics of this task, on the one hand, we construct and find that a multi-stage loss is helpful for fine-grained detection. On the other hand, we find that a reasonable sliding window cropping strategy for test samples has better performance for actual multi-size samples. Extensive experimental results show that the proposed scheme achieves state-of-the-art (SOTA) performance. Notably, the proposed scheme won the third place in the "ICPR 2024 Resource-Limited Infrared Small Target Detection Challenge Track 1: Weakly Supervised Infrared Small Target Detection".
<div id='section'>Paperid: <span id='pid'>417, <a href='https://arxiv.org/pdf/2408.00672.pdf' target='_blank'>https://arxiv.org/pdf/2408.00672.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kumar Ashutosh, Tushar Nagarajan, Georgios Pavlakos, Kris Kitani, Kristen Grauman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.00672">ExpertAF: Expert Actionable Feedback from Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Feedback is essential for learning a new skill or improving one's current skill-level. However, current methods for skill-assessment from video only provide scores or compare demonstrations, leaving the burden of knowing what to do differently on the user. We introduce a novel method to generate actionable feedback (AF) from video of a person doing a physical activity, such as basketball or soccer. Our method takes a video demonstration and its accompanying 3D body pose and generates (1) free-form expert commentary describing what the person is doing well and what they could improve, and (2) a visual expert demonstration that incorporates the required corrections. We show how to leverage Ego-Exo4D's [29] videos of skilled activity and expert commentary together with a strong language model to create a weakly-supervised training dataset for this task, and we devise a multimodal video-language model to infer coaching feedback. Our method is able to reason across multi-modal input combinations to output full spectrum, actionable coaching-expert commentary, expert video retrieval, and expert pose generation-outperforming strong vision-language models on both established metrics and human preference studies.
<div id='section'>Paperid: <span id='pid'>418, <a href='https://arxiv.org/pdf/2406.02251.pdf' target='_blank'>https://arxiv.org/pdf/2406.02251.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lukas Christ, Shahin Amiriparian, Manuel Milling, Ilhan Aslan, BjÃ¶rn W. Schuller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.02251">Modeling Emotional Trajectories in Written Stories Utilizing Transformers and Weakly-Supervised Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Telling stories is an integral part of human communication which can evoke emotions and influence the affective states of the audience. Automatically modeling emotional trajectories in stories has thus attracted considerable scholarly interest. However, as most existing works have been limited to unsupervised dictionary-based approaches, there is no benchmark for this task. We address this gap by introducing continuous valence and arousal labels for an existing dataset of children's stories originally annotated with discrete emotion categories. We collect additional annotations for this data and map the categorical labels to the continuous valence and arousal space. For predicting the thus obtained emotionality signals, we fine-tune a DeBERTa model and improve upon this baseline via a weakly supervised learning approach. The best configuration achieves a Concordance Correlation Coefficient (CCC) of $.8221$ for valence and $.7125$ for arousal on the test set, demonstrating the efficacy of our proposed approach. A detailed analysis shows the extent to which the results vary depending on factors such as the author, the individual story, or the section within the story. In addition, we uncover the weaknesses of our approach by investigating examples that prove to be difficult to predict.
<div id='section'>Paperid: <span id='pid'>419, <a href='https://arxiv.org/pdf/2405.20402.pdf' target='_blank'>https://arxiv.org/pdf/2405.20402.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhong-Qiu Wang, Anurag Kumar, Shinji Watanabe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.20402">Cross-Talk Reduction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While far-field multi-talker mixtures are recorded, each speaker can wear a close-talk microphone so that close-talk mixtures can be recorded at the same time. Although each close-talk mixture has a high signal-to-noise ratio (SNR) of the wearer, it has a very limited range of applications, as it also contains significant cross-talk speech by other speakers and is not clean enough. In this context, we propose a novel task named cross-talk reduction (CTR) which aims at reducing cross-talk speech, and a novel solution named CTRnet which is based on unsupervised or weakly-supervised neural speech separation. In unsupervised CTRnet, close-talk and far-field mixtures are stacked as input for a DNN to estimate the close-talk speech of each speaker. It is trained in an unsupervised, discriminative way such that the DNN estimate for each speaker can be linearly filtered to cancel out the speaker's cross-talk speech captured at other microphones. In weakly-supervised CTRnet, we assume the availability of each speaker's activity timestamps during training, and leverage them to improve the training of unsupervised CTRnet. Evaluation results on a simulated two-speaker CTR task and on a real-recorded conversational speech separation and recognition task show the effectiveness and potential of CTRnet.
<div id='section'>Paperid: <span id='pid'>420, <a href='https://arxiv.org/pdf/2405.18012.pdf' target='_blank'>https://arxiv.org/pdf/2405.18012.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Adi Nugroho, Sangmin Woo, Sumin Lee, Jinyoung Park, Yooseung Wang, Donguk Kim, Changick Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.18012">Flow-Assisted Motion Learning Network for Weakly-Supervised Group Activity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-Supervised Group Activity Recognition (WSGAR) aims to understand the activity performed together by a group of individuals with the video-level label and without actor-level labels. We propose Flow-Assisted Motion Learning Network (Flaming-Net) for WSGAR, which consists of the motion-aware actor encoder to extract actor features and the two-pathways relation module to infer the interaction among actors and their activity. Flaming-Net leverages an additional optical flow modality in the training stage to enhance its motion awareness when finding locally active actors. The first pathway of the relation module, the actor-centric path, initially captures the temporal dynamics of individual actors and then constructs inter-actor relationships. In parallel, the group-centric path starts by building spatial connections between actors within the same timeframe and then captures simultaneous spatio-temporal dynamics among them. We demonstrate that Flaming-Net achieves new state-of-the-art WSGAR results on two benchmarks, including a 2.8%p higher MPCA score on the NBA dataset. Importantly, we use the optical flow modality only for training and not for inference.
<div id='section'>Paperid: <span id='pid'>421, <a href='https://arxiv.org/pdf/2403.08801.pdf' target='_blank'>https://arxiv.org/pdf/2403.08801.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Woojung Han, Seil Kang, Kyobin Choo, Seong Jae Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.08801">CoBra: Complementary Branch Fusing Class and Semantic Knowledge for Robust Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Leveraging semantically precise pseudo masks derived from image-level class knowledge for segmentation, namely image-level Weakly Supervised Semantic Segmentation (WSSS), still remains challenging. While Class Activation Maps (CAMs) using CNNs have steadily been contributing to the success of WSSS, the resulting activation maps often narrowly focus on class-specific parts (e.g., only face of human). On the other hand, recent works based on vision transformers (ViT) have shown promising results based on their self-attention mechanism to capture the semantic parts but fail in capturing complete class-specific details (e.g., entire body parts of human but also with a dog nearby). In this work, we propose Complementary Branch (CoBra), a novel dual branch framework consisting of two distinct architectures which provide valuable complementary knowledge of class (from CNN) and semantic (from ViT) to each branch. In particular, we learn Class-Aware Projection (CAP) for the CNN branch and Semantic-Aware Projection (SAP) for the ViT branch to explicitly fuse their complementary knowledge and facilitate a new type of extra patch-level supervision. Our model, through CoBra, fuses CNN and ViT's complementary outputs to create robust pseudo masks that integrate both class and semantic information effectively. Extensive experiments qualitatively and quantitatively investigate how CNN and ViT complement each other on the PASCAL VOC 2012 dataset, showing a state-of-the-art WSSS result. This includes not only the masks generated by our model, but also the segmentation results derived from utilizing these masks as pseudo labels.
<div id='section'>Paperid: <span id='pid'>422, <a href='https://arxiv.org/pdf/2401.13537.pdf' target='_blank'>https://arxiv.org/pdf/2401.13537.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tobias Golling, Lukas Heinrich, Michael Kagan, Samuel Klein, Matthew Leigh, Margarita Osadchy, John Andrew Raine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.13537">Masked Particle Modeling on Sets: Towards Self-Supervised High Energy Physics Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose masked particle modeling (MPM) as a self-supervised method for learning generic, transferable, and reusable representations on unordered sets of inputs for use in high energy physics (HEP) scientific data. This work provides a novel scheme to perform masked modeling based pre-training to learn permutation invariant functions on sets. More generally, this work provides a step towards building large foundation models for HEP that can be generically pre-trained with self-supervised learning and later fine-tuned for a variety of down-stream tasks. In MPM, particles in a set are masked and the training objective is to recover their identity, as defined by a discretized token representation of a pre-trained vector quantized variational autoencoder. We study the efficacy of the method in samples of high energy jets at collider physics experiments, including studies on the impact of discretization, permutation invariance, and ordering. We also study the fine-tuning capability of the model, showing that it can be adapted to tasks such as supervised and weakly supervised jet classification, and that the model can transfer efficiently with small fine-tuning data sets to new classes and new data domains.
<div id='section'>Paperid: <span id='pid'>423, <a href='https://arxiv.org/pdf/2512.13806.pdf' target='_blank'>https://arxiv.org/pdf/2512.13806.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siegfried Ludwig, Stylianos Bakas, Konstantinos Barmpas, Georgios Zoumpourlis, Dimitrios A. Adamos, Nikolaos Laskaris, Yannis Panagakis, Stefanos Zafeiriou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.13806">EEG-D3: A Solution to the Hidden Overfitting Problem of Deep Learning Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning for decoding EEG signals has gained traction, with many claims to state-of-the-art accuracy. However, despite the convincing benchmark performance, successful translation to real applications is limited. The frequent disconnect between performance on controlled BCI benchmarks and its lack of generalisation to practical settings indicates hidden overfitting problems. We introduce Disentangled Decoding Decomposition (D3), a weakly supervised method for training deep learning models across EEG datasets. By predicting the place in the respective trial sequence from which the input window was sampled, EEG-D3 separates latent components of brain activity, akin to non-linear ICA. We utilise a novel model architecture with fully independent sub-networks for strict interpretability. We outline a feature interpretation paradigm to contrast the component activation profiles on different datasets and inspect the associated temporal and spatial filters. The proposed method reliably separates latent components of brain activity on motor imagery data. Training downstream classifiers on an appropriate subset of these components prevents hidden overfitting caused by task-correlated artefacts, which severely affects end-to-end classifiers. We further exploit the linearly separable latent space for effective few-shot learning on sleep stage classification. The ability to distinguish genuine components of brain activity from spurious features results in models that avoid the hidden overfitting problem and generalise well to real-world applications, while requiring only minimal labelled data. With interest to the neuroscience community, the proposed method gives researchers a tool to separate individual brain processes and potentially even uncover heretofore unknown dynamics.
<div id='section'>Paperid: <span id='pid'>424, <a href='https://arxiv.org/pdf/2511.18319.pdf' target='_blank'>https://arxiv.org/pdf/2511.18319.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xian Yeow Lee, Lasitha Vidyaratne, Gregory Sin, Ahmed Farahat, Chetan Gupta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.18319">Weakly-supervised Latent Models for Task-specific Visual-Language Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous inspection in hazardous environments requires AI agents that can interpret high-level goals and execute precise control. A key capability for such agents is spatial grounding, for example when a drone must center a detected object in its camera view to enable reliable inspection. While large language models provide a natural interface for specifying goals, using them directly for visual control achieves only 58\% success in this task. We envision that equipping agents with a world model as a tool would allow them to roll out candidate actions and perform better in spatially grounded settings, but conventional world models are data and compute intensive. To address this, we propose a task-specific latent dynamics model that learns state-specific action-induced shifts in a shared latent space using only goal-state supervision. The model leverages global action embeddings and complementary training losses to stabilize learning. In experiments, our approach achieves 71\% success and generalizes to unseen images and instructions, highlighting the potential of compact, domain-specific latent dynamics models for spatial alignment in autonomous inspection.
<div id='section'>Paperid: <span id='pid'>425, <a href='https://arxiv.org/pdf/2509.01899.pdf' target='_blank'>https://arxiv.org/pdf/2509.01899.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhimeng Luo, Zhendong Wang, Rui Meng, Diyang Xue, Adam Frisch, Daqing He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01899">Weakly Supervised Medical Entity Extraction and Linking for Chief Complaints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A Chief complaint (CC) is the reason for the medical visit as stated in the patient's own words. It helps medical professionals to quickly understand a patient's situation, and also serves as a short summary for medical text mining. However, chief complaint records often take a variety of entering methods, resulting in a wide variation of medical notations, which makes it difficult to standardize across different medical institutions for record keeping or text mining. In this study, we propose a weakly supervised method to automatically extract and link entities in chief complaints in the absence of human annotation. We first adopt a split-and-match algorithm to produce weak annotations, including entity mention spans and class labels, on 1.2 million real-world de-identified and IRB approved chief complaint records. Then we train a BERT-based model with generated weak labels to locate entity mentions in chief complaint text and link them to a pre-defined ontology. We conducted extensive experiments, and the results showed that our Weakly Supervised Entity Extraction and Linking (\ours) method produced superior performance over previous methods without any human annotation.
<div id='section'>Paperid: <span id='pid'>426, <a href='https://arxiv.org/pdf/2508.07877.pdf' target='_blank'>https://arxiv.org/pdf/2508.07877.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>WonJun Moon, Hyun Seok Seong, Jae-Pil Heo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07877">Selective Contrastive Learning for Weakly Supervised Affordance Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Facilitating an entity's interaction with objects requires accurately identifying parts that afford specific actions. Weakly supervised affordance grounding (WSAG) seeks to imitate human learning from third-person demonstrations, where humans intuitively grasp functional parts without needing pixel-level annotations. To achieve this, grounding is typically learned using a shared classifier across images from different perspectives, along with distillation strategies incorporating part discovery process. However, since affordance-relevant parts are not always easily distinguishable, models primarily rely on classification, often focusing on common class-specific patterns that are unrelated to affordance. To address this limitation, we move beyond isolated part-level learning by introducing selective prototypical and pixel contrastive objectives that adaptively learn affordance-relevant cues at both the part and object levels, depending on the granularity of the available information. Initially, we find the action-associated objects in both egocentric (object-focused) and exocentric (third-person example) images by leveraging CLIP. Then, by cross-referencing the discovered objects of complementary views, we excavate the precise part-level affordance clues in each perspective. By consistently learning to distinguish affordance-relevant regions from affordance-irrelevant background context, our approach effectively shifts activation from irrelevant areas toward meaningful affordance cues. Experimental results demonstrate the effectiveness of our method. Codes are available at github.com/hynnsk/SelectiveCL.
<div id='section'>Paperid: <span id='pid'>427, <a href='https://arxiv.org/pdf/2505.17905.pdf' target='_blank'>https://arxiv.org/pdf/2505.17905.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xie Ting, Ye Huang, Zhilin Liu, Lixin Duan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17905">Semantic segmentation with reward</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In real-world scenarios, pixel-level labeling is not always available. Sometimes, we need a semantic segmentation network, and even a visual encoder can have a high compatibility, and can be trained using various types of feedback beyond traditional labels, such as feedback that indicates the quality of the parsing results. To tackle this issue, we proposed RSS (Reward in Semantic Segmentation), the first practical application of reward-based reinforcement learning on pure semantic segmentation offered in two granular levels (pixel-level and image-level). RSS incorporates various novel technologies, such as progressive scale rewards (PSR) and pair-wise spatial difference (PSD), to ensure that the reward facilitates the convergence of the semantic segmentation network, especially under image-level rewards. Experiments and visualizations on benchmark datasets demonstrate that the proposed RSS can successfully ensure the convergence of the semantic segmentation network on two levels of rewards. Additionally, the RSS, which utilizes an image-level reward, outperforms existing weakly supervised methods that also rely solely on image-level signals during training.
<div id='section'>Paperid: <span id='pid'>428, <a href='https://arxiv.org/pdf/2505.17088.pdf' target='_blank'>https://arxiv.org/pdf/2505.17088.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmed Adel Attia, Dorottya Demszky, Jing Liu, Carol Espy-Wilson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17088">From Weak Labels to Strong Results: Utilizing 5,000 Hours of Noisy Classroom Transcripts with Minimal Accurate Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent progress in speech recognition has relied on models trained on vast amounts of labeled data. However, classroom Automatic Speech Recognition (ASR) faces the real-world challenge of abundant weak transcripts paired with only a small amount of accurate, gold-standard data. In such low-resource settings, high transcription costs make re-transcription impractical. To address this, we ask: what is the best approach when abundant inexpensive weak transcripts coexist with limited gold-standard data, as is the case for classroom speech data? We propose Weakly Supervised Pretraining (WSP), a two-step process where models are first pretrained on weak transcripts in a supervised manner, and then fine-tuned on accurate data. Our results, based on both synthetic and real weak transcripts, show that WSP outperforms alternative methods, establishing it as an effective training methodology for low-resource ASR in real-world scenarios.
<div id='section'>Paperid: <span id='pid'>429, <a href='https://arxiv.org/pdf/2504.01452.pdf' target='_blank'>https://arxiv.org/pdf/2504.01452.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Encheng Su, Hu Cao, Alois Knoll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01452">BiSeg-SAM: Weakly-Supervised Post-Processing Framework for Boosting Binary Segmentation in Segment Anything Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate segmentation of polyps and skin lesions is essential for diagnosing colorectal and skin cancers. While various segmentation methods for polyps and skin lesions using fully supervised deep learning techniques have been developed, the pixel-level annotation of medical images by doctors is both time-consuming and costly. Foundational vision models like the Segment Anything Model (SAM) have demonstrated superior performance; however, directly applying SAM to medical segmentation may not yield satisfactory results due to the lack of domain-specific medical knowledge. In this paper, we propose BiSeg-SAM, a SAM-guided weakly supervised prompting and boundary refinement network for the segmentation of polyps and skin lesions. Specifically, we fine-tune SAM combined with a CNN module to learn local features. We introduce a WeakBox with two functions: automatically generating box prompts for the SAM model and using our proposed Multi-choice Mask-to-Box (MM2B) transformation for rough mask-to-box conversion, addressing the mismatch between coarse labels and precise predictions. Additionally, we apply scale consistency (SC) loss for prediction scale alignment. Our DetailRefine module enhances boundary precision and segmentation accuracy by refining coarse predictions using a limited amount of ground truth labels. This comprehensive approach enables BiSeg-SAM to achieve excellent multi-task segmentation performance. Our method demonstrates significant superiority over state-of-the-art (SOTA) methods when tested on five polyp datasets and one skin cancer dataset.
<div id='section'>Paperid: <span id='pid'>430, <a href='https://arxiv.org/pdf/2412.12870.pdf' target='_blank'>https://arxiv.org/pdf/2412.12870.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenjiang Mao, Ivan Ruchkin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12870">Towards Physically Interpretable World Models: Meaningful Weakly Supervised Representations for Visual Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning models are increasingly employed for perception, prediction, and control in robotic systems. For for achieving realistic and consistent outputs, it is crucial to embed physical knowledge into their learned representations. However, doing so is difficult due to high-dimensional observation data, such as images, particularly under conditions of incomplete system knowledge and imprecise state sensing. To address this, we propose Physically Interpretable World Models, a novel architecture that aligns learned latent representations with real-world physical quantities. To this end, our architecture combines three key elements: (1) a vector-quantized image autoencoder, (2) a transformer-based physically interpretable autoencoder, and (3) a partially known dynamical model. The training incorporates weak interval-based supervision to eliminate the impractical reliance on ground-truth physical knowledge. Three case studies demonstrate that our approach achieves physical interpretability and accurate state predictions, thus advancing representation learning for robotics.
<div id='section'>Paperid: <span id='pid'>431, <a href='https://arxiv.org/pdf/2409.15801.pdf' target='_blank'>https://arxiv.org/pdf/2409.15801.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soojin Jang, Jungmin Yun, Junehyoung Kwon, Eunju Lee, Youngbin Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.15801">DIAL: Dense Image-text ALignment for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation (WSSS) approaches typically rely on class activation maps (CAMs) for initial seed generation, which often fail to capture global context due to limited supervision from image-level labels. To address this issue, we introduce DALNet, Dense Alignment Learning Network that leverages text embeddings to enhance the comprehensive understanding and precise localization of objects across different levels of granularity. Our key insight is to employ a dual-level alignment strategy: (1) Global Implicit Alignment (GIA) to capture global semantics by maximizing the similarity between the class token and the corresponding text embeddings while minimizing the similarity with background embeddings, and (2) Local Explicit Alignment (LEA) to improve object localization by utilizing spatial information from patch tokens. Moreover, we propose a cross-contrastive learning approach that aligns foreground features between image and text modalities while separating them from the background, encouraging activation in missing regions and suppressing distractions. Through extensive experiments on the PASCAL VOC and MS COCO datasets, we demonstrate that DALNet significantly outperforms state-of-the-art WSSS methods. Our approach, in particular, allows for more efficient end-to-end process as a single-stage method.
<div id='section'>Paperid: <span id='pid'>432, <a href='https://arxiv.org/pdf/2409.04011.pdf' target='_blank'>https://arxiv.org/pdf/2409.04011.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weijie He, Mushui Liu, Yunlong Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.04011">Hybrid Mask Generation for Infrared Small Target Detection with Single-Point Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single-frame infrared small target (SIRST) detection poses a significant challenge due to the requirement to discern minute targets amidst complex infrared background clutter. In this paper, we focus on a weakly-supervised paradigm to obtain high-quality pseudo masks from the point-level annotation by integrating a novel learning-free method with the hybrid of the learning-based method. The learning-free method adheres to a sequential process, progressing from a point annotation to the bounding box that encompasses the target, and subsequently to detailed pseudo masks, while the hybrid is achieved through filtering out false alarms and retrieving missed detections in the network's prediction to provide a reliable supplement for learning-free masks. The experimental results show that our learning-free method generates pseudo masks with an average Intersection over Union (IoU) that is 4.3% higher than the second-best learning-free competitor across three datasets, while the hybrid learning-based method further enhances the quality of pseudo masks, achieving an additional average IoU increase of 3.4%.
<div id='section'>Paperid: <span id='pid'>433, <a href='https://arxiv.org/pdf/2407.20818.pdf' target='_blank'>https://arxiv.org/pdf/2407.20818.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingcheng Zhou, Deyu Fu, Walter Zimmer, Mingyu Liu, Venkatnarayanan Lakshminarasimhan, Leah Strand, Alois C. Knoll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.20818">WARM-3D: A Weakly-Supervised Sim2Real Domain Adaptation Framework for Roadside Monocular 3D Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing roadside perception systems are limited by the absence of publicly available, large-scale, high-quality 3D datasets. Exploring the use of cost-effective, extensive synthetic datasets offers a viable solution to tackle this challenge and enhance the performance of roadside monocular 3D detection. In this study, we introduce the TUMTraf Synthetic Dataset, offering a diverse and substantial collection of high-quality 3D data to augment scarce real-world datasets. Besides, we present WARM-3D, a concise yet effective framework to aid the Sim2Real domain transfer for roadside monocular 3D detection. Our method leverages cheap synthetic datasets and 2D labels from an off-the-shelf 2D detector for weak supervision. We show that WARM-3D significantly enhances performance, achieving a +12.40% increase in mAP 3D over the baseline with only pseudo-2D supervision. With 2D GT as weak labels, WARM-3D even reaches performance close to the Oracle baseline. Moreover, WARM-3D improves the ability of 3D detectors to unseen sample recognition across various real-world environments, highlighting its potential for practical applications.
<div id='section'>Paperid: <span id='pid'>434, <a href='https://arxiv.org/pdf/2404.00257.pdf' target='_blank'>https://arxiv.org/pdf/2404.00257.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qian Wan, Xiang Xiang, Qinhao Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00257">YOLOOC: YOLO-based Open-Class Incremental Object Detection with Novel Class Discovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Because of its use in practice, open-world object detection (OWOD) has gotten a lot of attention recently. The challenge is how can a model detect novel classes and then incrementally learn them without forgetting previously known classes. Previous approaches hinge on strongly-supervised or weakly-supervised novel-class data for novel-class detection, which may not apply to real applications. We construct a new benchmark that novel classes are only encountered at the inference stage. And we propose a new OWOD detector YOLOOC, based on the YOLO architecture yet for the Open-Class setup. We introduce label smoothing to prevent the detector from over-confidently mapping novel classes to known classes and to discover novel classes. Extensive experiments conducted on our more realistic setup demonstrate the effectiveness of our method for discovering novel classes in our new benchmark.
<div id='section'>Paperid: <span id='pid'>435, <a href='https://arxiv.org/pdf/2401.09709.pdf' target='_blank'>https://arxiv.org/pdf/2401.09709.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zipeng Wang, Xuehui Yu, Xumeng Han, Wenwen Yu, Zhixun Huang, Jianbin Jiao, Zhenjun Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.09709">P2Seg: Pointly-supervised Segmentation via Mutual Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Point-level Supervised Instance Segmentation (PSIS) aims to enhance the applicability and scalability of instance segmentation by utilizing low-cost yet instance-informative annotations. Existing PSIS methods usually rely on positional information to distinguish objects, but predicting precise boundaries remains challenging due to the lack of contour annotations. Nevertheless, weakly supervised semantic segmentation methods are proficient in utilizing intra-class feature consistency to capture the boundary contours of the same semantic regions. In this paper, we design a Mutual Distillation Module (MDM) to leverage the complementary strengths of both instance position and semantic information and achieve accurate instance-level object perception. The MDM consists of Semantic to Instance (S2I) and Instance to Semantic (I2S). S2I is guided by the precise boundaries of semantic regions to learn the association between annotated points and instance contours. I2S leverages discriminative relationships between instances to facilitate the differentiation of various objects within the semantic map. Extensive experiments substantiate the efficacy of MDM in fostering the synergy between instance and semantic information, consequently improving the quality of instance-level object representations. Our method achieves 55.7 mAP$_{50}$ and 17.6 mAP on the PASCAL VOC and MS COCO datasets, significantly outperforming recent PSIS methods and several box-supervised instance segmentation competitors.
<div id='section'>Paperid: <span id='pid'>436, <a href='https://arxiv.org/pdf/2512.20260.pdf' target='_blank'>https://arxiv.org/pdf/2512.20260.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei Ge, Jiuxin Cao, Xinyi Li, Xuelin Zhu, Chang Liu, Bo Liu, Chen Feng, Ioannis Patras
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.20260">${D}^{3}${ETOR}: ${D}$ebate-Enhanced Pseudo Labeling and Frequency-Aware Progressive ${D}$ebiasing for Weakly-Supervised Camouflaged Object ${D}$etection with Scribble Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-Supervised Camouflaged Object Detection (WSCOD) aims to locate and segment objects that are visually concealed within their surrounding scenes, relying solely on sparse supervision such as scribble annotations. Despite recent progress, existing WSCOD methods still lag far behind fully supervised ones due to two major limitations: (1) the pseudo masks generated by general-purpose segmentation models (e.g., SAM) and filtered via rules are often unreliable, as these models lack the task-specific semantic understanding required for effective pseudo labeling in COD; and (2) the neglect of inherent annotation bias in scribbles, which hinders the model from capturing the global structure of camouflaged objects. To overcome these challenges, we propose ${D}^{3}$ETOR, a two-stage WSCOD framework consisting of Debate-Enhanced Pseudo Labeling and Frequency-Aware Progressive Debiasing. In the first stage, we introduce an adaptive entropy-driven point sampling method and a multi-agent debate mechanism to enhance the capability of SAM for COD, improving the interpretability and precision of pseudo masks. In the second stage, we design FADeNet, which progressively fuses multi-level frequency-aware features to balance global semantic understanding with local detail modeling, while dynamically reweighting supervision strength across regions to alleviate scribble bias. By jointly exploiting the supervision signals from both the pseudo masks and scribble semantics, ${D}^{3}$ETOR significantly narrows the gap between weakly and fully supervised COD, achieving state-of-the-art performance on multiple benchmarks.
<div id='section'>Paperid: <span id='pid'>437, <a href='https://arxiv.org/pdf/2511.17619.pdf' target='_blank'>https://arxiv.org/pdf/2511.17619.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qinghao Meng, Junbo Yin, Jianbing Shen, Yunde Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17619">Rethinking the Encoding and Annotating of 3D Bounding Box: Corner-Aware 3D Object Detection from Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Center-aligned regression remains dominant in LiDAR-based 3D object detection, yet it suffers from fundamental instability: object centers often fall in sparse or empty regions of the bird's-eye-view (BEV) due to the front-surface-biased nature of LiDAR point clouds, leading to noisy and inaccurate bounding box predictions. To circumvent this limitation, we revisit bounding box representation and propose corner-aligned regression, which shifts the prediction target from unstable centers to geometrically informative corners that reside in dense, observable regions. Leveraging the inherent geometric constraints among corners and image 2D boxes, partial parameters of 3D bounding boxes can be recovered from corner annotations, enabling a weakly supervised paradigm without requiring complete 3D labels. We design a simple yet effective corner-aware detection head that can be plugged into existing detectors. Experiments on KITTI show our method improves performance by 3.5% AP over center-based baseline, and achieves 83% of fully supervised accuracy using only BEV corner clicks, demonstrating the effectiveness of our corner-aware regression strategy.
<div id='section'>Paperid: <span id='pid'>438, <a href='https://arxiv.org/pdf/2511.14832.pdf' target='_blank'>https://arxiv.org/pdf/2511.14832.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marie Hein, Gregor Kasieczka, Michael Krämer, Louis Moureaux, Alexander Mück, David Shih
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.14832">How to pick the best anomaly detector?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anomaly detection has the potential to discover new physics in unexplored regions of the data. However, choosing the best anomaly detector for a given data set in a model-agnostic way is an important challenge which has hitherto largely been neglected. In this paper, we introduce the data-driven ARGOS metric, which has a sound theoretical foundation and is empirically shown to robustly select the most sensitive anomaly detection model given the data. Focusing on weakly-supervised, classifier-based anomaly detection methods, we show that the ARGOS metric outperforms other model selection metrics previously used in the literature, in particular the binary cross-entropy loss. We explore several realistic applications, including hyperparameter tuning as well as architecture and feature selection, and in all cases we demonstrate that ARGOS is robust to the noisy conditions of anomaly detection.
<div id='section'>Paperid: <span id='pid'>439, <a href='https://arxiv.org/pdf/2509.13116.pdf' target='_blank'>https://arxiv.org/pdf/2509.13116.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruibo Li, Hanyu Shi, Zhe Wang, Guosheng Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13116">Weakly and Self-Supervised Class-Agnostic Motion Prediction for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding motion in dynamic environments is critical for autonomous driving, thereby motivating research on class-agnostic motion prediction. In this work, we investigate weakly and self-supervised class-agnostic motion prediction from LiDAR point clouds. Outdoor scenes typically consist of mobile foregrounds and static backgrounds, allowing motion understanding to be associated with scene parsing. Based on this observation, we propose a novel weakly supervised paradigm that replaces motion annotations with fully or partially annotated (1%, 0.1%) foreground/background masks for supervision. To this end, we develop a weakly supervised approach utilizing foreground/background cues to guide the self-supervised learning of motion prediction models. Since foreground motion generally occurs in non-ground regions, non-ground/ground masks can serve as an alternative to foreground/background masks, further reducing annotation effort. Leveraging non-ground/ground cues, we propose two additional approaches: a weakly supervised method requiring fewer (0.01%) foreground/background annotations, and a self-supervised method without annotations. Furthermore, we design a Robust Consistency-aware Chamfer Distance loss that incorporates multi-frame information and robust penalty functions to suppress outliers in self-supervised learning. Experiments show that our weakly and self-supervised models outperform existing self-supervised counterparts, and our weakly supervised models even rival some supervised ones. This demonstrates that our approaches effectively balance annotation effort and performance.
<div id='section'>Paperid: <span id='pid'>440, <a href='https://arxiv.org/pdf/2506.18261.pdf' target='_blank'>https://arxiv.org/pdf/2506.18261.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Su, Dong Xu, Luping Zhou, Wanli Ouyang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18261">Improving Weakly Supervised Temporal Action Localization by Exploiting Multi-resolution Information in Temporal Domain</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised temporal action localization is a challenging task as only the video-level annotation is available during the training process. To address this problem, we propose a two-stage approach to fully exploit multi-resolution information in the temporal domain and generate high quality frame-level pseudo labels based on both appearance and motion streams. Specifically, in the first stage, we generate reliable initial frame-level pseudo labels, and in the second stage, we iteratively refine the pseudo labels and use a set of selected frames with highly confident pseudo labels to train neural networks and better predict action class scores at each frame. We fully exploit temporal information at multiple scales to improve temporal action localization performance. Specifically, in order to obtain reliable initial frame-level pseudo labels, in the first stage, we propose an Initial Label Generation (ILG) module, which leverages temporal multi-resolution consistency to generate high quality class activation sequences (CASs), which consist of a number of sequences with each sequence measuring how likely each video frame belongs to one specific action class. In the second stage, we propose a Progressive Temporal Label Refinement (PTLR) framework. In our PTLR framework, two networks called Network-OTS and Network-RTS, which are respectively used to generate CASs for the original temporal scale and the reduced temporal scales, are used as two streams (i.e., the OTS stream and the RTS stream) to refine the pseudo labels in turn. By this way, the multi-resolution information in the temporal domain is exchanged at the pseudo label level, and our work can help improve each stream (i.e., the OTS/RTS stream) by exploiting the refined pseudo labels from another stream (i.e., the RTS/OTS stream).
<div id='section'>Paperid: <span id='pid'>441, <a href='https://arxiv.org/pdf/2504.18866.pdf' target='_blank'>https://arxiv.org/pdf/2504.18866.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxu Leng, Zhanjie Wu, Mingpi Tan, Mengjingcheng Mo, Jiankang Zheng, Qingqing Li, Ji Gan, Xinbo Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.18866">PiercingEye: Dual-Space Video Violence Detection with Hyperbolic Vision-Language Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing weakly supervised video violence detection (VVD) methods primarily rely on Euclidean representation learning, which often struggles to distinguish visually similar yet semantically distinct events due to limited hierarchical modeling and insufficient ambiguous training samples. To address this challenge, we propose PiercingEye, a novel dual-space learning framework that synergizes Euclidean and hyperbolic geometries to enhance discriminative feature representation. Specifically, PiercingEye introduces a layer-sensitive hyperbolic aggregation strategy with hyperbolic Dirichlet energy constraints to progressively model event hierarchies, and a cross-space attention mechanism to facilitate complementary feature interactions between Euclidean and hyperbolic spaces. Furthermore, to mitigate the scarcity of ambiguous samples, we leverage large language models to generate logic-guided ambiguous event descriptions, enabling explicit supervision through a hyperbolic vision-language contrastive loss that prioritizes high-confusion samples via dynamic similarity-aware weighting. Extensive experiments on XD-Violence and UCF-Crime benchmarks demonstrate that PiercingEye achieves state-of-the-art performance, with particularly strong results on a newly curated ambiguous event subset, validating its superior capability in fine-grained violence detection.
<div id='section'>Paperid: <span id='pid'>442, <a href='https://arxiv.org/pdf/2502.03382.pdf' target='_blank'>https://arxiv.org/pdf/2502.03382.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tom Labiausse, Laurent MazarÃ©, Edouard Grave, Patrick PÃ©rez, Alexandre DÃ©fossez, Neil Zeghidour
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03382">High-Fidelity Simultaneous Speech-To-Speech Translation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Hibiki, a decoder-only model for simultaneous speech translation. Hibiki leverages a multistream language model to synchronously process source and target speech, and jointly produces text and audio tokens to perform speech-to-text and speech-to-speech translation. We furthermore address the fundamental challenge of simultaneous interpretation, which unlike its consecutive counterpart, where one waits for the end of the source utterance to start translating, adapts its flow to accumulate just enough context to produce a correct translation in real-time, chunk by chunk. To do so, we introduce a weakly-supervised method that leverages the perplexity of an off-the-shelf text translation system to identify optimal delays on a per-word basis and create aligned synthetic data. After supervised training, Hibiki performs adaptive, simultaneous speech translation with vanilla temperature sampling. On a French-English simultaneous speech translation task, Hibiki demonstrates state-of-the-art performance in translation quality, speaker fidelity and naturalness. Moreover, the simplicity of its inference process makes it compatible with batched translation and even real-time on-device deployment. We provide examples as well as models and inference code.
<div id='section'>Paperid: <span id='pid'>443, <a href='https://arxiv.org/pdf/2502.01455.pdf' target='_blank'>https://arxiv.org/pdf/2502.01455.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrea Marelli, Luca Magri, Federica Arrigoni, Giacomo Boracchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01455">Temporal-consistent CAMs for Weakly Supervised Video Segmentation in Waste Sorting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In industrial settings, weakly supervised (WS) methods are usually preferred over their fully supervised (FS) counterparts as they do not require costly manual annotations. Unfortunately, the segmentation masks obtained in the WS regime are typically poor in terms of accuracy. In this work, we present a WS method capable of producing accurate masks for semantic segmentation in the case of video streams. More specifically, we build saliency maps that exploit the temporal coherence between consecutive frames in a video, promoting consistency when objects appear in different frames. We apply our method in a waste-sorting scenario, where we perform weakly supervised video segmentation (WSVS) by training an auxiliary classifier that distinguishes between videos recorded before and after a human operator, who manually removes specific wastes from a conveyor belt. The saliency maps of this classifier identify materials to be removed, and we modify the classifier training to minimize differences between the saliency map of a central frame and those in adjacent frames, after having compensated object displacement. Experiments on a real-world dataset demonstrate the benefits of integrating temporal coherence directly during the training phase of the classifier. Code and dataset are available upon request.
<div id='section'>Paperid: <span id='pid'>444, <a href='https://arxiv.org/pdf/2412.18842.pdf' target='_blank'>https://arxiv.org/pdf/2412.18842.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Heng-Bo Fan, Ming-Kun Xie, Jia-Hao Xiao, Sheng-Jun Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.18842">Context-Based Semantic-Aware Alignment for Semi-Supervised Multi-Label Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the lack of extensive precisely-annotated multi-label data in real word, semi-supervised multi-label learning (SSMLL) has gradually gained attention. Abundant knowledge embedded in vision-language models (VLMs) pre-trained on large-scale image-text pairs could alleviate the challenge of limited labeled data under SSMLL setting.Despite existing methods based on fine-tuning VLMs have achieved advances in weakly-supervised multi-label learning, they failed to fully leverage the information from labeled data to enhance the learning of unlabeled data. In this paper, we propose a context-based semantic-aware alignment method to solve the SSMLL problem by leveraging the knowledge of VLMs. To address the challenge of handling multiple semantics within an image, we introduce a novel framework design to extract label-specific image features. This design allows us to achieve a more compact alignment between text features and label-specific image features, leading the model to generate high-quality pseudo-labels. To incorporate the model with comprehensive understanding of image, we design a semi-supervised context identification auxiliary task to enhance the feature representation by capturing co-occurrence information. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our proposed method.
<div id='section'>Paperid: <span id='pid'>445, <a href='https://arxiv.org/pdf/2410.14790.pdf' target='_blank'>https://arxiv.org/pdf/2410.14790.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianchao Ci, Eldert J. van Henten, Xin Wang, Akshay K. Burusa, Gert Kootstra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14790">SSL-NBV: A Self-Supervised-Learning-Based Next-Best-View algorithm for Efficient 3D Plant Reconstruction by a Robot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The 3D reconstruction of plants is challenging due to their complex shape causing many occlusions. Next-Best-View (NBV) methods address this by iteratively selecting new viewpoints to maximize information gain (IG). Deep-learning-based NBV (DL-NBV) methods demonstrate higher computational efficiency over classic voxel-based NBV approaches but current methods require extensive training using ground-truth plant models, making them impractical for real-world plants. These methods, moreover, rely on offline training with pre-collected data, limiting adaptability in changing agricultural environments. This paper proposes a self-supervised learning-based NBV method (SSL-NBV) that uses a deep neural network to predict the IG for candidate viewpoints. The method allows the robot to gather its own training data during task execution by comparing new 3D sensor data to the earlier gathered data and by employing weakly-supervised learning and experience replay for efficient online learning. Comprehensive evaluations were conducted in simulation and real-world environments using cross-validation. The results showed that SSL-NBV required fewer views for plant reconstruction than non-NBV methods and was over 800 times faster than a voxel-based method. SSL-NBV reduced training annotations by over 90% compared to a baseline DL-NBV. Furthermore, SSL-NBV could adapt to novel scenarios through online fine-tuning. Also using real plants, the results showed that the proposed method can learn to effectively plan new viewpoints for 3D plant reconstruction. Most importantly, SSL-NBV automated the entire network training and uses continuous online learning, allowing it to operate in changing agricultural environments.
<div id='section'>Paperid: <span id='pid'>446, <a href='https://arxiv.org/pdf/2409.19252.pdf' target='_blank'>https://arxiv.org/pdf/2409.19252.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxu Leng, Zhanjie Wu, Mingpi Tan, Yiran Liu, Ji Gan, Haosheng Chen, Xinbo Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19252">Beyond Euclidean: Dual-Space Representation Learning for Weakly Supervised Video Violence Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While numerous Video Violence Detection (VVD) methods have focused on representation learning in Euclidean space, they struggle to learn sufficiently discriminative features, leading to weaknesses in recognizing normal events that are visually similar to violent events (\emph{i.e.}, ambiguous violence). In contrast, hyperbolic representation learning, renowned for its ability to model hierarchical and complex relationships between events, has the potential to amplify the discrimination between visually similar events. Inspired by these, we develop a novel Dual-Space Representation Learning (DSRL) method for weakly supervised VVD to utilize the strength of both Euclidean and hyperbolic geometries, capturing the visual features of events while also exploring the intrinsic relations between events, thereby enhancing the discriminative capacity of the features. DSRL employs a novel information aggregation strategy to progressively learn event context in hyperbolic spaces, which selects aggregation nodes through layer-sensitive hyperbolic association degrees constrained by hyperbolic Dirichlet energy. Furthermore, DSRL attempts to break the cyber-balkanization of different spaces, utilizing cross-space attention to facilitate information interactions between Euclidean and hyperbolic space to capture better discriminative features for final violence detection. Comprehensive experiments demonstrate the effectiveness of our proposed DSRL.
<div id='section'>Paperid: <span id='pid'>447, <a href='https://arxiv.org/pdf/2409.06210.pdf' target='_blank'>https://arxiv.org/pdf/2409.06210.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ji Ha Jang, Hoigi Seo, Se Young Chun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.06210">INTRA: Interaction Relationship-aware Weakly Supervised Affordance Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Affordance denotes the potential interactions inherent in objects. The perception of affordance can enable intelligent agents to navigate and interact with new environments efficiently. Weakly supervised affordance grounding teaches agents the concept of affordance without costly pixel-level annotations, but with exocentric images. Although recent advances in weakly supervised affordance grounding yielded promising results, there remain challenges including the requirement for paired exocentric and egocentric image dataset, and the complexity in grounding diverse affordances for a single object. To address them, we propose INTeraction Relationship-aware weakly supervised Affordance grounding (INTRA). Unlike prior arts, INTRA recasts this problem as representation learning to identify unique features of interactions through contrastive learning with exocentric images only, eliminating the need for paired datasets. Moreover, we leverage vision-language model embeddings for performing affordance grounding flexibly with any text, designing text-conditioned affordance map generation to reflect interaction relationship for contrastive learning and enhancing robustness with our text synonym augmentation. Our method outperformed prior arts on diverse datasets such as AGD20K, IIT-AFF, CAD and UMD. Additionally, experimental results demonstrate that our method has remarkable domain scalability for synthesized images / illustrations and is capable of performing affordance grounding for novel interactions and objects.
<div id='section'>Paperid: <span id='pid'>448, <a href='https://arxiv.org/pdf/2407.13157.pdf' target='_blank'>https://arxiv.org/pdf/2407.13157.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jin Zhang, Ruiheng Zhang, Yanjiao Shi, Zhe Cao, Nian Liu, Fahad Shahbaz Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.13157">Learning Camouflaged Object Detection from Noisy Pseudo Label</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing Camouflaged Object Detection (COD) methods rely heavily on large-scale pixel-annotated training sets, which are both time-consuming and labor-intensive. Although weakly supervised methods offer higher annotation efficiency, their performance is far behind due to the unclear visual demarcations between foreground and background in camouflaged images. In this paper, we explore the potential of using boxes as prompts in camouflaged scenes and introduce the first weakly semi-supervised COD method, aiming for budget-efficient and high-precision camouflaged object segmentation with an extremely limited number of fully labeled images. Critically, learning from such limited set inevitably generates pseudo labels with serious noisy pixels. To address this, we propose a noise correction loss that facilitates the model's learning of correct pixels in the early learning stage, and corrects the error risk gradients dominated by noisy pixels in the memorization stage, ultimately achieving accurate segmentation of camouflaged objects from noisy labels. When using only 20% of fully labeled data, our method shows superior performance over the state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>449, <a href='https://arxiv.org/pdf/2407.01869.pdf' target='_blank'>https://arxiv.org/pdf/2407.01869.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenyi Lian, Joakim Lindblad, Christina Runow Stark, Jan-MichaÃ©l Hirsch, NataÅ¡a Sladoje
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.01869">Let it shine: Autofluorescence of Papanicolaou-stain improves AI-based cytological oral cancer detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Oral cancer is a global health challenge. It is treatable if detected early, but it is often fatal in late stages. There is a shift from the invasive and time-consuming tissue sampling and histological examination, toward non-invasive brush biopsies and cytological examination. Reliable computer-assisted methods are essential for cost-effective and accurate cytological analysis, but the lack of detailed cell-level annotations impairs model effectiveness. This study aims to improve AI-based oral cancer detection using multimodal imaging and deep fusion. We combine brightfield and fluorescence whole slide microscopy imaging to analyze Papanicolaou-stained liquid-based cytology slides of brush biopsies collected from both healthy and cancer patients. Due to limited cytological annotations, we utilize a weakly supervised deep learning approach using only patient-level labels. We evaluate various multimodal fusion strategies, including early, late, and three recent intermediate fusion methods. Our results show: (i) fluorescence imaging of Papanicolaou-stained samples provides substantial diagnostic information; (ii) multimodal fusion enhances classification and cancer detection accuracy over single-modality methods. Intermediate fusion is the leading method among the studied approaches. Specifically, the Co-Attention Fusion Network (CAFNet) model excels with an F1 score of 83.34% and accuracy of 91.79%, surpassing human performance on the task. Additional tests highlight the need for precise image registration to optimize multimodal analysis benefits. This study advances cytopathology by combining deep learning and multimodal imaging to enhance early, non-invasive detection of oral cancer, improving diagnostic accuracy and streamlining clinical workflows. The developed pipeline is also applicable in other cytological settings. Our codes and dataset are available online for further research.
<div id='section'>Paperid: <span id='pid'>450, <a href='https://arxiv.org/pdf/2403.01214.pdf' target='_blank'>https://arxiv.org/pdf/2403.01214.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyi Yu, Ling Yan, Pengtao Jiang, Hao Chen, Bo Li, Lin Yuanbo Wu, Linlin Ou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.01214">Boosting Box-supervised Instance Segmentation with Pseudo Depth</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The realm of Weakly Supervised Instance Segmentation (WSIS) under box supervision has garnered substantial attention, showcasing remarkable advancements in recent years. However, the limitations of box supervision become apparent in its inability to furnish effective information for distinguishing foreground from background within the specified target box. This research addresses this challenge by introducing pseudo-depth maps into the training process of the instance segmentation network, thereby boosting its performance by capturing depth differences between instances. These pseudo-depth maps are generated using a readily available depth predictor and are not necessary during the inference stage. To enable the network to discern depth features when predicting masks, we integrate a depth prediction layer into the mask prediction head. This innovative approach empowers the network to simultaneously predict masks and depth, enhancing its ability to capture nuanced depth-related information during the instance segmentation process. We further utilize the mask generated in the training process as supervision to distinguish the foreground from the background. When selecting the best mask for each box through the Hungarian algorithm, we use depth consistency as one calculation cost item. The proposed method achieves significant improvements on Cityscapes and COCO dataset.
<div id='section'>Paperid: <span id='pid'>451, <a href='https://arxiv.org/pdf/2401.13611.pdf' target='_blank'>https://arxiv.org/pdf/2401.13611.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rhiannon Mogridge, George Close, Robert Sutherland, Thomas Hain, Jon Barker, Stefan Goetze, Anton Ragni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.13611">Non-Intrusive Speech Intelligibility Prediction for Hearing-Impaired Users using Intermediate ASR Features and Human Memory Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural networks have been successfully used for non-intrusive speech intelligibility prediction. Recently, the use of feature representations sourced from intermediate layers of pre-trained self-supervised and weakly-supervised models has been found to be particularly useful for this task. This work combines the use of Whisper ASR decoder layer representations as neural network input features with an exemplar-based, psychologically motivated model of human memory to predict human intelligibility ratings for hearing-aid users. Substantial performance improvement over an established intrusive HASPI baseline system is found, including on enhancement systems and listeners unseen in the training data, with a root mean squared error of 25.3 compared with the baseline of 28.7.
<div id='section'>Paperid: <span id='pid'>452, <a href='https://arxiv.org/pdf/2512.16243.pdf' target='_blank'>https://arxiv.org/pdf/2512.16243.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Zhang, Yunfei Gong, Zhidan Xie, Zhizi Wang, Antoni B. Chan, Hui Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.16243">Semi-Supervised Multi-View Crowd Counting by Ranking Multi-View Fusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-view crowd counting has been proposed to deal with the severe occlusion issue of crowd counting in large and wide scenes. However, due to the difficulty of collecting and annotating multi-view images, the datasets for multi-view counting have a limited number of multi-view frames and scenes. To solve the problem of limited data, one approach is to collect synthetic data to bypass the annotating step, while another is to propose semi- or weakly-supervised or unsupervised methods that demand less multi-view data. In this paper, we propose two semi-supervised multi-view crowd counting frameworks by ranking the multi-view fusion models of different numbers of input views, in terms of the model predictions or the model uncertainties. Specifically, for the first method (vanilla model), we rank the multi-view fusion models' prediction results of different numbers of camera-view inputs, namely, the model's predictions with fewer camera views shall not be larger than the predictions with more camera views. For the second method, we rank the estimated model uncertainties of the multi-view fusion models with a variable number of view inputs, guided by the multi-view fusion models' prediction errors, namely, the model uncertainties with more camera views shall not be larger than those with fewer camera views. These constraints are introduced into the model training in a semi-supervised fashion for multi-view counting with limited labeled data. The experiments demonstrate the advantages of the proposed multi-view model ranking methods compared with other semi-supervised counting methods.
<div id='section'>Paperid: <span id='pid'>453, <a href='https://arxiv.org/pdf/2512.10316.pdf' target='_blank'>https://arxiv.org/pdf/2512.10316.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Khang Le, Ha Thach, Anh M. Vu, Trang T. K. Vo, Han H. Huynh, David Yang, Minh H. N. Le, Thanh-Huy Nguyen, Akash Awasthi, Chandra Mohan, Zhu Han, Hien Van Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.10316">ConStruct: Structural Distillation of Foundation Models for Prototype-Based Weakly Supervised Histopathology Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation (WSSS) in histopathology relies heavily on classification backbones, yet these models often localize only the most discriminative regions and struggle to capture the full spatial extent of tissue structures. Vision-language models such as CONCH offer rich semantic alignment and morphology-aware representations, while modern segmentation backbones like SegFormer preserve fine-grained spatial cues. However, combining these complementary strengths remains challenging, especially under weak supervision and without dense annotations. We propose a prototype learning framework for WSSS in histopathological images that integrates morphology-aware representations from CONCH, multi-scale structural cues from SegFormer, and text-guided semantic alignment to produce prototypes that are simultaneously semantically discriminative and spatially coherent. To effectively leverage these heterogeneous sources, we introduce text-guided prototype initialization that incorporates pathology descriptions to generate more complete and semantically accurate pseudo-masks. A structural distillation mechanism transfers spatial knowledge from SegFormer to preserve fine-grained morphological patterns and local tissue boundaries during prototype learning. Our approach produces high-quality pseudo masks without pixel-level annotations, improves localization completeness, and enhances semantic consistency across tissue types. Experiments on BCSS-WSSS datasets demonstrate that our prototype learning framework outperforms existing WSSS methods while remaining computationally efficient through frozen foundation model backbones and lightweight trainable adapters.
<div id='section'>Paperid: <span id='pid'>454, <a href='https://arxiv.org/pdf/2512.10314.pdf' target='_blank'>https://arxiv.org/pdf/2512.10314.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anh M. Vu, Khang P. Le, Trang T. K. Vo, Ha Thach, Huy Hung Nguyen, David Yang, Han H. Huynh, Quynh Nguyen, Tuan M. Pham, Tuan-Anh Le, Minh H. N. Le, Thanh-Huy Nguyen, Akash Awasthi, Chandra Mohan, Zhu Han, Hien Van Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.10314">DualProtoSeg: Simple and Efficient Design with Text- and Image-Guided Prototype Learning for Weakly Supervised Histopathology Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation (WSSS) in histopathology seeks to reduce annotation cost by learning from image-level labels, yet it remains limited by inter-class homogeneity, intra-class heterogeneity, and the region-shrinkage effect of CAM-based supervision. We propose a simple and effective prototype-driven framework that leverages vision-language alignment to improve region discovery under weak supervision. Our method integrates CoOp-style learnable prompt tuning to generate text-based prototypes and combines them with learnable image prototypes, forming a dual-modal prototype bank that captures both semantic and appearance cues. To address oversmoothing in ViT representations, we incorporate a multi-scale pyramid module that enhances spatial precision and improves localization quality. Experiments on the BCSS-WSSS benchmark show that our approach surpasses existing state-of-the-art methods, and detailed analyses demonstrate the benefits of text description diversity, context length, and the complementary behavior of text and image prototypes. These results highlight the effectiveness of jointly leveraging textual semantics and visual prototype learning for WSSS in digital pathology.
<div id='section'>Paperid: <span id='pid'>455, <a href='https://arxiv.org/pdf/2509.22132.pdf' target='_blank'>https://arxiv.org/pdf/2509.22132.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingjing Lu, Huilong Pi, Yunchuan Qin, Zhuo Tang, Ruihui Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22132">Self-Supervised Point Cloud Completion based on Multi-View Augmentations of Single Partial Point Cloud</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Point cloud completion aims to reconstruct complete shapes from partial observations. Although current methods have achieved remarkable performance, they still have some limitations: Supervised methods heavily rely on ground truth, which limits their generalization to real-world datasets due to the synthetic-to-real domain gap. Unsupervised methods require complete point clouds to compose unpaired training data, and weakly-supervised methods need multi-view observations of the object. Existing self-supervised methods frequently produce unsatisfactory predictions due to the limited capabilities of their self-supervised signals. To overcome these challenges, we propose a novel self-supervised point cloud completion method. We design a set of novel self-supervised signals based on multi-view augmentations of the single partial point cloud. Additionally, to enhance the model's learning ability, we first incorporate Mamba into self-supervised point cloud completion task, encouraging the model to generate point clouds with better quality. Experiments on synthetic and real-world datasets demonstrate that our method achieves state-of-the-art results.
<div id='section'>Paperid: <span id='pid'>456, <a href='https://arxiv.org/pdf/2509.08129.pdf' target='_blank'>https://arxiv.org/pdf/2509.08129.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Francisco M. Castro-MacÃ­as, Francisco J. SÃ¡ez-Maldonado, Pablo Morales-Ãlvarez, Rafael Molina
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08129">torchmil: A PyTorch-based library for deep Multiple Instance Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple Instance Learning (MIL) is a powerful framework for weakly supervised learning, particularly useful when fine-grained annotations are unavailable. Despite growing interest in deep MIL methods, the field lacks standardized tools for model development, evaluation, and comparison, which hinders reproducibility and accessibility. To address this, we present torchmil, an open-source Python library built on PyTorch. torchmil offers a unified, modular, and extensible framework, featuring basic building blocks for MIL models, a standardized data format, and a curated collection of benchmark datasets and models. The library includes comprehensive documentation and tutorials to support both practitioners and researchers. torchmil aims to accelerate progress in MIL and lower the entry barrier for new users. Available at https://torchmil.readthedocs.io.
<div id='section'>Paperid: <span id='pid'>457, <a href='https://arxiv.org/pdf/2508.12322.pdf' target='_blank'>https://arxiv.org/pdf/2508.12322.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Deutges, Chen Yang, Raheleh Salehi, Nassir Navab, Carsten Marr, Ario Sadafi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12322">Neural Cellular Automata for Weakly Supervised Segmentation of White Blood Cells</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The detection and segmentation of white blood cells in blood smear images is a key step in medical diagnostics, supporting various downstream tasks such as automated blood cell counting, morphological analysis, cell classification, and disease diagnosis and monitoring. Training robust and accurate models requires large amounts of labeled data, which is both time-consuming and expensive to acquire. In this work, we propose a novel approach for weakly supervised segmentation using neural cellular automata (NCA-WSS). By leveraging the feature maps generated by NCA during classification, we can extract segmentation masks without the need for retraining with segmentation labels. We evaluate our method on three white blood cell microscopy datasets and demonstrate that NCA-WSS significantly outperforms existing weakly supervised approaches. Our work illustrates the potential of NCA for both classification and segmentation in a weakly supervised framework, providing a scalable and efficient solution for medical image analysis.
<div id='section'>Paperid: <span id='pid'>458, <a href='https://arxiv.org/pdf/2508.03997.pdf' target='_blank'>https://arxiv.org/pdf/2508.03997.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Zhang, Tianzhuzi Tan, Guanchun Yin, Bo Zhang, Xiuzhuang Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03997">JanusNet: Hierarchical Slice-Block Shuffle and Displacement for Semi-Supervised 3D Multi-Organ Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Limited by the scarcity of training samples and annotations, weakly supervised medical image segmentation often employs data augmentation to increase data diversity, while randomly mixing volumetric blocks has demonstrated strong performance. However, this approach disrupts the inherent anatomical continuity of 3D medical images along orthogonal axes, leading to severe structural inconsistencies and insufficient training in challenging regions, such as small-sized organs, etc. To better comply with and utilize human anatomical information, we propose JanusNet}, a data augmentation framework for 3D medical data that globally models anatomical continuity while locally focusing on hard-to-segment regions. Specifically, our Slice-Block Shuffle step performs aligned shuffling of same-index slice blocks across volumes along a random axis, while preserving the anatomical context on planes perpendicular to the perturbation axis. Concurrently, the Confidence-Guided Displacement step uses prediction reliability to replace blocks within each slice, amplifying signals from difficult areas. This dual-stage, axis-aligned framework is plug-and-play, requiring minimal code changes for most teacher-student schemes. Extensive experiments on the Synapse and AMOS datasets demonstrate that JanusNet significantly surpasses state-of-the-art methods, achieving, for instance, a 4% DSC gain on the Synapse dataset with only 20% labeled data.
<div id='section'>Paperid: <span id='pid'>459, <a href='https://arxiv.org/pdf/2508.01697.pdf' target='_blank'>https://arxiv.org/pdf/2508.01697.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiqi Huang, Tingfa Xu, Wen Yan, Dean Barratt, Yipeng Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01697">Register Anything: Estimating "Corresponding Prompts" for Segment Anything Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Establishing pixel/voxel-level or region-level correspondences is the core challenge in image registration. The latter, also known as region-based correspondence representation, leverages paired regions of interest (ROIs) to enable regional matching while preserving fine-grained capability at pixel/voxel level. Traditionally, this representation is implemented via two steps: segmenting ROIs in each image then matching them between the two images. In this paper, we simplify this into one step by directly "searching for corresponding prompts", using extensively pre-trained segmentation models (e.g., SAM) for a training-free registration approach, PromptReg. Firstly, we introduce the "corresponding prompt problem", which aims to identify a corresponding Prompt Y in Image Y for any given visual Prompt X in Image X, such that the two respectively prompt-conditioned segmentations are a pair of corresponding ROIs from the two images. Secondly, we present an "inverse prompt" solution that generates primary and optionally auxiliary prompts, inverting Prompt X into the prompt space of Image Y. Thirdly, we propose a novel registration algorithm that identifies multiple paired corresponding ROIs by marginalizing the inverted Prompt X across both prompt and spatial dimensions. Comprehensive experiments are conducted on five applications of registering 3D prostate MR, 3D abdomen MR, 3D lung CT, 2D histopathology and, as a non-medical example, 2D aerial images. Based on metrics including Dice and target registration errors on anatomical structures, the proposed registration outperforms both intensity-based iterative algorithms and learning-based DDF-predicting networks, even yielding competitive performance with weakly-supervised approaches that require fully-segmented training data.
<div id='section'>Paperid: <span id='pid'>460, <a href='https://arxiv.org/pdf/2506.23460.pdf' target='_blank'>https://arxiv.org/pdf/2506.23460.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dewen Zeng, Xinrong Hu, Yu-Jen Chen, Yawen Wu, Xiaowei Xu, Yiyu Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23460">Contrastive Learning with Diffusion Features for Weakly Supervised Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation (WSSS) methods using class labels often rely on class activation maps (CAMs) to localize objects. However, traditional CAM-based methods struggle with partial activations and imprecise object boundaries due to optimization discrepancies between classification and segmentation. Recently, the conditional diffusion model (CDM) has been used as an alternative for generating segmentation masks in WSSS, leveraging its strong image generation capabilities tailored to specific class distributions. By modifying or perturbing the condition during diffusion sampling, the related objects can be highlighted in the generated images. Yet, the saliency maps generated by CDMs are prone to noise from background alterations during reverse diffusion. To alleviate the problem, we introduce Contrastive Learning with Diffusion Features (CLDF), a novel method that uses contrastive learning to train a pixel decoder to map the diffusion features from a frozen CDM to a low-dimensional embedding space for segmentation. Specifically, we integrate gradient maps generated from CDM external classifier with CAMs to identify foreground and background pixels with fewer false positives/negatives for contrastive learning, enabling robust pixel embedding learning. Experimental results on four segmentation tasks from two public medical datasets demonstrate that our method significantly outperforms existing baselines.
<div id='section'>Paperid: <span id='pid'>461, <a href='https://arxiv.org/pdf/2506.06006.pdf' target='_blank'>https://arxiv.org/pdf/2506.06006.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifu Qiu, Yftah Ziser, Anna Korhonen, Shay B. Cohen, Edoardo M. Ponti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06006">Bootstrapping World Models from Dynamics Models in Multimodal Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To what extent do vision-and-language foundation models possess a realistic world model (observation $\times$ action $\rightarrow$ observation) and a dynamics model (observation $\times$ observation $\rightarrow$ action), when actions are expressed through language? While open-source foundation models struggle with both, we find that fine-tuning them to acquire a dynamics model through supervision is significantly easier than acquiring a world model. In turn, dynamics models can be used to bootstrap world models through two main strategies: 1) weakly supervised learning from synthetic data and 2) inference time verification. Firstly, the dynamics model can annotate actions for unlabelled pairs of video frame observations to expand the training data. We further propose a new objective, where image tokens in observation pairs are weighted by their importance, as predicted by a recognition model. Secondly, the dynamics models can assign rewards to multiple samples of the world model to score them, effectively guiding search at inference time. We evaluate the world models resulting from both strategies through the task of action-centric image editing on Aurora-Bench. Our best model achieves a performance competitive with state-of-the-art image editing models, improving on them by a margin of $15\%$ on real-world subsets according to GPT4o-as-judge, and achieving the best average human evaluation across all subsets of Aurora-Bench.
<div id='section'>Paperid: <span id='pid'>462, <a href='https://arxiv.org/pdf/2504.13405.pdf' target='_blank'>https://arxiv.org/pdf/2504.13405.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengqin Jiang, Linfei Li, Haokui Zhang, Qingshan Liu, Amin Beheshti, Jian Yang, Anton van den Hengel, Quan Z. Sheng, Yuankai Qi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13405">ProgRoCC: A Progressive Approach to Rough Crowd Counting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As the number of individuals in a crowd grows, enumeration-based techniques become increasingly infeasible and their estimates increasingly unreliable. We propose instead an estimation-based version of the problem: we label Rough Crowd Counting that delivers better accuracy on the basis of training data that is easier to acquire. Rough crowd counting requires only rough annotations of the number of targets in an image, instead of the more traditional, and far more expensive, per-target annotations. We propose an approach to the rough crowd counting problem based on CLIP, termed ProgRoCC. Specifically, we introduce a progressive estimation learning strategy that determines the object count through a coarse-to-fine approach. This approach delivers answers quickly, outperforms the state-of-the-art in semi- and weakly-supervised crowd counting. In addition, we design a vision-language matching adapter that optimizes key-value pairs by mining effective matches of two modalities to refine the visual features, thereby improving the final performance. Extensive experimental results on three widely adopted crowd counting datasets demonstrate the effectiveness of our method.
<div id='section'>Paperid: <span id='pid'>463, <a href='https://arxiv.org/pdf/2503.20826.pdf' target='_blank'>https://arxiv.org/pdf/2503.20826.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiwei Yang, Yucong Meng, Kexue Fu, Feilong Tang, Shuo Wang, Zhijian Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20826">Exploring CLIP's Dense Knowledge for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Semantic Segmentation (WSSS) with image-level labels aims to achieve pixel-level predictions using Class Activation Maps (CAMs). Recently, Contrastive Language-Image Pre-training (CLIP) has been introduced in WSSS. However, recent methods primarily focus on image-text alignment for CAM generation, while CLIP's potential in patch-text alignment remains unexplored. In this work, we propose ExCEL to explore CLIP's dense knowledge via a novel patch-text alignment paradigm for WSSS. Specifically, we propose Text Semantic Enrichment (TSE) and Visual Calibration (VC) modules to improve the dense alignment across both text and vision modalities. To make text embeddings semantically informative, our TSE module applies Large Language Models (LLMs) to build a dataset-wide knowledge base and enriches the text representations with an implicit attribute-hunting process. To mine fine-grained knowledge from visual features, our VC module first proposes Static Visual Calibration (SVC) to propagate fine-grained knowledge in a non-parametric manner. Then Learnable Visual Calibration (LVC) is further proposed to dynamically shift the frozen features towards distributions with diverse semantics. With these enhancements, ExCEL not only retains CLIP's training-free advantages but also significantly outperforms other state-of-the-art methods with much less training cost on PASCAL VOC and MS COCO.
<div id='section'>Paperid: <span id='pid'>464, <a href='https://arxiv.org/pdf/2411.08753.pdf' target='_blank'>https://arxiv.org/pdf/2411.08753.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sagnik Majumder, Tushar Nagarajan, Ziad Al-Halah, Reina Pradhan, Kristen Grauman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08753">Which Viewpoint Shows it Best? Language for Weakly Supervising View Selection in Multi-view Instructional Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Given a multi-view video, which viewpoint is most informative for a human observer? Existing methods rely on heuristics or expensive "best-view" supervision to answer this question, limiting their applicability. We propose a weakly supervised approach that leverages language accompanying an instructional multi-view video as a means to recover its most informative viewpoint(s). Our key hypothesis is that the more accurately an individual view can predict a view-agnostic text summary, the more informative it is. To put this into action, we propose LangView, a framework that uses the relative accuracy of view-dependent caption predictions as a proxy for best view pseudo-labels. Then, those pseudo-labels are used to train a view selector, together with an auxiliary camera pose predictor that enhances view-sensitivity. During inference, our model takes as input only a multi-view video--no language or camera poses--and returns the best viewpoint to watch at each timestep. On two challenging datasets comprised of diverse multi-camera setups and how-to activities, our model consistently outperforms state-of-the-art baselines, both with quantitative metrics and human evaluation. Project page: https://vision.cs.utexas.edu/projects/which-view-shows-it-best.
<div id='section'>Paperid: <span id='pid'>465, <a href='https://arxiv.org/pdf/2408.06743.pdf' target='_blank'>https://arxiv.org/pdf/2408.06743.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jialiang Wang, Ning Zhang, Shimin Di, Ruidong Wang, Lei Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.06743">Class-aware and Augmentation-free Contrastive Learning from Label Proportion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning from Label Proportion (LLP) is a weakly supervised learning scenario in which training data is organized into predefined bags of instances, disclosing only the class label proportions per bag. This paradigm is essential for user modeling and personalization, where user privacy is paramount, offering insights into user preferences without revealing individual data. LLP faces a unique difficulty: the misalignment between bag-level supervision and the objective of instance-level prediction, primarily due to the inherent ambiguity in label proportion matching. Previous studies have demonstrated deep representation learning can generate auxiliary signals to promote the supervision level in the image domain. However, applying these techniques to tabular data presents significant challenges: 1) they rely heavily on label-invariant augmentation to establish multi-view, which is not feasible with the heterogeneous nature of tabular datasets, and 2) tabular datasets often lack sufficient semantics for perfect class distinction, making them prone to suboptimality caused by the inherent ambiguity of label proportion matching.
  To address these challenges, we propose an augmentation-free contrastive framework TabLLP-BDC that introduces class-aware supervision (explicitly aware of class differences) at the instance level. Our solution features a two-stage Bag Difference Contrastive (BDC) learning mechanism that establishes robust class-aware instance-level supervision by disassembling the nuance between bag label proportions, without relying on augmentations. Concurrently, our model presents a pioneering multi-task pretraining pipeline tailored for tabular-based LLP, capturing intrinsic tabular feature correlations in alignment with label proportion distribution. Extensive experiments demonstrate that TabLLP-BDC achieves state-of-the-art performance for LLP in the tabular domain.
<div id='section'>Paperid: <span id='pid'>466, <a href='https://arxiv.org/pdf/2407.08971.pdf' target='_blank'>https://arxiv.org/pdf/2407.08971.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qianhan Feng, Wenshuo Li, Tong Lin, Xinghao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08971">Full-Stage Pseudo Label Quality Enhancement for Weakly-supervised Temporal Action Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised Temporal Action Localization (WSTAL) aims to localize actions in untrimmed videos using only video-level supervision. Latest WSTAL methods introduce pseudo label learning framework to bridge the gap between classification-based training and inferencing targets at localization, and achieve cutting-edge results. In these frameworks, a classification-based model is used to generate pseudo labels for a regression-based student model to learn from. However, the quality of pseudo labels in the framework, which is a key factor to the final result, is not carefully studied. In this paper, we propose a set of simple yet efficient pseudo label quality enhancement mechanisms to build our FuSTAL framework. FuSTAL enhances pseudo label quality at three stages: cross-video contrastive learning at proposal Generation-Stage, prior-based filtering at proposal Selection-Stage and EMA-based distillation at Training-Stage. These designs enhance pseudo label quality at different stages in the framework, and help produce more informative, less false and smoother action proposals. With the help of these comprehensive designs at all stages, FuSTAL achieves an average mAP of 50.8% on THUMOS'14, outperforming the previous best method by 1.2%, and becomes the first method to reach the milestone of 50%.
<div id='section'>Paperid: <span id='pid'>467, <a href='https://arxiv.org/pdf/2407.02920.pdf' target='_blank'>https://arxiv.org/pdf/2407.02920.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ramy Battrawy, RenÃ© Schuster, Didier Stricker
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.02920">EgoFlowNet: Non-Rigid Scene Flow from Point Clouds with Ego-Motion Support</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent weakly-supervised methods for scene flow estimation from LiDAR point clouds are limited to explicit reasoning on object-level. These methods perform multiple iterative optimizations for each rigid object, which makes them vulnerable to clustering robustness. In this paper, we propose our EgoFlowNet - a point-level scene flow estimation network trained in a weakly-supervised manner and without object-based abstraction. Our approach predicts a binary segmentation mask that implicitly drives two parallel branches for ego-motion and scene flow. Unlike previous methods, we provide both branches with all input points and carefully integrate the binary mask into the feature extraction and losses. We also use a shared cost volume with local refinement that is updated at multiple scales without explicit clustering or rigidity assumptions. On realistic KITTI scenes, we show that our EgoFlowNet performs better than state-of-the-art methods in the presence of ground surface points.
<div id='section'>Paperid: <span id='pid'>468, <a href='https://arxiv.org/pdf/2405.19387.pdf' target='_blank'>https://arxiv.org/pdf/2405.19387.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Moshira Abdalla, Sajid Javed, Muaz Al Radi, Anwaar Ulhaq, Naoufel Werghi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19387">Video Anomaly Detection in 10 Years: A Survey and Outlook</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video anomaly detection (VAD) holds immense importance across diverse domains such as surveillance, healthcare, and environmental monitoring. While numerous surveys focus on conventional VAD methods, they often lack depth in exploring specific approaches and emerging trends. This survey explores deep learning-based VAD, expanding beyond traditional supervised training paradigms to encompass emerging weakly supervised, self-supervised, and unsupervised approaches. A prominent feature of this review is the investigation of core challenges within the VAD paradigms including large-scale datasets, features extraction, learning methods, loss functions, regularization, and anomaly score prediction. Moreover, this review also investigates the vision language models (VLMs) as potent feature extractors for VAD. VLMs integrate visual data with textual descriptions or spoken language from videos, enabling a nuanced understanding of scenes crucial for anomaly detection. By addressing these challenges and proposing future research directions, this review aims to foster the development of robust and efficient VAD systems leveraging the capabilities of VLMs for enhanced anomaly detection in complex real-world scenarios. This comprehensive analysis seeks to bridge existing knowledge gaps, provide researchers with valuable insights, and contribute to shaping the future of VAD research.
<div id='section'>Paperid: <span id='pid'>469, <a href='https://arxiv.org/pdf/2405.10879.pdf' target='_blank'>https://arxiv.org/pdf/2405.10879.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiqi Huang, Tingfa Xu, Ziyi Shen, Shaheer Ullah Saeed, Wen Yan, Dean Barratt, Yipeng Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.10879">One registration is worth two segmentations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The goal of image registration is to establish spatial correspondence between two or more images, traditionally through dense displacement fields (DDFs) or parametric transformations (e.g., rigid, affine, and splines). Rethinking the existing paradigms of achieving alignment via spatial transformations, we uncover an alternative but more intuitive correspondence representation: a set of corresponding regions-of-interest (ROI) pairs, which we demonstrate to have sufficient representational capability as other correspondence representation methods.Further, it is neither necessary nor sufficient for these ROIs to hold specific anatomical or semantic significance. In turn, we formulate image registration as searching for the same set of corresponding ROIs from both moving and fixed images - in other words, two multi-class segmentation tasks on a pair of images. For a general-purpose and practical implementation, we integrate the segment anything model (SAM) into our proposed algorithms, resulting in a SAM-enabled registration (SAMReg) that does not require any training data, gradient-based fine-tuning or engineered prompts. We experimentally show that the proposed SAMReg is capable of segmenting and matching multiple ROI pairs, which establish sufficiently accurate correspondences, in three clinical applications of registering prostate MR, cardiac MR and abdominal CT images. Based on metrics including Dice and target registration errors on anatomical structures, the proposed registration outperforms both intensity-based iterative algorithms and DDF-predicting learning-based networks, even yielding competitive performance with weakly-supervised registration which requires fully-segmented training data.
<div id='section'>Paperid: <span id='pid'>470, <a href='https://arxiv.org/pdf/2404.09475.pdf' target='_blank'>https://arxiv.org/pdf/2404.09475.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Byeongkeun Kang, Sinhae Cha, Yeejin Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.09475">Improving Weakly-Supervised Object Localization Using Adversarial Erasing and Pseudo Label</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised learning approaches have gained significant attention due to their ability to reduce the effort required for human annotations in training neural networks. This paper investigates a framework for weakly-supervised object localization, which aims to train a neural network capable of predicting both the object class and its location using only images and their image-level class labels. The proposed framework consists of a shared feature extractor, a classifier, and a localizer. The localizer predicts pixel-level class probabilities, while the classifier predicts the object class at the image level. Since image-level class labels are insufficient for training the localizer, weakly-supervised object localization methods often encounter challenges in accurately localizing the entire object region. To address this issue, the proposed method incorporates adversarial erasing and pseudo labels to improve localization accuracy. Specifically, novel losses are designed to utilize adversarially erased foreground features and adversarially erased feature maps, reducing dependence on the most discriminative region. Additionally, the proposed method employs pseudo labels to suppress activation values in the background while increasing them in the foreground. The proposed method is applied to two backbone networks (MobileNetV1 and InceptionV3) and is evaluated on three publicly available datasets (ILSVRC-2012, CUB-200-2011, and PASCAL VOC 2012). The experimental results demonstrate that the proposed method outperforms previous state-of-the-art methods across all evaluated metrics.
<div id='section'>Paperid: <span id='pid'>471, <a href='https://arxiv.org/pdf/2404.08195.pdf' target='_blank'>https://arxiv.org/pdf/2404.08195.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiwei Yang, Yucong Meng, Kexue Fu, Shuo Wang, Zhijian Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.08195">Tackling Ambiguity from Perspective of Uncertainty Inference and Affinity Diversification for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation (WSSS) with image-level labels intends to achieve dense tasks without laborious annotations. However, due to the ambiguous contexts and fuzzy regions, the performance of WSSS, especially the stages of generating Class Activation Maps (CAMs) and refining pseudo masks, widely suffers from ambiguity while being barely noticed by previous literature. In this work, we propose UniA, a unified single-staged WSSS framework, to efficiently tackle this issue from the perspective of uncertainty inference and affinity diversification, respectively. When activating class objects, we argue that the false activation stems from the bias to the ambiguous regions during the feature extraction. Therefore, we design a more robust feature representation with a probabilistic Gaussian distribution and introduce the uncertainty estimation to avoid the bias. A distribution loss is particularly proposed to supervise the process, which effectively captures the ambiguity and models the complex dependencies among features. When refining pseudo labels, we observe that the affinity from the prevailing refinement methods intends to be similar among ambiguities. To this end, an affinity diversification module is proposed to promote diversity among semantics. A mutual complementing refinement is proposed to initially rectify the ambiguous affinity with multiple inferred pseudo labels. More importantly, a contrastive affinity loss is further designed to diversify the relations among unrelated semantics, which reliably propagates the diversity into the whole feature representations and helps generate better pseudo masks. Extensive experiments are conducted on PASCAL VOC, MS COCO, and medical ACDC datasets, which validate the efficiency of UniA tackling ambiguity and the superiority over recent single-staged or even most multi-staged competitors.
<div id='section'>Paperid: <span id='pid'>472, <a href='https://arxiv.org/pdf/2510.22209.pdf' target='_blank'>https://arxiv.org/pdf/2510.22209.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sofoklis Kitharidis, Cor J. Veenman, Thomas Bäck, Niki van Stein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.22209">Visual Model Selection using Feature Importance Clusters in Fairness-Performance Similarity Optimized Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the context of algorithmic decision-making, fair machine learning methods often yield multiple models that balance predictive fairness and performance in varying degrees. This diversity introduces a challenge for stakeholders who must select a model that aligns with their specific requirements and values. To address this, we propose an interactive framework that assists in navigating and interpreting the trade-offs across a portfolio of models. Our approach leverages weakly supervised metric learning to learn a Mahalanobis distance that reflects similarity in fairness and performance outcomes, effectively structuring the feature importance space of the models according to stakeholder-relevant criteria. We then apply clustering technique (k-means) to group models based on their transformed representations of feature importances, allowing users to explore clusters of models with similar predictive behaviors and fairness characteristics. This facilitates informed decision-making by helping users understand how models differ not only in their fairness-performance balance but also in the features that drive their predictions.
<div id='section'>Paperid: <span id='pid'>473, <a href='https://arxiv.org/pdf/2510.17484.pdf' target='_blank'>https://arxiv.org/pdf/2510.17484.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Umer Ramzan, Ali Zia, Abdelwahed Khamis, Noman Ali, Usman Ali, Wei Xiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17484">Split-Fuse-Transport: Annotation-Free Saliency via Dual Clustering and Optimal Transport Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Salient object detection (SOD) aims to segment visually prominent regions in images and serves as a foundational task for various computer vision applications. We posit that SOD can now reach near-supervised accuracy without a single pixel-level label, but only when reliable pseudo-masks are available. We revisit the prototype-based line of work and make two key observations. First, boundary pixels and interior pixels obey markedly different geometry; second, the global consistency enforced by optimal transport (OT) is underutilized if prototype quality is weak. To address this, we introduce POTNet, an adaptation of Prototypical Optimal Transport that replaces POT's single k-means step with an entropy-guided dual-clustering head: high-entropy pixels are organized by spectral clustering, low-entropy pixels by k-means, and the two prototype sets are subsequently aligned by OT. This split-fuse-transport design yields sharper, part-aware pseudo-masks in a single forward pass, without handcrafted priors. Those masks supervise a standard MaskFormer-style encoder-decoder, giving rise to AutoSOD, an end-to-end unsupervised SOD pipeline that eliminates SelfMask's offline voting yet improves both accuracy and training efficiency. Extensive experiments on five benchmarks show that AutoSOD outperforms unsupervised methods by up to 26% and weakly supervised methods by up to 36% in F-measure, further narrowing the gap to fully supervised models.
<div id='section'>Paperid: <span id='pid'>474, <a href='https://arxiv.org/pdf/2508.11472.pdf' target='_blank'>https://arxiv.org/pdf/2508.11472.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Wang, Yaxin Zhao, Xinyu Jiao, Sihan Xu, Xiangrui Cai, Ying Zhang, Xiaojie Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11472">RMSL: Weakly-Supervised Insider Threat Detection with Robust Multi-sphere Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Insider threat detection aims to identify malicious user behavior by analyzing logs that record user interactions. Due to the lack of fine-grained behavior-level annotations, detecting specific behavior-level anomalies within user behavior sequences is challenging. Unsupervised methods face high false positive rates and miss rates due to the inherent ambiguity between normal and anomalous behaviors. In this work, we instead introduce weak labels of behavior sequences, which have lower annotation costs, i.e., the training labels (anomalous or normal) are at sequence-level instead of behavior-level, to enhance the detection capability for behavior-level anomalies by learning discriminative features. To achieve this, we propose a novel framework called Robust Multi-sphere Learning (RMSL). RMSL uses multiple hyper-spheres to represent the normal patterns of behaviors. Initially, a one-class classifier is constructed as a good anomaly-supervision-free starting point. Building on this, using multiple instance learning and adaptive behavior-level self-training debiasing based on model prediction confidence, the framework further refines hyper-spheres and feature representations using weak sequence-level labels. This approach enhances the model's ability to distinguish between normal and anomalous behaviors. Extensive experiments demonstrate that RMSL significantly improves the performance of behavior-level insider threat detection.
<div id='section'>Paperid: <span id='pid'>475, <a href='https://arxiv.org/pdf/2507.15203.pdf' target='_blank'>https://arxiv.org/pdf/2507.15203.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyue Liu, Xicheng Sheng, Xiahai Zhuang, Vicente Grau, Mark YY Chan, Ching-Hui Sia, Lei Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15203">Personalized 4D Whole Heart Geometry Reconstruction from Cine MRI for Cardiac Digital Twins</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cardiac digital twins (CDTs) provide personalized in-silico cardiac representations and hold great potential for precision medicine in cardiology. However, whole-heart CDT models that simulate the full organ-scale electromechanics of all four heart chambers remain limited. In this work, we propose a weakly supervised learning model to reconstruct 4D (3D+t) heart mesh directly from multi-view 2D cardiac cine MRIs. This is achieved by learning a self-supervised mapping between cine MRIs and 4D cardiac meshes, enabling the generation of personalized heart models that closely correspond to input cine MRIs. The resulting 4D heart meshes can facilitate the automatic extraction of key cardiac variables, including ejection fraction and dynamic chamber volume changes with high temporal resolution. It demonstrates the feasibility of inferring personalized 4D heart models from cardiac MRIs, paving the way for an efficient CDT platform for precision medicine. The code will be publicly released once the manuscript is accepted.
<div id='section'>Paperid: <span id='pid'>476, <a href='https://arxiv.org/pdf/2503.11376.pdf' target='_blank'>https://arxiv.org/pdf/2503.11376.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Panggih Kusuma Ningrum, Philipp Mayr, Nina Smirnova, Iana Atanassova
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11376">Annotating Scientific Uncertainty: A comprehensive model using linguistic patterns and comparison with existing approaches</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>UnScientify, a system designed to detect scientific uncertainty in scholarly full text. The system utilizes a weakly supervised technique to identify verbally expressed uncertainty in scientific texts and their authorial references. The core methodology of UnScientify is based on a multi-faceted pipeline that integrates span pattern matching, complex sentence analysis and author reference checking. This approach streamlines the labeling and annotation processes essential for identifying scientific uncertainty, covering a variety of uncertainty expression types to support diverse applications including information retrieval, text mining and scientific document processing. The evaluation results highlight the trade-offs between modern large language models (LLMs) and the UnScientify system. UnScientify, which employs more traditional techniques, achieved superior performance in the scientific uncertainty detection task, attaining an accuracy score of 0.808. This finding underscores the continued relevance and efficiency of UnScientify's simple rule-based and pattern matching strategy for this specific application. The results demonstrate that in scenarios where resource efficiency, interpretability, and domain-specific adaptability are critical, traditional methods can still offer significant advantages.
<div id='section'>Paperid: <span id='pid'>477, <a href='https://arxiv.org/pdf/2410.19176.pdf' target='_blank'>https://arxiv.org/pdf/2410.19176.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dachun Sun, Ruijie Wang, Jinning Li, Ruipeng Han, Xinyi Liu, You Lyu, Tarek Abdelzaher
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.19176">Perturbation-based Graph Active Learning for Weakly-Supervised Belief Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the problem of optimizing the allocation of labeling resources for semi-supervised belief representation learning in social networks. The objective is to strategically identify valuable messages on social media graphs that are worth labeling within a constrained budget, ultimately maximizing the task's performance. Despite the progress in unsupervised or semi-supervised methods in advancing belief and ideology representation learning on social networks and the remarkable efficacy of graph learning techniques, the availability of high-quality curated labeled social data can greatly benefit and further improve performances. Consequently, allocating labeling efforts is a critical research problem in scenarios where labeling resources are limited. This paper proposes a graph data augmentation-inspired perturbation-based active learning strategy (PerbALGraph) that progressively selects messages for labeling according to an automatic estimator, obviating human guidance. This estimator is based on the principle that messages in the network that exhibit heightened sensitivity to structural features of the observational data indicate landmark quality that significantly influences semi-supervision processes. We design the estimator to be the prediction variance under a set of designed graph perturbations, which is model-agnostic and application-independent. Extensive experiment results demonstrate the effectiveness of the proposed strategy for belief representation learning tasks.
<div id='section'>Paperid: <span id='pid'>478, <a href='https://arxiv.org/pdf/2410.08091.pdf' target='_blank'>https://arxiv.org/pdf/2410.08091.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyi Pan, Wei Gao, Shan Liu, Ge Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08091">Distribution Guidance Network for Weakly Supervised Point Cloud Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite alleviating the dependence on dense annotations inherent to fully supervised methods, weakly supervised point cloud semantic segmentation suffers from inadequate supervision signals. In response to this challenge, we introduce a novel perspective that imparts auxiliary constraints by regulating the feature space under weak supervision. Our initial investigation identifies which distributions accurately characterize the feature space, subsequently leveraging this priori to guide the alignment of the weakly supervised embeddings. Specifically, we analyze the superiority of the mixture of von Mises-Fisher distributions (moVMF) among several common distribution candidates. Accordingly, we develop a Distribution Guidance Network (DGNet), which comprises a weakly supervised learning branch and a distribution alignment branch. Leveraging reliable clustering initialization derived from the weakly supervised learning branch, the distribution alignment branch alternately updates the parameters of the moVMF and the network, ensuring alignment with the moVMF-defined latent space. Extensive experiments validate the rationality and effectiveness of our distribution choice and network design. Consequently, DGNet achieves state-of-the-art performance under multiple datasets and various weakly supervised settings.
<div id='section'>Paperid: <span id='pid'>479, <a href='https://arxiv.org/pdf/2409.16566.pdf' target='_blank'>https://arxiv.org/pdf/2409.16566.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kartikeya Singh, Yash Turkar, Christo Aluckal, Charuvarahan Adhivarahan, Karthik Dantu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16566">PANOS: Payload-Aware Navigation in Offroad Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nature has evolved humans to walk on different terrains by developing a detailed understanding of their physical characteristics. Similarly, legged robots need to develop their capability to walk on complex terrains with a variety of task-dependent payloads to achieve their goals. However, conventional terrain adaptation methods are susceptible to failure with varying payloads. In this work, we introduce PANOS, a weakly supervised approach that integrates proprioception and exteroception from onboard sensing to achieve a stable gait while walking by a legged robot over various terrains. Our work also provides evidence of its adaptability over varying payloads. We evaluate our method on multiple terrains and payloads using a legged robot. PANOS improves the stability up to 44% without any payload and 53% with 15 lbs payload. We also notice a reduction in the vibration cost of 20% with the payload for various terrain types when compared to state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>480, <a href='https://arxiv.org/pdf/2407.10814.pdf' target='_blank'>https://arxiv.org/pdf/2407.10814.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Linhao Qu, Dingkang Yang, Dan Huang, Qinhao Guo, Rongkui Luo, Shaoting Zhang, Xiaosong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10814">Pathology-knowledge Enhanced Multi-instance Prompt Learning for Few-shot Whole Slide Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current multi-instance learning algorithms for pathology image analysis often require a substantial number of Whole Slide Images for effective training but exhibit suboptimal performance in scenarios with limited learning data. In clinical settings, restricted access to pathology slides is inevitable due to patient privacy concerns and the prevalence of rare or emerging diseases. The emergence of the Few-shot Weakly Supervised WSI Classification accommodates the significant challenge of the limited slide data and sparse slide-level labels for diagnosis. Prompt learning based on the pre-trained models (\eg, CLIP) appears to be a promising scheme for this setting; however, current research in this area is limited, and existing algorithms often focus solely on patch-level prompts or confine themselves to language prompts. This paper proposes a multi-instance prompt learning framework enhanced with pathology knowledge, \ie, integrating visual and textual prior knowledge into prompts at both patch and slide levels. The training process employs a combination of static and learnable prompts, effectively guiding the activation of pre-trained models and further facilitating the diagnosis of key pathology patterns. Lightweight Messenger (self-attention) and Summary (attention-pooling) layers are introduced to model relationships between patches and slides within the same patient data. Additionally, alignment-wise contrastive losses ensure the feature-level alignment between visual and textual learnable prompts for both patches and slides. Our method demonstrates superior performance in three challenging clinical tasks, significantly outperforming comparative few-shot methods.
<div id='section'>Paperid: <span id='pid'>481, <a href='https://arxiv.org/pdf/2406.19638.pdf' target='_blank'>https://arxiv.org/pdf/2406.19638.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junsung Park, Hyunjung Shim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.19638">Precision matters: Precision-aware ensemble for weakly supervised semantic segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Semantic Segmentation (WSSS) employs weak supervision, such as image-level labels, to train the segmentation model. Despite the impressive achievement in recent WSSS methods, we identify that introducing weak labels with high mean Intersection of Union (mIoU) does not guarantee high segmentation performance. Existing studies have emphasized the importance of prioritizing precision and reducing noise to improve overall performance. In the same vein, we propose ORANDNet, an advanced ensemble approach tailored for WSSS. ORANDNet combines Class Activation Maps (CAMs) from two different classifiers to increase the precision of pseudo-masks (PMs). To further mitigate small noise in the PMs, we incorporate curriculum learning. This involves training the segmentation model initially with pairs of smaller-sized images and corresponding PMs, gradually transitioning to the original-sized pairs. By combining the original CAMs of ResNet-50 and ViT, we significantly improve the segmentation performance over the single-best model and the naive ensemble model, respectively. We further extend our ensemble method to CAMs from AMN (ResNet-like) and MCTformer (ViT-like) models, achieving performance benefits in advanced WSSS models. It highlights the potential of our ORANDNet as a final add-on module for WSSS models.
<div id='section'>Paperid: <span id='pid'>482, <a href='https://arxiv.org/pdf/2406.15284.pdf' target='_blank'>https://arxiv.org/pdf/2406.15284.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Georgios Paraskevopoulos, Chara Tsoukala, Athanasios Katsamanis, Vassilis Katsouros
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.15284">The Greek podcast corpus: Competitive speech models for low-resourced languages with weakly supervised data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of speech technologies for languages with limited digital representation poses significant challenges, primarily due to the scarcity of available data. This issue is exacerbated in the era of large, data-intensive models. Recent research has underscored the potential of leveraging weak supervision to augment the pool of available data. In this study, we compile an 800-hour corpus of Modern Greek from podcasts and employ Whisper large-v3 to generate silver transcriptions. This corpus is utilized to fine-tune our models, aiming to assess the efficacy of this approach in enhancing ASR performance. Our analysis spans 16 distinct podcast domains, alongside evaluations on established datasets for Modern Greek. The findings indicate consistent WER improvements, correlating with increases in both data volume and model size. Our study confirms that assembling large, weakly supervised corpora serves as a cost-effective strategy for advancing speech technologies in under-resourced languages.
<div id='section'>Paperid: <span id='pid'>483, <a href='https://arxiv.org/pdf/2406.01791.pdf' target='_blank'>https://arxiv.org/pdf/2406.01791.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weitong Cai, Jiabo Huang, Shaogang Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01791">Hybrid-Learning Video Moment Retrieval across Multi-Domain Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video moment retrieval (VMR) is to search for a visual temporal moment in an untrimmed raw video by a given text query description (sentence). Existing studies either start from collecting exhaustive frame-wise annotations on the temporal boundary of target moments (fully-supervised), or learn with only the video-level video-text pairing labels (weakly-supervised). The former is poor in generalisation to unknown concepts and/or novel scenes due to restricted dataset scale and diversity under expensive annotation costs; the latter is subject to visual-textual mis-correlations from incomplete labels. In this work, we introduce a new approach called hybrid-learning video moment retrieval to solve the problem by knowledge transfer through adapting the video-text matching relationships learned from a fully-supervised source domain to a weakly-labelled target domain when they do not share a common label space. Our aim is to explore shared universal knowledge between the two domains in order to improve model learning in the weakly-labelled target domain. Specifically, we introduce a multiplE branch Video-text Alignment model (EVA) that performs cross-modal (visual-textual) matching information sharing and multi-modal feature alignment to optimise domain-invariant visual and textual features as well as per-task discriminative joint video-text representations. Experiments show EVA's effectiveness in exploring temporal segment annotations in a source domain to help learn video moment retrieval without temporal labels in a target domain.
<div id='section'>Paperid: <span id='pid'>484, <a href='https://arxiv.org/pdf/2405.04086.pdf' target='_blank'>https://arxiv.org/pdf/2405.04086.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongqi Tong, Sizhe Wang, Dawei Li, Yifan Wang, Simeng Han, Zi Lin, Chengsong Huang, Jiaxin Huang, Jingbo Shang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04086">Optimizing Language Model's Reasoning Abilities with Weak Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While Large Language Models (LLMs) have demonstrated proficiency in handling complex queries, much of the past work has depended on extensively annotated datasets by human experts. However, this reliance on fully-supervised annotations poses scalability challenges, particularly as models and data requirements grow. To mitigate this, we explore the potential of enhancing LLMs' reasoning abilities with minimal human supervision. In this work, we introduce self-reinforcement, which begins with Supervised Fine-Tuning (SFT) of the model using a small collection of annotated questions. Then it iteratively improves LLMs by learning from the differences in responses from the SFT and unfinetuned models on unlabeled questions. Our approach provides an efficient approach without relying heavily on extensive human-annotated explanations. However, current reasoning benchmarks typically only include golden-reference answers or rationales. Therefore, we present \textsc{PuzzleBen}, a weakly supervised benchmark that comprises 25,147 complex questions, answers, and human-generated rationales across various domains, such as brainteasers, puzzles, riddles, parajumbles, and critical reasoning tasks. A unique aspect of our dataset is the inclusion of 10,000 unannotated questions, enabling us to explore utilizing fewer supersized data to boost LLMs' inference capabilities. Our experiments underscore the significance of \textsc{PuzzleBen}, as well as the effectiveness of our methodology as a promising direction in future endeavors. Our dataset and code will be published soon on \texttt{Anonymity Link}.
<div id='section'>Paperid: <span id='pid'>485, <a href='https://arxiv.org/pdf/2404.12784.pdf' target='_blank'>https://arxiv.org/pdf/2404.12784.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Myrna C. Silva, Mahtab Dahaghin, Matteo Toso, Alessio Del Bue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.12784">Contrastive Gaussian Clustering: Weakly Supervised 3D Scene Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Contrastive Gaussian Clustering, a novel approach capable of provide segmentation masks from any viewpoint and of enabling 3D segmentation of the scene. Recent works in novel-view synthesis have shown how to model the appearance of a scene via a cloud of 3D Gaussians, and how to generate accurate images from a given viewpoint by projecting on it the Gaussians before $Î±$ blending their color. Following this example, we train a model to include also a segmentation feature vector for each Gaussian. These can then be used for 3D scene segmentation, by clustering Gaussians according to their feature vectors; and to generate 2D segmentation masks, by projecting the Gaussians on a plane and $Î±$ blending over their segmentation features. Using a combination of contrastive learning and spatial regularization, our method can be trained on inconsistent 2D segmentation masks, and still learn to generate segmentation masks consistent across all views. Moreover, the resulting model is extremely accurate, improving the IoU accuracy of the predicted masks by $+8\%$ over the state of the art. Code and trained models will be released soon.
<div id='section'>Paperid: <span id='pid'>486, <a href='https://arxiv.org/pdf/2403.04558.pdf' target='_blank'>https://arxiv.org/pdf/2403.04558.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tim Lenz, Omar S. M. El Nahhas, Marta Ligero, Jakob Nikolas Kather
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.04558">Reducing self-supervised learning complexity improves weakly-supervised classification performance in computational pathology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep Learning models have been successfully utilized to extract clinically actionable insights from routinely available histology data. Generally, these models require annotations performed by clinicians, which are scarce and costly to generate. The emergence of self-supervised learning (SSL) methods remove this barrier, allowing for large-scale analyses on non-annotated data. However, recent SSL approaches apply increasingly expansive model architectures and larger datasets, causing the rapid escalation of data volumes, hardware prerequisites, and overall expenses, limiting access to these resources to few institutions. Therefore, we investigated the complexity of contrastive SSL in computational pathology in relation to classification performance with the utilization of consumer-grade hardware. Specifically, we analyzed the effects of adaptations in data volume, architecture, and algorithms on downstream classification tasks, emphasizing their impact on computational resources. We trained breast cancer foundation models on a large public patient cohort and validated them on various downstream classification tasks in a weakly supervised manner on two external public patient cohorts. Our experiments demonstrate that we can improve downstream classification performance whilst reducing SSL training duration by 90%. In summary, we propose a set of adaptations which enable the utilization of SSL in computational pathology in non-resource abundant environments.
<div id='section'>Paperid: <span id='pid'>487, <a href='https://arxiv.org/pdf/2402.17601.pdf' target='_blank'>https://arxiv.org/pdf/2402.17601.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matthias Boeker, Vajira Thambawita, Michael Riegler, PÃ¥l Halvorsen, Hugo L. Hammer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17601">Advancing sleep detection by modelling weak label sets: A novel weakly supervised learning approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding sleep and activity patterns plays a crucial role in physical and mental health. This study introduces a novel approach for sleep detection using weakly supervised learning for scenarios where reliable ground truth labels are unavailable. The proposed method relies on a set of weak labels, derived from the predictions generated by conventional sleep detection algorithms. Introducing a novel approach, we suggest a novel generalised non-linear statistical model in which the number of weak sleep labels is modelled as outcome of a binomial distribution. The probability of sleep in the binomial distribution is linked to the outcomes of neural networks trained to detect sleep based on actigraphy. We show that maximizing the likelihood function of the model, is equivalent to minimizing the soft cross-entropy loss. Additionally, we explored the use of the Brier score as a loss function for weak labels. The efficacy of the suggested modelling framework was demonstrated using the Multi-Ethnic Study of Atherosclerosis dataset. A \gls{lstm} trained on the soft cross-entropy outperformed conventional sleep detection algorithms, other neural network architectures and loss functions in accuracy and model calibration. This research not only advances sleep detection techniques in scenarios where ground truth data is scarce but also contributes to the broader field of weakly supervised learning by introducing innovative approach in modelling sets of weak labels.
<div id='section'>Paperid: <span id='pid'>488, <a href='https://arxiv.org/pdf/2402.10851.pdf' target='_blank'>https://arxiv.org/pdf/2402.10851.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mobina Mansoori, Sajjad Shahabodini, Jamshid Abouei, Arash Mohammadi, Konstantinos N. Plataniotis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.10851">HistoSegCap: Capsules for Weakly-Supervised Semantic Segmentation of Histological Tissue Type in Whole Slide Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Digital pathology involves converting physical tissue slides into high-resolution Whole Slide Images (WSIs), which pathologists analyze for disease-affected tissues. However, large histology slides with numerous microscopic fields pose challenges for visual search. To aid pathologists, Computer Aided Diagnosis (CAD) systems offer visual assistance in efficiently examining WSIs and identifying diagnostically relevant regions. This paper presents a novel histopathological image analysis method employing Weakly Supervised Semantic Segmentation (WSSS) based on Capsule Networks, the first such application. The proposed model is evaluated using the Atlas of Digital Pathology (ADP) dataset and its performance is compared with other histopathological semantic segmentation methodologies. The findings underscore the potential of Capsule Networks in enhancing the precision and efficiency of histopathological image analysis. Experimental results show that the proposed model outperforms traditional methods in terms of accuracy and the mean Intersection-over-Union (mIoU) metric.
<div id='section'>Paperid: <span id='pid'>489, <a href='https://arxiv.org/pdf/2402.08892.pdf' target='_blank'>https://arxiv.org/pdf/2402.08892.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiqi Peng, Bolin Lai, Guangyu Yao, Xiaoyun Zhang, Ya Zhang, Yan-Feng Wang, Hui Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.08892">Weakly Supervised Segmentation of Vertebral Bodies with Iterative Slice-propagation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vertebral body (VB) segmentation is an important preliminary step towards medical visual diagnosis for spinal diseases. However, most previous works require pixel/voxel-wise strong supervisions, which is expensive, tedious and time-consuming for experts to annotate. In this paper, we propose a Weakly supervised Iterative Spinal Segmentation (WISS) method leveraging only four corner landmark weak labels on a single sagittal slice to achieve automatic volumetric segmentation from CT images for VBs. WISS first segments VBs on an annotated sagittal slice in an iterative self-training manner. This self-training method alternates between training and refining labels in the training set. Then WISS proceeds to segment the whole VBs slice by slice with a slice-propagation method to obtain volumetric segmentations. We evaluate the performance of WISS on a private spinal metastases CT dataset and the public lumbar CT dataset. On the first dataset, WISS achieves distinct improvements with regard to two different backbones. For the second dataset, WISS achieves dice coefficients of $91.7\%$ and $83.7\%$ for mid-sagittal slices and 3D CT volumes, respectively, saving a lot of labeling costs and only sacrificing a little segmentation performance.
<div id='section'>Paperid: <span id='pid'>490, <a href='https://arxiv.org/pdf/2402.03345.pdf' target='_blank'>https://arxiv.org/pdf/2402.03345.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Antoine Collas, RÃ©mi Flamary, Alexandre Gramfort
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.03345">Weakly supervised covariance matrices alignment through Stiefel matrices estimation for MEG applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel domain adaptation technique for time series data, called Mixing model Stiefel Adaptation (MSA), specifically addressing the challenge of limited labeled signals in the target dataset. Leveraging a domain-dependent mixing model and the optimal transport domain adaptation assumption, we exploit abundant unlabeled data in the target domain to ensure effective prediction by establishing pairwise correspondence with equivalent signal variances between domains. Theoretical foundations are laid for identifying crucial Stiefel matrices, essential for recovering underlying signal variances from a Riemannian representation of observed signal covariances. We propose an integrated cost function that simultaneously learns these matrices, pairwise domain relationships, and a predictor, classifier, or regressor, depending on the task. Applied to neuroscience problems, MSA outperforms recent methods in brain-age regression with task variations using magnetoencephalography (MEG) signals from the Cam-CAN dataset.
<div id='section'>Paperid: <span id='pid'>491, <a href='https://arxiv.org/pdf/2402.00899.pdf' target='_blank'>https://arxiv.org/pdf/2402.00899.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ivan Y. Tyukin, Tatiana Tyukina, Daniel van Helden, Zedong Zheng, Evgeny M. Mirkes, Oliver J. Sutton, Qinghua Zhou, Alexander N. Gorban, Penelope Allison
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.00899">Weakly Supervised Learners for Correction of AI Errors with Provable Performance Guarantees</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a new methodology for handling AI errors by introducing weakly supervised AI error correctors with a priori performance guarantees. These AI correctors are auxiliary maps whose role is to moderate the decisions of some previously constructed underlying classifier by either approving or rejecting its decisions. The rejection of a decision can be used as a signal to suggest abstaining from making a decision. A key technical focus of the work is in providing performance guarantees for these new AI correctors through bounds on the probabilities of incorrect decisions. These bounds are distribution agnostic and do not rely on assumptions on the data dimension. Our empirical example illustrates how the framework can be applied to improve the performance of an image classifier in a challenging real-world task where training data are scarce.
<div id='section'>Paperid: <span id='pid'>492, <a href='https://arxiv.org/pdf/2512.17788.pdf' target='_blank'>https://arxiv.org/pdf/2512.17788.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Tang, Yin-Fang Yang, Weijia Zhang, Min-Ling Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.17788">Calibratable Disambiguation Loss for Multi-Instance Partial-Label Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-instance partial-label learning (MIPL) is a weakly supervised framework that extends the principles of multi-instance learning (MIL) and partial-label learning (PLL) to address the challenges of inexact supervision in both instance and label spaces. However, existing MIPL approaches often suffer from poor calibration, undermining classifier reliability. In this work, we propose a plug-and-play calibratable disambiguation loss (CDL) that simultaneously improves classification accuracy and calibration performance. The loss has two instantiations: the first one calibrates predictions based on probabilities from the candidate label set, while the second one integrates probabilities from both candidate and non-candidate label sets. The proposed CDL can be seamlessly incorporated into existing MIPL and PLL frameworks. We provide a theoretical analysis that establishes the lower bound and regularization properties of CDL, demonstrating its superiority over conventional disambiguation losses. Experimental results on benchmark and real-world datasets confirm that our CDL significantly enhances both classification and calibration performance.
<div id='section'>Paperid: <span id='pid'>493, <a href='https://arxiv.org/pdf/2512.01701.pdf' target='_blank'>https://arxiv.org/pdf/2512.01701.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiuli Bi, Die Xiao, Junchao Fan, Bin Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.01701">SSR: Semantic and Spatial Rectification for CLIP-based Weakly Supervised Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, Contrastive Language-Image Pretraining (CLIP) has been widely applied to Weakly Supervised Semantic Segmentation (WSSS) tasks due to its powerful cross-modal semantic understanding capabilities. This paper proposes a novel Semantic and Spatial Rectification (SSR) method to address the limitations of existing CLIP-based weakly supervised semantic segmentation approaches: over-activation in non-target foreground regions and background areas. Specifically, at the semantic level, the Cross-Modal Prototype Alignment (CMPA) establishes a contrastive learning mechanism to enforce feature space alignment across modalities, reducing inter-class overlap while enhancing semantic correlations, to rectify over-activation in non-target foreground regions effectively; at the spatial level, the Superpixel-Guided Correction (SGC) leverages superpixel-based spatial priors to precisely filter out interference from non-target regions during affinity propagation, significantly rectifying background over-activation. Extensive experiments on the PASCAL VOC and MS COCO datasets demonstrate that our method outperforms all single-stage approaches, as well as more complex multi-stage approaches, achieving mIoU scores of 79.5% and 50.6%, respectively.
<div id='section'>Paperid: <span id='pid'>494, <a href='https://arxiv.org/pdf/2511.01131.pdf' target='_blank'>https://arxiv.org/pdf/2511.01131.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Nahiduzzaman, Steven Korevaar, Alireza Bab-Hadiashar, Ruwan Tennakoon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.01131">Weakly Supervised Concept Learning with Class-Level Priors for Interpretable Medical Diagnosis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-interpretable predictions are essential for deploying AI in medical imaging, yet most interpretable-by-design (IBD) frameworks require concept annotations for training data, which are costly and impractical to obtain in clinical contexts. Recent attempts to bypass annotation, such as zero-shot vision-language models or concept-generation frameworks, struggle to capture domain-specific medical features, leading to poor reliability. In this paper, we propose a novel Prior-guided Concept Predictor (PCP), a weakly supervised framework that enables concept answer prediction without explicit supervision or reliance on language models. PCP leverages class-level concept priors as weak supervision and incorporates a refinement mechanism with KL divergence and entropy regularization to align predictions with clinical reasoning. Experiments on PH2 (dermoscopy) and WBCatt (hematology) show that PCP improves concept-level F1-score by over 33% compared to zero-shot baselines, while delivering competitive classification performance on four medical datasets (PH2, WBCatt, HAM10000, and CXR4) relative to fully supervised concept bottleneck models (CBMs) and V-IP.
<div id='section'>Paperid: <span id='pid'>495, <a href='https://arxiv.org/pdf/2510.17875.pdf' target='_blank'>https://arxiv.org/pdf/2510.17875.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoxu Xu, Xuexun Liu, Jinlong Li, Yitian Yuan, Qiudan Zhang, Lin Ma, Nicu Sebe, Xu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17875">3D Weakly Supervised Semantic Segmentation via Class-Aware and Geometry-Guided Pseudo-Label Refinement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D weakly supervised semantic segmentation (3D WSSS) aims to achieve semantic segmentation by leveraging sparse or low-cost annotated data, significantly reducing reliance on dense point-wise annotations. Previous works mainly employ class activation maps or pre-trained vision-language models to address this challenge. However, the low quality of pseudo-labels and the insufficient exploitation of 3D geometric priors jointly create significant technical bottlenecks in developing high-performance 3D WSSS models. In this paper, we propose a simple yet effective 3D weakly supervised semantic segmentation method that integrates 3D geometric priors into a class-aware guidance mechanism to generate high-fidelity pseudo labels. Concretely, our designed methodology first employs Class-Aware Label Refinement module to generate more balanced and accurate pseudo labels for semantic categrories. This initial refinement stage focuses on enhancing label quality through category-specific optimization. Subsequently, the Geometry-Aware Label Refinement component is developed, which strategically integrates implicit 3D geometric constraints to effectively filter out low-confidence pseudo labels that fail to comply with geometric plausibility. Moreover, to address the challenge of extensive unlabeled regions, we propose a Label Update strategy that integrates Self-Training to propagate labels into these areas. This iterative process continuously enhances pseudo-label quality while expanding label coverage, ultimately fostering the development of high-performance 3D WSSS models. Comprehensive experimental validation reveals that our proposed methodology achieves state-of-the-art performance on both ScanNet and S3DIS benchmarks while demonstrating remarkable generalization capability in unsupervised settings, maintaining competitive accuracy through its robust design.
<div id='section'>Paperid: <span id='pid'>496, <a href='https://arxiv.org/pdf/2510.17566.pdf' target='_blank'>https://arxiv.org/pdf/2510.17566.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nachuan Ma, Zhengfei Song, Qiang Hu, Xiaoyu Tang, Chengxi Zhang, Rui Fan, Lihua Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17566">WP-CrackNet: A Collaborative Adversarial Learning Framework for End-to-End Weakly-Supervised Road Crack Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Road crack detection is essential for intelligent infrastructure maintenance in smart cities. To reduce reliance on costly pixel-level annotations, we propose WP-CrackNet, an end-to-end weakly-supervised method that trains with only image-level labels for pixel-wise crack detection. WP-CrackNet integrates three components: a classifier generating class activation maps (CAMs), a reconstructor measuring feature inferability, and a detector producing pixel-wise road crack detection results. During training, the classifier and reconstructor alternate in adversarial learning to encourage crack CAMs to cover complete crack regions, while the detector learns from pseudo labels derived from post-processed crack CAMs. This mutual feedback among the three components improves learning stability and detection accuracy. To further boost detection performance, we design a path-aware attention module (PAAM) that fuses high-level semantics from the classifier with low-level structural cues from the reconstructor by modeling spatial and channel-wise dependencies. Additionally, a center-enhanced CAM consistency module (CECCM) is proposed to refine crack CAMs using center Gaussian weighting and consistency constraints, enabling better pseudo-label generation. We create three image-level datasets and extensive experiments show that WP-CrackNet achieves comparable results to supervised methods and outperforms existing weakly-supervised methods, significantly advancing scalable road inspection. The source code package and datasets are available at https://mias.group/WP-CrackNet/.
<div id='section'>Paperid: <span id='pid'>497, <a href='https://arxiv.org/pdf/2509.14573.pdf' target='_blank'>https://arxiv.org/pdf/2509.14573.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Takamasa Yamaguchi, Brian Kenji Iwana, Ryoma Bise, Shota Harada, Takumi Okuo, Kiyohito Tanaka, Kaito Shiku
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14573">Domain Adaptation for Ulcerative Colitis Severity Estimation Using Patient-Level Diagnoses</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of methods to estimate the severity of Ulcerative Colitis (UC) is of significant importance. However, these methods often suffer from domain shifts caused by differences in imaging devices and clinical settings across hospitals. Although several domain adaptation methods have been proposed to address domain shift, they still struggle with the lack of supervision in the target domain or the high cost of annotation. To overcome these challenges, we propose a novel Weakly Supervised Domain Adaptation method that leverages patient-level diagnostic results, which are routinely recorded in UC diagnosis, as weak supervision in the target domain. The proposed method aligns class-wise distributions across domains using Shared Aggregation Tokens and a Max-Severity Triplet Loss, which leverages the characteristic that patient-level diagnoses are determined by the most severe region within each patient. Experimental results demonstrate that our method outperforms comparative DA approaches, improving UC severity estimation in a domain-shifted setting.
<div id='section'>Paperid: <span id='pid'>498, <a href='https://arxiv.org/pdf/2508.06318.pdf' target='_blank'>https://arxiv.org/pdf/2508.06318.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giacomo D'Amicantonio, Snehashis Majhi, Quan Kong, Lorenzo Garattoni, Gianpiero Francesca, FranÃ§ois Bremond, Egor Bondarev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06318">Mixture of Experts Guided by Gaussian Splatters Matters: A new Approach to Weakly-Supervised Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Anomaly Detection (VAD) is a challenging task due to the variability of anomalous events and the limited availability of labeled data. Under the Weakly-Supervised VAD (WSVAD) paradigm, only video-level labels are provided during training, while predictions are made at the frame level. Although state-of-the-art models perform well on simple anomalies (e.g., explosions), they struggle with complex real-world events (e.g., shoplifting). This difficulty stems from two key issues: (1) the inability of current models to address the diversity of anomaly types, as they process all categories with a shared model, overlooking category-specific features; and (2) the weak supervision signal, which lacks precise temporal information, limiting the ability to capture nuanced anomalous patterns blended with normal events. To address these challenges, we propose Gaussian Splatting-guided Mixture of Experts (GS-MoE), a novel framework that employs a set of expert models, each specialized in capturing specific anomaly types. These experts are guided by a temporal Gaussian splatting loss, enabling the model to leverage temporal consistency and enhance weak supervision. The Gaussian splatting approach encourages a more precise and comprehensive representation of anomalies by focusing on temporal segments most likely to contain abnormal events. The predictions from these specialized experts are integrated through a mixture-of-experts mechanism to model complex relationships across diverse anomaly patterns. Our approach achieves state-of-the-art performance, with a 91.58% AUC on the UCF-Crime dataset, and demonstrates superior results on XD-Violence and MSAD datasets. By leveraging category-specific expertise and temporal guidance, GS-MoE sets a new benchmark for VAD under weak supervision.
<div id='section'>Paperid: <span id='pid'>499, <a href='https://arxiv.org/pdf/2506.05912.pdf' target='_blank'>https://arxiv.org/pdf/2506.05912.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adrien Petralia, Paul Boniol, Philippe Charpentier, Themis Palpanas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05912">DeviceScope: An Interactive App to Detect and Localize Appliance Patterns in Electricity Consumption Time Series</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, electricity suppliers have installed millions of smart meters worldwide to improve the management of the smart grid system. These meters collect a large amount of electrical consumption data to produce valuable information to help consumers reduce their electricity footprint. However, having non-expert users (e.g., consumers or sales advisors) understand these data and derive usage patterns for different appliances has become a significant challenge for electricity suppliers because these data record the aggregated behavior of all appliances. At the same time, ground-truth labels (which could train appliance detection and localization models) are expensive to collect and extremely scarce in practice. This paper introduces DeviceScope, an interactive tool designed to facilitate understanding smart meter data by detecting and localizing individual appliance patterns within a given time period. Our system is based on CamAL (Class Activation Map-based Appliance Localization), a novel weakly supervised approach for appliance localization that only requires the knowledge of the existence of an appliance in a household to be trained. This paper appeared in ICDE 2025.
<div id='section'>Paperid: <span id='pid'>500, <a href='https://arxiv.org/pdf/2505.13123.pdf' target='_blank'>https://arxiv.org/pdf/2505.13123.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Snehashis Majhi, Giacomo D'Amicantonio, Antitza Dantcheva, Quan Kong, Lorenzo Garattoni, Gianpiero Francesca, Egor Bondarev, Francois Bremond
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13123">Just Dance with $Ï$! A Poly-modal Inductor for Weakly-supervised Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised methods for video anomaly detection (VAD) are conventionally based merely on RGB spatio-temporal features, which continues to limit their reliability in real-world scenarios. This is due to the fact that RGB-features are not sufficiently distinctive in setting apart categories such as shoplifting from visually similar events. Therefore, towards robust complex real-world VAD, it is essential to augment RGB spatio-temporal features by additional modalities. Motivated by this, we introduce the Poly-modal Induced framework for VAD: "PI-VAD", a novel approach that augments RGB representations by five additional modalities. Specifically, the modalities include sensitivity to fine-grained motion (Pose), three dimensional scene and entity representation (Depth), surrounding objects (Panoptic masks), global motion (optical flow), as well as language cues (VLM). Each modality represents an axis of a polygon, streamlined to add salient cues to RGB. PI-VAD includes two plug-in modules, namely Pseudo-modality Generation module and Cross Modal Induction module, which generate modality-specific prototypical representation and, thereby, induce multi-modal information into RGB cues. These modules operate by performing anomaly-aware auxiliary tasks and necessitate five modality backbones -- only during training. Notably, PI-VAD achieves state-of-the-art accuracy on three prominent VAD datasets encompassing real-world scenarios, without requiring the computational overhead of five modality backbones at inference.
<div id='section'>Paperid: <span id='pid'>501, <a href='https://arxiv.org/pdf/2505.08687.pdf' target='_blank'>https://arxiv.org/pdf/2505.08687.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hangwei Zhang, Zhimu Huang, Yan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08687">AC-PKAN: Attention-Enhanced and Chebyshev Polynomial-Based Physics-Informed Kolmogorov-Arnold Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Kolmogorov-Arnold Networks (KANs) have recently shown promise for solving partial differential equations (PDEs). Yet their original formulation is computationally and memory intensive, motivating the introduction of Chebyshev Type-I-based KANs (Chebyshev1KANs). Although Chebyshev1KANs have outperformed the vanilla KANs architecture, our rigorous theoretical analysis reveals that they still suffer from rank collapse, ultimately limiting their expressive capacity. To overcome these limitations, we enhance Chebyshev1KANs by integrating wavelet-activated MLPs with learnable parameters and an internal attention mechanism. We prove that this design preserves a full-rank Jacobian and is capable of approximating solutions to PDEs of arbitrary order. Furthermore, to alleviate the loss instability and imbalance introduced by the Chebyshev polynomial basis, we externally incorporate a Residual Gradient Attention (RGA) mechanism that dynamically re-weights individual loss terms according to their gradient norms and residual magnitudes. By jointly leveraging internal and external attention, we present AC-PKAN, a novel architecture that constitutes an enhancement to weakly supervised Physics-Informed Neural Networks (PINNs) and extends the expressive power of KANs. Experimental results from nine benchmark tasks across three domains show that AC-PKAN consistently outperforms or matches state-of-the-art models such as PINNsFormer, establishing it as a highly effective tool for solving complex real-world engineering problems in zero-data or data-sparse regimes. The code will be made publicly available upon acceptance.
<div id='section'>Paperid: <span id='pid'>502, <a href='https://arxiv.org/pdf/2505.01064.pdf' target='_blank'>https://arxiv.org/pdf/2505.01064.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hari Chandana Kuchibhotla, Sai Srinivas Kancheti, Abbavaram Gowtham Reddy, Vineeth N Balasubramanian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01064">Efficient Vocabulary-Free Fine-Grained Visual Recognition in the Age of Multimodal LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fine-grained Visual Recognition (FGVR) involves distinguishing between visually similar categories, which is inherently challenging due to subtle inter-class differences and the need for large, expert-annotated datasets. In domains like medical imaging, such curated datasets are unavailable due to issues like privacy concerns and high annotation costs. In such scenarios lacking labeled data, an FGVR model cannot rely on a predefined set of training labels, and hence has an unconstrained output space for predictions. We refer to this task as Vocabulary-Free FGVR (VF-FGVR), where a model must predict labels from an unconstrained output space without prior label information. While recent Multimodal Large Language Models (MLLMs) show potential for VF-FGVR, querying these models for each test input is impractical because of high costs and prohibitive inference times. To address these limitations, we introduce \textbf{Nea}rest-Neighbor Label \textbf{R}efinement (NeaR), a novel approach that fine-tunes a downstream CLIP model using labels generated by an MLLM. Our approach constructs a weakly supervised dataset from a small, unlabeled training set, leveraging MLLMs for label generation. NeaR is designed to handle the noise, stochasticity, and open-endedness inherent in labels generated by MLLMs, and establishes a new benchmark for efficient VF-FGVR.
<div id='section'>Paperid: <span id='pid'>503, <a href='https://arxiv.org/pdf/2411.10497.pdf' target='_blank'>https://arxiv.org/pdf/2411.10497.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xavier Bou, Gabriele Facciolo, Rafael Grompone von Gioi, Jean-Michel Morel, Thibaud Ehret
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10497">Structure Tensor Representation for Robust Oriented Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Oriented object detection predicts orientation in addition to object location and bounding box. Precisely predicting orientation remains challenging due to angular periodicity, which introduces boundary discontinuity issues and symmetry ambiguities. Inspired by classical works on edge and corner detection, this paper proposes to represent orientation in oriented bounding boxes as a structure tensor. This representation combines the strengths of Gaussian-based methods and angle-coder solutions, providing a simple yet efficient approach that is robust to angular periodicity issues without additional hyperparameters. Extensive evaluations across five datasets demonstrate that the proposed structure tensor representation outperforms previous methods in both fully-supervised and weakly supervised tasks, achieving high precision in angular prediction with minimal computational overhead. Thus, this work establishes structure tensors as a robust and modular alternative for encoding orientation in oriented object detection. We make our code publicly available, allowing for seamless integration into existing object detectors.
<div id='section'>Paperid: <span id='pid'>504, <a href='https://arxiv.org/pdf/2411.02466.pdf' target='_blank'>https://arxiv.org/pdf/2411.02466.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Robin Trombetta, Olivier RouviÃ¨re, Carole Lartizien
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.02466">Weakly supervised deep learning model with size constraint for prostate cancer detection in multiparametric MRI and generalization to unseen domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fully supervised deep models have shown promising performance for many medical segmentation tasks. Still, the deployment of these tools in clinics is limited by the very timeconsuming collection of manually expert-annotated data. Moreover, most of the state-ofthe-art models have been trained and validated on moderately homogeneous datasets. It is known that deep learning methods are often greatly degraded by domain or label shifts and are yet to be built in such a way as to be robust to unseen data or label distributions. In the clinical setting, this problematic is particularly relevant as the deployment institutions may have different scanners or acquisition protocols than those from which the data has been collected to train the model. In this work, we propose to address these two challenges on the detection of clinically significant prostate cancer (csPCa) from bi-parametric MRI. We evaluate the method proposed by (Kervadec et al., 2018), which introduces a size constaint loss to produce fine semantic cancer lesions segmentations from weak circle scribbles annotations. Performance of the model is based on two public (PI-CAI and Prostate158) and one private databases. First, we show that the model achieves on-par performance with strong fully supervised baseline models, both on in-distribution validation data and unseen test images. Second, we observe a performance decrease for both fully supervised and weakly supervised models when tested on unseen data domains. This confirms the crucial need for efficient domain adaptation methods if deep learning models are aimed to be deployed in a clinical environment. Finally, we show that ensemble predictions from multiple trainings increase generalization performance.
<div id='section'>Paperid: <span id='pid'>505, <a href='https://arxiv.org/pdf/2410.19836.pdf' target='_blank'>https://arxiv.org/pdf/2410.19836.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ronan Docherty, Antonis Vamvakeros, Samuel J. Cooper
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.19836">Upsampling DINOv2 features for unsupervised vision tasks and weakly supervised materials segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The features of self-supervised vision transformers (ViTs) contain strong semantic and positional information relevant to downstream tasks like object localization and segmentation. Recent works combine these features with traditional methods like clustering, graph partitioning or region correlations to achieve impressive baselines without finetuning or training additional networks. We leverage upsampled features from ViT networks (e.g DINOv2) in two workflows: in a clustering based approach for object localization and segmentation, and paired with standard classifiers in weakly supervised materials segmentation. Both show strong performance on benchmarks, especially in weakly supervised segmentation where the ViT features capture complex relationships inaccessible to classical approaches. We expect the flexibility and generalizability of these features will both speed up and strengthen materials characterization, from segmentation to property-prediction.
<div id='section'>Paperid: <span id='pid'>506, <a href='https://arxiv.org/pdf/2410.15657.pdf' target='_blank'>https://arxiv.org/pdf/2410.15657.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianjun Gao, Chen Cai, Ruoyu Wang, Wenyang Liu, Kim-Hui Yap, Kratika Garg, Boon-Siew Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15657">CL-HOI: Cross-Level Human-Object Interaction Distillation from Vision Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-object interaction (HOI) detection has seen advancements with Vision Language Models (VLMs), but these methods often depend on extensive manual annotations. Vision Large Language Models (VLLMs) can inherently recognize and reason about interactions at the image level but are computationally heavy and not designed for instance-level HOI detection. To overcome these limitations, we propose a Cross-Level HOI distillation (CL-HOI) framework, which distills instance-level HOIs from VLLMs image-level understanding without the need for manual annotations. Our approach involves two stages: context distillation, where a Visual Linguistic Translator (VLT) converts visual information into linguistic form, and interaction distillation, where an Interaction Cognition Network (ICN) reasons about spatial, visual, and context relations. We design contrastive distillation losses to transfer image-level context and interaction knowledge from the teacher to the student model, enabling instance-level HOI detection. Evaluations on HICO-DET and V-COCO datasets demonstrate that our CL-HOI surpasses existing weakly supervised methods and VLLM supervised methods, showing its efficacy in detecting HOIs without manual labels.
<div id='section'>Paperid: <span id='pid'>507, <a href='https://arxiv.org/pdf/2408.10641.pdf' target='_blank'>https://arxiv.org/pdf/2408.10641.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxiao Wang, Yu Lei, Li Cui, Weiying Xue, Qi Liu, Zhenao Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.10641">A Review of Human-Object Interaction Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-object interaction (HOI) detection plays a key role in high-level visual understanding, facilitating a deep comprehension of human activities. Specifically, HOI detection aims to locate the humans and objects involved in interactions within images or videos and classify the specific interactions between them. The success of this task is influenced by several key factors, including the accurate localization of human and object instances, as well as the correct classification of object categories and interaction relationships. This paper systematically summarizes and discusses the recent work in image-based HOI detection. First, the mainstream datasets involved in HOI relationship detection are introduced. Furthermore, starting with two-stage methods and end-to-end one-stage detection approaches, this paper comprehensively discusses the current developments in image-based HOI detection, analyzing the strengths and weaknesses of these two methods. Additionally, the advancements of zero-shot learning, weakly supervised learning, and the application of large-scale language models in HOI detection are discussed. Finally, the current challenges in HOI detection are outlined, and potential research directions and future trends are explored.
<div id='section'>Paperid: <span id='pid'>508, <a href='https://arxiv.org/pdf/2407.12342.pdf' target='_blank'>https://arxiv.org/pdf/2407.12342.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jintang Xue, Yun-Cheng Wang, Chengwei Wei, C. -C. Jay Kuo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.12342">Word Embedding Dimension Reduction via Weakly-Supervised Feature Selection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As a fundamental task in natural language processing, word embedding converts each word into a representation in a vector space. A challenge with word embedding is that as the vocabulary grows, the vector space's dimension increases, which can lead to a vast model size. Storing and processing word vectors are resource-demanding, especially for mobile edge-devices applications. This paper explores word embedding dimension reduction. To balance computational costs and performance, we propose an efficient and effective weakly-supervised feature selection method named WordFS. It has two variants, each utilizing novel criteria for feature selection. Experiments on various tasks (e.g., word and sentence similarity and binary and multi-class classification) indicate that the proposed WordFS model outperforms other dimension reduction methods at lower computational costs. We have released the code for reproducibility along with the paper.
<div id='section'>Paperid: <span id='pid'>509, <a href='https://arxiv.org/pdf/2407.07566.pdf' target='_blank'>https://arxiv.org/pdf/2407.07566.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arnon Turetzky, Or Tal, Yael Segal-Feldman, Yehoshua Dissen, Ella Zeldes, Amit Roth, Eyal Cohen, Yosi Shrem, Bronya R. Chernyak, Olga Seleznova, Joseph Keshet, Yossi Adi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.07566">HebDB: a Weakly Supervised Dataset for Hebrew Speech Processing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present HebDB, a weakly supervised dataset for spoken language processing in the Hebrew language. HebDB offers roughly 2500 hours of natural and spontaneous speech recordings in the Hebrew language, consisting of a large variety of speakers and topics. We provide raw recordings together with a pre-processed, weakly supervised, and filtered version. The goal of HebDB is to further enhance research and development of spoken language processing tools for the Hebrew language. Hence, we additionally provide two baseline systems for Automatic Speech Recognition (ASR): (i) a self-supervised model; and (ii) a fully supervised model. We present the performance of these two methods optimized on HebDB and compare them to current multi-lingual ASR alternatives. Results suggest the proposed method reaches better results than the evaluated baselines considering similar model sizes. Dataset, code, and models are publicly available under https://pages.cs.huji.ac.il/adiyoss-lab/HebDB/.
<div id='section'>Paperid: <span id='pid'>510, <a href='https://arxiv.org/pdf/2405.19638.pdf' target='_blank'>https://arxiv.org/pdf/2405.19638.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyang Huang, Chuang Zhu, Kebin Liu, Ruiying Ren, Shengjie Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19638">Learning Robust Correlation with Foundation Model for Weakly-Supervised Few-Shot Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing few-shot segmentation (FSS) only considers learning support-query correlation and segmenting unseen categories under the precise pixel masks. However, the cost of a large number of pixel masks during training is expensive. This paper considers a more challenging scenario, weakly-supervised few-shot segmentation (WS-FSS), which only provides category ($i.e.$ image-level) labels. It requires the model to learn robust support-query information when the generated mask is inaccurate. In this work, we design a Correlation Enhancement Network (CORENet) with foundation model, which utilizes multi-information guidance to learn robust correlation. Specifically, correlation-guided transformer (CGT) utilizes self-supervised ViT tokens to learn robust correlation from both local and global perspectives. From the perspective of semantic categories, the class-guided module (CGM) guides the model to locate valuable correlations through the pre-trained CLIP. Finally, the embedding-guided module (EGM) implicitly guides the model to supplement the inevitable information loss during the correlation learning by the original appearance embedding and finally generates the query mask. Extensive experiments on PASCAL-5$^i$ and COCO-20$^i$ have shown that CORENet exhibits excellent performance compared to existing methods.
<div id='section'>Paperid: <span id='pid'>511, <a href='https://arxiv.org/pdf/2405.06586.pdf' target='_blank'>https://arxiv.org/pdf/2405.06586.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elham Ravanbakhsh, Cheng Niu, Yongqing Liang, J. Ramanujam, Xin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06586">Enhancing Weakly Supervised Semantic Segmentation with Multi-modal Foundation Models: An End-to-End Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semantic segmentation is a core computer vision problem, but the high costs of data annotation have hindered its wide application. Weakly-Supervised Semantic Segmentation (WSSS) offers a cost-efficient workaround to extensive labeling in comparison to fully-supervised methods by using partial or incomplete labels. Existing WSSS methods have difficulties in learning the boundaries of objects leading to poor segmentation results. We propose a novel and effective framework that addresses these issues by leveraging visual foundation models inside the bounding box. Adopting a two-stage WSSS framework, our proposed network consists of a pseudo-label generation module and a segmentation module. The first stage leverages Segment Anything Model (SAM) to generate high-quality pseudo-labels. To alleviate the problem of delineating precise boundaries, we adopt SAM inside the bounding box with the help of another pre-trained foundation model (e.g., Grounding-DINO). Furthermore, we eliminate the necessity of using the supervision of image labels, by employing CLIP in classification. Then in the second stage, the generated high-quality pseudo-labels are used to train an off-the-shelf segmenter that achieves the state-of-the-art performance on PASCAL VOC 2012 and MS COCO 2014.
<div id='section'>Paperid: <span id='pid'>512, <a href='https://arxiv.org/pdf/2404.09504.pdf' target='_blank'>https://arxiv.org/pdf/2404.09504.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiangqiang Wu, Antoni B. Chan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.09504">Learning Tracking Representations from Single Point Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing deep trackers are typically trained with largescale video frames with annotated bounding boxes. However, these bounding boxes are expensive and time-consuming to annotate, in particular for large scale datasets. In this paper, we propose to learn tracking representations from single point annotations (i.e., 4.5x faster to annotate than the traditional bounding box) in a weakly supervised manner. Specifically, we propose a soft contrastive learning (SoCL) framework that incorporates target objectness prior into end-to-end contrastive learning. Our SoCL consists of adaptive positive and negative sample generation, which is memory-efficient and effective for learning tracking representations. We apply the learned representation of SoCL to visual tracking and show that our method can 1) achieve better performance than the fully supervised baseline trained with box annotations under the same annotation time cost; 2) achieve comparable performance of the fully supervised baseline by using the same number of training frames and meanwhile reducing annotation time cost by 78% and total fees by 85%; 3) be robust to annotation noise.
<div id='section'>Paperid: <span id='pid'>513, <a href='https://arxiv.org/pdf/2403.15977.pdf' target='_blank'>https://arxiv.org/pdf/2403.15977.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Timur Ibrayev, Amitangshu Mukherjee, Sai Aparna Aketi, Kaushik Roy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15977">Towards Two-Stream Foveation-based Active Vision Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural network (DNN) based machine perception frameworks process the entire input in a one-shot manner to provide answers to both "what object is being observed" and "where it is located". In contrast, the "two-stream hypothesis" from neuroscience explains the neural processing in the human visual cortex as an active vision system that utilizes two separate regions of the brain to answer the what and the where questions. In this work, we propose a machine learning framework inspired by the "two-stream hypothesis" and explore the potential benefits that it offers. Specifically, the proposed framework models the following mechanisms: 1) ventral (what) stream focusing on the input regions perceived by the fovea part of an eye (foveation), 2) dorsal (where) stream providing visual guidance, and 3) iterative processing of the two streams to calibrate visual focus and process the sequence of focused image patches. The training of the proposed framework is accomplished by label-based DNN training for the ventral stream model and reinforcement learning for the dorsal stream model. We show that the two-stream foveation-based learning is applicable to the challenging task of weakly-supervised object localization (WSOL), where the training data is limited to the object class or its attributes. The framework is capable of both predicting the properties of an object and successfully localizing it by predicting its bounding box. We also show that, due to the independent nature of the two streams, the dorsal model can be applied on its own to unseen images to localize objects from different datasets.
<div id='section'>Paperid: <span id='pid'>514, <a href='https://arxiv.org/pdf/2403.01840.pdf' target='_blank'>https://arxiv.org/pdf/2403.01840.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Liu, Yuxiao Wang, Xinyu Jiang, Wolin Liang, Zhenao Wei, Yu Lei, Nan Zhuang, Weiying Xue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.01840">FreeA: Human-object Interaction Detection using Free Annotation Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent human-object interaction (HOI) detection methods depend on extensively annotated image datasets, which require a significant amount of manpower. In this paper, we propose a novel self-adaptive, language-driven HOI detection method, termed FreeA. This method leverages the adaptability of the text-image model to generate latent HOI labels without requiring manual annotation. Specifically, FreeA aligns image features of human-object pairs with HOI text templates and employs a knowledge-based masking technique to decrease improbable interactions. Furthermore, FreeA implements a proposed method for matching interaction correlations to increase the probability of actions associated with a particular action, thereby improving the generated HOI labels. Experiments on two benchmark datasets showcase that FreeA achieves state-of-the-art performance among weakly supervised HOI competitors. Our proposal gets +\textbf{13.29} (\textbf{159\%$\uparrow$}) mAP and +\textbf{17.30} (\textbf{98\%$\uparrow$}) mAP than the newest ``Weakly'' supervised model, and +\textbf{7.19} (\textbf{28\%$\uparrow$}) mAP and +\textbf{14.69} (\textbf{34\%$\uparrow$}) mAP than the latest ``Weakly+'' supervised model, respectively, on HICO-DET and V-COCO datasets, more accurate in localizing and classifying the interactive actions. The source code will be made public.
<div id='section'>Paperid: <span id='pid'>515, <a href='https://arxiv.org/pdf/2402.04419.pdf' target='_blank'>https://arxiv.org/pdf/2402.04419.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fakrul Islam Tushar, Vincent M. D'Anniballe, Geoffrey D. Rubin, Joseph Y. Lo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04419">What limits performance of weakly supervised deep learning for chest CT classification?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised learning with noisy data has drawn attention in the medical imaging community due to the sparsity of high-quality disease labels. However, little is known about the limitations of such weakly supervised learning and the effect of these constraints on disease classification performance. In this paper, we test the effects of such weak supervision by examining model tolerance for three conditions. First, we examined model tolerance for noisy data by incrementally increasing error in the labels within the training data. Second, we assessed the impact of dataset size by varying the amount of training data. Third, we compared performance differences between binary and multi-label classification. Results demonstrated that the model could endure up to 10% added label error before experiencing a decline in disease classification performance. Disease classification performance steadily rose as the amount of training data was increased for all disease classes, before experiencing a plateau in performance at 75% of training data. Last, the binary model outperformed the multilabel model in every disease category. However, such interpretations may be misleading, as the binary model was heavily influenced by co-occurring diseases and may not have learned the specific features of the disease in the image. In conclusion, this study may help the medical imaging community understand the benefits and risks of weak supervision with noisy labels. Such studies demonstrate the need to build diverse, large-scale datasets and to develop explainable and responsible AI.
<div id='section'>Paperid: <span id='pid'>516, <a href='https://arxiv.org/pdf/2402.03783.pdf' target='_blank'>https://arxiv.org/pdf/2402.03783.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fudan Zheng, Jindong Cao, Weijiang Yu, Zhiguang Chen, Nong Xiao, Yutong Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.03783">Exploring Low-Resource Medical Image Classification with Weakly Supervised Prompt Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most advances in medical image recognition supporting clinical auxiliary diagnosis meet challenges due to the low-resource situation in the medical field, where annotations are highly expensive and professional. This low-resource problem can be alleviated by leveraging the transferable representations of large-scale pre-trained vision-language models via relevant medical text prompts. However, existing pre-trained vision-language models require domain experts to carefully design the medical prompts, which greatly increases the burden on clinicians. To address this problem, we propose a weakly supervised prompt learning method MedPrompt to automatically generate medical prompts, which includes an unsupervised pre-trained vision-language model and a weakly supervised prompt learning model. The unsupervised pre-trained vision-language model utilizes the natural correlation between medical images and corresponding medical texts for pre-training, without any manual annotations. The weakly supervised prompt learning model only utilizes the classes of images in the dataset to guide the learning of the specific class vector in the prompt, while the learning of other context vectors in the prompt requires no manual annotations for guidance. To the best of our knowledge, this is the first model to automatically generate medical prompts. With these prompts, the pre-trained vision-language model can be freed from the strong expert dependency of manual annotation and manual prompt design. Experimental results show that the model using our automatically generated prompts outperforms its full-shot learning hand-crafted prompts counterparts with only a minimal number of labeled samples for few-shot learning, and reaches superior or comparable accuracy on zero-shot image classification. The proposed prompt generator is lightweight and therefore can be embedded into any network architecture.
<div id='section'>Paperid: <span id='pid'>517, <a href='https://arxiv.org/pdf/2512.10408.pdf' target='_blank'>https://arxiv.org/pdf/2512.10408.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiyue Sun, Tailin Chen, Yinghui Zhang, Yuchen Zhang, Jiangbei Yue, Jianbo Jiao, Zeyu Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.10408">MultiHateLoc: Towards Temporal Localisation of Multimodal Hate Content in Online Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid growth of video content on platforms such as TikTok and YouTube has intensified the spread of multimodal hate speech, where harmful cues emerge subtly and asynchronously across visual, acoustic, and textual streams. Existing research primarily focuses on video-level classification, leaving the practically crucial task of temporal localisation, identifying when hateful segments occur, largely unaddressed. This challenge is even more noticeable under weak supervision, where only video-level labels are available, and static fusion or classification-based architectures struggle to capture cross-modal and temporal dynamics. To address these challenges, we propose MultiHateLoc, the first framework designed for weakly-supervised multimodal hate localisation. MultiHateLoc incorporates (1) modality-aware temporal encoders to model heterogeneous sequential patterns, including a tailored text-based preprocessing module for feature enhancement; (2) dynamic cross-modal fusion to adaptively emphasise the most informative modality at each moment and a cross-modal contrastive alignment strategy to enhance multimodal feature consistency; (3) a modality-aware MIL objective to identify discriminative segments under video-level supervision. Despite relying solely on coarse labels, MultiHateLoc produces fine-grained, interpretable frame-level predictions. Experiments on HateMM and MultiHateClip show that our method achieves state-of-the-art performance in the localisation task.
<div id='section'>Paperid: <span id='pid'>518, <a href='https://arxiv.org/pdf/2512.06845.pdf' target='_blank'>https://arxiv.org/pdf/2512.06845.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Satoshi Hashimoto, Hitoshi Nishimura, Yanan Wang, Mori Kurokawa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.06845">Pseudo Anomalies Are All You Need: Diffusion-Based Generation for Weakly-Supervised Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deploying video anomaly detection in practice is hampered by the scarcity and collection cost of real abnormal footage. We address this by training without any real abnormal videos while evaluating under the standard weakly supervised split, and we introduce PA-VAD, a generation-driven approach that learns a detector from synthesized pseudo-abnormal videos paired with real normal videos, using only a small set of real normal images to drive synthesis. For synthesis, we select class-relevant initial images with CLIP and refine textual prompts with a vision-language model to improve fidelity and scene consistency before invoking a video diffusion model. For training, we mitigate excessive spatiotemporal magnitude in synthesized anomalies by an domain-aligned regularized module that combines domain alignment and memory usage-aware updates. Extensive experiments show that our approach reaches 98.2% on ShanghaiTech and 82.5% on UCF-Crime, surpassing the strongest real-abnormal method on ShanghaiTech by +0.6% and outperforming the UVAD state-of-the-art on UCF-Crime by +1.9%. The results demonstrate that high-accuracy anomaly detection can be obtained without collecting real anomalies, providing a practical path toward scalable deployment.
<div id='section'>Paperid: <span id='pid'>519, <a href='https://arxiv.org/pdf/2510.16450.pdf' target='_blank'>https://arxiv.org/pdf/2510.16450.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shan Xiong, Jiabao Chen, Ye Wang, Jialin Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.16450">Instance-Aware Pseudo-Labeling and Class-Focused Contrastive Learning for Weakly Supervised Domain Adaptive Segmentation of Electron Microscopy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Annotation-efficient segmentation of the numerous mitochondria instances from various electron microscopy (EM) images is highly valuable for biological and neuroscience research. Although unsupervised domain adaptation (UDA) methods can help mitigate domain shifts and reduce the high costs of annotating each domain, they typically have relatively low performance in practical applications. Thus, we investigate weakly supervised domain adaptation (WDA) that utilizes additional sparse point labels on the target domain, which require minimal annotation effort and minimal expert knowledge. To take full use of the incomplete and imprecise point annotations, we introduce a multitask learning framework that jointly conducts segmentation and center detection with a novel cross-teaching mechanism and class-focused cross-domain contrastive learning. While leveraging unlabeled image regions is essential, we introduce segmentation self-training with a novel instance-aware pseudo-label (IPL) selection strategy. Unlike existing methods that typically rely on pixel-wise pseudo-label filtering, the IPL semantically selects reliable and diverse pseudo-labels with the help of the detection task. Comprehensive validations and comparisons on challenging datasets demonstrate that our method outperforms existing UDA and WDA methods, significantly narrowing the performance gap with the supervised upper bound. Furthermore, under the UDA setting, our method also achieves substantial improvements over other UDA techniques.
<div id='section'>Paperid: <span id='pid'>520, <a href='https://arxiv.org/pdf/2509.18973.pdf' target='_blank'>https://arxiv.org/pdf/2509.18973.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiabao Chen, Shan Xiong, Jialin Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18973">Prompt-DAS: Annotation-Efficient Prompt Learning for Domain Adaptive Semantic Segmentation of Electron Microscopy Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain adaptive segmentation (DAS) of numerous organelle instances from large-scale electron microscopy (EM) is a promising way to enable annotation-efficient learning. Inspired by SAM, we propose a promptable multitask framework, namely Prompt-DAS, which is flexible enough to utilize any number of point prompts during the adaptation training stage and testing stage. Thus, with varying prompt configurations, Prompt-DAS can perform unsupervised domain adaptation (UDA) and weakly supervised domain adaptation (WDA), as well as interactive segmentation during testing. Unlike the foundation model SAM, which necessitates a prompt for each individual object instance, Prompt-DAS is only trained on a small dataset and can utilize full points on all instances, sparse points on partial instances, or even no points at all, facilitated by the incorporation of an auxiliary center-point detection task. Moreover, a novel prompt-guided contrastive learning is proposed to enhance discriminative feature learning. Comprehensive experiments conducted on challenging benchmarks demonstrate the effectiveness of the proposed approach over existing UDA, WDA, and SAM-based approaches.
<div id='section'>Paperid: <span id='pid'>521, <a href='https://arxiv.org/pdf/2509.01209.pdf' target='_blank'>https://arxiv.org/pdf/2509.01209.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>MaÃ«lic Neau, Zoe Falomir, CÃ©dric Buche, Akihiro Sugimoto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01209">Measuring Image-Relation Alignment: Reference-Free Evaluation of VLMs and Synthetic Pre-training for Open-Vocabulary Scene Graph Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene Graph Generation (SGG) encodes visual relationships between objects in images as graph structures. Thanks to the advances of Vision-Language Models (VLMs), the task of Open-Vocabulary SGG has been recently proposed where models are evaluated on their functionality to learn a wide and diverse range of relations. Current benchmarks in SGG, however, possess a very limited vocabulary, making the evaluation of open-source models inefficient. In this paper, we propose a new reference-free metric to fairly evaluate the open-vocabulary capabilities of VLMs for relation prediction. Another limitation of Open-Vocabulary SGG is the reliance on weakly supervised data of poor quality for pre-training. We also propose a new solution for quickly generating high-quality synthetic data through region-specific prompt tuning of VLMs. Experimental results show that pre-training with this new data split can benefit the generalization capabilities of Open-Voc SGG models.
<div id='section'>Paperid: <span id='pid'>522, <a href='https://arxiv.org/pdf/2508.19909.pdf' target='_blank'>https://arxiv.org/pdf/2508.19909.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lechun You, Zhonghua Wu, Weide Liu, Xulei Yang, Jun Cheng, Wei Zhou, Bharadwaj Veeravalli, Guosheng Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19909">Integrating SAM Supervision for 3D Weakly Supervised Point Cloud Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current methods for 3D semantic segmentation propose training models with limited annotations to address the difficulty of annotating large, irregular, and unordered 3D point cloud data. They usually focus on the 3D domain only, without leveraging the complementary nature of 2D and 3D data. Besides, some methods extend original labels or generate pseudo labels to guide the training, but they often fail to fully use these labels or address the noise within them. Meanwhile, the emergence of comprehensive and adaptable foundation models has offered effective solutions for segmenting 2D data. Leveraging this advancement, we present a novel approach that maximizes the utility of sparsely available 3D annotations by incorporating segmentation masks generated by 2D foundation models. We further propagate the 2D segmentation masks into the 3D space by establishing geometric correspondences between 3D scenes and 2D views. We extend the highly sparse annotations to encompass the areas delineated by 3D masks, thereby substantially augmenting the pool of available labels. Furthermore, we apply confidence- and uncertainty-based consistency regularization on augmentations of the 3D point cloud and select the reliable pseudo labels, which are further spread on the 3D masks to generate more labels. This innovative strategy bridges the gap between limited 3D annotations and the powerful capabilities of 2D foundation models, ultimately improving the performance of 3D weakly supervised segmentation.
<div id='section'>Paperid: <span id='pid'>523, <a href='https://arxiv.org/pdf/2507.02399.pdf' target='_blank'>https://arxiv.org/pdf/2507.02399.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peilin Zhang, Shaouxan Wua, Jun Feng, Zhuo Jin, Zhizezhang Gao, Jingkun Chen, Yaqiong Xing, Xiao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02399">TABNet: A Triplet Augmentation Self-Recovery Framework with Boundary-Aware Pseudo-Labels for Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Background and objective: Medical image segmentation is a core task in various clinical applications. However, acquiring large-scale, fully annotated medical image datasets is both time-consuming and costly. Scribble annotations, as a form of sparse labeling, provide an efficient and cost-effective alternative for medical image segmentation. However, the sparsity of scribble annotations limits the feature learning of the target region and lacks sufficient boundary supervision, which poses significant challenges for training segmentation networks. Methods: We propose TAB Net, a novel weakly-supervised medical image segmentation framework, consisting of two key components: the triplet augmentation self-recovery (TAS) module and the boundary-aware pseudo-label supervision (BAP) module. The TAS module enhances feature learning through three complementary augmentation strategies: intensity transformation improves the model's sensitivity to texture and contrast variations, cutout forces the network to capture local anatomical structures by masking key regions, and jigsaw augmentation strengthens the modeling of global anatomical layout by disrupting spatial continuity. By guiding the network to recover complete masks from diverse augmented inputs, TAS promotes a deeper semantic understanding of medical images under sparse supervision. The BAP module enhances pseudo-supervision accuracy and boundary modeling by fusing dual-branch predictions into a loss-weighted pseudo-label and introducing a boundary-aware loss for fine-grained contour refinement. Results: Experimental evaluations on two public datasets, ACDC and MSCMR seg, demonstrate that TAB Net significantly outperforms state-of-the-art methods for scribble-based weakly supervised segmentation. Moreover, it achieves performance comparable to that of fully supervised methods.
<div id='section'>Paperid: <span id='pid'>524, <a href='https://arxiv.org/pdf/2506.19222.pdf' target='_blank'>https://arxiv.org/pdf/2506.19222.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinke Ma, Yongsheng Pan, Qingjie Zeng, Mengkang Lu, Bolysbek Murat Yerzhanuly, Bazargul Matkerim, Yong Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19222">Deformable Medical Image Registration with Effective Anatomical Structure Representation and Divide-and-Conquer Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective representation of Regions of Interest (ROI) and independent alignment of these ROIs can significantly enhance the performance of deformable medical image registration (DMIR). However, current learning-based DMIR methods have limitations. Unsupervised techniques disregard ROI representation and proceed directly with aligning pairs of images, while weakly-supervised methods heavily depend on label constraints to facilitate registration. To address these issues, we introduce a novel ROI-based registration approach named EASR-DCN. Our method represents medical images through effective ROIs and achieves independent alignment of these ROIs without requiring labels. Specifically, we first used a Gaussian mixture model for intensity analysis to represent images using multiple effective ROIs with distinct intensities. Furthermore, we propose a novel Divide-and-Conquer Network (DCN) to process these ROIs through separate channels to learn feature alignments for each ROI. The resultant correspondences are seamlessly integrated to generate a comprehensive displacement vector field. Extensive experiments were performed on three MRI and one CT datasets to showcase the superior accuracy and deformation reduction efficacy of our EASR-DCN. Compared to VoxelMorph, our EASR-DCN achieved improvements of 10.31\% in the Dice score for brain MRI, 13.01\% for cardiac MRI, and 5.75\% for hippocampus MRI, highlighting its promising potential for clinical applications. The code for this work will be released upon acceptance of the paper.
<div id='section'>Paperid: <span id='pid'>525, <a href='https://arxiv.org/pdf/2506.10633.pdf' target='_blank'>https://arxiv.org/pdf/2506.10633.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Konstantinos Vilouras, Ilias Stogiannidis, Junyu Yan, Alison Q. O'Neil, Sotirios A. Tsaftaris
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10633">Anatomy-Grounded Weakly Supervised Prompt Tuning for Chest X-ray Latent Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Latent Diffusion Models have shown remarkable results in text-guided image synthesis in recent years. In the domain of natural (RGB) images, recent works have shown that such models can be adapted to various vision-language downstream tasks with little to no supervision involved. On the contrary, text-to-image Latent Diffusion Models remain relatively underexplored in the field of medical imaging, primarily due to limited data availability (e.g., due to privacy concerns). In this work, focusing on the chest X-ray modality, we first demonstrate that a standard text-conditioned Latent Diffusion Model has not learned to align clinically relevant information in free-text radiology reports with the corresponding areas of the given scan. Then, to alleviate this issue, we propose a fine-tuning framework to improve multi-modal alignment in a pre-trained model such that it can be efficiently repurposed for downstream tasks such as phrase grounding. Our method sets a new state-of-the-art on a standard benchmark dataset (MS-CXR), while also exhibiting robust performance on out-of-distribution data (VinDr-CXR). Our code will be made publicly available.
<div id='section'>Paperid: <span id='pid'>526, <a href='https://arxiv.org/pdf/2506.04351.pdf' target='_blank'>https://arxiv.org/pdf/2506.04351.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maksym Ivashechkin, Oscar Mendez, Richard Bowden
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.04351">HuGeDiff: 3D Human Generation via Diffusion with Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D human generation is an important problem with a wide range of applications in computer vision and graphics. Despite recent progress in generative AI such as diffusion models or rendering methods like Neural Radiance Fields or Gaussian Splatting, controlling the generation of accurate 3D humans from text prompts remains an open challenge. Current methods struggle with fine detail, accurate rendering of hands and faces, human realism, and controlability over appearance. The lack of diversity, realism, and annotation in human image data also remains a challenge, hindering the development of a foundational 3D human model. We present a weakly supervised pipeline that tries to address these challenges. In the first step, we generate a photorealistic human image dataset with controllable attributes such as appearance, race, gender, etc using a state-of-the-art image diffusion model. Next, we propose an efficient mapping approach from image features to 3D point clouds using a transformer-based architecture. Finally, we close the loop by training a point-cloud diffusion model that is conditioned on the same text prompts used to generate the original samples. We demonstrate orders-of-magnitude speed-ups in 3D human generation compared to the state-of-the-art approaches, along with significantly improved text-prompt alignment, realism, and rendering quality. We will make the code and dataset available.
<div id='section'>Paperid: <span id='pid'>527, <a href='https://arxiv.org/pdf/2503.18082.pdf' target='_blank'>https://arxiv.org/pdf/2503.18082.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nachuan Ma, Zhengfei Song, Qiang Hu, Chuang-Wei Liu, Yu Han, Yanting Zhang, Rui Fan, Lihua Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18082">Vehicular Road Crack Detection with Deep Learning: A New Online Benchmark for Comprehensive Evaluation of Existing Algorithms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the emerging field of urban digital twins (UDTs), advancing intelligent road inspection (IRI) vehicles with automatic road crack detection systems is essential for maintaining civil infrastructure. Over the past decade, deep learning-based road crack detection methods have been developed to detect cracks more efficiently, accurately, and objectively, with the goal of replacing manual visual inspection. Nonetheless, there is a lack of systematic reviews on state-of-the-art (SoTA) deep learning techniques, especially data-fusion and label-efficient algorithms for this task. This paper thoroughly reviews the SoTA deep learning-based algorithms, including (1) supervised, (2) unsupervised, (3) semi-supervised, and (4) weakly-supervised methods developed for road crack detection. Also, we create a dataset called UDTIRI-Crack, comprising $2,500$ high-quality images from seven public annotated sources, as the first extensive online benchmark in this field. Comprehensive experiments are conducted to compare the detection performance, computational efficiency, and generalizability of public SoTA deep learning-based algorithms for road crack detection. In addition, the feasibility of foundation models and large language models (LLMs) for road crack detection is explored. Afterwards, the existing challenges and future development trends of deep learning-based road crack detection algorithms are discussed. We believe this review can serve as practical guidance for developing intelligent road detection vehicles with the next-generation road condition assessment systems. The released benchmark UDTIRI-Crack is available at https://udtiri.com/submission/.
<div id='section'>Paperid: <span id='pid'>528, <a href='https://arxiv.org/pdf/2503.10287.pdf' target='_blank'>https://arxiv.org/pdf/2503.10287.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Zhou, Xiaobao Guo, Yuzhe Zhu, Adams Wai-Kin Kong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10287">MACS: Multi-source Audio-to-image Generation with Contextual Significance and Semantic Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Propelled by the breakthrough in deep generative models, audio-to-image generation has emerged as a pivotal cross-model task that converts complex auditory signals into rich visual representations. However, previous works only focus on single-source audio inputs for image generation, ignoring the multi-source characteristic in natural auditory scenes, thus limiting the performance in generating comprehensive visual content. To bridge this gap, a method called MACS is proposed to conduct multi-source audio-to-image generation. This is the first work that explicitly separates multi-source audio to capture the rich audio components before image generation. MACS is a two-stage method. In the first stage, multi-source audio inputs are separated by a weakly supervised method, where the audio and text labels are semantically aligned by casting into a common space using the large pre-trained CLAP model. We introduce a ranking loss to consider the contextual significance of the separated audio signals. In the second stage, efficient image generation is achieved by mapping the separated audio signals to the generation condition using only a trainable adapter and a MLP layer. We preprocess the LLP dataset as the first full multi-source audio-to-image generation benchmark. The experiments are conducted on multi-source, mixed-source, and single-source audio-to-image generation tasks. The proposed MACS outperforms the current state-of-the-art methods in 17 of the 21 evaluation indexes on all tasks and delivers superior visual quality. The code will be publicly available.
<div id='section'>Paperid: <span id='pid'>529, <a href='https://arxiv.org/pdf/2502.06591.pdf' target='_blank'>https://arxiv.org/pdf/2502.06591.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ron Shapira Weber, Oren Freifeld
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.06591">Diffeomorphic Temporal Alignment Nets for Time-series Joint Alignment and Averaging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In time-series analysis, nonlinear temporal misalignment remains a pivotal challenge that forestalls even simple averaging. Since its introduction, the Diffeomorphic Temporal Alignment Net (DTAN), which we first introduced (Weber et al., 2019) and further developed in (Weber & Freifeld, 2023), has proven itself as an effective solution for this problem (these conference papers are earlier partial versions of the current manuscript). DTAN predicts and applies diffeomorphic transformations in an input-dependent manner, thus facilitating the joint alignment (JA) and averaging of time-series ensembles in an unsupervised or a weakly-supervised manner. The inherent challenges of the weakly/unsupervised setting, particularly the risk of trivial solutions through excessive signal distortion, are mitigated using either one of two distinct strategies: 1) a regularization term for warps; 2) using the Inverse Consistency Averaging Error (ICAE). The latter is a novel, regularization-free approach which also facilitates the JA of variable-length signals. We also further extend our framework to incorporate multi-task learning (MT-DTAN), enabling simultaneous time-series alignment and classification. Additionally, we conduct a comprehensive evaluation of different backbone architectures, demonstrating their efficacy in time-series alignment tasks. Finally, we showcase the utility of our approach in enabling Principal Component Analysis (PCA) for misaligned time-series data. Extensive experiments across 128 UCR datasets validate the superiority of our approach over contemporary averaging methods, including both traditional and learning-based approaches, marking a significant advancement in the field of time-series analysis.
<div id='section'>Paperid: <span id='pid'>530, <a href='https://arxiv.org/pdf/2501.19345.pdf' target='_blank'>https://arxiv.org/pdf/2501.19345.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Masahiro Kato, Fumiaki Kozai, Ryo Inokuchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.19345">PUATE: Efficient Average Treatment Effect Estimation from Treated (Positive) and Unlabeled Units</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The estimation of average treatment effects (ATEs), defined as the difference in expected outcomes between treatment and control groups, is a central topic in causal inference. This study develops semiparametric efficient estimators for ATE in a setting where only a treatment group and an unlabeled group, consisting of units whose treatment status is unknown, are observed. This scenario constitutes a variant of learning from positive and unlabeled data (PU learning) and can be viewed as a special case of ATE estimation with missing data. For this setting, we derive the semiparametric efficiency bounds, which characterize the lowest achievable asymptotic variance for regular estimators. We then construct semiparametric efficient ATE estimators that attain these bounds. Our results contribute to the literature on causal inference with missing data and weakly supervised learning.
<div id='section'>Paperid: <span id='pid'>531, <a href='https://arxiv.org/pdf/2501.01845.pdf' target='_blank'>https://arxiv.org/pdf/2501.01845.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunshuang Yuan, Frank Thiemann, Monika Sester
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01845">Semantic Segmentation for Sequential Historical Maps by Learning from Only One Map</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Historical maps are valuable resources that capture detailed geographical information from the past. However, these maps are typically available in printed formats, which are not conducive to modern computer-based analyses. Digitizing these maps into a machine-readable format enables efficient computational analysis. In this paper, we propose an automated approach to digitization using deep-learning-based semantic segmentation, which assigns a semantic label to each pixel in scanned historical maps. A key challenge in this process is the lack of ground-truth annotations required for training deep neural networks, as manual labeling is time-consuming and labor-intensive. To address this issue, we introduce a weakly-supervised age-tracing strategy for model fine-tuning. This approach exploits the similarity in appearance and land-use patterns between historical maps from neighboring time periods to guide the training process. Specifically, model predictions for one map are utilized as pseudo-labels for training on maps from adjacent time periods. Experiments conducted on our newly curated \textit{Hameln} dataset demonstrate that the proposed age-tracing strategy significantly enhances segmentation performance compared to baseline models. In the best-case scenario, the mean Intersection over Union (mIoU) achieved 77.3\%, reflecting an improvement of approximately 20\% over baseline methods. Additionally, the fine-tuned model achieved an average overall accuracy of 97\%, highlighting the effectiveness of our approach for digitizing historical maps.
<div id='section'>Paperid: <span id='pid'>532, <a href='https://arxiv.org/pdf/2410.05900.pdf' target='_blank'>https://arxiv.org/pdf/2410.05900.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiling Zhang, Erkut Akdag, Egor Bondarev, Peter H. N. De With
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05900">MTFL: Multi-Timescale Feature Learning for Weakly-Supervised Anomaly Detection in Surveillance Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detection of anomaly events is relevant for public safety and requires a combination of fine-grained motion information and contextual events at variable time-scales. To this end, we propose a Multi-Timescale Feature Learning (MTFL) method to enhance the representation of anomaly features. Short, medium, and long temporal tubelets are employed to extract spatio-temporal video features using a Video Swin Transformer. Experimental results demonstrate that MTFL outperforms state-of-the-art methods on the UCF-Crime dataset, achieving an anomaly detection performance 89.78% AUC. Moreover, it performs complementary to SotA with 95.32% AUC on the ShanghaiTech and 84.57% AP on the XD-Violence dataset. Furthermore, we generate an extended dataset of the UCF-Crime for development and evaluation on a wider range of anomalies, namely Video Anomaly Detection Dataset (VADD), involving 2,591 videos in 18 classes with extensive coverage of realistic anomalies.
<div id='section'>Paperid: <span id='pid'>533, <a href='https://arxiv.org/pdf/2409.20253.pdf' target='_blank'>https://arxiv.org/pdf/2409.20253.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Iira HÃ¤kkinen, Iaroslav Melekhov, Erik Englesson, Hossein Azizpour, Juho Kannala
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.20253">Medical Image Segmentation with SAM-generated Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The field of medical image segmentation is hindered by the scarcity of large, publicly available annotated datasets. Not all datasets are made public for privacy reasons, and creating annotations for a large dataset is time-consuming and expensive, as it requires specialized expertise to accurately identify regions of interest (ROIs) within the images. To address these challenges, we evaluate the performance of the Segment Anything Model (SAM) as an annotation tool for medical data by using it to produce so-called "pseudo labels" on the Medical Segmentation Decathlon (MSD) computed tomography (CT) tasks. The pseudo labels are then used in place of ground truth labels to train a UNet model in a weakly-supervised manner. We experiment with different prompt types on SAM and find that the bounding box prompt is a simple yet effective method for generating pseudo labels. This method allows us to develop a weakly-supervised model that performs comparably to a fully supervised model.
<div id='section'>Paperid: <span id='pid'>534, <a href='https://arxiv.org/pdf/2409.19587.pdf' target='_blank'>https://arxiv.org/pdf/2409.19587.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abhijeet Patil, Harsh Diwakar, Jay Sawant, Nikhil Cherian Kurian, Subhash Yadav, Swapnil Rane, Tripti Bameta, Amit Sethi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19587">Efficient Quality Control of Whole Slide Pathology Images with Human-in-the-loop Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Histopathology whole slide images (WSIs) are being widely used to develop deep learning-based diagnostic solutions, especially for precision oncology. Most of these diagnostic softwares are vulnerable to biases and impurities in the training and test data which can lead to inaccurate diagnoses. For instance, WSIs contain multiple types of tissue regions, at least some of which might not be relevant to the diagnosis. We introduce HistoROI, a robust yet lightweight deep learning-based classifier to segregate WSI into six broad tissue regions -- epithelium, stroma, lymphocytes, adipose, artifacts, and miscellaneous. HistoROI is trained using a novel human-in-the-loop and active learning paradigm that ensures variations in training data for labeling-efficient generalization. HistoROI consistently performs well across multiple organs, despite being trained on only a single dataset, demonstrating strong generalization. Further, we have examined the utility of HistoROI in improving the performance of downstream deep learning-based tasks using the CAMELYON breast cancer lymph node and TCGA lung cancer datasets. For the former dataset, the area under the receiver operating characteristic curve (AUC) for metastasis versus normal tissue of a neural network trained using weakly supervised learning increased from 0.88 to 0.92 by filtering the data using HistoROI. Similarly, the AUC increased from 0.88 to 0.93 for the classification between adenocarcinoma and squamous cell carcinoma on the lung cancer dataset. We also found that the performance of the HistoROI improves upon HistoQC for artifact detection on a test dataset of 93 annotated WSIs. The limitations of the proposed model are analyzed, and potential extensions are also discussed.
<div id='section'>Paperid: <span id='pid'>535, <a href='https://arxiv.org/pdf/2409.02145.pdf' target='_blank'>https://arxiv.org/pdf/2409.02145.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zekang Yang, Hong Liu, Xiangdong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.02145">A Multimodal Object-level Contrast Learning Method for Cancer Survival Risk Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computer-aided cancer survival risk prediction plays an important role in the timely treatment of patients. This is a challenging weakly supervised ordinal regression task associated with multiple clinical factors involved such as pathological images, genomic data and etc. In this paper, we propose a new training method, multimodal object-level contrast learning, for cancer survival risk prediction. First, we construct contrast learning pairs based on the survival risk relationship among the samples in the training sample set. Then we introduce the object-level contrast learning method to train the survival risk predictor. We further extend it to the multimodal scenario by applying cross-modal constrast. Considering the heterogeneity of pathological images and genomics data, we construct a multimodal survival risk predictor employing attention-based and self-normalizing based nerural network respectively. Finally, the survival risk predictor trained by our proposed method outperforms state-of-the-art methods on two public multimodal cancer datasets for survival risk prediction.
<div id='section'>Paperid: <span id='pid'>536, <a href='https://arxiv.org/pdf/2407.20461.pdf' target='_blank'>https://arxiv.org/pdf/2407.20461.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pascal Spiegler, Amirhossein Rasoulian, Yiming Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.20461">Weakly Supervised Intracranial Hemorrhage Segmentation with YOLO and an Uncertainty Rectified Segment Anything Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intracranial hemorrhage (ICH) is a life-threatening condition that requires rapid and accurate diagnosis to improve treatment outcomes and patient survival rates. Recent advancements in supervised deep learning have greatly improved the analysis of medical images, but often rely on extensive datasets with high-quality annotations, which are costly, time-consuming, and require medical expertise to prepare. To mitigate the need for large amounts of expert-prepared segmentation data, we have developed a novel weakly supervised ICH segmentation method that utilizes the YOLO object detection model and an uncertainty-rectified Segment Anything Model (SAM). In addition, we have proposed a novel point prompt generator for this model to further improve segmentation results with YOLO-predicted bounding box prompts. Our approach achieved a high accuracy of 0.933 and an AUC of 0.796 in ICH detection, along with a mean Dice score of 0.629 for ICH segmentation, outperforming existing weakly supervised and popular supervised (UNet and Swin-UNETR) approaches. Overall, the proposed method provides a robust and accurate alternative to the more commonly used supervised techniques for ICH quantification without requiring refined segmentation ground truths during model training.
<div id='section'>Paperid: <span id='pid'>537, <a href='https://arxiv.org/pdf/2407.15794.pdf' target='_blank'>https://arxiv.org/pdf/2407.15794.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guiqiu Liao, Matjaz Jogan, Sai Koushik, Eric Eaton, Daniel A. Hashimoto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.15794">Disentangling spatio-temporal knowledge for weakly supervised object detection and segmentation in surgical video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised video object segmentation (WSVOS) enables the identification of segmentation maps without requiring an extensive training dataset of object masks, relying instead on coarse video labels indicating object presence. Current state-of-the-art methods either require multiple independent stages of processing that employ motion cues or, in the case of end-to-end trainable networks, lack in segmentation accuracy, in part due to the difficulty of learning segmentation maps from videos with transient object presence. This limits the application of WSVOS for semantic annotation of surgical videos where multiple surgical tools frequently move in and out of the field of view, a problem that is more difficult than typically encountered in WSVOS. This paper introduces Video Spatio-Temporal Disentanglement Networks (VDST-Net), a framework to disentangle spatiotemporal information using semi-decoupled knowledge distillation to predict high-quality class activation maps (CAMs). A teacher network designed to resolve temporal conflicts when specifics about object location and timing in the video are not provided works with a student network that integrates information over time by leveraging temporal dependencies. We demonstrate the efficacy of our framework on a public reference dataset and on a more challenging surgical video dataset where objects are, on average, present in less than 60\% of annotated frames. Our method outperforms state-of-the-art techniques and generates superior segmentation masks under video-level weak supervision.
<div id='section'>Paperid: <span id='pid'>538, <a href='https://arxiv.org/pdf/2406.05308.pdf' target='_blank'>https://arxiv.org/pdf/2406.05308.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Heming Yao, Phil Hanslovsky, Jan-Christian Huetter, Burkhard Hoeckendorf, David Richmond
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.05308">Weakly Supervised Set-Consistency Learning Improves Morphological Profiling of Single-Cell Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Optical Pooled Screening (OPS) is a powerful tool combining high-content microscopy with genetic engineering to investigate gene function in disease. The characterization of high-content images remains an active area of research and is currently undergoing rapid innovation through the application of self-supervised learning and vision transformers. In this study, we propose a set-level consistency learning algorithm, Set-DINO, that combines self-supervised learning with weak supervision to improve learned representations of perturbation effects in single-cell images. Our method leverages the replicate structure of OPS experiments (i.e., cells undergoing the same genetic perturbation, both within and across batches) as a form of weak supervision. We conduct extensive experiments on a large-scale OPS dataset with more than 5000 genetic perturbations, and demonstrate that Set-DINO helps mitigate the impact of confounders and encodes more biologically meaningful information. In particular, Set-DINO recalls known biological relationships with higher accuracy compared to commonly used methods for morphological profiling, suggesting that it can generate more reliable insights from drug target discovery campaigns leveraging OPS.
<div id='section'>Paperid: <span id='pid'>539, <a href='https://arxiv.org/pdf/2405.10690.pdf' target='_blank'>https://arxiv.org/pdf/2405.10690.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Faegheh Sardari, Armin Mustafa, Philip J. B. Jackson, Adrian Hilton
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.10690">CoLeaF: A Contrastive-Collaborative Learning Framework for Weakly Supervised Audio-Visual Video Parsing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised audio-visual video parsing (AVVP) methods aim to detect audible-only, visible-only, and audible-visible events using only video-level labels. Existing approaches tackle this by leveraging unimodal and cross-modal contexts. However, we argue that while cross-modal learning is beneficial for detecting audible-visible events, in the weakly supervised scenario, it negatively impacts unaligned audible or visible events by introducing irrelevant modality information. In this paper, we propose CoLeaF, a novel learning framework that optimizes the integration of cross-modal context in the embedding space such that the network explicitly learns to combine cross-modal information for audible-visible events while filtering them out for unaligned events. Additionally, as videos often involve complex class relationships, modelling them improves performance. However, this introduces extra computational costs into the network. Our framework is designed to leverage cross-class relationships during training without incurring additional computations at inference. Furthermore, we propose new metrics to better evaluate a method's capabilities in performing AVVP. Our extensive experiments demonstrate that CoLeaF significantly improves the state-of-the-art results by an average of 1.9% and 2.4% F-score on the LLP and UnAV-100 datasets, respectively.
<div id='section'>Paperid: <span id='pid'>540, <a href='https://arxiv.org/pdf/2405.08699.pdf' target='_blank'>https://arxiv.org/pdf/2405.08699.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenrui Li, Wei Zhang, Qinghao Zhang, Xuegong Zhang, Xiaowo Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.08699">Weakly-supervised causal discovery based on fuzzy knowledge and complex data complementarity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Causal discovery based on observational data is important for deciphering the causal mechanism behind complex systems. However, the effectiveness of existing causal discovery methods is limited due to inferior prior knowledge, domain inconsistencies, and the challenges of high-dimensional datasets with small sample sizes. To address this gap, we propose a novel weakly-supervised fuzzy knowledge and data co-driven causal discovery method named KEEL. KEEL adopts a fuzzy causal knowledge schema to encapsulate diverse types of fuzzy knowledge, and forms corresponding weakened constraints. This schema not only lessens the dependency on expertise but also allows various types of limited and error-prone fuzzy knowledge to guide causal discovery. It can enhance the generalization and robustness of causal discovery, especially in high-dimensional and small-sample scenarios. In addition, we integrate the extended linear causal model (ELCM) into KEEL for dealing with the multi-distribution and incomplete data. Extensive experiments with different datasets demonstrate the superiority of KEEL over several state-of-the-art methods in accuracy, robustness and computational efficiency. For causal discovery in real protein signal transduction processes, KEEL outperforms the benchmark method with limited data. In summary, KEEL is effective to tackle the causal discovery tasks with higher accuracy while alleviating the requirement for extensive domain expertise.
<div id='section'>Paperid: <span id='pid'>541, <a href='https://arxiv.org/pdf/2404.17324.pdf' target='_blank'>https://arxiv.org/pdf/2404.17324.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jyri MaanpÃ¤Ã¤, Julius Pesonen, Heikki Hyyti, Iaroslav Melekhov, Juho Kannala, Petri Manninen, Antero Kukko, Juha HyyppÃ¤
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.17324">Dense Road Surface Grip Map Prediction from Multimodal Image Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Slippery road weather conditions are prevalent in many regions and cause a regular risk for traffic. Still, there has been less research on how autonomous vehicles could detect slippery driving conditions on the road to drive safely. In this work, we propose a method to predict a dense grip map from the area in front of the car, based on postprocessed multimodal sensor data. We trained a convolutional neural network to predict pixelwise grip values from fused RGB camera, thermal camera, and LiDAR reflectance images, based on weakly supervised ground truth from an optical road weather sensor.
  The experiments show that it is possible to predict dense grip values with good accuracy from the used data modalities as the produced grip map follows both ground truth measurements and local weather conditions, such as snowy areas on the road. The model using only the RGB camera or LiDAR reflectance modality provided good baseline results for grip prediction accuracy while using models fusing the RGB camera, thermal camera, and LiDAR modalities improved the grip predictions significantly.
<div id='section'>Paperid: <span id='pid'>542, <a href='https://arxiv.org/pdf/2404.13311.pdf' target='_blank'>https://arxiv.org/pdf/2404.13311.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yangcen Liu, Ziyi Liu, Yuanhao Zhai, Wen Li, David Doerman, Junsong Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.13311">STAT: Towards Generalizable Temporal Action Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised temporal action localization (WTAL) aims to recognize and localize action instances with only video-level labels. Despite the significant progress, existing methods suffer from severe performance degradation when transferring to different distributions and thus may hardly adapt to real-world scenarios . To address this problem, we propose the Generalizable Temporal Action Localization task (GTAL), which focuses on improving the generalizability of action localization methods. We observed that the performance decline can be primarily attributed to the lack of generalizability to different action scales. To address this problem, we propose STAT (Self-supervised Temporal Adaptive Teacher), which leverages a teacher-student structure for iterative refinement. Our STAT features a refinement module and an alignment module. The former iteratively refines the model's output by leveraging contextual information and helps adapt to the target scale. The latter improves the refinement process by promoting a consensus between student and teacher models. We conduct extensive experiments on three datasets, THUMOS14, ActivityNet1.2, and HACS, and the results show that our method significantly improves the Baseline methods under the cross-distribution evaluation setting, even approaching the same-distribution evaluation performance.
<div id='section'>Paperid: <span id='pid'>543, <a href='https://arxiv.org/pdf/2404.12015.pdf' target='_blank'>https://arxiv.org/pdf/2404.12015.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Claudia Cuttano, Gabriele Rosi, Gabriele Trivigno, Giuseppe Averta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.12015">What does CLIP know about peeling a banana?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans show an innate capability to identify tools to support specific actions. The association between objects parts and the actions they facilitate is usually named affordance. Being able to segment objects parts depending on the tasks they afford is crucial to enable intelligent robots to use objects of daily living. Traditional supervised learning methods for affordance segmentation require costly pixel-level annotations, while weakly supervised approaches, though less demanding, still rely on object-interaction examples and support a closed set of actions. These limitations hinder scalability, may introduce biases, and usually restrict models to a limited set of predefined actions. This paper proposes AffordanceCLIP, to overcome these limitations by leveraging the implicit affordance knowledge embedded within large pre-trained Vision-Language models like CLIP. We experimentally demonstrate that CLIP, although not explicitly trained for affordances detection, retains valuable information for the task. Our AffordanceCLIP achieves competitive zero-shot performance compared to methods with specialized training, while offering several advantages: i) it works with any action prompt, not just a predefined set; ii) it requires training only a small number of additional parameters compared to existing solutions and iii) eliminates the need for direct supervision on action-object pairs, opening new perspectives for functionality-based reasoning of models.
<div id='section'>Paperid: <span id='pid'>544, <a href='https://arxiv.org/pdf/2404.02527.pdf' target='_blank'>https://arxiv.org/pdf/2404.02527.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xu Wang, Yifan Li, Qiudan Zhang, Wenhui Wu, Mark Junjie Li, Jianmin Jinag
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.02527">Weakly-Supervised 3D Scene Graph Generation via Visual-Linguistic Assisted Pseudo-labeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning to build 3D scene graphs is essential for real-world perception in a structured and rich fashion. However, previous 3D scene graph generation methods utilize a fully supervised learning manner and require a large amount of entity-level annotation data of objects and relations, which is extremely resource-consuming and tedious to obtain. To tackle this problem, we propose 3D-VLAP, a weakly-supervised 3D scene graph generation method via Visual-Linguistic Assisted Pseudo-labeling. Specifically, our 3D-VLAP exploits the superior ability of current large-scale visual-linguistic models to align the semantics between texts and 2D images, as well as the naturally existing correspondences between 2D images and 3D point clouds, and thus implicitly constructs correspondences between texts and 3D point clouds. First, we establish the positional correspondence from 3D point clouds to 2D images via camera intrinsic and extrinsic parameters, thereby achieving alignment of 3D point clouds and 2D images. Subsequently, a large-scale cross-modal visual-linguistic model is employed to indirectly align 3D instances with the textual category labels of objects by matching 2D images with object category labels. The pseudo labels for objects and relations are then produced for 3D-VLAP model training by calculating the similarity between visual embeddings and textual category embeddings of objects and relations encoded by the visual-linguistic model, respectively. Ultimately, we design an edge self-attention based graph neural network to generate scene graphs of 3D point cloud scenes. Extensive experiments demonstrate that our 3D-VLAP achieves comparable results with current advanced fully supervised methods, meanwhile significantly alleviating the pressure of data annotation.
<div id='section'>Paperid: <span id='pid'>545, <a href='https://arxiv.org/pdf/2404.00667.pdf' target='_blank'>https://arxiv.org/pdf/2404.00667.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dafei Qiu, Shan Xiong, Jiajin Yi, Jialin Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00667">Weakly-Supervised Cross-Domain Segmentation of Electron Microscopy with Sparse Point Annotation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate segmentation of organelle instances from electron microscopy (EM) images plays an essential role in many neuroscience researches. However, practical scenarios usually suffer from high annotation costs, label scarcity, and large domain diversity. While unsupervised domain adaptation (UDA) that assumes no annotation effort on the target data is promising to alleviate these challenges, its performance on complicated segmentation tasks is still far from practical usage. To address these issues, we investigate a highly annotation-efficient weak supervision, which assumes only sparse center-points on a small subset of object instances in the target training images. To achieve accurate segmentation with partial point annotations, we introduce instance counting and center detection as auxiliary tasks and design a multitask learning framework to leverage correlations among the counting, detection, and segmentation, which are all tasks with partial or no supervision. Building upon the different domain-invariances of the three tasks, we enforce counting estimation with a novel soft consistency loss as a global prior for center detection, which further guides the per-pixel segmentation. To further compensate for annotation sparsity, we develop a cross-position cut-and-paste for label augmentation and an entropy-based pseudo-label selection. The experimental results highlight that, by simply using extremely weak annotation, e.g., 15\% sparse points, for model training, the proposed model is capable of significantly outperforming UDA methods and produces comparable performance as the supervised counterpart. The high robustness of our model shown in the validations and the low requirement of expert knowledge for sparse point annotation further improve the potential application value of our model.
<div id='section'>Paperid: <span id='pid'>546, <a href='https://arxiv.org/pdf/2403.14366.pdf' target='_blank'>https://arxiv.org/pdf/2403.14366.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lizhe Liu, Bohua Wang, Hongwei Xie, Daqi Liu, Li Liu, Zhiqiang Tian, Kuiyuan Yang, Bing Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.14366">SurroundSDF: Implicit 3D Scene Understanding Based on Signed Distance Field</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-centric 3D environment understanding is both vital and challenging for autonomous driving systems. Recently, object-free methods have attracted considerable attention. Such methods perceive the world by predicting the semantics of discrete voxel grids but fail to construct continuous and accurate obstacle surfaces. To this end, in this paper, we propose SurroundSDF to implicitly predict the signed distance field (SDF) and semantic field for the continuous perception from surround images. Specifically, we introduce a query-based approach and utilize SDF constrained by the Eikonal formulation to accurately describe the surfaces of obstacles. Furthermore, considering the absence of precise SDF ground truth, we propose a novel weakly supervised paradigm for SDF, referred to as the Sandwich Eikonal formulation, which emphasizes applying correct and dense constraints on both sides of the surface, thereby enhancing the perceptual accuracy of the surface. Experiments suggest that our method achieves SOTA for both occupancy prediction and 3D scene reconstruction tasks on the nuScenes dataset.
<div id='section'>Paperid: <span id='pid'>547, <a href='https://arxiv.org/pdf/2403.14240.pdf' target='_blank'>https://arxiv.org/pdf/2403.14240.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wang-Wang Yu, Xian-Shi Zhang, Fu-Ya Luo, Yijun Cao, Kai-Fu Yang, Hong-Mei Yan, Yong-Jie Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.14240">Weak Supervision with Arbitrary Single Frame for Micro- and Macro-expression Spotting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Frame-level micro- and macro-expression spotting methods require time-consuming frame-by-frame observation during annotation. Meanwhile, video-level spotting lacks sufficient information about the location and number of expressions during training, resulting in significantly inferior performance compared with fully-supervised spotting. To bridge this gap, we propose a point-level weakly-supervised expression spotting (PWES) framework, where each expression requires to be annotated with only one random frame (i.e., a point). To mitigate the issue of sparse label distribution, the prevailing solution is pseudo-label mining, which, however, introduces new problems: localizing contextual background snippets results in inaccurate boundaries and discarding foreground snippets leads to fragmentary predictions. Therefore, we design the strategies of multi-refined pseudo label generation (MPLG) and distribution-guided feature contrastive learning (DFCL) to address these problems. Specifically, MPLG generates more reliable pseudo labels by merging class-specific probabilities, attention scores, fused features, and point-level labels. DFCL is utilized to enhance feature similarity for the same categories and feature variability for different categories while capturing global representations across the entire datasets. Extensive experiments on the CAS(ME)^2, CAS(ME)^3, and SAMM-LV datasets demonstrate PWES achieves promising performance comparable to that of recent fully-supervised methods.
<div id='section'>Paperid: <span id='pid'>548, <a href='https://arxiv.org/pdf/2403.10298.pdf' target='_blank'>https://arxiv.org/pdf/2403.10298.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qin Xu, Sitong Li, Jiahui Wang, Bo Jiang, Jinhui Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10298">Context-Semantic Quality Awareness Network for Fine-Grained Visual Categorization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Exploring and mining subtle yet distinctive features between sub-categories with similar appearances is crucial for fine-grained visual categorization (FGVC). However, less effort has been devoted to assessing the quality of extracted visual representations. Intuitively, the network may struggle to capture discriminative features from low-quality samples, which leads to a significant decline in FGVC performance. To tackle this challenge, we propose a weakly supervised Context-Semantic Quality Awareness Network (CSQA-Net) for FGVC. In this network, to model the spatial contextual relationship between rich part descriptors and global semantics for capturing more discriminative details within the object, we design a novel multi-part and multi-scale cross-attention (MPMSCA) module. Before feeding to the MPMSCA module, the part navigator is developed to address the scale confusion problems and accurately identify the local distinctive regions. Furthermore, we propose a generic multi-level semantic quality evaluation module (MLSQE) to progressively supervise and enhance hierarchical semantics from different levels of the backbone network. Finally, context-aware features from MPMSCA and semantically enhanced features from MLSQE are fed into the corresponding quality probing classifiers to evaluate their quality in real-time, thus boosting the discriminability of feature representations. Comprehensive experiments on four popular and highly competitive FGVC datasets demonstrate the superiority of the proposed CSQA-Net in comparison with the state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>549, <a href='https://arxiv.org/pdf/2402.13079.pdf' target='_blank'>https://arxiv.org/pdf/2402.13079.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Charles Arnal, Vivien Cabannes, Vianney Perchet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.13079">Mode Estimation with Partial Feedback</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The combination of lightly supervised pre-training and online fine-tuning has played a key role in recent AI developments. These new learning pipelines call for new theoretical frameworks. In this paper, we formalize core aspects of weakly supervised and active learning with a simple problem: the estimation of the mode of a distribution using partial feedback. We show how entropy coding allows for optimal information acquisition from partial feedback, develop coarse sufficient statistics for mode identification, and adapt bandit algorithms to our new setting. Finally, we combine those contributions into a statistically and computationally efficient solution to our problem.
<div id='section'>Paperid: <span id='pid'>550, <a href='https://arxiv.org/pdf/2510.15666.pdf' target='_blank'>https://arxiv.org/pdf/2510.15666.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Shi, Gang Li, Junxing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.15666">Uncertainty-Aware Extreme Point Tracing for Weakly Supervised Ultrasound Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic medical image segmentation is a fundamental step in computer-aided diagnosis, yet fully supervised approaches demand extensive pixel-level annotations that are costly and time-consuming. To alleviate this burden, we propose a weakly supervised segmentation framework that leverages only four extreme points as annotation. Specifically, bounding boxes derived from the extreme points are used as prompts for the Segment Anything Model 2 (SAM2) to generate reliable initial pseudo labels. These pseudo labels are progressively refined by an enhanced Feature-Guided Extreme Point Masking (FGEPM) algorithm, which incorporates Monte Carlo dropout-based uncertainty estimation to construct a unified gradient uncertainty cost map for boundary tracing. Furthermore, a dual-branch Uncertainty-aware Scale Consistency (USC) loss and a box alignment loss are introduced to ensure spatial consistency and precise boundary alignment during training. Extensive experiments on two public ultrasound datasets, BUSI and UNS, demonstrate that our method achieves performance comparable to, and even surpassing fully supervised counterparts while significantly reducing annotation cost. These results validate the effectiveness and practicality of the proposed weakly supervised framework for ultrasound image segmentation.
<div id='section'>Paperid: <span id='pid'>551, <a href='https://arxiv.org/pdf/2509.23376.pdf' target='_blank'>https://arxiv.org/pdf/2509.23376.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinghong Zheng, Changlong Jiang, Jiaqi Li, Haohong Kuang, Hang Xu, Tingbing Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23376">UniPose: Unified Cross-modality Pose Prior Propagation towards RGB-D data for Weakly Supervised 3D Human Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present UniPose, a unified cross-modality pose prior propagation method for weakly supervised 3D human pose estimation (HPE) using unannotated single-view RGB-D sequences (RGB, depth, and point cloud data). UniPose transfers 2D HPE annotations from large-scale RGB datasets (e.g., MS COCO) to the 3D domain via self-supervised learning on easily acquired RGB-D sequences, eliminating the need for labor-intensive 3D keypoint annotations. This approach bridges the gap between 2D and 3D domains without suffering from issues related to multi-view camera calibration or synthetic-to-real data shifts. During training, UniPose leverages off-the-shelf 2D pose estimations as weak supervision for point cloud networks, incorporating spatial-temporal constraints like body symmetry and joint motion. The 2D-to-3D back-projection loss and cross-modality interaction further enhance this process. By treating the point cloud network's 3D HPE results as pseudo ground truth, our anchor-to-joint prediction method performs 3D lifting on RGB and depth networks, making it more robust against inaccuracies in 2D HPE results compared to state-of-the-art methods. Experiments on CMU Panoptic and ITOP datasets show that UniPose achieves comparable performance to fully supervised methods. Incorporating large-scale unlabeled data (e.g., NTU RGB+D 60) enhances its performance under challenging conditions, demonstrating its potential for practical applications. Our proposed 3D lifting method also achieves state-of-the-art results.
<div id='section'>Paperid: <span id='pid'>552, <a href='https://arxiv.org/pdf/2509.19028.pdf' target='_blank'>https://arxiv.org/pdf/2509.19028.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ioannis Sarafis, Alexandros Papadopoulos, Anastasios Delopoulos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19028">Weakly Supervised Food Image Segmentation using Vision Transformers and Segment Anything Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a weakly supervised semantic segmentation approach for food images which takes advantage of the zero-shot capabilities and promptability of the Segment Anything Model (SAM) along with the attention mechanisms of Vision Transformers (ViTs). Specifically, we use class activation maps (CAMs) from ViTs to generate prompts for SAM, resulting in masks suitable for food image segmentation. The ViT model, a Swin Transformer, is trained exclusively using image-level annotations, eliminating the need for pixel-level annotations during training. Additionally, to enhance the quality of the SAM-generated masks, we examine the use of image preprocessing techniques in combination with single-mask and multi-mask SAM generation strategies. The methodology is evaluated on the FoodSeg103 dataset, generating an average of 2.4 masks per image (excluding background), and achieving an mIoU of 0.54 for the multi-mask scenario. We envision the proposed approach as a tool to accelerate food image annotation tasks or as an integrated component in food and nutrition tracking applications.
<div id='section'>Paperid: <span id='pid'>553, <a href='https://arxiv.org/pdf/2509.17971.pdf' target='_blank'>https://arxiv.org/pdf/2509.17971.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tan-Ha Mai, Hsuan-Tien Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17971">Intra-Cluster Mixup: An Effective Data Augmentation Technique for Complementary-Label Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we investigate the challenges of complementary-label learning (CLL), a specialized form of weakly-supervised learning (WSL) where models are trained with labels indicating classes to which instances do not belong, rather than standard ordinary labels. This alternative supervision is appealing because collecting complementary labels is generally cheaper and less labor-intensive. Although most existing research in CLL emphasizes the development of novel loss functions, the potential of data augmentation in this domain remains largely underexplored. In this work, we uncover that the widely-used Mixup data augmentation technique is ineffective when directly applied to CLL. Through in-depth analysis, we identify that the complementary-label noise generated by Mixup negatively impacts the performance of CLL models. We then propose an improved technique called Intra-Cluster Mixup (ICM), which only synthesizes augmented data from nearby examples, to mitigate the noise effect. ICM carries the benefits of encouraging complementary label sharing of nearby examples, and leads to substantial performance improvements across synthetic and real-world labeled datasets. In particular, our wide spectrum of experimental results on both balanced and imbalanced CLL settings justifies the potential of ICM in allying with state-of-the-art CLL algorithms, achieving significant accuracy increases of 30% and 10% on MNIST and CIFAR datasets, respectively.
<div id='section'>Paperid: <span id='pid'>554, <a href='https://arxiv.org/pdf/2508.18790.pdf' target='_blank'>https://arxiv.org/pdf/2508.18790.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhui Tao, Yizhe Zhang, Qiang Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18790">A Closer Look at Edema Area Segmentation in SD-OCT Images Using Adversarial Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of artificial intelligence models for macular edema (ME) analy-sis always relies on expert-annotated pixel-level image datasets which are expen-sive to collect prospectively. While anomaly-detection-based weakly-supervised methods have shown promise in edema area (EA) segmentation task, their per-formance still lags behind fully-supervised approaches. In this paper, we leverage the strong correlation between EA and retinal layers in spectral-domain optical coherence tomography (SD-OCT) images, along with the update characteristics of weakly-supervised learning, to enhance an off-the-shelf adversarial framework for EA segmentation with a novel layer-structure-guided post-processing step and a test-time-adaptation (TTA) strategy. By incorporating additional retinal lay-er information, our framework reframes the dense EA prediction task as one of confirming intersection points between the EA contour and retinal layers, result-ing in predictions that better align with the shape prior of EA. Besides, the TTA framework further helps address discrepancies in the manifestations and presen-tations of EA between training and test sets. Extensive experiments on two pub-licly available datasets demonstrate that these two proposed ingredients can im-prove the accuracy and robustness of EA segmentation, bridging the gap between weakly-supervised and fully-supervised models.
<div id='section'>Paperid: <span id='pid'>555, <a href='https://arxiv.org/pdf/2508.06115.pdf' target='_blank'>https://arxiv.org/pdf/2508.06115.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weichen Zhang, Kebin Liu, Fan Dang, Zhui Zhu, Xikai Sun, Yunhao Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06115">SynSeg: Feature Synergy for Multi-Category Contrastive Learning in Open-Vocabulary Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semantic segmentation in open-vocabulary scenarios presents significant challenges due to the wide range and granularity of semantic categories. Existing weakly-supervised methods often rely on category-specific supervision and ill-suited feature construction methods for contrastive learning, leading to semantic misalignment and poor performance. In this work, we propose a novel weakly-supervised approach, SynSeg, to address the challenges. SynSeg performs Multi-Category Contrastive Learning (MCCL) as a stronger training signal with a new feature reconstruction framework named Feature Synergy Structure (FSS). Specifically, MCCL strategy robustly combines both intra- and inter-category alignment and separation in order to make the model learn the knowledge of correlations from different categories within the same image. Moreover, FSS reconstructs discriminative features for contrastive learning through prior fusion and semantic-activation-map enhancement, effectively avoiding the foreground bias introduced by the visual encoder. In general, SynSeg effectively improves the abilities in semantic localization and discrimination under weak supervision. Extensive experiments on benchmarks demonstrate that our method outperforms state-of-the-art (SOTA) performance. For instance, SynSeg achieves higher accuracy than SOTA baselines by 4.5\% on VOC, 8.9\% on Context, 2.6\% on Object and 2.0\% on City.
<div id='section'>Paperid: <span id='pid'>556, <a href='https://arxiv.org/pdf/2508.00563.pdf' target='_blank'>https://arxiv.org/pdf/2508.00563.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hannah Kniesel, Leon Sick, Tristan Payer, Tim Bergner, Kavitha Shaga Devan, Clarissa Read, Paul Walther, Timo Ropinski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00563">Weakly Supervised Virus Capsid Detection with Image-Level Annotations in Electron Microscopy Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current state-of-the-art methods for object detection rely on annotated bounding boxes of large data sets for training. However, obtaining such annotations is expensive and can require up to hundreds of hours of manual labor. This poses a challenge, especially since such annotations can only be provided by experts, as they require knowledge about the scientific domain. To tackle this challenge, we propose a domain-specific weakly supervised object detection algorithm that only relies on image-level annotations, which are significantly easier to acquire. Our method distills the knowledge of a pre-trained model, on the task of predicting the presence or absence of a virus in an image, to obtain a set of pseudo-labels that can be used to later train a state-of-the-art object detection model. To do so, we use an optimization approach with a shrinking receptive field to extract virus particles directly without specific network architectures. Through a set of extensive studies, we show how the proposed pseudo-labels are easier to obtain, and, more importantly, are able to outperform other existing weak labeling methods, and even ground truth labels, in cases where the time to obtain the annotation is limited.
<div id='section'>Paperid: <span id='pid'>557, <a href='https://arxiv.org/pdf/2507.14237.pdf' target='_blank'>https://arxiv.org/pdf/2507.14237.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Louis Bahrman, Mathieu Fontaine, GaÃ«l Richard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14237">U-DREAM: Unsupervised Dereverberation guided by a Reverberation Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper explores the outcome of training state-ofthe-art dereverberation models with supervision settings ranging from weakly-supervised to fully unsupervised, relying solely on reverberant signals and an acoustic model for training. Most of the existing deep learning approaches typically require paired dry and reverberant data, which are difficult to obtain in practice. We develop instead a sequential learning strategy motivated by a bayesian formulation of the dereverberation problem, wherein acoustic parameters and dry signals are estimated from reverberant inputs using deep neural networks, guided by a reverberation matching loss. Our most data-efficient variant requires only 100 reverberation-parameter-labelled samples to outperform an unsupervised baseline, demonstrating the effectiveness and practicality of the proposed method in low-resource scenarios.
<div id='section'>Paperid: <span id='pid'>558, <a href='https://arxiv.org/pdf/2507.02454.pdf' target='_blank'>https://arxiv.org/pdf/2507.02454.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiwei Duan, Luping Ji, Shengjia Chen, Sicheng Zhu, Jianghong Huang, Mao Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02454">Weakly-supervised Contrastive Learning with Quantity Prompts for Moving Infrared Small Target Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Different from general object detection, moving infrared small target detection faces huge challenges due to tiny target size and weak background contrast.Currently, most existing methods are fully-supervised, heavily relying on a large number of manual target-wise annotations. However, manually annotating video sequences is often expensive and time-consuming, especially for low-quality infrared frame images. Inspired by general object detection, non-fully supervised strategies ($e.g.$, weakly supervised) are believed to be potential in reducing annotation requirements. To break through traditional fully-supervised frameworks, as the first exploration work, this paper proposes a new weakly-supervised contrastive learning (WeCoL) scheme, only requires simple target quantity prompts during model training.Specifically, in our scheme, based on the pretrained segment anything model (SAM), a potential target mining strategy is designed to integrate target activation maps and multi-frame energy accumulation.Besides, contrastive learning is adopted to further improve the reliability of pseudo-labels, by calculating the similarity between positive and negative samples in feature subspace.Moreover, we propose a long-short term motion-aware learning scheme to simultaneously model the local motion patterns and global motion trajectory of small targets.The extensive experiments on two public datasets (DAUB and ITSDT-15K) verify that our weakly-supervised scheme could often outperform early fully-supervised methods. Even, its performance could reach over 90\% of state-of-the-art (SOTA) fully-supervised ones.
<div id='section'>Paperid: <span id='pid'>559, <a href='https://arxiv.org/pdf/2507.02393.pdf' target='_blank'>https://arxiv.org/pdf/2507.02393.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seokyeong Lee, Sithu Aung, Junyong Choi, Seungryong Kim, Ig-Jae Kim, Junghyun Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02393">PLOT: Pseudo-Labeling via Video Object Tracking for Scalable Monocular 3D Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monocular 3D object detection (M3OD) has long faced challenges due to data scarcity caused by high annotation costs and inherent 2D-to-3D ambiguity. Although various weakly supervised methods and pseudo-labeling methods have been proposed to address these issues, they are mostly limited by domain-specific learning or rely solely on shape information from a single observation. In this paper, we propose a novel pseudo-labeling framework that uses only video data and is more robust to occlusion, without requiring a multi-view setup, additional sensors, camera poses, or domain-specific training. Specifically, we explore a technique for aggregating the pseudo-LiDARs of both static and dynamic objects across temporally adjacent frames using object point tracking, enabling 3D attribute extraction in scenarios where 3D data acquisition is infeasible. Extensive experiments demonstrate that our method ensures reliable accuracy and strong scalability, making it a practical and effective solution for M3OD.
<div id='section'>Paperid: <span id='pid'>560, <a href='https://arxiv.org/pdf/2506.21883.pdf' target='_blank'>https://arxiv.org/pdf/2506.21883.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Basudha Pal, Sharif Amit Kamran, Brendon Lutnick, Molly Lucas, Chaitanya Parmar, Asha Patel Shah, David Apfel, Steven Fakharzadeh, Lloyd Miller, Gabriela Cula, Kristopher Standish
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21883">GRASP-PsONet: Gradient-based Removal of Spurious Patterns for PsOriasis Severity Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Psoriasis (PsO) severity scoring is important for clinical trials but is hindered by inter-rater variability and the burden of in person clinical evaluation. Remote imaging using patient captured mobile photos offers scalability but introduces challenges, such as variation in lighting, background, and device quality that are often imperceptible to humans but can impact model performance. These factors, along with inconsistencies in dermatologist annotations, reduce the reliability of automated severity scoring. We propose a framework to automatically flag problematic training images that introduce spurious correlations which degrade model generalization, using a gradient based interpretability approach. By tracing the gradients of misclassified validation images, we detect training samples where model errors align with inconsistently rated examples or are affected by subtle, nonclinical artifacts. We apply this method to a ConvNeXT based weakly supervised model designed to classify PsO severity from phone images. Removing 8.2% of flagged images improves model AUC-ROC by 5% (85% to 90%) on a held out test set. Commonly, multiple annotators and an adjudication process ensure annotation accuracy, which is expensive and time consuming. Our method detects training images with annotation inconsistencies, potentially removing the need for manual review. When applied to a subset of training data rated by two dermatologists, the method identifies over 90% of cases with inter-rater disagreement by reviewing only the top 30% of samples. This improves automated scoring for remote assessments, ensuring robustness despite data collection variability.
<div id='section'>Paperid: <span id='pid'>561, <a href='https://arxiv.org/pdf/2506.07652.pdf' target='_blank'>https://arxiv.org/pdf/2506.07652.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hangbei Cheng, Xiaorong Dong, Xueyu Liu, Jianan Zhang, Xuetao Ma, Mingqiang Wei, Liansheng Wang, Junxin Chen, Yongfei Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07652">FMaMIL: Frequency-Driven Mamba Multi-Instance Learning for Weakly Supervised Lesion Segmentation in Medical Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate lesion segmentation in histopathology images is essential for diagnostic interpretation and quantitative analysis, yet it remains challenging due to the limited availability of costly pixel-level annotations. To address this, we propose FMaMIL, a novel two-stage framework for weakly supervised lesion segmentation based solely on image-level labels. In the first stage, a lightweight Mamba-based encoder is introduced to capture long-range dependencies across image patches under the MIL paradigm. To enhance spatial sensitivity and structural awareness, we design a learnable frequency-domain encoding module that supplements spatial-domain features with spectrum-based information. CAMs generated in this stage are used to guide segmentation training. In the second stage, we refine the initial pseudo labels via a CAM-guided soft-label supervision and a self-correction mechanism, enabling robust training even under label noise. Extensive experiments on both public and private histopathology datasets demonstrate that FMaMIL outperforms state-of-the-art weakly supervised methods without relying on pixel-level annotations, validating its effectiveness and potential for digital pathology applications.
<div id='section'>Paperid: <span id='pid'>562, <a href='https://arxiv.org/pdf/2506.03229.pdf' target='_blank'>https://arxiv.org/pdf/2506.03229.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qian-Wei Wang, Yuqiu Xie, Letian Zhang, Zimo Liu, Shu-Tao Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03229">Pre-trained Vision-Language Models Assisted Noisy Partial Label Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the context of noisy partial label learning (NPLL), each training sample is associated with a set of candidate labels annotated by multiple noisy annotators. With the emergence of high-performance pre-trained vision-language models (VLMs) such as CLIP, LLaVa and GPT-4V, the direction of using these models to replace time-consuming manual annotation workflows and achieve "manual-annotation-free" training for downstream tasks has become a highly promising research avenue. This paper focuses on learning from noisy partial labels annotated by pre-trained VLMs and proposes an innovative collaborative consistency regularization (Co-Reg) method. Unlike the symmetric noise primarily addressed in traditional noisy label learning, the noise generated by pre-trained models is instance-dependent, embodying the underlying patterns of the pre-trained models themselves, which significantly increases the learning difficulty for the model. To address this, we simultaneously train two neural networks that implement collaborative purification of training labels through a "Co-Pseudo-Labeling" mechanism, while enforcing consistency regularization constraints in both the label space and feature representation space. Our method can also leverage few-shot manually annotated valid labels to further enhance its performances. Comparative experiments with different denoising and disambiguation algorithms, annotation manners, and pre-trained model application schemes fully validate the effectiveness of the proposed method, while revealing the broad prospects of integrating weakly-supervised learning techniques into the knowledge distillation process of pre-trained models.
<div id='section'>Paperid: <span id='pid'>563, <a href='https://arxiv.org/pdf/2505.23341.pdf' target='_blank'>https://arxiv.org/pdf/2505.23341.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daoxi Cao, Hangbei Cheng, Yijin Li, Ruolin Zhou, Xuehan Zhang, Xinyi Li, Binwei Li, Xuancheng Gu, Jianan Zhang, Xueyu Liu, Yongfei Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23341">DSAGL: Dual-Stream Attention-Guided Learning for Weakly Supervised Whole Slide Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Whole-slide images (WSIs) are critical for cancer diagnosis due to their ultra-high resolution and rich semantic content. However, their massive size and the limited availability of fine-grained annotations pose substantial challenges for conventional supervised learning. We propose DSAGL (Dual-Stream Attention-Guided Learning), a novel weakly supervised classification framework that combines a teacher-student architecture with a dual-stream design. DSAGL explicitly addresses instance-level ambiguity and bag-level semantic consistency by generating multi-scale attention-based pseudo labels and guiding instance-level learning. A shared lightweight encoder (VSSMamba) enables efficient long-range dependency modeling, while a fusion-attentive module (FASA) enhances focus on sparse but diagnostically relevant regions. We further introduce a hybrid loss to enforce mutual consistency between the two streams. Experiments on CIFAR-10, NCT-CRC, and TCGA-Lung datasets demonstrate that DSAGL consistently outperforms state-of-the-art MIL baselines, achieving superior discriminative performance and robustness under weak supervision.
<div id='section'>Paperid: <span id='pid'>564, <a href='https://arxiv.org/pdf/2505.06710.pdf' target='_blank'>https://arxiv.org/pdf/2505.06710.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yicheng Song, Tiancheng Lin, Die Peng, Su Yang, Yi Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06710">SimMIL: A Universal Weakly Supervised Pre-Training Framework for Multi-Instance Learning in Whole Slide Pathology Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Various multi-instance learning (MIL) based approaches have been developed and successfully applied to whole-slide pathological images (WSI). Existing MIL methods emphasize the importance of feature aggregators, but largely neglect the instance-level representation learning. They assume that the availability of a pre-trained feature extractor can be directly utilized or fine-tuned, which is not always the case. This paper proposes to pre-train feature extractor for MIL via a weakly-supervised scheme, i.e., propagating the weak bag-level labels to the corresponding instances for supervised learning. To learn effective features for MIL, we further delve into several key components, including strong data augmentation, a non-linear prediction head and the robust loss function. We conduct experiments on common large-scale WSI datasets and find it achieves better performance than other pre-training schemes (e.g., ImageNet pre-training and self-supervised learning) in different downstream tasks. We further show the compatibility and scalability of the proposed scheme by deploying it in fine-tuning the pathological-specific models and pre-training on merged multiple datasets. To our knowledge, this is the first work focusing on the representation learning for MIL.
<div id='section'>Paperid: <span id='pid'>565, <a href='https://arxiv.org/pdf/2504.05403.pdf' target='_blank'>https://arxiv.org/pdf/2504.05403.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Manahil Raza, Muhammad Dawood, Talha Qaiser, Nasir M. Rajpoot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.05403">A Novel Approach to Linking Histology Images with DNA Methylation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>DNA methylation is an epigenetic mechanism that regulates gene expression by adding methyl groups to DNA. Abnormal methylation patterns can disrupt gene expression and have been linked to cancer development. To quantify DNA methylation, specialized assays are typically used. However, these assays are often costly and have lengthy processing times, which limits their widespread availability in routine clinical practice. In contrast, whole slide images (WSIs) for the majority of cancer patients can be more readily available. As such, given the ready availability of WSIs, there is a compelling need to explore the potential relationship between WSIs and DNA methylation patterns. To address this, we propose an end-to-end graph neural network based weakly supervised learning framework to predict the methylation state of gene groups exhibiting coherent patterns across samples. Using data from three cohorts from The Cancer Genome Atlas (TCGA) - TCGA-LGG (Brain Lower Grade Glioma), TCGA-GBM (Glioblastoma Multiforme) ($n$=729) and TCGA-KIRC (Kidney Renal Clear Cell Carcinoma) ($n$=511) - we demonstrate that the proposed approach achieves significantly higher AUROC scores than the state-of-the-art (SOTA) methods, by more than $20\%$. We conduct gene set enrichment analyses on the gene groups and show that majority of the gene groups are significantly enriched in important hallmarks and pathways. We also generate spatially enriched heatmaps to further investigate links between histological patterns and DNA methylation states. To the best of our knowledge, this is the first study that explores association of spatially resolved histological patterns with gene group methylation states across multiple cancer types using weakly supervised deep learning.
<div id='section'>Paperid: <span id='pid'>566, <a href='https://arxiv.org/pdf/2503.15260.pdf' target='_blank'>https://arxiv.org/pdf/2503.15260.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Shi, Xi Fang, Naiyu Wang, Junxing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15260">DEPT: Deep Extreme Point Tracing for Ultrasound Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic medical image segmentation plays a crucial role in computer aided diagnosis. However, fully supervised learning approaches often require extensive and labor-intensive annotation efforts. To address this challenge, weakly supervised learning methods, particularly those using extreme points as supervisory signals, have the potential to offer an effective solution. In this paper, we introduce Deep Extreme Point Tracing (DEPT) integrated with Feature-Guided Extreme Point Masking (FGEPM) algorithm for ultrasound image segmentation. Notably, our method generates pseudo labels by identifying the lowest-cost path that connects all extreme points on the feature map-based cost matrix. Additionally, an iterative training strategy is proposed to refine pseudo labels progressively, enabling continuous network improvement. Experimental results on two public datasets demonstrate the effectiveness of our proposed method. The performance of our method approaches that of the fully supervised method and outperforms several existing weakly supervised methods.
<div id='section'>Paperid: <span id='pid'>567, <a href='https://arxiv.org/pdf/2502.20249.pdf' target='_blank'>https://arxiv.org/pdf/2502.20249.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pierre Vuillecard, Jean-Marc Odobez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20249">Enhancing 3D Gaze Estimation in the Wild using Weak Supervision with Gaze Following Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate 3D gaze estimation in unconstrained real-world environments remains a significant challenge due to variations in appearance, head pose, occlusion, and the limited availability of in-the-wild 3D gaze datasets. To address these challenges, we introduce a novel Self-Training Weakly-Supervised Gaze Estimation framework (ST-WSGE). This two-stage learning framework leverages diverse 2D gaze datasets, such as gaze-following data, which offer rich variations in appearances, natural scenes, and gaze distributions, and proposes an approach to generate 3D pseudo-labels and enhance model generalization. Furthermore, traditional modality-specific models, designed separately for images or videos, limit the effective use of available training data. To overcome this, we propose the Gaze Transformer (GaT), a modality-agnostic architecture capable of simultaneously learning static and dynamic gaze information from both image and video datasets. By combining 3D video datasets with 2D gaze target labels from gaze following tasks, our approach achieves the following key contributions: (i) Significant state-of-the-art improvements in within-domain and cross-domain generalization on unconstrained benchmarks like Gaze360 and GFIE, with notable cross-modal gains in video gaze estimation; (ii) Superior cross-domain performance on datasets such as MPIIFaceGaze and Gaze360 compared to frontal face methods. Code and pre-trained models will be released to the community.
<div id='section'>Paperid: <span id='pid'>568, <a href='https://arxiv.org/pdf/2502.11743.pdf' target='_blank'>https://arxiv.org/pdf/2502.11743.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tobias Fuchs, Florian Kalinke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.11743">Robust Partial-Label Learning by Leveraging Class Activation Values</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-world training data is often noisy; for example, human annotators assign conflicting class labels to the same instances. Partial-label learning (PLL) is a weakly supervised learning paradigm that allows training classifiers in this context without manual data cleaning. While state-of-the-art methods have good predictive performance, their predictions are sensitive to high noise levels, out-of-distribution data, and adversarial perturbations. We propose a novel PLL method based on subjective logic, which explicitly represents uncertainty by leveraging the magnitudes of the underlying neural network's class activation values. Thereby, we effectively incorporate prior knowledge about the class labels by using a novel label weight re-distribution strategy that we prove to be optimal. We empirically show that our method yields more robust predictions in terms of predictive performance under high PLL noise levels, handling out-of-distribution examples, and handling adversarial perturbations on the test instances.
<div id='section'>Paperid: <span id='pid'>569, <a href='https://arxiv.org/pdf/2502.06839.pdf' target='_blank'>https://arxiv.org/pdf/2502.06839.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Louis Bahrman, Mathieu Fontaine, Gael Richard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.06839">A Hybrid Model for Weakly-Supervised Speech Dereverberation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a new training strategy to improve speech dereverberation systems using minimal acoustic information and reverberant (wet) speech. Most existing algorithms rely on paired dry/wet data, which is difficult to obtain, or on target metrics that may not adequately capture reverberation characteristics and can lead to poor results on non-target metrics. Our approach uses limited acoustic information, like the reverberation time (RT60), to train a dereverberation system. The system's output is resynthesized using a generated room impulse response and compared with the original reverberant speech, providing a novel reverberation matching loss replacing the standard target metrics. During inference, only the trained dereverberation model is used. Experimental results demonstrate that our method achieves more consistent performance across various objective metrics used in speech dereverberation than the state-of-the-art.
<div id='section'>Paperid: <span id='pid'>570, <a href='https://arxiv.org/pdf/2412.20455.pdf' target='_blank'>https://arxiv.org/pdf/2412.20455.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ayush Ghadiya, Purbayan Kar, Vishal Chudasama, Pankaj Wasnik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20455">Cross-Modal Fusion and Attention Mechanism for Weakly Supervised Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, weakly supervised video anomaly detection (WS-VAD) has emerged as a contemporary research direction to identify anomaly events like violence and nudity in videos using only video-level labels. However, this task has substantial challenges, including addressing imbalanced modality information and consistently distinguishing between normal and abnormal features. In this paper, we address these challenges and propose a multi-modal WS-VAD framework to accurately detect anomalies such as violence and nudity. Within the proposed framework, we introduce a new fusion mechanism known as the Cross-modal Fusion Adapter (CFA), which dynamically selects and enhances highly relevant audio-visual features in relation to the visual modality. Additionally, we introduce a Hyperbolic Lorentzian Graph Attention (HLGAtt) to effectively capture the hierarchical relationships between normal and abnormal representations, thereby enhancing feature separation accuracy. Through extensive experiments, we demonstrate that the proposed model achieves state-of-the-art results on benchmark datasets of violence and nudity detection.
<div id='section'>Paperid: <span id='pid'>571, <a href='https://arxiv.org/pdf/2412.20201.pdf' target='_blank'>https://arxiv.org/pdf/2412.20201.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wen-Dong Jiang, Chih-Yung Chang, Hsiang-Chuan Chang, Ji-Yuan Chen, Diptendu Sinha Roy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20201">Injecting Explainability and Lightweight Design into Weakly Supervised Video Anomaly Detection Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Monitoring Anomaly Detection (WSMAD) utilizes weak supervision learning to identify anomalies, a critical task for smart city monitoring. However, existing multimodal approaches often fail to meet the real-time and interpretability requirements of edge devices due to their complexity. This paper presents TCVADS (Two-stage Cross-modal Video Anomaly Detection System), which leverages knowledge distillation and cross-modal contrastive learning to enable efficient, accurate, and interpretable anomaly detection on edge devices.TCVADS operates in two stages: coarse-grained rapid classification and fine-grained detailed analysis. In the first stage, TCVADS extracts features from video frames and inputs them into a time series analysis module, which acts as the teacher model. Insights are then transferred via knowledge distillation to a simplified convolutional network (student model) for binary classification. Upon detecting an anomaly, the second stage is triggered, employing a fine-grained multi-class classification model. This stage uses CLIP for cross-modal contrastive learning with text and images, enhancing interpretability and achieving refined classification through specially designed triplet textual relationships. Experimental results demonstrate that TCVADS significantly outperforms existing methods in model performance, detection efficiency, and interpretability, offering valuable contributions to smart city monitoring applications.
<div id='section'>Paperid: <span id='pid'>572, <a href='https://arxiv.org/pdf/2412.19504.pdf' target='_blank'>https://arxiv.org/pdf/2412.19504.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Li, Bo Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19504">Hear the Scene: Audio-Enhanced Text Spotting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in scene text spotting have focused on end-to-end methodologies that heavily rely on precise location annotations, which are often costly and labor-intensive to procure. In this study, we introduce an innovative approach that leverages only transcription annotations for training text spotting models, substantially reducing the dependency on elaborate annotation processes. Our methodology employs a query-based paradigm that facilitates the learning of implicit location features through the interaction between text queries and image embeddings. These features are later refined during the text recognition phase using an attention activation map. Addressing the challenges associated with training a weakly-supervised model from scratch, we implement a circular curriculum learning strategy to enhance model convergence. Additionally, we introduce a coarse-to-fine cross-attention localization mechanism for more accurate text instance localization. Notably, our framework supports audio-based annotation, which significantly diminishes annotation time and provides an inclusive alternative for individuals with disabilities. Our approach achieves competitive performance against existing benchmarks, demonstrating that high accuracy in text spotting can be attained without extensive location annotations.
<div id='section'>Paperid: <span id='pid'>573, <a href='https://arxiv.org/pdf/2411.12276.pdf' target='_blank'>https://arxiv.org/pdf/2411.12276.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nai-Xuan Ye, Tan-Ha Mai, Hsiu-Hsuan Wang, Wei-I Lin, Hsuan-Tien Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.12276">libcll: an Extendable Python Toolkit for Complementary-Label Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Complementary-label learning (CLL) is a weakly supervised learning paradigm for multiclass classification, where only complementary labels -- indicating classes an instance does not belong to -- are provided to the learning algorithm. Despite CLL's increasing popularity, previous studies highlight two main challenges: (1) inconsistent results arising from varied assumptions on complementary label generation, and (2) high barriers to entry due to the lack of a standardized evaluation platform across datasets and algorithms. To address these challenges, we introduce \texttt{libcll}, an extensible Python toolkit for CLL research. \texttt{libcll} provides a universal interface that supports a wide range of generation assumptions, both synthetic and real-world datasets, and key CLL algorithms. The toolkit is designed to mitigate inconsistencies and streamline the research process, with easy installation, comprehensive usage guides, and quickstart tutorials that facilitate efficient adoption and implementation of CLL techniques. Extensive ablation studies conducted with \texttt{libcll} demonstrate its utility in generating valuable insights to advance future CLL research.
<div id='section'>Paperid: <span id='pid'>574, <a href='https://arxiv.org/pdf/2410.21991.pdf' target='_blank'>https://arxiv.org/pdf/2410.21991.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wen-Dong Jiang, Chih-Yung Chang, Ssu-Chi Kuai, Diptendu Sinha Roy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.21991">A Lightweight Dual-Branch System for Weakly-Supervised Video Anomaly Detection on Consumer Edge Devices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing demand for intelligent security in consumer electronics, such as smart home cameras and personal monitoring systems, is often hindered by the high computational cost and large model sizes of advanced AI. These limitations prevent the effective deployment of real-time Video Anomaly Detection (VAD) on resource-constrained edge devices. To bridge this gap, this paper introduces Rule-based Video Anomaly Detection (RuleVAD), a novel, lightweight system engineered for high-efficiency and low-complexity threat detection directly on consumer hardware. RuleVAD features an innovative decoupled dual-branch architecture to minimize computational load. An implicit branch uses visual features for rapid, coarse-grained binary classification, efficiently filtering out normal activity to avoid unnecessary processing. For potentially anomalous or complex events, a multimodal explicit branch takes over. This branch leverages YOLO-World to detect objects and applies data mining to generate interpretable, text-based association rules from the scene. By aligning these rules with visual data, RuleVAD achieves a more nuanced, fine-grained classification, significantly reducing the false alarms common in vision-only systems. Extensive experiments on the XD-Violence and UCF-Crime benchmark datasets show that RuleVAD achieves superior performance, surpassing existing state-of-the-art methods in both accuracy and speed. Crucially, the entire system is optimized for low-power operation and is fully deployable on an NVIDIA Jetson Nano board, demonstrating its practical feasibility for bringing advanced, real-time security monitoring to everyday consumer electronic devices.
<div id='section'>Paperid: <span id='pid'>575, <a href='https://arxiv.org/pdf/2410.15051.pdf' target='_blank'>https://arxiv.org/pdf/2410.15051.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vittorio Torri, Elisa Barbieri, Anna Cantarutti, Carlo Giaquinto, Francesca Ieva
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15051">Weakly-supervised diagnosis identification from Italian discharge letters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Objective: Recognizing diseases from discharge letters is crucial for cohort selection and epidemiological analyses, as this is the only type of data consistently produced across hospitals. This is a classic document classification problem, typically requiring supervised learning. However, manual annotation of large datasets of discharge letters is uncommon since it is extremely time-consuming. We propose a novel weakly-supervised pipeline to recognize diseases from Italian discharge letters. Methods: Our Natural Language Processing pipeline is based on a fine-tuned version of the Italian Umberto model. The pipeline extracts diagnosis-related sentences from a subset of letters and applies a two-level clustering using the embeddings generated by the fine-tuned Umberto model. These clusters are summarized and those mapped to the diseases of interest are selected as weak labels. Finally, the same BERT-based model is trained using these weak labels to detect the targeted diseases. Results: A case study related to the identification of bronchiolitis with 33'176 Italian discharge letters from 44 hospitals in the Veneto Region shows the potential of our method, with an AUC of 77.7 % and an F1-Score of 75.1 % on manually annotated labels, improving compared to other non-supervised methods and with a limited loss compared to fully supervised methods. Results are robust to the cluster selection and the identified clusters highlight the potential to recognize a variety of diseases. Conclusions: This study demonstrates the feasibility of diagnosis identification from Italian discharge letters in the absence of labelled data. Our pipeline showed strong performance and robustness, and its flexibility allows for easy adaptation to various diseases. This approach offers a scalable solution for clinical text classification, reducing the need for manual annotation while maintaining good accuracy.
<div id='section'>Paperid: <span id='pid'>576, <a href='https://arxiv.org/pdf/2410.00536.pdf' target='_blank'>https://arxiv.org/pdf/2410.00536.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Krishna Chaitanya, Pablo F. Damasceno, Shreyas Fadnavis, Pooya Mobadersany, Chaitanya Parmar, Emily Scherer, Natalia Zemlianskaia, Lindsey Surace, Louis R. Ghanem, Oana Gabriela Cula, Tommaso Mansi, Kristopher Standish
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.00536">Arges: Spatio-Temporal Transformer for Ulcerative Colitis Severity Assessment in Endoscopy Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate assessment of disease severity from endoscopy videos in ulcerative colitis (UC) is crucial for evaluating drug efficacy in clinical trials. Severity is often measured by the Mayo Endoscopic Subscore (MES) and Ulcerative Colitis Endoscopic Index of Severity (UCEIS) score. However, expert MES/UCEIS annotation is time-consuming and susceptible to inter-rater variability, factors addressable by automation. Automation attempts with frame-level labels face challenges in fully-supervised solutions due to the prevalence of video-level labels in clinical trials. CNN-based weakly-supervised models (WSL) with end-to-end (e2e) training lack generalization to new disease scores and ignore spatio-temporal information crucial for accurate scoring. To address these limitations, we propose "Arges", a deep learning framework that utilizes a transformer with positional encoding to incorporate spatio-temporal information from frame features to estimate disease severity scores in endoscopy video. Extracted features are derived from a foundation model (ArgesFM), pre-trained on a large diverse dataset from multiple clinical trials (61M frames, 3927 videos). We evaluate four UC disease severity scores, including MES and three UCEIS component scores. Test set evaluation indicates significant improvements, with F1 scores increasing by 4.1% for MES and 18.8%, 6.6%, 3.8% for the three UCEIS component scores compared to state-of-the-art methods. Prospective validation on previously unseen clinical trial data further demonstrates the model's successful generalization.
<div id='section'>Paperid: <span id='pid'>577, <a href='https://arxiv.org/pdf/2408.08444.pdf' target='_blank'>https://arxiv.org/pdf/2408.08444.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinming Nian, Zhiyuan Peng, Qifan Wang, Yi Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.08444">W-RAG: Weakly Supervised Dense Retrieval in RAG for Open-domain Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In knowledge-intensive tasks such as open-domain question answering (OpenQA), large language models (LLMs) often struggle to generate factual answers, relying solely on their internal (parametric) knowledge. To address this limitation, Retrieval-Augmented Generation (RAG) systems enhance LLMs by retrieving relevant information from external sources, thereby positioning the retriever as a pivotal component. Although dense retrieval demonstrates state-of-the-art performance, its training poses challenges due to the scarcity of ground-truth evidence, largely attributed to the high costs of human annotation. In this paper, we propose W-RAG, a method that draws weak training signals from the downstream task (such as OpenQA) of an LLM, and fine-tunes the retriever to prioritize passages that most benefit the task. Specifically, we rerank the top-$k$ passages retrieved via BM25 by assessing the probability that the LLM will generate the correct answer for a question given each passage. The highest-ranking passages are then used as positive fine-tuning examples for dense retrieval. We conduct comprehensive experiments across four publicly available OpenQA datasets to demonstrate that our approach enhances both retrieval and OpenQA performance compared to baseline models, achieving results comparable to models fine-tuned with human-labeled data.
<div id='section'>Paperid: <span id='pid'>578, <a href='https://arxiv.org/pdf/2407.21384.pdf' target='_blank'>https://arxiv.org/pdf/2407.21384.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanxu Mao, Xiaohui Chen, Peipei Liu, Tiehan Cui, Zuhui Yue, Zheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.21384">GEGA: Graph Convolutional Networks and Evidence Retrieval Guided Attention for Enhanced Document-level Relation Extraction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Document-level relation extraction (DocRE) aims to extract relations between entities from unstructured document text. Compared to sentence-level relation extraction, it requires more complex semantic understanding from a broader text context. Currently, some studies are utilizing logical rules within evidence sentences to enhance the performance of DocRE. However, in the data without provided evidence sentences, researchers often obtain a list of evidence sentences for the entire document through evidence retrieval (ER). Therefore, DocRE suffers from two challenges: firstly, the relevance between evidence and entity pairs is weak; secondly, there is insufficient extraction of complex cross-relations between long-distance multi-entities. To overcome these challenges, we propose GEGA, a novel model for DocRE. The model leverages graph neural networks to construct multiple weight matrices, guiding attention allocation to evidence sentences. It also employs multi-scale representation aggregation to enhance ER. Subsequently, we integrate the most efficient evidence information to implement both fully supervised and weakly supervised training processes for the model. We evaluate the GEGA model on three widely used benchmark datasets: DocRED, Re-DocRED, and Revisit-DocRED. The experimental results indicate that our model has achieved comprehensive improvements compared to the existing SOTA model.
<div id='section'>Paperid: <span id='pid'>579, <a href='https://arxiv.org/pdf/2407.13553.pdf' target='_blank'>https://arxiv.org/pdf/2407.13553.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingyue Zhao, Peiqi Li, Xiangde Luo, Meng Yang, Shi Chang, Zhongyu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.13553">SAM-Driven Weakly Supervised Nodule Segmentation with Uncertainty-Aware Cross Teaching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automated nodule segmentation is essential for computer-assisted diagnosis in ultrasound images. Nevertheless, most existing methods depend on precise pixel-level annotations by medical professionals, a process that is both costly and labor-intensive. Recently, segmentation foundation models like SAM have shown impressive generalizability on natural images, suggesting their potential as pseudo-labelers. However, accurate prompts remain crucial for their success in medical images. In this work, we devise a novel weakly supervised framework that effectively utilizes the segmentation foundation model to generate pseudo-labels from aspect ration annotations for automatic nodule segmentation. Specifically, we develop three types of bounding box prompts based on scalable shape priors, followed by an adaptive pseudo-label selection module to fully exploit the prediction capabilities of the foundation model for nodules. We also present a SAM-driven uncertainty-aware cross-teaching strategy. This approach integrates SAM-based uncertainty estimation and label-space perturbations into cross-teaching to mitigate the impact of pseudo-label inaccuracies on model training. Extensive experiments on two clinically collected ultrasound datasets demonstrate the superior performance of our proposed method.
<div id='section'>Paperid: <span id='pid'>580, <a href='https://arxiv.org/pdf/2407.13363.pdf' target='_blank'>https://arxiv.org/pdf/2407.13363.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chang Liu, Giulia Rizzoli, Pietro Zanuttigh, Fu Li, Yi Niu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.13363">Learning from the Web: Language Drives Weakly-Supervised Incremental Learning for Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current weakly-supervised incremental learning for semantic segmentation (WILSS) approaches only consider replacing pixel-level annotations with image-level labels, while the training images are still from well-designed datasets. In this work, we argue that widely available web images can also be considered for the learning of new classes. To achieve this, firstly we introduce a strategy to select web images which are similar to previously seen examples in the latent space using a Fourier-based domain discriminator. Then, an effective caption-driven reharsal strategy is proposed to preserve previously learnt classes. To our knowledge, this is the first work to rely solely on web images for both the learning of new concepts and the preservation of the already learned ones in WILSS. Experimental results show that the proposed approach can reach state-of-the-art performances without using manually selected and annotated data in the incremental steps.
<div id='section'>Paperid: <span id='pid'>581, <a href='https://arxiv.org/pdf/2407.10131.pdf' target='_blank'>https://arxiv.org/pdf/2407.10131.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinjian Wu, Ruisong Zhang, Jie Qin, Shijie Ma, Cheng-Lin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10131">WPS-SAM: Towards Weakly-Supervised Part Segmentation with Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Segmenting and recognizing diverse object parts is crucial in computer vision and robotics. Despite significant progress in object segmentation, part-level segmentation remains underexplored due to complex boundaries and scarce annotated data. To address this, we propose a novel Weakly-supervised Part Segmentation (WPS) setting and an approach called WPS-SAM, built on the large-scale pre-trained vision foundation model, Segment Anything Model (SAM). WPS-SAM is an end-to-end framework designed to extract prompt tokens directly from images and perform pixel-level segmentation of part regions. During its training phase, it only uses weakly supervised labels in the form of bounding boxes or points. Extensive experiments demonstrate that, through exploiting the rich knowledge embedded in pre-trained foundation models, WPS-SAM outperforms other segmentation models trained with pixel-level strong annotations. Specifically, WPS-SAM achieves 68.93% mIOU and 79.53% mACC on the PartImageNet dataset, surpassing state-of-the-art fully supervised methods by approximately 4% in terms of mIOU.
<div id='section'>Paperid: <span id='pid'>582, <a href='https://arxiv.org/pdf/2406.18550.pdf' target='_blank'>https://arxiv.org/pdf/2406.18550.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qian-Wei Wang, Yuqiu Xie, Letian Zhang, Zimo Liu, Shu-Tao Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.18550">Pre-Trained Vision-Language Models as Partial Annotators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pre-trained vision-language models learn massive data to model unified representations of images and natural languages, which can be widely applied to downstream machine learning tasks. In addition to zero-shot inference, in order to better adapt pre-trained models to the requirements of downstream tasks, people usually use methods such as few-shot or parameter-efficient fine-tuning and knowledge distillation. However, annotating samples is laborious, while a large number of unlabeled samples can be easily obtained. In this paper, we investigate a novel "pre-trained annotating - weakly-supervised learning" paradigm for pre-trained model application and experiment on image classification tasks. Specifically, based on CLIP, we annotate image samples with multiple prompt templates to obtain multiple candidate labels to form the noisy partial label dataset, and design a collaborative consistency regularization algorithm to solve this problem. Our method simultaneously trains two neural networks, which collaboratively purify training labels for each other and obtain pseudo-labels for self-training, while adopting prototypical similarity alignment and noisy supervised contrastive learning to optimize model representation. In experiments, our method achieves performances far beyond zero-shot inference without introducing additional label information, and outperforms other weakly supervised learning and few-shot fine-tuning methods, and obtains smaller deployed models. Our code is available at: \url{https://anonymous.4open.science/r/Co-Reg-8CF9}.
<div id='section'>Paperid: <span id='pid'>583, <a href='https://arxiv.org/pdf/2405.07655.pdf' target='_blank'>https://arxiv.org/pdf/2405.07655.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liuxin Bao, Xiaofei Zhou, Xiankai Lu, Yaoqi Sun, Haibing Yin, Zhenghui Hu, Jiyong Zhang, Chenggang Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.07655">Quality-aware Selective Fusion Network for V-D-T Salient Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Depth images and thermal images contain the spatial geometry information and surface temperature information, which can act as complementary information for the RGB modality. However, the quality of the depth and thermal images is often unreliable in some challenging scenarios, which will result in the performance degradation of the two-modal based salient object detection (SOD). Meanwhile, some researchers pay attention to the triple-modal SOD task, where they attempt to explore the complementarity of the RGB image, the depth image, and the thermal image. However, existing triple-modal SOD methods fail to perceive the quality of depth maps and thermal images, which leads to performance degradation when dealing with scenes with low-quality depth and thermal images. Therefore, we propose a quality-aware selective fusion network (QSF-Net) to conduct VDT salient object detection, which contains three subnets including the initial feature extraction subnet, the quality-aware region selection subnet, and the region-guided selective fusion subnet. Firstly, except for extracting features, the initial feature extraction subnet can generate a preliminary prediction map from each modality via a shrinkage pyramid architecture. Then, we design the weakly-supervised quality-aware region selection subnet to generate the quality-aware maps. Concretely, we first find the high-quality and low-quality regions by using the preliminary predictions, which further constitute the pseudo label that can be used to train this subnet. Finally, the region-guided selective fusion subnet purifies the initial features under the guidance of the quality-aware maps, and then fuses the triple-modal features and refines the edge details of prediction maps through the intra-modality and inter-modality attention (IIA) module and the edge refinement (ER) module, respectively. Extensive experiments are performed on VDT-2048
<div id='section'>Paperid: <span id='pid'>584, <a href='https://arxiv.org/pdf/2404.15683.pdf' target='_blank'>https://arxiv.org/pdf/2404.15683.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Che, Fazle Rafsani, Jay Shah, Md Mahfuzur Rahman Siddiquee, Teresa Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.15683">AnoFPDM: Anomaly Segmentation with Forward Process of Diffusion Models for Brain MRI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised diffusion models (DMs) in anomaly segmentation, leveraging image-level labels, have attracted significant attention for their superior performance compared to unsupervised methods. It eliminates the need for pixel-level labels in training, offering a more cost-effective alternative to supervised methods. However, existing methods are not fully weakly-supervised because they heavily rely on costly pixel-level labels for hyperparameter tuning in inference. To tackle this challenge, we introduce Anomaly Segmentation with Forward Process of Diffusion Models (AnoFPDM), a fully weakly-supervised framework that operates without the need of pixel-level labels. Leveraging the unguided forward process as a reference for the guided forward process, we select hyperparameters such as the noise scale, the threshold for segmentation and the guidance strength. We aggregate anomaly maps from guided forward process, enhancing the signal strength of anomalous regions. Remarkably, our proposed method outperforms recent state-of-the-art weakly-supervised approaches, even without utilizing pixel-level labels.
<div id='section'>Paperid: <span id='pid'>585, <a href='https://arxiv.org/pdf/2404.12861.pdf' target='_blank'>https://arxiv.org/pdf/2404.12861.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yilong Chen, Zongyi Xu, xiaoshui Huang, Ruicheng Zhang, Xinqi Jiang, Xinbo Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.12861">Weakly Supervised LiDAR Semantic Segmentation via Scatter Image Annotation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised LiDAR semantic segmentation has made significant strides with limited labeled data. However, most existing methods focus on the network training under weak supervision, while efficient annotation strategies remain largely unexplored. To tackle this gap, we implement LiDAR semantic segmentation using scatter image annotation, effectively integrating an efficient annotation strategy with network training. Specifically, we propose employing scatter images to annotate LiDAR point clouds, combining a pre-trained optical flow estimation network with a foundation image segmentation model to rapidly propagate manual annotations into dense labels for both images and point clouds. Moreover, we propose ScatterNet, a network that includes three pivotal strategies to reduce the performance gap caused by such annotations. Firstly, it utilizes dense semantic labels as supervision for the image branch, alleviating the modality imbalance between point clouds and images. Secondly, an intermediate fusion branch is proposed to obtain multimodal texture and structural features. Lastly, a perception consistency loss is introduced to determine which information needs to be fused and which needs to be discarded during the fusion process. Extensive experiments on the nuScenes and SemanticKITTI datasets have demonstrated that our method requires less than 0.02% of the labeled points to achieve over 95% of the performance of fully-supervised methods. Notably, our labeled points are only 5% of those used in the most advanced weakly supervised methods.
<div id='section'>Paperid: <span id='pid'>586, <a href='https://arxiv.org/pdf/2404.05022.pdf' target='_blank'>https://arxiv.org/pdf/2404.05022.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Valentin Koch, Sophia J. Wagner, Salome Kazeminia, Ece Sancar, Matthias Hehr, Julia Schnabel, Tingying Peng, Carsten Marr
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.05022">DinoBloom: A Foundation Model for Generalizable Cell Embeddings in Hematology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In hematology, computational models offer significant potential to improve diagnostic accuracy, streamline workflows, and reduce the tedious work of analyzing single cells in peripheral blood or bone marrow smears. However, clinical adoption of computational models has been hampered by the lack of generalization due to large batch effects, small dataset sizes, and poor performance in transfer learning from natural images. To address these challenges, we introduce DinoBloom, the first foundation model for single cell images in hematology, utilizing a tailored DINOv2 pipeline. Our model is built upon an extensive collection of 13 diverse, publicly available datasets of peripheral blood and bone marrow smears, the most substantial open-source cohort in hematology so far, comprising over 380,000 white blood cell images. To assess its generalization capability, we evaluate it on an external dataset with a challenging domain shift. We show that our model outperforms existing medical and non-medical vision models in (i) linear probing and k-nearest neighbor evaluations for cell-type classification on blood and bone marrow smears and (ii) weakly supervised multiple instance learning for acute myeloid leukemia subtyping by a large margin. A family of four DinoBloom models (small, base, large, and giant) can be adapted for a wide range of downstream applications, be a strong baseline for classification problems, and facilitate the assessment of batch effects in new datasets. All models are available at github.com/marrlab/DinoBloom.
<div id='section'>Paperid: <span id='pid'>587, <a href='https://arxiv.org/pdf/2404.01740.pdf' target='_blank'>https://arxiv.org/pdf/2404.01740.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tanvir Mahmud, Saeed Amizadeh, Kazuhito Koishida, Diana Marculescu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01740">Weakly-supervised Audio Separation via Bi-modal Semantic Similarity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conditional sound separation in multi-source audio mixtures without having access to single source sound data during training is a long standing challenge. Existing mix-and-separate based methods suffer from significant performance drop with multi-source training mixtures due to the lack of supervision signal for single source separation cases during training. However, in the case of language-conditional audio separation, we do have access to corresponding text descriptions for each audio mixture in our training data, which can be seen as (rough) representations of the audio samples in the language modality. To this end, in this paper, we propose a generic bi-modal separation framework which can enhance the existing unsupervised frameworks to separate single-source signals in a target modality (i.e., audio) using the easily separable corresponding signals in the conditioning modality (i.e., language), without having access to single-source samples in the target modality during training. We empirically show that this is well within reach if we have access to a pretrained joint embedding model between the two modalities (i.e., CLAP). Furthermore, we propose to incorporate our framework into two fundamental scenarios to enhance separation performance. First, we show that our proposed methodology significantly improves the performance of purely unsupervised baselines by reducing the distribution shift between training and test samples. In particular, we show that our framework can achieve 71% boost in terms of Signal-to-Distortion Ratio (SDR) over the baseline, reaching 97.5% of the supervised learning performance. Second, we show that we can further improve the performance of the supervised learning itself by 17% if we augment it by our proposed weakly-supervised framework, that enables a powerful semi-supervised framework for audio separation.
<div id='section'>Paperid: <span id='pid'>588, <a href='https://arxiv.org/pdf/2403.19786.pdf' target='_blank'>https://arxiv.org/pdf/2403.19786.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingxing Rao, Yinhong Qin, Soheil Kolouri, Jie Ying Wu, Daniel Moyer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.19786">Zero-shot Prompt-based Video Encoder for Surgical Gesture Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Purpose: In order to produce a surgical gesture recognition system that can support a wide variety of procedures, either a very large annotated dataset must be acquired, or fitted models must generalize to new labels (so called "zero-shot" capability). In this paper we investigate the feasibility of latter option. Methods: Leveraging the Bridge-Prompt framework, we prompt-tune a pre-trained vision-text model (CLIP) for gesture recognition in surgical videos. This can utilize extensive outside video data such as text, but also make use of label meta-data and weakly supervised contrastive losses. Results: Our experiments show that prompt-based video encoder outperforms standard encoders in surgical gesture recognition tasks. Notably, it displays strong performance in zero-shot scenarios, where gestures/tasks that were not provided during the encoder training phase are included in the prediction phase. Additionally, we measure the benefit of inclusion text descriptions in the feature extractor training schema. Conclusion Bridge-Prompt and similar pre-trained+prompt-tuned video encoder models present significant visual representation for surgical robotics, especially in gesture recognition tasks. Given the diverse range of surgical tasks (gestures), the ability of these models to zero-shot transfer without the need for any task (gesture) specific retraining makes them invaluable.
<div id='section'>Paperid: <span id='pid'>589, <a href='https://arxiv.org/pdf/2403.14390.pdf' target='_blank'>https://arxiv.org/pdf/2403.14390.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingwen Lin, Boyan Xu, Zhengting Huang, Ruichu Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.14390">From Large to Tiny: Distilling and Refining Mathematical Expertise for Math Word Problems with Weakly Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Addressing the challenge of high annotation costs in solving Math Word Problems (MWPs) through full supervision with intermediate equations, recent works have proposed weakly supervised task settings that rely solely on the final answer as a supervised signal. Existing leading approaches typically employ various search techniques to infer intermediate equations, but cannot ensure their semantic consistency with natural language descriptions. The rise of Large Language Models (LLMs) like ChatGPT has opened up new possibilities for addressing MWPs directly. However, the computational demands of LLMs make them less than ideal for use in settings where resources are tight. In light of these challenges, we introduce an innovative two-stage framework that adeptly transfers mathematical Expertise from large to tiny language models. In \emph{Distillation Stage}, we propose a series of extraction processes that satisfy the properties of MWPs to distill mathematical knowledge from LLMs to construct problem-equation pairs required for supervised training. In \emph{Refinement Stage}, Due to Knowledge distilling method cannot guarantee the full utilization of all data, we further utilize the unsuccessfully searched data effectively by Knowledge Refine method. Finally, We train a small model using distilled data generated through two-stage methods. As our method fully leverages the semantic understanding capabilities during the searching 'problem-equation' pair, it demonstrates significantly improved performance on the Math23K and Weak12K datasets compared to existing small model methods, while maintaining a much lower computational cost than ChatGPT.
<div id='section'>Paperid: <span id='pid'>590, <a href='https://arxiv.org/pdf/2403.09315.pdf' target='_blank'>https://arxiv.org/pdf/2403.09315.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Xiong, Churan Wang, Wenxue Li, Guanbin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09315">Semi- and Weakly-Supervised Learning for Mammogram Mass Segmentation with Limited Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate identification of breast masses is crucial in diagnosing breast cancer; however, it can be challenging due to their small size and being camouflaged in surrounding normal glands. Worse still, it is also expensive in clinical practice to obtain adequate pixel-wise annotations for training deep neural networks. To overcome these two difficulties with one stone, we propose a semi- and weakly-supervised learning framework for mass segmentation that utilizes limited strongly-labeled samples and sufficient weakly-labeled samples to achieve satisfactory performance. The framework consists of an auxiliary branch to exclude lesion-irrelevant background areas, a segmentation branch for final prediction, and a spatial prompting module to integrate the complementary information of the two branches. We further disentangle encoded obscure features into lesion-related and others to boost performance. Experiments on CBIS-DDSM and INbreast datasets demonstrate the effectiveness of our method.
<div id='section'>Paperid: <span id='pid'>591, <a href='https://arxiv.org/pdf/2402.00592.pdf' target='_blank'>https://arxiv.org/pdf/2402.00592.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tobias Fuchs, Florian Kalinke, Klemens BÃ¶hm
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.00592">Partial-Label Learning with a Reject Option</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In real-world applications, one often encounters ambiguously labeled data, where different annotators assign conflicting class labels. Partial-label learning allows training classifiers in this weakly supervised setting, where state-of-the-art methods already show good predictive performance. However, even the best algorithms give incorrect predictions, which can have severe consequences when they impact actions or decisions. We propose a novel risk-consistent nearest-neighbor-based partial-label learning algorithm with a reject option, that is, the algorithm can reject unsure predictions. Extensive experiments on artificial and real-world datasets show that our method provides the best trade-off between the number and accuracy of non-rejected predictions when compared to our competitors, which use confidence thresholds for rejecting unsure predictions. When evaluated without the reject option, our nearest-neighbor-based approach also achieves competitive prediction performance.
<div id='section'>Paperid: <span id='pid'>592, <a href='https://arxiv.org/pdf/2401.13551.pdf' target='_blank'>https://arxiv.org/pdf/2401.13551.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongwei Nie, Hao Huang, Chengjiang Long, Qing Zhang, Pradipta Maji, Hongmin Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.13551">Interleaving One-Class and Weakly-Supervised Models with Adaptive Thresholding for Unsupervised Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Anomaly Detection (VAD) has been extensively studied under the settings of One-Class Classification (OCC) and Weakly-Supervised learning (WS), which however both require laborious human-annotated normal/abnormal labels. In this paper, we study Unsupervised VAD (UVAD) that does not depend on any label by combining OCC and WS into a unified training framework. Specifically, we extend OCC to weighted OCC (wOCC) and propose a wOCC-WS interleaving training module, where the two models automatically generate pseudo-labels for each other. We face two challenges to make the combination effective: (1) Models' performance fluctuates occasionally during the training process due to the inevitable randomness of the pseudo labels. (2) Thresholds are needed to divide pseudo labels, making the training depend on the accuracy of user intervention. For the first problem, we propose to use wOCC requiring soft labels instead of OCC trained with hard zero/one labels, as soft labels exhibit high consistency throughout different training cycles while hard labels are prone to sudden changes. For the second problem, we repeat the interleaving training module multiple times, during which we propose an adaptive thresholding strategy that can progressively refine a rough threshold to a relatively optimal threshold, which reduces the influence of user interaction. A benefit of employing OCC and WS methods to compose a UVAD method is that we can incorporate the most recent OCC or WS model into our framework. Experiments demonstrate the effectiveness of the proposed UVAD framework.
<div id='section'>Paperid: <span id='pid'>593, <a href='https://arxiv.org/pdf/2401.10711.pdf' target='_blank'>https://arxiv.org/pdf/2401.10711.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haibo Wang, Chenghang Lai, Yixuan Sun, Weifeng Ge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.10711">Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal Models for Video Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Question Answering (VideoQA) aims to answer natural language questions based on the information observed in videos. Despite the recent success of Large Multimodal Models (LMMs) in image-language understanding and reasoning, they deal with VideoQA insufficiently, by simply taking uniformly sampled frames as visual inputs, which ignores question-relevant visual clues. Moreover, there are no human annotations for question-critical timestamps in existing VideoQA datasets. In light of this, we propose a novel weakly supervised framework to enforce the LMMs to reason out the answers with question-critical moments as visual inputs. Specifically, we first fuse the question and answer pairs as event descriptions to find multiple keyframes as target moments and pseudo-labels, with the visual-language alignment capability of the CLIP models. With these pseudo-labeled keyframes as additionally weak supervision, we devise a lightweight Gaussian-based Contrastive Grounding (GCG) module. GCG learns multiple Gaussian functions to characterize the temporal structure of the video, and sample question-critical frames as positive moments to be the visual inputs of LMMs. Extensive experiments on several benchmarks verify the effectiveness of our framework, and we achieve substantial improvements compared to previous state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>594, <a href='https://arxiv.org/pdf/2401.04720.pdf' target='_blank'>https://arxiv.org/pdf/2401.04720.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benedikt Roth, Valentin Koch, Sophia J. Wagner, Julia A. Schnabel, Carsten Marr, Tingying Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.04720">Low-resource finetuning of foundation models beats state-of-the-art in histopathology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To handle the large scale of whole slide images in computational pathology, most approaches first tessellate the images into smaller patches, extract features from these patches, and finally aggregate the feature vectors with weakly-supervised learning. The performance of this workflow strongly depends on the quality of the extracted features. Recently, foundation models in computer vision showed that leveraging huge amounts of data through supervised or self-supervised learning improves feature quality and generalizability for a variety of tasks. In this study, we benchmark the most popular vision foundation models as feature extractors for histopathology data. We evaluate the models in two settings: slide-level classification and patch-level classification. We show that foundation models are a strong baseline. Our experiments demonstrate that by finetuning a foundation model on a single GPU for only two hours or three days depending on the dataset, we can match or outperform state-of-the-art feature extractors for computational pathology. These findings imply that even with little resources one can finetune a feature extractor tailored towards a specific downstream task and dataset. This is a considerable shift from the current state, where only few institutions with large amounts of resources and datasets are able to train a feature extractor. We publish all code used for training and evaluation as well as the finetuned models.
<div id='section'>Paperid: <span id='pid'>595, <a href='https://arxiv.org/pdf/2512.13008.pdf' target='_blank'>https://arxiv.org/pdf/2512.13008.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xi Luo, Shixin Xu, Ying Xie, JianZhong Hu, Yuwei He, Yuhui Deng, Huaxiong Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.13008">TWLR: Text-Guided Weakly-Supervised Lesion Localization and Severity Regression for Explainable Diabetic Retinopathy Grading</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate medical image analysis can greatly assist clinical diagnosis, but its effectiveness relies on high-quality expert annotations Obtaining pixel-level labels for medical images, particularly fundus images, remains costly and time-consuming. Meanwhile, despite the success of deep learning in medical imaging, the lack of interpretability limits its clinical adoption. To address these challenges, we propose TWLR, a two-stage framework for interpretable diabetic retinopathy (DR) assessment. In the first stage, a vision-language model integrates domain-specific ophthalmological knowledge into text embeddings to jointly perform DR grading and lesion classification, effectively linking semantic medical concepts with visual features. The second stage introduces an iterative severity regression framework based on weakly-supervised semantic segmentation. Lesion saliency maps generated through iterative refinement direct a progressive inpainting mechanism that systematically eliminates pathological features, effectively downgrading disease severity toward healthier fundus appearances. Critically, this severity regression approach achieves dual benefits: accurate lesion localization without pixel-level supervision and providing an interpretable visualization of disease-to-healthy transformations. Experimental results on the FGADR, DDR, and a private dataset demonstrate that TWLR achieves competitive performance in both DR classification and lesion segmentation, offering a more explainable and annotation-efficient solution for automated retinal image analysis.
<div id='section'>Paperid: <span id='pid'>596, <a href='https://arxiv.org/pdf/2512.06840.pdf' target='_blank'>https://arxiv.org/pdf/2512.06840.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Satoshi Hashimoto, Tatsuya Konishi, Tomoya Kaichi, Kazunori Matsumoto, Mori Kurokawa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.06840">CADE: Continual Weakly-supervised Video Anomaly Detection with Ensembles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video anomaly detection (VAD) has long been studied as a crucial problem in public security and crime prevention. In recent years, weakly-supervised VAD (WVAD) have attracted considerable attention due to their easy annotation process and promising research results. While existing WVAD methods tackle mainly on static datasets, the possibility that the domain of data can vary has been neglected. To adapt such domain-shift, the continual learning (CL) perspective is required because otherwise additional training only with new coming data could easily cause performance degradation for previous data, i.e., forgetting. Therefore, we propose a brand-new approach, called Continual Anomaly Detection with Ensembles (CADE) that is the first work combining CL and WVAD viewpoints. Specifically, CADE uses the Dual-Generator(DG) to address data imbalance and label uncertainty in WVAD. We also found that forgetting exacerbates the "incompleteness'' where the model becomes biased towards certain anomaly modes, leading to missed detections of various anomalies. To address this, we propose to ensemble Multi-Discriminator (MD) that capture missed anomalies in past scenes due to forgetting, using multiple models. Extensive experiments show that CADE significantly outperforms existing VAD methods on the common multi-scene VAD datasets, such as ShanghaiTech and Charlotte Anomaly datasets.
<div id='section'>Paperid: <span id='pid'>597, <a href='https://arxiv.org/pdf/2511.19765.pdf' target='_blank'>https://arxiv.org/pdf/2511.19765.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Torabi, Sanjog Gaihre, Yaqoob Majeed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19765">Lightweight Transformer Framework for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation (WSSS) must learn dense masks from noisy, under-specified cues. We revisit the SegFormer decoder and show that three small, synergistic changes make weak supervision markedly more effective-without altering the MiT backbone or relying on heavy post-processing. Our method, CrispFormer, augments the decoder with: (1) a boundary branch that supervises thin object contours using a lightweight edge head and a boundary-aware loss; (2) an uncertainty-guided refiner that predicts per-pixel aleatoric uncertainty and uses it to weight losses and gate a residual correction of the segmentation logits; and (3) a dynamic multi-scale fusion layer that replaces static concatenation with spatial softmax gating over multi-resolution features, optionally modulated by uncertainty. The result is a single-pass model that preserves crisp boundaries, selects appropriate scales per location, and resists label noise from weak cues. Integrated into a standard WSSS pipeline (seed, student, and EMA relabeling), CrispFormer consistently improves boundary F-score, small-object recall, and mIoU over SegFormer baselines trained on the same seeds, while adding minimal compute. Our decoder-centric formulation is simple to implement, broadly compatible with existing SegFormer variants, and offers a reproducible path to higher-fidelity masks from image-level supervision.
<div id='section'>Paperid: <span id='pid'>598, <a href='https://arxiv.org/pdf/2511.16268.pdf' target='_blank'>https://arxiv.org/pdf/2511.16268.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Erwan Dereure, Robin Louiset, Laura Parkkinen, David A Menassa, David Holcman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.16268">Weakly Supervised Segmentation and Classification of Alpha-Synuclein Aggregates in Brightfield Midbrain Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Parkinson's disease (PD) is a neurodegenerative disorder associated with the accumulation of misfolded alpha-synuclein aggregates, forming Lewy bodies and neuritic shape used for pathology diagnostics. Automatic analysis of immunohistochemistry histopathological images with Deep Learning provides a promising tool for better understanding the spatial organization of these aggregates. In this study, we develop an automated image processing pipeline to segment and classify these aggregates in whole-slide images (WSIs) of midbrain tissue from PD and incidental Lewy Body Disease (iLBD) cases based on weakly supervised segmentation, robust to immunohistochemical labelling variability, with a ResNet50 classifier. Our approach allows to differentiate between major aggregate morphologies, including Lewy bodies and neurites with a balanced accuracy of $80\%$. This framework paves the way for large-scale characterization of the spatial distribution and heterogeneity of alpha-synuclein aggregates in brightfield immunohistochemical tissue, and for investigating their poorly understood relationships with surrounding cells such as microglia and astrocytes.
<div id='section'>Paperid: <span id='pid'>599, <a href='https://arxiv.org/pdf/2511.13891.pdf' target='_blank'>https://arxiv.org/pdf/2511.13891.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seyed Mohamad Ali Tousi, John A. Lory, G. N. DeSouza
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13891">Weakly Supervised Ephemeral Gully Detection In Remote Sensing Images Using Vision Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Among soil erosion problems, Ephemeral Gullies are one of the most concerning phenomena occurring in agricultural fields. Their short temporal cycles increase the difficulty in automatically detecting them using classical computer vision approaches and remote sensing. Also, due to scarcity of and the difficulty in producing accurate labeled data, automatic detection of ephemeral gullies using Machine Learning is limited to zero-shot approaches which are hard to implement. To overcome these challenges, we present the first weakly supervised pipeline for detection of ephemeral gullies. Our method relies on remote sensing and uses Vision Language Models (VLMs) to drastically reduce the labor-intensive task of manual labeling. In order to achieve that, the method exploits: 1) the knowledge embedded in the VLM's pretraining; 2) a teacher-student model where the teacher learns from noisy labels coming from the VLMs, and the student learns by weak supervision using teacher-generate labels and a noise-aware loss function. We also make available the first-of-its-kind dataset for semi-supervised detection of ephemeral gully from remote-sensed images. The dataset consists of a number of locations labeled by a group of soil and plant scientists, as well as a large number of unlabeled locations. The dataset represent more than 18,000 high-resolution remote-sensing images obtained over the course of 13 years. Our experimental results demonstrate the validity of our approach by showing superior performances compared to VLMs and the label model itself when using weak supervision to train an student model. The code and dataset for this work are made publicly available.
<div id='section'>Paperid: <span id='pid'>600, <a href='https://arxiv.org/pdf/2511.10958.pdf' target='_blank'>https://arxiv.org/pdf/2511.10958.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gunho Jung, Heejo Kong, Seong-Whan Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.10958">Text-guided Weakly Supervised Framework for Dynamic Facial Expression Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic facial expression recognition (DFER) aims to identify emotional states by modeling the temporal changes in facial movements across video sequences. A key challenge in DFER is the many-to-one labeling problem, where a video composed of numerous frames is assigned a single emotion label. A common strategy to mitigate this issue is to formulate DFER as a Multiple Instance Learning (MIL) problem. However, MIL-based approaches inherently suffer from the visual diversity of emotional expressions and the complexity of temporal dynamics. To address this challenge, we propose TG-DFER, a text-guided weakly supervised framework that enhances MIL-based DFER by incorporating semantic guidance and coherent temporal modeling. We incorporate a vision-language pre-trained (VLP) model is integrated to provide semantic guidance through fine-grained textual descriptions of emotional context. Furthermore, we introduce visual prompts, which align enriched textual emotion labels with visual instance features, enabling fine-grained reasoning and frame-level relevance estimation. In addition, a multi-grained temporal network is designed to jointly capture short-term facial dynamics and long-range emotional flow, ensuring coherent affective understanding across time. Extensive results demonstrate that TG-DFER achieves improved generalization, interpretability, and temporal sensitivity under weak supervision.
<div id='section'>Paperid: <span id='pid'>601, <a href='https://arxiv.org/pdf/2510.11047.pdf' target='_blank'>https://arxiv.org/pdf/2510.11047.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nivea Roy, Son Tran, Atul Sajjanhar, K. Devaraja, Prakashini Koteshwara, Yong Xiang, Divya Rao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.11047">Benchmarking Deep Learning Models for Laryngeal Cancer Staging Using the LaryngealCT Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Laryngeal cancer imaging research lacks standardised datasets to enable reproducible deep learning (DL) model development. We present LaryngealCT, a curated benchmark of 1,029 computed tomography (CT) scans aggregated from six collections from The Cancer Imaging Archive (TCIA). Uniform 1 mm isotropic volumes of interest encompassing the larynx were extracted using a weakly supervised parameter search framework validated by clinical experts. 3D DL architectures (3D CNN, ResNet18,50,101, DenseNet121) were benchmarked on (i) early (Tis,T1,T2) vs. advanced (T3,T4) and (ii) T4 vs. non-T4 classification tasks. 3D CNN (AUC-0.881, F1-macro-0.821) and ResNet18 (AUC-0.892, F1-macro-0.646) respectively outperformed the other models in the two tasks. Model explainability assessed using 3D GradCAMs with thyroid cartilage overlays revealed greater peri-cartilage attention in non-T4 cases and focal activations in T4 predictions. Through open-source data, pretrained models, and integrated explainability tools, LaryngealCT offers a reproducible foundation for AI-driven research to support clinical decisions in laryngeal oncology.
<div id='section'>Paperid: <span id='pid'>602, <a href='https://arxiv.org/pdf/2509.12496.pdf' target='_blank'>https://arxiv.org/pdf/2509.12496.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Torabi, Sanjog Gaihre, MD Mahbubur Rahman, Yaqoob Majeed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12496">Instance-Guided Class Activation Mapping for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Semantic Segmentation (WSSS) addresses the challenge of training segmentation models using only image-level annotations, eliminating the need for expensive pixel-level labeling. While existing methods struggle with precise object boundary localization and often focus only on the most discriminative regions, we propose IG-CAM (Instance-Guided Class Activation Mapping), a novel approach that leverages instance-level cues and influence functions to generate high-quality, boundary-aware localization maps. Our method introduces three key innovations: (1) Instance-Guided Refinement that uses ground truth segmentation masks to guide CAM generation, ensuring complete object coverage rather than just discriminative parts; (2) Influence Function Integration that captures the relationship between training samples and model predictions, leading to more robust feature representations; and (3) Multi-Scale Boundary Enhancement that employs progressive refinement strategies to achieve sharp, precise object boundaries. IG-CAM achieves state-of-the-art performance on the PASCAL VOC 2012 dataset with an mIoU of 82.3% before post-processing, which further improves to 86.6% after applying Conditional Random Field (CRF) refinement, significantly outperforming previous WSSS methods. Our approach demonstrates superior localization accuracy, with complete object coverage and precise boundary delineation, while maintaining computational efficiency. Extensive ablation studies validate the contribution of each component, and qualitative comparisons across 600 diverse images showcase the method's robustness and generalization capability. The results establish IG-CAM as a new benchmark for weakly supervised semantic segmentation, offering a practical solution for scenarios where pixel-level annotations are unavailable or prohibitively expensive.
<div id='section'>Paperid: <span id='pid'>603, <a href='https://arxiv.org/pdf/2509.10641.pdf' target='_blank'>https://arxiv.org/pdf/2509.10641.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikita Rajaneesh, Thomas Zollo, Richard Zemel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10641">Test-Time Warmup for Multimodal Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal Large Language Models (MLLMs) hold great promise for advanced reasoning at the intersection of text and images, yet they have not fully realized this potential. MLLMs typically integrate an LLM, a vision encoder, and a connector that maps the vision encoder's embeddings into the LLM's text embedding space. Although each component is pretrained on massive datasets with billions of samples, the entire multimodal model is typically trained on only thousands (or a few million) samples, which can result in weak performance on complex reasoning tasks. To address these shortcomings, instead of relying on extensive labeled datasets for fine-tuning, we propose a Test-Time Warmup method that adapts the MLLM per test instance by leveraging data from weakly supervised auxiliary tasks. With our approach, we observe a relative performance improvement of 4.03% on MMMU, 5.28% on VQA-Rad, and 1.63% on GQA on the Llama-Vision-Instruct model. Our method demonstrates that 'warming up' before inference can enhance MLLMs' robustness across diverse reasoning tasks.
<div id='section'>Paperid: <span id='pid'>604, <a href='https://arxiv.org/pdf/2508.19647.pdf' target='_blank'>https://arxiv.org/pdf/2508.19647.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bikash Kumar Badatya, Vipul Baghel, Ravi Hegde
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19647">UTAL-GNN: Unsupervised Temporal Action Localization using Graph Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fine-grained action localization in untrimmed sports videos presents a significant challenge due to rapid and subtle motion transitions over short durations. Existing supervised and weakly supervised solutions often rely on extensive annotated datasets and high-capacity models, making them computationally intensive and less adaptable to real-world scenarios. In this work, we introduce a lightweight and unsupervised skeleton-based action localization pipeline that leverages spatio-temporal graph neural representations. Our approach pre-trains an Attention-based Spatio-Temporal Graph Convolutional Network (ASTGCN) on a pose-sequence denoising task with blockwise partitions, enabling it to learn intrinsic motion dynamics without any manual labeling. At inference, we define a novel Action Dynamics Metric (ADM), computed directly from low-dimensional ASTGCN embeddings, which detects motion boundaries by identifying inflection points in its curvature profile. Our method achieves a mean Average Precision (mAP) of 82.66% and average localization latency of 29.09 ms on the DSV Diving dataset, matching state-of-the-art supervised performance while maintaining computational efficiency. Furthermore, it generalizes robustly to unseen, in-the-wild diving footage without retraining, demonstrating its practical applicability for lightweight, real-time action analysis systems in embedded or dynamic environments.
<div id='section'>Paperid: <span id='pid'>605, <a href='https://arxiv.org/pdf/2508.08876.pdf' target='_blank'>https://arxiv.org/pdf/2508.08876.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaiyu Wang, Lin Mu, Zhiyao Yang, Ximing Li, Xiaotang Zhou Wanfu Gao, Huimao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08876">Weakly Supervised Fine-grained Span-Level Framework for Chinese Radiology Report Quality Assurance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quality Assurance (QA) for radiology reports refers to judging whether the junior reports (written by junior doctors) are qualified. The QA scores of one junior report are given by the senior doctor(s) after reviewing the image and junior report. This process requires intensive labor costs for senior doctors. Additionally, the QA scores may be inaccurate for reasons like diagnosis bias, the ability of senior doctors, and so on. To address this issue, we propose a Span-level Quality Assurance EvaluaTOR (Sqator) to mark QA scores automatically. Unlike the common document-level semantic comparison method, we try to analyze the semantic difference by exploring more fine-grained text spans. Specifically, Sqator measures QA scores by measuring the importance of revised spans between junior and senior reports, and outputs the final QA scores by merging all revised span scores. We evaluate Sqator using a collection of 12,013 radiology reports. Experimental results show that Sqator can achieve competitive QA scores. Moreover, the importance scores of revised spans can be also consistent with the judgments of senior doctors.
<div id='section'>Paperid: <span id='pid'>606, <a href='https://arxiv.org/pdf/2508.05108.pdf' target='_blank'>https://arxiv.org/pdf/2508.05108.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tomoya Tate, Kosuke Sugiyama, Masato Uchida
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05108">Learning from Similarity-Confidence and Confidence-Difference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In practical machine learning applications, it is often challenging to assign accurate labels to data, and increasing the number of labeled instances is often limited. In such cases, Weakly Supervised Learning (WSL), which enables training with incomplete or imprecise supervision, provides a practical and effective solution. However, most existing WSL methods focus on leveraging a single type of weak supervision. In this paper, we propose a novel WSL framework that leverages complementary weak supervision signals from multiple relational perspectives, which can be especially valuable when labeled data is limited. Specifically, we introduce SconfConfDiff Classification, a method that integrates two distinct forms of weaklabels: similarity-confidence and confidence-difference, which are assigned to unlabeled data pairs. To implement this method, we derive two types of unbiased risk estimators for classification: one based on a convex combination of existing estimators, and another newly designed by modeling the interaction between two weak labels. We prove that both estimators achieve optimal convergence rates with respect to estimation error bounds. Furthermore, we introduce a risk correction approach to mitigate overfitting caused by negative empirical risk, and provide theoretical analysis on the robustness of the proposed method against inaccurate class prior probability and label noise. Experimental results demonstrate that the proposed method consistently outperforms existing baselines across a variety of settings.
<div id='section'>Paperid: <span id='pid'>607, <a href='https://arxiv.org/pdf/2508.01338.pdf' target='_blank'>https://arxiv.org/pdf/2508.01338.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqi Sheng, Junyan Wu, Wei Lu, Jiantao Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01338">Weakly-Supervised Image Forgery Localization via Vision-Language Collaborative Reasoning Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image forgery localization aims to precisely identify tampered regions within images, but it commonly depends on costly pixel-level annotations. To alleviate this annotation burden, weakly supervised image forgery localization (WSIFL) has emerged, yet existing methods still achieve limited localization performance as they mainly exploit intra-image consistency clues and lack external semantic guidance to compensate for weak supervision. In this paper, we propose ViLaCo, a vision-language collaborative reasoning framework that introduces auxiliary semantic supervision distilled from pre-trained vision-language models (VLMs), enabling accurate pixel-level localization using only image-level labels. Specifically, ViLaCo first incorporates semantic knowledge through a vision-language feature modeling network, which jointly extracts textual and visual priors using pre-trained VLMs. Next, an adaptive vision-language reasoning network aligns textual semantics and visual features through mutual interactions, producing semantically aligned representations. Subsequently, these representations are passed into dual prediction heads, where the coarse head performs image-level classification and the fine head generates pixel-level localization masks, thereby bridging the gap between weak supervision and fine-grained localization. Moreover, a contrastive patch consistency module is introduced to cluster tampered features while separating authentic ones, facilitating more reliable forgery discrimination. Extensive experiments on multiple public datasets demonstrate that ViLaCo substantially outperforms existing WSIFL methods, achieving state-of-the-art performance in both detection and localization accuracy.
<div id='section'>Paperid: <span id='pid'>608, <a href='https://arxiv.org/pdf/2507.02998.pdf' target='_blank'>https://arxiv.org/pdf/2507.02998.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kimberly F. Greco, Zongxin Yang, Mengyan Li, Han Tong, Sara Morini Sweet, Alon Geva, Kenneth D. Mandl, Benjamin A. Raby, Tianxi Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02998">A Weakly Supervised Transformer to Support Rare Disease Diagnosis from Electronic Health Records: Methods and Applications in Rare Pulmonary Disease</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rare diseases affect an estimated 300-400 million people worldwide, yet individual conditions often remain poorly characterized and difficult to diagnose due to their low prevalence and limited clinician familiarity. While computational phenotyping algorithms show promise for automating rare disease detection, their development is hindered by the scarcity of labeled data and biases in existing label sources. Gold-standard labels from registries and expert chart reviews are highly accurate but constrained by selection bias and the cost of manual review. In contrast, labels derived from electronic health records (EHRs) cover a broader range of patients but can introduce substantial noise. To address these challenges, we propose a weakly supervised, transformer-based framework that combines a small set of gold-standard labels with a large volume of iteratively updated silver-standard labels derived from EHR data. This hybrid approach enables the training of a highly accurate and generalizable phenotyping model that scales rare disease detection beyond the scope of individual clinical expertise. Our method is initialized by learning embeddings of medical concepts based on their semantic meaning or co-occurrence patterns in EHRs, which are then refined and aggregated into patient-level representations via a multi-layer transformer architecture. Using two rare pulmonary diseases as a case study, we validate our model on EHR data from Boston Children's Hospital. Our framework demonstrates notable improvements in phenotype classification, identification of clinically meaningful subphenotypes through patient clustering, and prediction of disease progression compared to baseline methods. These results highlight the potential of our approach to enable scalable identification and stratification of rare disease patients for clinical care and research applications.
<div id='section'>Paperid: <span id='pid'>609, <a href='https://arxiv.org/pdf/2504.09582.pdf' target='_blank'>https://arxiv.org/pdf/2504.09582.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christos Theodoropoulos, Andrei Catalin Coman, James Henderson, Marie-Francine Moens
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09582">Reduction of Supervision for Biomedical Knowledge Discovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Knowledge discovery is hindered by the increasing volume of publications and the scarcity of extensive annotated data. To tackle the challenge of information overload, it is essential to employ automated methods for knowledge extraction and processing. Finding the right balance between the level of supervision and the effectiveness of models poses a significant challenge. While supervised techniques generally result in better performance, they have the major drawback of demanding labeled data. This requirement is labor-intensive and time-consuming and hinders scalability when exploring new domains. In this context, our study addresses the challenge of identifying semantic relationships between biomedical entities (e.g., diseases, proteins) in unstructured text while minimizing dependency on supervision. We introduce a suite of unsupervised algorithms based on dependency trees and attention mechanisms and employ a range of pointwise binary classification methods. Transitioning from weakly supervised to fully unsupervised settings, we assess the methods' ability to learn from data with noisy labels. The evaluation on biomedical benchmark datasets explores the effectiveness of the methods. Our approach tackles a central issue in knowledge discovery: balancing performance with minimal supervision. By gradually decreasing supervision, we assess the robustness of pointwise binary classification techniques in handling noisy labels, revealing their capability to shift from weakly supervised to entirely unsupervised scenarios. Comprehensive benchmarking offers insights into the effectiveness of these techniques, suggesting an encouraging direction toward adaptable knowledge discovery systems, representing progress in creating data-efficient methodologies for extracting useful insights when annotated data is limited.
<div id='section'>Paperid: <span id='pid'>610, <a href='https://arxiv.org/pdf/2503.18725.pdf' target='_blank'>https://arxiv.org/pdf/2503.18725.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zimin Xia, Alexandre Alahi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18725">FG$^2$: Fine-Grained Cross-View Localization by Fine-Grained Feature Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel fine-grained cross-view localization method that estimates the 3 Degrees of Freedom pose of a ground-level image in an aerial image of the surroundings by matching fine-grained features between the two images. The pose is estimated by aligning a point plane generated from the ground image with a point plane sampled from the aerial image. To generate the ground points, we first map ground image features to a 3D point cloud. Our method then learns to select features along the height dimension to pool the 3D points to a Bird's-Eye-View (BEV) plane. This selection enables us to trace which feature in the ground image contributes to the BEV representation. Next, we sample a set of sparse matches from computed point correspondences between the two point planes and compute their relative pose using Procrustes alignment. Compared to the previous state-of-the-art, our method reduces the mean localization error by 28% on the VIGOR cross-area test set. Qualitative results show that our method learns semantically consistent matches across ground and aerial views through weakly supervised learning from the camera pose.
<div id='section'>Paperid: <span id='pid'>611, <a href='https://arxiv.org/pdf/2503.03042.pdf' target='_blank'>https://arxiv.org/pdf/2503.03042.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Han, Soumava Kumar Roy, Mehrtash Harandi, Lars Petersson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.03042">Learning from Noisy Labels with Contrastive Co-Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning with noisy labels is an interesting challenge in weakly supervised learning. Despite their significant learning capacity, CNNs have a tendency to overfit in the presence of samples with noisy labels. Alleviating this issue, the well known Co-Training framework is used as a fundamental basis for our work. In this paper, we introduce a Contrastive Co-Transformer framework, which is simple and fast, yet able to improve the performance by a large margin compared to the state-of-the-art approaches. We argue the robustness of transformers when dealing with label noise. Our Contrastive Co-Transformer approach is able to utilize all samples in the dataset, irrespective of whether they are clean or noisy. Transformers are trained by a combination of contrastive loss and classification loss. Extensive experimental results on corrupted data from six standard benchmark datasets including Clothing1M, demonstrate that our Contrastive Co-Transformer is superior to existing state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>612, <a href='https://arxiv.org/pdf/2502.20678.pdf' target='_blank'>https://arxiv.org/pdf/2502.20678.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aaryan Garg, Akash Kumar, Yogesh S Rawat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20678">STPro: Spatial and Temporal Progressive Learning for Weakly Supervised Spatio-Temporal Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work we study Weakly Supervised Spatio-Temporal Video Grounding (WSTVG), a challenging task of localizing subjects spatio-temporally in videos using only textual queries and no bounding box supervision. Inspired by recent advances in vision-language foundation models, we investigate their utility for WSTVG, leveraging their zero-shot grounding capabilities. However, we find that a simple adaptation lacks essential spatio-temporal grounding abilities. To bridge this gap, we introduce Tubelet Referral Grounding (TRG), which connects textual queries to tubelets to enable spatio-temporal predictions. Despite its promise, TRG struggles with compositional action understanding and dense scene scenarios. To address these limitations, we propose STPro, a novel progressive learning framework with two key modules: (1) Sub-Action Temporal Curriculum Learning (SA-TCL), which incrementally builds compositional action understanding, and (2) Congestion-Guided Spatial Curriculum Learning (CG-SCL), which adapts the model to complex scenes by spatially increasing task difficulty. STPro achieves state-of-the-art results on three benchmark datasets, with improvements of 1.0% on VidSTG-Declarative and 3.0% on HCSTVG-v1.
<div id='section'>Paperid: <span id='pid'>613, <a href='https://arxiv.org/pdf/2409.06471.pdf' target='_blank'>https://arxiv.org/pdf/2409.06471.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujiao Shi, Hongdong Li, Akhil Perincherry, Ankit Vora
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.06471">Weakly-supervised Camera Localization by Ground-to-satellite Image Registration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ground-to-satellite image matching/retrieval was initially proposed for city-scale ground camera localization. This work addresses the problem of improving camera pose accuracy by ground-to-satellite image matching after a coarse location and orientation have been obtained, either from the city-scale retrieval or from consumer-level GPS and compass sensors. Existing learning-based methods for solving this task require accurate GPS labels of ground images for network training. However, obtaining such accurate GPS labels is difficult, often requiring an expensive {\color{black}Real Time Kinematics (RTK)} setup and suffering from signal occlusion, multi-path signal disruptions, \etc. To alleviate this issue, this paper proposes a weakly supervised learning strategy for ground-to-satellite image registration when only noisy pose labels for ground images are available for network training. It derives positive and negative satellite images for each ground image and leverages contrastive learning to learn feature representations for ground and satellite images useful for translation estimation. We also propose a self-supervision strategy for cross-view image relative rotation estimation, which trains the network by creating pseudo query and reference image pairs. Experimental results show that our weakly supervised learning strategy achieves the best performance on cross-area evaluation compared to recent state-of-the-art methods that are reliant on accurate pose labels for supervision.
<div id='section'>Paperid: <span id='pid'>614, <a href='https://arxiv.org/pdf/2408.04813.pdf' target='_blank'>https://arxiv.org/pdf/2408.04813.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingfan Ma, Xiaoyuan Luo, Mingzhi Yuan, Xinrong Chen, Manning Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.04813">Rethinking Multiple Instance Learning: Developing an Instance-Level Classifier via Weakly-Supervised Self-Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple instance learning (MIL) problem is currently solved from either bag-classification or instance-classification perspective, both of which ignore important information contained in some instances and result in limited performance. For example, existing methods often face difficulty in learning hard positive instances. In this paper, we formulate MIL as a semi-supervised instance classification problem, so that all the labeled and unlabeled instances can be fully utilized to train a better classifier. The difficulty in this formulation is that all the labeled instances are negative in MIL, and traditional self-training techniques used in semi-supervised learning tend to degenerate in generating pseudo labels for the unlabeled instances in this scenario. To resolve this problem, we propose a weakly-supervised self-training method, in which we utilize the positive bag labels to construct a global constraint and a local constraint on the pseudo labels to prevent them from degenerating and force the classifier to learn hard positive instances. It is worth noting that easy positive instances are instances are far from the decision boundary in the classification process, while hard positive instances are those close to the decision boundary. Through iterative optimization, the pseudo labels can gradually approach the true labels. Extensive experiments on two MNIST synthetic datasets, five traditional MIL benchmark datasets and two histopathology whole slide image datasets show that our method achieved new SOTA performance on all of them. The code will be publicly available.
<div id='section'>Paperid: <span id='pid'>615, <a href='https://arxiv.org/pdf/2407.12814.pdf' target='_blank'>https://arxiv.org/pdf/2407.12814.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Priyanshu Priya, Mauajama Firdaus, Asif Ekbal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.12814">Computational Politeness in Natural Language Processing: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computational approach to politeness is the task of automatically predicting and generating politeness in text. This is a pivotal task for conversational analysis, given the ubiquity and challenges of politeness in interactions. The computational approach to politeness has witnessed great interest from the conversational analysis community. This article is a compilation of past works in computational politeness in natural language processing. We view four milestones in the research so far, viz. supervised and weakly-supervised feature extraction to identify and induce politeness in a given text, incorporation of context beyond the target text, study of politeness across different social factors, and study the relationship between politeness and various sociolinguistic cues. In this article, we describe the datasets, approaches, trends, and issues in computational politeness research. We also discuss representative performance values and provide pointers to future works, as given in the prior works. In terms of resources to understand the state-of-the-art, this survey presents several valuable illustrations, most prominently, a table summarizing the past papers along different dimensions, such as the types of features, annotation techniques, and datasets used.
<div id='section'>Paperid: <span id='pid'>616, <a href='https://arxiv.org/pdf/2406.17469.pdf' target='_blank'>https://arxiv.org/pdf/2406.17469.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaichen Chi, Wei Jing, Junjie Li, Qiang Li, Qi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.17469">Cross-Modal Spherical Aggregation for Weakly Supervised Remote Sensing Shadow Removal</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Remote sensing shadow removal, which aims to recover contaminated surface information, is tricky since shadows typically display overwhelmingly low illumination intensities. In contrast, the infrared image is robust toward significant light changes, providing visual clues complementary to the visible image. Nevertheless, the existing methods ignore the collaboration between heterogeneous modalities, leading to undesired quality degradation. To fill this gap, we propose a weakly supervised shadow removal network with a spherical feature space, dubbed S2-ShadowNet, to explore the best of both worlds for visible and infrared modalities. Specifically, we employ a modal translation (visible-to-infrared) model to learn the cross-domain mapping, thus generating realistic infrared samples. Then, Swin Transformer is utilized to extract strong representational visible/infrared features. Simultaneously, the extracted features are mapped to the smooth spherical manifold, which alleviates the domain shift through regularization. Well-designed similarity loss and orthogonality loss are embedded into the spherical space, prompting the separation of private visible/infrared features and the alignment of shared visible/infrared features through constraints on both representation content and orientation. Such a manner encourages implicit reciprocity between modalities, thus providing a novel insight into shadow removal. Notably, ground truth is not available in practice, thus S2-ShadowNet is trained by cropping shadow and shadow-free patches from the shadow image itself, avoiding stereotypical and strict pair data acquisition. More importantly, we contribute a large-scale weakly supervised shadow removal benchmark, including 4000 shadow images with corresponding shadow masks.
<div id='section'>Paperid: <span id='pid'>617, <a href='https://arxiv.org/pdf/2406.15755.pdf' target='_blank'>https://arxiv.org/pdf/2406.15755.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xu Yin, Woobin Im, Dongbo Min, Yuchi Huo, Fei Pan, Sung-Eui Yoon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.15755">Fine-grained Background Representation for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating reliable pseudo masks from image-level labels is challenging in the weakly supervised semantic segmentation (WSSS) task due to the lack of spatial information. Prevalent class activation map (CAM)-based solutions are challenged to discriminate the foreground (FG) objects from the suspicious background (BG) pixels (a.k.a. co-occurring) and learn the integral object regions. This paper proposes a simple fine-grained background representation (FBR) method to discover and represent diverse BG semantics and address the co-occurring problems. We abandon using the class prototype or pixel-level features for BG representation. Instead, we develop a novel primitive, negative region of interest (NROI), to capture the fine-grained BG semantic information and conduct the pixel-to-NROI contrast to distinguish the confusing BG pixels. We also present an active sampling strategy to mine the FG negatives on-the-fly, enabling efficient pixel-to-pixel intra-foreground contrastive learning to activate the entire object region. Thanks to the simplicity of design and convenience in use, our proposed method can be seamlessly plugged into various models, yielding new state-of-the-art results under various WSSS settings across benchmarks. Leveraging solely image-level (I) labels as supervision, our method achieves 73.2 mIoU and 45.6 mIoU segmentation results on Pascal Voc and MS COCO test sets, respectively. Furthermore, by incorporating saliency maps as an additional supervision signal (I+S), we attain 74.9 mIoU on Pascal Voc test set. Concurrently, our FBR approach demonstrates meaningful performance gains in weakly-supervised instance segmentation (WSIS) tasks, showcasing its robustness and strong generalization capabilities across diverse domains.
<div id='section'>Paperid: <span id='pid'>618, <a href='https://arxiv.org/pdf/2406.14745.pdf' target='_blank'>https://arxiv.org/pdf/2406.14745.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sefika Efeoglu, Adrian Paschke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.14745">Relation Extraction with Fine-Tuned Large Language Models in Retrieval Augmented Generation Frameworks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Information Extraction (IE) is crucial for converting unstructured data into structured formats like Knowledge Graphs (KGs). A key task within IE is Relation Extraction (RE), which identifies relationships between entities in text. Various RE methods exist, including supervised, unsupervised, weakly supervised, and rule-based approaches. Recent studies leveraging pre-trained language models (PLMs) have shown significant success in this area. In the current era dominated by Large Language Models (LLMs), fine-tuning these models can overcome limitations associated with zero-shot LLM prompting-based RE methods, especially regarding domain adaptation challenges and identifying implicit relations between entities in sentences. These implicit relations, which cannot be easily extracted from a sentence's dependency tree, require logical inference for accurate identification. This work explores the performance of fine-tuned LLMs and their integration into the Retrieval Augmented-based (RAG) RE approach to address the challenges of identifying implicit relations at the sentence level, particularly when LLMs act as generators within the RAG framework. Empirical evaluations on the TACRED, TACRED-Revisited (TACREV), Re-TACRED, and SemEVAL datasets show significant performance improvements with fine-tuned LLMs, including Llama2-7B, Mistral-7B, and T5 (Large). Notably, our approach achieves substantial gains on SemEVAL, where implicit relations are common, surpassing previous results on this dataset. Additionally, our method outperforms previous works on TACRED, TACREV, and Re-TACRED, demonstrating exceptional performance across diverse evaluation scenarios.
<div id='section'>Paperid: <span id='pid'>619, <a href='https://arxiv.org/pdf/2405.14239.pdf' target='_blank'>https://arxiv.org/pdf/2405.14239.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammed Baharoon, Jonathan Klein, Dominik L. Michels
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14239">Harmony: A Joint Self-Supervised and Weakly-Supervised Framework for Learning General Purpose Visual Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language contrastive learning frameworks such as CLIP enable learning representations from natural language supervision and provide strong zero-shot classification capabilities. However, due to the nature of the supervisory signal in these paradigms, they lack the ability to learn localized features, leading to degraded performance on dense prediction tasks such as segmentation and detection. On the other hand, self-supervised learning methods have shown the ability to learn granular representations, complementing the high-level features in vision-language training. In this work, we present Harmony, a framework that combines vision-language training with discriminative and generative self-supervision to learn visual features that can be generalized across different downstream vision tasks. Our framework is specifically designed to work on web-scraped data by not relying on negative examples in the self-supervised learning path and addressing the one-to-one correspondence issue using soft CLIP targets generated by an EMA model. Moreover, Harmony optimizes for five different objectives simultaneously, efficiently utilizing the supervision in each data example, making it even more suited in data-constrained settings. We comprehensively evaluate Harmony across various vision downstream tasks and find that it significantly outperforms the baseline CLIP and outperforms the previously leading joint self- and weakly supervised methods, SLIP, MaskCLIP, and DetailCLIP.
<div id='section'>Paperid: <span id='pid'>620, <a href='https://arxiv.org/pdf/2405.09697.pdf' target='_blank'>https://arxiv.org/pdf/2405.09697.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jadie Adams, Krithika Iyer, Shireen Elhabian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.09697">Weakly Supervised Bayesian Shape Modeling from Unsegmented Medical Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anatomical shape analysis plays a pivotal role in clinical research and hypothesis testing, where the relationship between form and function is paramount. Correspondence-based statistical shape modeling (SSM) facilitates population-level morphometrics but requires a cumbersome, potentially bias-inducing construction pipeline. Recent advancements in deep learning have streamlined this process in inference by providing SSM prediction directly from unsegmented medical images. However, the proposed approaches are fully supervised and require utilizing a traditional SSM construction pipeline to create training data, thus inheriting the associated burdens and limitations. To address these challenges, we introduce a weakly supervised deep learning approach to predict SSM from images using point cloud supervision. Specifically, we propose reducing the supervision associated with the state-of-the-art fully Bayesian variational information bottleneck DeepSSM (BVIB-DeepSSM) model. BVIB-DeepSSM is an effective, principled framework for predicting probabilistic anatomical shapes from images with quantification of both aleatoric and epistemic uncertainties. Whereas the original BVIB-DeepSSM method requires strong supervision in the form of ground truth correspondence points, the proposed approach utilizes weak supervision via point cloud surface representations, which are more readily obtainable. Furthermore, the proposed approach learns correspondence in a completely data-driven manner without prior assumptions about the expected variability in shape cohort. Our experiments demonstrate that this approach yields similar accuracy and uncertainty estimation to the fully supervised scenario while substantially enhancing the feasibility of model training for SSM construction.
<div id='section'>Paperid: <span id='pid'>621, <a href='https://arxiv.org/pdf/2403.17254.pdf' target='_blank'>https://arxiv.org/pdf/2403.17254.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gaurav Negi, Rajdeep Sarkar, Omnia Zayed, Paul Buitelaar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17254">A Hybrid Approach To Aspect Based Sentiment Analysis Using Transfer Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Aspect-Based Sentiment Analysis (ABSA) aims to identify terms or multiword expressions (MWEs) on which sentiments are expressed and the sentiment polarities associated with them. The development of supervised models has been at the forefront of research in this area. However, training these models requires the availability of manually annotated datasets which is both expensive and time-consuming. Furthermore, the available annotated datasets are tailored to a specific domain, language, and text type. In this work, we address this notable challenge in current state-of-the-art ABSA research. We propose a hybrid approach for Aspect Based Sentiment Analysis using transfer learning. The approach focuses on generating weakly-supervised annotations by exploiting the strengths of both large language models (LLM) and traditional syntactic dependencies. We utilise syntactic dependency structures of sentences to complement the annotations generated by LLMs, as they may overlook domain-specific aspect terms. Extensive experimentation on multiple datasets is performed to demonstrate the efficacy of our hybrid method for the tasks of aspect term extraction and aspect sentiment classification.
  Keywords: Aspect Based Sentiment Analysis, Syntactic Parsing, large language model (LLM)
<div id='section'>Paperid: <span id='pid'>622, <a href='https://arxiv.org/pdf/2403.09551.pdf' target='_blank'>https://arxiv.org/pdf/2403.09551.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiyuan Wang, Yanzhe Liu, Shang Zhao, Rong Liu, S. Kevin Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09551">WeakSurg: Weakly supervised surgical instrument segmentation using temporal equivariance and semantic continuity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For robotic surgical videos, instrument presence annotations are typically recorded with video streams, which offering the potential to reduce the manually annotated costs for segmentation. However, weakly supervised surgical instrument segmentation with only instrument presence labels has been rarely explored in surgical domain due to the highly under-constrained challenges. Temporal properties can enhance representation learning by capturing sequential dependencies and patterns over time even in incomplete supervision situations. From this, we take the inherent temporal attributes of surgical video into account and extend a two-stage weakly supervised segmentation paradigm from different perspectives. Firstly, we make temporal equivariance constraint to enhance pixel-wise temporal consistency between adjacent features. Secondly, we constrain class-aware semantic continuity between global and local regions across temporal dimension. Finally, we generate temporal-enhanced pseudo masks from consecutive frames to suppress irrelevant regions. Extensive experiments are validated on two surgical video datasets, including one cholecystectomy surgery benchmark and one real robotic left lateral segment liver surgery dataset. We annotate instance-wise instrument labels with fixed time-steps which are double checked by a clinician with 3-years experience to evaluate segmentation results. Experimental results demonstrate the promising performances of our method, which consistently achieves comparable or favorable results with previous state-of-the-art approaches.
<div id='section'>Paperid: <span id='pid'>623, <a href='https://arxiv.org/pdf/2402.15477.pdf' target='_blank'>https://arxiv.org/pdf/2402.15477.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Renan D. B. Brotto, Jean-Michel Loubes, Laurent Risser, Jean-Pierre Florens, Kenji Nose-Filho, JoÃ£o M. T. Romano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.15477">Debiasing Machine Learning Models by Using Weakly Supervised Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We tackle the problem of bias mitigation of algorithmic decisions in a setting where both the output of the algorithm and the sensitive variable are continuous. Most of prior work deals with discrete sensitive variables, meaning that the biases are measured for subgroups of persons defined by a label, leaving out important algorithmic bias cases, where the sensitive variable is continuous. Typical examples are unfair decisions made with respect to the age or the financial status. In our work, we then propose a bias mitigation strategy for continuous sensitive variables, based on the notion of endogeneity which comes from the field of econometrics. In addition to solve this new problem, our bias mitigation strategy is a weakly supervised learning method which requires that a small portion of the data can be measured in a fair manner. It is model agnostic, in the sense that it does not make any hypothesis on the prediction model. It also makes use of a reasonably large amount of input observations and their corresponding predictions. Only a small fraction of the true output predictions should be known. This therefore limits the need for expert interventions. Results obtained on synthetic data show the effectiveness of our approach for examples as close as possible to real-life applications in econometrics.
<div id='section'>Paperid: <span id='pid'>624, <a href='https://arxiv.org/pdf/2402.12913.pdf' target='_blank'>https://arxiv.org/pdf/2402.12913.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengcheng Wei, Ze Chen, Songtan Fang, Jiarong He, Max Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.12913">OPDAI at SemEval-2024 Task 6: Small LLMs can Accelerate Hallucination Detection with Weakly Supervised Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper mainly describes a unified system for hallucination detection of LLMs, which wins the second prize in the model-agnostic track of the SemEval-2024 Task 6, and also achieves considerable results in the model-aware track. This task aims to detect hallucination with LLMs for three different text-generation tasks without labeled training data. We utilize prompt engineering and few-shot learning to verify the performance of different LLMs on the validation data. Then we select the LLMs with better performance to generate high-quality weakly supervised training data, which not only satisfies the consistency of different LLMs, but also satisfies the consistency of the optimal LLM with different sampling parameters. Furthermore, we finetune different LLMs by using the constructed training data, and finding that a relatively small LLM can achieve a competitive level of performance in hallucination detection, when compared to the large LLMs and the prompt-based approaches using GPT-4.
<div id='section'>Paperid: <span id='pid'>625, <a href='https://arxiv.org/pdf/2401.00128.pdf' target='_blank'>https://arxiv.org/pdf/2401.00128.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lujia Wang, Hairong Wang, Fulvio D'Angelo, Lee Curtin, Christopher P. Sereduk, Gustavo De Leon, Kyle W. Singleton, Javier Urcuyo, Andrea Hawkins-Daarud, Pamela R. Jackson, Chandan Krishna, Richard S. Zimmerman, Devi P. Patra, Bernard R. Bendok, Kris A. Smith, Peter Nakaji, Kliment Donev, Leslie C. Baxter, Maciej M. MrugaÅa, Michele Ceccarelli, Antonio Iavarone, Kristin R. Swanson, Nhan L. Tran, Leland S. Hu, Jing Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.00128">Quantifying intra-tumoral genetic heterogeneity of glioblastoma toward precision medicine using MRI and a data-inclusive machine learning algorithm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Glioblastoma (GBM) is one of the most aggressive and lethal human cancers. Intra-tumoral genetic heterogeneity poses a significant challenge for treatment. Biopsy is invasive, which motivates the development of non-invasive, MRI-based machine learning (ML) models to quantify intra-tumoral genetic heterogeneity for each patient. This capability holds great promise for enabling better therapeutic selection to improve patient outcomes. We proposed a novel Weakly Supervised Ordinal Support Vector Machine (WSO-SVM) to predict regional genetic alteration status within each GBM tumor using MRI. WSO-SVM was applied to a unique dataset of 318 image-localized biopsies with spatially matched multiparametric MRI from 74 GBM patients. The model was trained to predict the regional genetic alteration of three GBM driver genes (EGFR, PDGFRA, and PTEN) based on features extracted from the corresponding region of five MRI contrast images. For comparison, a variety of existing ML algorithms were also applied. The classification accuracy of each gene was compared between the different algorithms. The SHapley Additive exPlanations (SHAP) method was further applied to compute contribution scores of different contrast images. Finally, the trained WSO-SVM was used to generate prediction maps within the tumoral area of each patient to help visualize the intra-tumoral genetic heterogeneity. This study demonstrated the feasibility of using MRI and WSO-SVM to enable non-invasive prediction of intra-tumoral regional genetic alteration for each GBM patient, which can inform future adaptive therapies for individualized oncology.
<div id='section'>Paperid: <span id='pid'>626, <a href='https://arxiv.org/pdf/2512.19713.pdf' target='_blank'>https://arxiv.org/pdf/2512.19713.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taoran Sheng, Manfred Huber
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.19713">Reducing Label Dependency in Human Activity Recognition with Wearables: From Supervised Learning to Novel Weakly Self-Supervised Approaches</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human activity recognition (HAR) using wearable sensors has advanced through various machine learning paradigms, each with inherent trade-offs between performance and labeling requirements. While fully supervised techniques achieve high accuracy, they demand extensive labeled datasets that are costly to obtain. Conversely, unsupervised methods eliminate labeling needs but often deliver suboptimal performance. This paper presents a comprehensive investigation across the supervision spectrum for wearable-based HAR, with particular focus on novel approaches that minimize labeling requirements while maintaining competitive accuracy. We develop and empirically compare: (1) traditional fully supervised learning, (2) basic unsupervised learning, (3) a weakly supervised learning approach with constraints, (4) a multi-task learning approach with knowledge sharing, (5) a self-supervised approach based on domain expertise, and (6) a novel weakly self-supervised learning framework that leverages domain knowledge and minimal labeled data. Experiments across benchmark datasets demonstrate that: (i) our weakly supervised methods achieve performance comparable to fully supervised approaches while significantly reducing supervision requirements; (ii) the proposed multi-task framework enhances performance through knowledge sharing between related tasks; (iii) our weakly self-supervised approach demonstrates remarkable efficiency with just 10\% of labeled data. These results not only highlight the complementary strengths of different learning paradigms, offering insights into tailoring HAR solutions based on the availability of labeled data, but also establish that our novel weakly self-supervised framework offers a promising solution for practical HAR applications where labeled data are limited.
<div id='section'>Paperid: <span id='pid'>627, <a href='https://arxiv.org/pdf/2512.01294.pdf' target='_blank'>https://arxiv.org/pdf/2512.01294.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Song Zhang, Ruohan Guo, Xiaohua Ge, Perter Mahon, Weixiang Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.01294">Experimental Methods, Health Indicators, and Diagnostic Strategies for Retired Lithium-ion Batteries: A Comprehensive Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reliable health assessment of retired lithium-ion batteries is essential for safe and economically viable second-life deployment, yet remains difficult due to sparse measurements, incomplete historical records, heterogeneous chemistries, and limited or noisy battery health labels. Conventional laboratory diagnostics, such as full charge-discharge cycling, pulse tests, Electrochemical Impedance Spectroscopy (EIS) measurements, and thermal characterization, provide accurate degradation information but are too time-consuming, equipment-intensive, or condition-sensitive to be applied at scale during retirement-stage sorting, leaving real-world datasets fragmented and inconsistent. This review synthesizes recent advances that address these constraints through physical health indicators, experiment testing methods, data-generation and augmentation techniques, and a spectrum of learning-based modeling routes spanning supervised, semi-supervised, weakly supervised, and unsupervised paradigms. We highlight how minimal-test features, synthetic data, domain-invariant representations, and uncertainty-aware prediction enable robust inference under limited or approximate labels and across mixed chemistries and operating histories. A comparative evaluation further reveals trade-offs in accuracy, interpretability, scalability, and computational burden. Looking forward, progress toward physically constrained generative models, cross-chemistry generalization, calibrated uncertainty estimation, and standardized benchmarks will be crucial for building reliable, scalable, and deployment-ready health prediction tools tailored to the realities of retired-battery applications.
<div id='section'>Paperid: <span id='pid'>628, <a href='https://arxiv.org/pdf/2511.17346.pdf' target='_blank'>https://arxiv.org/pdf/2511.17346.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marius Rodrigues, Louis Bahrman, Roland Badeau, Gaël Richard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17346">Is Phase Really Needed for Weakly-Supervised Dereverberation ?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In unsupervised or weakly-supervised approaches for speech dereverberation, the target clean (dry) signals are considered to be unknown during training. In that context, evaluating to what extent information can be retrieved from the sole knowledge of reverberant (wet) speech becomes critical. This work investigates the role of the reverberant (wet) phase in the time-frequency domain. Based on Statistical Wave Field Theory, we show that late reverberation perturbs phase components with white, uniformly distributed noise, except at low frequencies. Consequently, the wet phase carries limited useful information and is not essential for weakly supervised dereverberation. To validate this finding, we train dereverberation models under a recent weak supervision framework and demonstrate that performance can be significantly improved by excluding the reverberant phase from the loss function.
<div id='section'>Paperid: <span id='pid'>629, <a href='https://arxiv.org/pdf/2510.09306.pdf' target='_blank'>https://arxiv.org/pdf/2510.09306.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alemu Sisay Nigru, Michele Svanera, Austin Dibble, Connor Dalby, Mattia Savardi, Sergio Benini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09306">Rewiring Development in Brain Segmentation: Leveraging Adult Brain Priors for Enhancing Infant MRI Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate segmentation of infant brain MRI is critical for studying early neurodevelopment and diagnosing neurological disorders. Yet, it remains a fundamental challenge due to continuously evolving anatomy of the subjects, motion artifacts, and the scarcity of high-quality labeled data. In this work, we present LODi, a novel framework that utilizes prior knowledge from an adult brain MRI segmentation model to enhance the segmentation performance of infant scans. Given the abundance of publicly available adult brain MRI data, we pre-train a segmentation model on a large adult dataset as a starting point. Through transfer learning and domain adaptation strategies, we progressively adapt the model to the 0-2 year-old population, enabling it to account for the anatomical and imaging variability typical of infant scans. The adaptation of the adult model is carried out using weakly supervised learning on infant brain scans, leveraging silver-standard ground truth labels obtained with FreeSurfer. By introducing a novel training strategy that integrates hierarchical feature refinement and multi-level consistency constraints, our method enables fast, accurate, age-adaptive segmentation, while mitigating scanner and site-specific biases. Extensive experiments on both internal and external datasets demonstrate the superiority of our approach over traditional supervised learning and domain-specific models. Our findings highlight the advantage of leveraging adult brain priors as a foundation for age-flexible neuroimaging analysis, paving the way for more reliable and generalizable brain MRI segmentation across the lifespan.
<div id='section'>Paperid: <span id='pid'>630, <a href='https://arxiv.org/pdf/2510.00654.pdf' target='_blank'>https://arxiv.org/pdf/2510.00654.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaocong Zhu, Zhiwei Li, Xinghua Li, Huanfeng Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00654">Weakly Supervised Cloud Detection Combining Spectral Features and Multi-Scale Deep Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Clouds significantly affect the quality of optical satellite images, which seriously limits their precise application. Recently, deep learning has been widely applied to cloud detection and has achieved satisfactory results. However, the lack of distinctive features in thin clouds and the low quality of training samples limit the cloud detection accuracy of deep learning methods, leaving space for further improvements. In this paper, we propose a weakly supervised cloud detection method that combines spectral features and multi-scale scene-level deep network (SpecMCD) to obtain highly accurate pixel-level cloud masks. The method first utilizes a progressive training framework with a multi-scale scene-level dataset to train the multi-scale scene-level cloud detection network. Pixel-level cloud probability maps are then obtained by combining the multi-scale probability maps and cloud thickness map based on the characteristics of clouds in dense cloud coverage and large cloud-area coverage images. Finally, adaptive thresholds are generated based on the differentiated regions of the scene-level cloud masks at different scales and combined with distance-weighted optimization to obtain binary cloud masks. Two datasets, WDCD and GF1MS-WHU, comprising a total of 60 Gaofen-1 multispectral (GF1-MS) images, were used to verify the effectiveness of the proposed method. Compared to the other weakly supervised cloud detection methods such as WDCD and WSFNet, the F1-score of the proposed SpecMCD method shows an improvement of over 7.82%, highlighting the superiority and potential of the SpecMCD method for cloud detection under different cloud coverage conditions.
<div id='section'>Paperid: <span id='pid'>631, <a href='https://arxiv.org/pdf/2509.17702.pdf' target='_blank'>https://arxiv.org/pdf/2509.17702.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Patrick Schmidt, Vasileios Belagiannis, Lazaros Nalpantidis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17702">Depth Edge Alignment Loss: DEALing with Depth in Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous robotic systems applied to new domains require an abundance of expensive, pixel-level dense labels to train robust semantic segmentation models under full supervision. This study proposes a model-agnostic Depth Edge Alignment Loss to improve Weakly Supervised Semantic Segmentation models across different datasets. The methodology generates pixel-level semantic labels from image-level supervision, avoiding expensive annotation processes. While weak supervision is widely explored in traditional computer vision, our approach adds supervision with pixel-level depth information, a modality commonly available in robotic systems. We demonstrate how our approach improves segmentation performance across datasets and models, but can also be combined with other losses for even better performance, with improvements up to +5.439, +1.274 and +16.416 points in mean Intersection over Union on the PASCAL VOC / MS COCO validation, and the HOPE static onboarding split, respectively. Our code will be made publicly available.
<div id='section'>Paperid: <span id='pid'>632, <a href='https://arxiv.org/pdf/2509.06485.pdf' target='_blank'>https://arxiv.org/pdf/2509.06485.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrea Marelli, Alberto Foresti, Leonardo Pesce, Giacomo Boracchi, Mario Grosso
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06485">WS$^2$: Weakly Supervised Segmentation using Before-After Supervision in Waste Sorting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In industrial quality control, to visually recognize unwanted items within a moving heterogeneous stream, human operators are often still indispensable. Waste-sorting stands as a significant example, where operators on multiple conveyor belts manually remove unwanted objects to select specific materials. To automate this recognition problem, computer vision systems offer great potential in accurately identifying and segmenting unwanted items in such settings. Unfortunately, considering the multitude and the variety of sorting tasks, fully supervised approaches are not a viable option to address this challange, as they require extensive labeling efforts. Surprisingly, weakly supervised alternatives that leverage the implicit supervision naturally provided by the operator in his removal action are relatively unexplored. In this paper, we define the concept of Before-After Supervision, illustrating how to train a segmentation network by leveraging only the visual differences between images acquired \textit{before} and \textit{after} the operator. To promote research in this direction, we introduce WS$^2$ (Weakly Supervised segmentation for Waste-Sorting), the first multiview dataset consisting of more than 11 000 high-resolution video frames captured on top of a conveyor belt, including "before" and "after" images. We also present a robust end-to-end pipeline, used to benchmark several state-of-the-art weakly supervised segmentation methods on WS$^2$.
<div id='section'>Paperid: <span id='pid'>633, <a href='https://arxiv.org/pdf/2507.10594.pdf' target='_blank'>https://arxiv.org/pdf/2507.10594.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengda Zhuo, Di Wu, Yi He, Shuqiang Huang, Xindong Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10594">Extension OL-MDISF: Online Learning from Mix-Typed, Drifted, and Incomplete Streaming Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Online learning, where feature spaces can change over time, offers a flexible learning paradigm that has attracted considerable attention. However, it still faces three significant challenges. First, the heterogeneity of real-world data streams with mixed feature types presents challenges for traditional parametric modeling. Second, data stream distributions can shift over time, causing an abrupt and substantial decline in model performance. Additionally, the time and cost constraints make it infeasible to label every data instance in a supervised setting. To overcome these challenges, we propose a new algorithm Online Learning from Mix-typed, Drifted, and Incomplete Streaming Features (OL-MDISF), which aims to relax restrictions on both feature types, data distribution, and supervision information. Our approach involves utilizing copula models to create a comprehensive latent space, employing an adaptive sliding window for detecting drift points to ensure model stability, and establishing label proximity information based on geometric structural relationships. To demonstrate the model's efficiency and effectiveness, we provide theoretical analysis and comprehensive experimental results.
  This extension serves as a standalone technical reference to the original OL-MDISF method. It provides (i) a contextual analysis of OL-MDISF within the broader landscape of online learning, covering recent advances in mixed-type feature modeling, concept drift adaptation, and weak supervision, and (ii) a comprehensive set of experiments across 14 real-world datasets under two types of drift scenarios. These include full CER trends, ablation studies, sensitivity analyses, and temporal ensemble dynamics. We hope this document can serve as a reproducible benchmark and technical resource for researchers working on nonstationary, heterogeneous, and weakly supervised data streams.
<div id='section'>Paperid: <span id='pid'>634, <a href='https://arxiv.org/pdf/2507.07297.pdf' target='_blank'>https://arxiv.org/pdf/2507.07297.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengfei Wu, Ronald Seoh, Bingxuan Li, Liqiang Zhang, Fengrong Han, Dan Goldwasser
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07297">MagiC: Evaluating Multimodal Cognition Toward Grounded Visual Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in large vision-language models have led to impressive performance in visual question answering and multimodal reasoning. However, it remains unclear whether these models genuinely perform grounded visual reasoning or rely on superficial patterns and dataset biases. In this work, we introduce MagiC, a comprehensive benchmark designed to evaluate grounded multimodal cognition, assessing not only answer accuracy but also the quality of step-by-step reasoning and its alignment with relevant visual evidence. Our benchmark includes approximately 5,500 weakly supervised QA examples generated from strong model outputs and 900 human-curated examples with fine-grained annotations, including answers, rationales, and bounding box groundings. We evaluate 15 vision-language models ranging from 7B to 70B parameters across four dimensions: final answer correctness, reasoning validity, grounding fidelity, and self-correction ability. MagiC further includes diagnostic settings to probe model robustness under adversarial visual cues and assess their capacity for introspective error correction. We introduce new metrics such as MagiScore and StepSense, and provide comprehensive analyses that reveal key limitations and opportunities in current approaches to grounded visual reasoning.
<div id='section'>Paperid: <span id='pid'>635, <a href='https://arxiv.org/pdf/2506.22866.pdf' target='_blank'>https://arxiv.org/pdf/2506.22866.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hang-Cheng Dong, Lu Zou, Bingguo Liu, Dong Ye, Guodong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22866">Region-Aware CAM: High-Resolution Weakly-Supervised Defect Segmentation via Salient Region Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Surface defect detection plays a critical role in industrial quality inspection. Recent advances in artificial intelligence have significantly enhanced the automation level of detection processes. However, conventional semantic segmentation and object detection models heavily rely on large-scale annotated datasets, which conflicts with the practical requirements of defect detection tasks. This paper proposes a novel weakly supervised semantic segmentation framework comprising two key components: a region-aware class activation map (CAM) and pseudo-label training. To address the limitations of existing CAM methods, especially low-resolution thermal maps, and insufficient detail preservation, we introduce filtering-guided backpropagation (FGBP), which refines target regions by filtering gradient magnitudes to identify areas with higher relevance to defects. Building upon this, we further develop a region-aware weighted module to enhance spatial precision. Finally, pseudo-label segmentation is implemented to refine the model's performance iteratively. Comprehensive experiments on industrial defect datasets demonstrate the superiority of our method. The proposed framework effectively bridges the gap between weakly supervised learning and high-precision defect segmentation, offering a practical solution for resource-constrained industrial scenarios.
<div id='section'>Paperid: <span id='pid'>636, <a href='https://arxiv.org/pdf/2505.23524.pdf' target='_blank'>https://arxiv.org/pdf/2505.23524.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Xia, Dan Jiang, Quan Zhang, Ke Zhang, Chun Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23524">CLIP-AE: CLIP-assisted Cross-view Audio-Visual Enhancement for Unsupervised Temporal Action Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Temporal Action Localization (TAL) has garnered significant attention in information retrieval. Existing supervised or weakly supervised methods heavily rely on labeled temporal boundaries and action categories, which are labor-intensive and time-consuming. Consequently, unsupervised temporal action localization (UTAL) has gained popularity. However, current methods face two main challenges: 1) Classification pre-trained features overly focus on highly discriminative regions; 2) Solely relying on visual modality information makes it difficult to determine contextual boundaries. To address these issues, we propose a CLIP-assisted cross-view audiovisual enhanced UTAL method. Specifically, we introduce visual language pre-training (VLP) and classification pre-training-based collaborative enhancement to avoid excessive focus on highly discriminative regions; we also incorporate audio perception to provide richer contextual boundary information. Finally, we introduce a self-supervised cross-view learning paradigm to achieve multi-view perceptual enhancement without additional annotations. Extensive experiments on two public datasets demonstrate our model's superiority over several state-of-the-art competitors.
<div id='section'>Paperid: <span id='pid'>637, <a href='https://arxiv.org/pdf/2503.20722.pdf' target='_blank'>https://arxiv.org/pdf/2503.20722.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>A. Candito, A. Dragan, R. Holbrey, A. Ribeiro, R. Donners, C. Messiou, N. Tunariu, D. -M. Koh, M. D. Blackledge, The Institute of Cancer Research, London, United Kingdom, The Royal Marsden NHS Foundation Trust, London, United Kingdom, University Hospital Basel, Basel, Switzerland
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20722">A weakly-supervised deep learning model for fast localisation and delineation of the skeleton, internal organs, and spinal canal on Whole-Body Diffusion-Weighted MRI (WB-DWI)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Background: Apparent Diffusion Coefficient (ADC) values and Total Diffusion Volume (TDV) from Whole-body diffusion-weighted MRI (WB-DWI) are recognized cancer imaging biomarkers. However, manual disease delineation for ADC and TDV measurements is unfeasible in clinical practice, demanding automation. As a first step, we propose an algorithm to generate fast and reproducible probability maps of the skeleton, adjacent internal organs (liver, spleen, urinary bladder, and kidneys), and spinal canal. Methods: We developed an automated deep-learning pipeline based on a 3D patch-based Residual U-Net architecture that localizes and delineates these anatomical structures on WB-DWI. The algorithm was trained using "soft-labels" (non-binary segmentations) derived from a computationally intensive atlas-based approach. For training and validation, we employed a multi-center WB-DWI dataset comprising 532 scans from patients with Advanced Prostate Cancer (APC) or Multiple Myeloma (MM), with testing on 45 patients. Results: Our weakly-supervised deep learning model achieved an average dice score/precision/recall of 0.66/0.6/0.73 for skeletal delineations, 0.8/0.79/0.81 for internal organs, and 0.85/0.79/0.94 for spinal canal, with surface distances consistently below 3 mm. Relative median ADC and log-transformed volume differences between automated and manual expert-defined full-body delineations were below 10% and 4%, respectively. The computational time for generating probability maps was 12x faster than the atlas-based registration algorithm (25 s vs. 5 min). An experienced radiologist rated the model's accuracy "good" or "excellent" on test datasets. Conclusion: Our model offers fast and reproducible probability maps for localizing and delineating body regions on WB-DWI, enabling ADC and TDV quantification, potentially supporting clinicians in disease staging and treatment response assessment.
<div id='section'>Paperid: <span id='pid'>638, <a href='https://arxiv.org/pdf/2503.04165.pdf' target='_blank'>https://arxiv.org/pdf/2503.04165.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bodong Zhang, Hamid Manoochehri, Xiwen Li, Beatrice S. Knudsen, Tolga Tasdizen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04165">WeakSupCon: Weakly Supervised Contrastive Learning for Encoder Pre-training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised multiple instance learning (MIL) is a challenging task given that only bag-level labels are provided, while each bag typically contains multiple instances. This topic has been extensively studied in histopathological image analysis, where labels are usually available only at the whole slide image (WSI) level, while each WSI could be divided into thousands of small image patches for training. The dominant MIL approaches focus on feature aggregation and take fixed patch features as inputs. However, weakly supervised feature representation learning in MIL settings is always neglected. Those features used to be generated by self-supervised learning methods that do not utilize weak labels, or by foundation encoders pre-trained on other large datasets. In this paper, we propose a novel weakly supervised feature representation learning method called Weakly Supervised Contrastive Learning (WeakSupCon) that utilizes bag-level labels. In our method, we employ multi-task learning and define distinct contrastive losses for samples with different bag labels. Our experiments demonstrate that the features generated using WeakSupCon with limited computing resources significantly enhance MIL classification performance compared to self-supervised approaches across three datasets. Our WeakSupCon code is available at github.com/BzhangURU/Paper_WeakSupCon
<div id='section'>Paperid: <span id='pid'>639, <a href='https://arxiv.org/pdf/2502.09080.pdf' target='_blank'>https://arxiv.org/pdf/2502.09080.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiwei Wang, Shaoxun Wu, Yujiao Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09080">BevSplat: Resolving Height Ambiguity via Feature-Based Gaussian Primitives for Weakly-Supervised Cross-View Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the problem of weakly supervised cross-view localization, where the goal is to estimate the pose of a ground camera relative to a satellite image with noisy ground truth annotations. A common approach to bridge the cross-view domain gap for pose estimation is Bird's-Eye View (BEV) synthesis. However, existing methods struggle with height ambiguity due to the lack of depth information in ground images and satellite height maps. Previous solutions either assume a flat ground plane or rely on complex models, such as cross-view transformers. We propose BevSplat, a novel method that resolves height ambiguity by using feature-based Gaussian primitives. Each pixel in the ground image is represented by a 3D Gaussian with semantic and spatial features, which are synthesized into a BEV feature map for relative pose estimation. Additionally, to address challenges with panoramic query images, we introduce an icosphere-based supervision strategy for the Gaussian primitives. We validate our method on the widely used KITTI and VIGOR datasets, which include both pinhole and panoramic query images. Experimental results show that BevSplat significantly improves localization accuracy over prior approaches.
<div id='section'>Paperid: <span id='pid'>640, <a href='https://arxiv.org/pdf/2501.19048.pdf' target='_blank'>https://arxiv.org/pdf/2501.19048.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rita Pereira, M. Rita Verdelho, Catarina Barata, Carlos Santiago
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.19048">The Role of Graph-based MIL and Interventional Training in the Generalization of WSI Classifiers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Whole Slide Imaging (WSI), which involves high-resolution digital scans of pathology slides, has become the gold standard for cancer diagnosis, but its gigapixel resolution and the scarcity of annotated datasets present challenges for deep learning models. Multiple Instance Learning (MIL), a widely-used weakly supervised approach, bypasses the need for patch-level annotations. However, conventional MIL methods overlook the spatial relationships between patches, which are crucial for tasks such as cancer grading and diagnosis. To address this, graph-based approaches have gained prominence by incorporating spatial information through node connections. Despite their potential, both MIL and graph-based models are vulnerable to learning spurious associations, like color variations in WSIs, affecting their robustness. In this dissertation, we conduct an extensive comparison of multiple graph construction techniques, MIL models, graph-MIL approaches, and interventional training, introducing a new framework, Graph-based Multiple Instance Learning with Interventional Training (GMIL-IT), for WSI classification. We evaluate their impact on model generalization through domain shift analysis and demonstrate that graph-based models alone achieve the generalization initially anticipated from interventional training. Our code is available here: github.com/ritamartinspereira/GMIL-IT
<div id='section'>Paperid: <span id='pid'>641, <a href='https://arxiv.org/pdf/2501.11124.pdf' target='_blank'>https://arxiv.org/pdf/2501.11124.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Quan Zhang, Yuxin Qi, Xi Tang, Rui Yuan, Xi Lin, Ke Zhang, Chun Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.11124">Rethinking Pseudo-Label Guided Learning for Weakly Supervised Temporal Action Localization from the Perspective of Noise Correction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pseudo-label learning methods have been widely applied in weakly-supervised temporal action localization. Existing works directly utilize weakly-supervised base model to generate instance-level pseudo-labels for training the fully-supervised detection head. We argue that the noise in pseudo-labels would interfere with the learning of fully-supervised detection head, leading to significant performance leakage. Issues with noisy labels include:(1) inaccurate boundary localization; (2) undetected short action clips; (3) multiple adjacent segments incorrectly detected as one segment. To target these issues, we introduce a two-stage noisy label learning strategy to harness every potential useful signal in noisy labels. First, we propose a frame-level pseudo-label generation model with a context-aware denoising algorithm to refine the boundaries. Second, we introduce an online-revised teacher-student framework with a missing instance compensation module and an ambiguous instance correction module to solve the short-action-missing and many-to-one problems. Besides, we apply a high-quality pseudo-label mining loss in our online-revised teacher-student framework to add different weights to the noisy labels to train more effectively. Our model outperforms the previous state-of-the-art method in detection accuracy and inference speed greatly upon the THUMOS14 and ActivityNet v1.2 benchmarks.
<div id='section'>Paperid: <span id='pid'>642, <a href='https://arxiv.org/pdf/2412.14295.pdf' target='_blank'>https://arxiv.org/pdf/2412.14295.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anna Manasyan, Maximilian Seitzer, Filip Radovic, Georg Martius, Andrii Zadaianchuk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14295">Temporally Consistent Object-Centric Learning by Contrasting Slots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsupervised object-centric learning from videos is a promising approach to extract structured representations from large, unlabeled collections of videos. To support downstream tasks like autonomous control, these representations must be both compositional and temporally consistent. Existing approaches based on recurrent processing often lack long-term stability across frames because their training objective does not enforce temporal consistency. In this work, we introduce a novel object-level temporal contrastive loss for video object-centric models that explicitly promotes temporal consistency. Our method significantly improves the temporal consistency of the learned object-centric representations, yielding more reliable video decompositions that facilitate challenging downstream tasks such as unsupervised object dynamics prediction. Furthermore, the inductive bias added by our loss strongly improves object discovery, leading to state-of-the-art results on both synthetic and real-world datasets, outperforming even weakly-supervised methods that leverage motion masks as additional cues.
<div id='section'>Paperid: <span id='pid'>643, <a href='https://arxiv.org/pdf/2411.08466.pdf' target='_blank'>https://arxiv.org/pdf/2411.08466.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Quan Zhang, Jinwei Fang, Rui Yuan, Xi Tang, Yuxin Qi, Ke Zhang, Chun Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08466">Weakly Supervised Temporal Action Localization via Dual-Prior Collaborative Learning Guided by Multimodal Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent breakthroughs in Multimodal Large Language Models (MLLMs) have gained significant recognition within the deep learning community, where the fusion of the Video Foundation Models (VFMs) and Large Language Models(LLMs) has proven instrumental in constructing robust video understanding systems, effectively surmounting constraints associated with predefined visual tasks. These sophisticated MLLMs exhibit remarkable proficiency in comprehending videos, swiftly attaining unprecedented performance levels across diverse benchmarks. However, their operation demands substantial memory and computational resources, underscoring the continued importance of traditional models in video comprehension tasks. In this paper, we introduce a novel learning paradigm termed MLLM4WTAL. This paradigm harnesses the potential of MLLM to offer temporal action key semantics and complete semantic priors for conventional Weakly-supervised Temporal Action Localization (WTAL) methods. MLLM4WTAL facilitates the enhancement of WTAL by leveraging MLLM guidance. It achieves this by integrating two distinct modules: Key Semantic Matching (KSM) and Complete Semantic Reconstruction (CSR). These modules work in tandem to effectively address prevalent issues like incomplete and over-complete outcomes common in WTAL methods. Rigorous experiments are conducted to validate the efficacy of our proposed approach in augmenting the performance of various heterogeneous WTAL models.
<div id='section'>Paperid: <span id='pid'>644, <a href='https://arxiv.org/pdf/2409.05199.pdf' target='_blank'>https://arxiv.org/pdf/2409.05199.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giannis Karamanolakis, Daniel Hsu, Luis Gravano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.05199">Interactive Machine Teaching by Labeling Rules and Instances</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised learning aims to reduce the cost of labeling data by using expert-designed labeling rules. However, existing methods require experts to design effective rules in a single shot, which is difficult in the absence of proper guidance and tooling. Therefore, it is still an open question whether experts should spend their limited time writing rules or instead providing instance labels via active learning. In this paper, we investigate how to exploit an expert's limited time to create effective supervision. First, to develop practical guidelines for rule creation, we conduct an exploratory analysis of diverse collections of existing expert-designed rules and find that rule precision is more important than coverage across datasets. Second, we compare rule creation to individual instance labeling via active learning and demonstrate the importance of both across 6 datasets. Third, we propose an interactive learning framework, INTERVAL, that achieves efficiency by automatically extracting candidate rules based on rich patterns (e.g., by prompting a language model), and effectiveness by soliciting expert feedback on both candidate rules and individual instances. Across 6 datasets, INTERVAL outperforms state-of-the-art weakly supervised approaches by 7% in F1. Furthermore, it requires as few as 10 queries for expert feedback to reach F1 values that existing active learning methods cannot match even with 100 queries.
<div id='section'>Paperid: <span id='pid'>645, <a href='https://arxiv.org/pdf/2408.10777.pdf' target='_blank'>https://arxiv.org/pdf/2408.10777.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huafeng Chen, Dian Shao, Guangqian Guo, Shan Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.10777">Just a Hint: Point-Supervised Camouflaged Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Camouflaged Object Detection (COD) demands models to expeditiously and accurately distinguish objects which conceal themselves seamlessly in the environment. Owing to the subtle differences and ambiguous boundaries, COD is not only a remarkably challenging task for models but also for human annotators, requiring huge efforts to provide pixel-wise annotations. To alleviate the heavy annotation burden, we propose to fulfill this task with the help of only one point supervision. Specifically, by swiftly clicking on each object, we first adaptively expand the original point-based annotation to a reasonable hint area. Then, to avoid partial localization around discriminative parts, we propose an attention regulator to scatter model attention to the whole object through partially masking labeled regions. Moreover, to solve the unstable feature representation of camouflaged objects under only point-based annotation, we perform unsupervised contrastive learning based on differently augmented image pairs (e.g. changing color or doing translation). On three mainstream COD benchmarks, experimental results show that our model outperforms several weakly-supervised methods by a large margin across various metrics.
<div id='section'>Paperid: <span id='pid'>646, <a href='https://arxiv.org/pdf/2408.10760.pdf' target='_blank'>https://arxiv.org/pdf/2408.10760.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huafeng Chen, Pengxu Wei, Guangqian Guo, Shan Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.10760">SAM-COD: SAM-guided Unified Framework for Weakly-Supervised Camouflaged Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most Camouflaged Object Detection (COD) methods heavily rely on mask annotations, which are time-consuming and labor-intensive to acquire. Existing weakly-supervised COD approaches exhibit significantly inferior performance compared to fully-supervised methods and struggle to simultaneously support all the existing types of camouflaged object labels, including scribbles, bounding boxes, and points. Even for Segment Anything Model (SAM), it is still problematic to handle the weakly-supervised COD and it typically encounters challenges of prompt compatibility of the scribble labels, extreme response, semantically erroneous response, and unstable feature representations, producing unsatisfactory results in camouflaged scenes. To mitigate these issues, we propose a unified COD framework in this paper, termed SAM-COD, which is capable of supporting arbitrary weakly-supervised labels. Our SAM-COD employs a prompt adapter to handle scribbles as prompts based on SAM. Meanwhile, we introduce response filter and semantic matcher modules to improve the quality of the masks obtained by SAM under COD prompts. To alleviate the negative impacts of inaccurate mask predictions, a new strategy of prompt-adaptive knowledge distillation is utilized to ensure a reliable feature representation. To validate the effectiveness of our approach, we have conducted extensive empirical experiments on three mainstream COD benchmarks. The results demonstrate the superiority of our method against state-of-the-art weakly-supervised and even fully-supervised methods.
<div id='section'>Paperid: <span id='pid'>647, <a href='https://arxiv.org/pdf/2408.09615.pdf' target='_blank'>https://arxiv.org/pdf/2408.09615.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyang Li, Xinyi Ying, Ruojing Li, Yongxian Liu, Yangsi Shi, Miao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09615">The First Competition on Resource-Limited Infrared Small Target Detection Challenge: Methods and Results</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we briefly summarize the first competition on resource-limited infrared small target detection (namely, LimitIRSTD). This competition has two tracks, including weakly-supervised infrared small target detection (Track 1) and lightweight infrared small target detection (Track 2). 46 and 60 teams successfully registered and took part in Tracks 1 and Track 2, respectively. The top-performing methods and their results in each track are described with details. This competition inspires the community to explore the tough problems in the application of infrared small target detection, and ultimately promote the deployment of this technology under limited resource.
<div id='section'>Paperid: <span id='pid'>648, <a href='https://arxiv.org/pdf/2407.09159.pdf' target='_blank'>https://arxiv.org/pdf/2407.09159.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abid Ali, Mahmoud Ali, Jean-Marc Odobez, Camilla Barbini, SÃ©verine Dubuisson, Francois Bremond, Susanne ThÃ¼mmler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.09159">Weakly-supervised Autism Severity Assessment in Long Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autism Spectrum Disorder (ASD) is a diverse collection of neurobiological conditions marked by challenges in social communication and reciprocal interactions, as well as repetitive and stereotypical behaviors. Atypical behavior patterns in a long, untrimmed video can serve as biomarkers for children with ASD. In this paper, we propose a video-based weakly-supervised method that takes spatio-temporal features of long videos to learn typical and atypical behaviors for autism detection. On top of that, we propose a shallow TCN-MLP network, which is designed to further categorize the severity score. We evaluate our method on actual evaluation videos of children with autism collected and annotated (for severity score) by clinical professionals. Experimental results demonstrate the effectiveness of behavioral biomarkers that could help clinicians in autism spectrum analysis.
<div id='section'>Paperid: <span id='pid'>649, <a href='https://arxiv.org/pdf/2406.14510.pdf' target='_blank'>https://arxiv.org/pdf/2406.14510.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rotem Shalev-Arkushin, Aharon Azulay, Tavi Halperin, Eitan Richardson, Amit H. Bermano, Ohad Fried
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.14510">V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion-based generative models have recently shown remarkable image and video editing capabilities. However, local video editing, particularly removal of small attributes like glasses, remains a challenge. Existing methods either alter the videos excessively, generate unrealistic artifacts, or fail to perform the requested edit consistently throughout the video. In this work, we focus on consistent and identity-preserving removal of glasses in videos, using it as a case study for consistent local attribute removal in videos. Due to the lack of paired data, we adopt a weakly supervised approach and generate synthetic imperfect data, using an adjusted pretrained diffusion model. We show that despite data imperfection, by learning from our generated data and leveraging the prior of pretrained diffusion models, our model is able to perform the desired edit consistently while preserving the original video content. Furthermore, we exemplify the generalization ability of our method to other local video editing tasks by applying it successfully to facial sticker-removal. Our approach demonstrates significant improvement over existing methods, showcasing the potential of leveraging synthetic data and strong video priors for local video editing tasks.
<div id='section'>Paperid: <span id='pid'>650, <a href='https://arxiv.org/pdf/2406.00474.pdf' target='_blank'>https://arxiv.org/pdf/2406.00474.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zimin Xia, Yujiao Shi, Hongdong Li, Julian F. P. Kooij
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00474">Adapting Fine-Grained Cross-View Localization to Areas without Fine Ground Truth</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Given a ground-level query image and a geo-referenced aerial image that covers the query's local surroundings, fine-grained cross-view localization aims to estimate the location of the ground camera inside the aerial image. Recent works have focused on developing advanced networks trained with accurate ground truth (GT) locations of ground images. However, the trained models always suffer a performance drop when applied to images in a new target area that differs from training. In most deployment scenarios, acquiring fine GT, i.e. accurate GT locations, for target-area images to re-train the network can be expensive and sometimes infeasible. In contrast, collecting images with noisy GT with errors of tens of meters is often easy. Motivated by this, our paper focuses on improving the performance of a trained model in a new target area by leveraging only the target-area images without fine GT. We propose a weakly supervised learning approach based on knowledge self-distillation. This approach uses predictions from a pre-trained model as pseudo GT to supervise a copy of itself. Our approach includes a mode-based pseudo GT generation for reducing uncertainty in pseudo GT and an outlier filtering method to remove unreliable pseudo GT. Our approach is validated using two recent state-of-the-art models on two benchmarks. The results demonstrate that it consistently and considerably boosts the localization accuracy in the target area.
<div id='section'>Paperid: <span id='pid'>651, <a href='https://arxiv.org/pdf/2405.18148.pdf' target='_blank'>https://arxiv.org/pdf/2405.18148.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>JuneHyoung Kwon, Eunju Lee, Yunsung Cho, YoungBin Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.18148">Learning to Detour: Shortcut Mitigating Augmentation for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation (WSSS) employing weak forms of labels has been actively studied to alleviate the annotation cost of acquiring pixel-level labels. However, classifiers trained on biased datasets tend to exploit shortcut features and make predictions based on spurious correlations between certain backgrounds and objects, leading to a poor generalization performance. In this paper, we propose shortcut mitigating augmentation (SMA) for WSSS, which generates synthetic representations of object-background combinations not seen in the training data to reduce the use of shortcut features. Our approach disentangles the object-relevant and background features. We then shuffle and combine the disentangled representations to create synthetic features of diverse object-background combinations. SMA-trained classifier depends less on contexts and focuses more on the target object when making predictions. In addition, we analyzed the behavior of the classifier on shortcut usage after applying our augmentation using an attribution method-based metric. The proposed method achieved the improved performance of semantic segmentation result on PASCAL VOC 2012 and MS COCO 2014 datasets.
<div id='section'>Paperid: <span id='pid'>652, <a href='https://arxiv.org/pdf/2404.12832.pdf' target='_blank'>https://arxiv.org/pdf/2404.12832.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dmytro Shvetsov, Joonas Ariva, Marharyta Domnich, Raul Vicente, Dmytro Fishman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.12832">COIN: Counterfactual inpainting for weakly supervised semantic segmentation for medical images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning is dramatically transforming the field of medical imaging and radiology, enabling the identification of pathologies in medical images, including computed tomography (CT) and X-ray scans. However, the performance of deep learning models, particularly in segmentation tasks, is often limited by the need for extensive annotated datasets. To address this challenge, the capabilities of weakly supervised semantic segmentation are explored through the lens of Explainable AI and the generation of counterfactual explanations. The scope of this research is development of a novel counterfactual inpainting approach (COIN) that flips the predicted classification label from abnormal to normal by using a generative model. For instance, if the classifier deems an input medical image X as abnormal, indicating the presence of a pathology, the generative model aims to inpaint the abnormal region, thus reversing the classifier's original prediction label. The approach enables us to produce precise segmentations for pathologies without depending on pre-existing segmentation masks. Crucially, image-level labels are utilized, which are substantially easier to acquire than creating detailed segmentation masks. The effectiveness of the method is demonstrated by segmenting synthetic targets and actual kidney tumors from CT images acquired from Tartu University Hospital in Estonia. The findings indicate that COIN greatly surpasses established attribution methods, such as RISE, ScoreCAM, and LayerCAM, as well as an alternative counterfactual explanation method introduced by Singla et al. This evidence suggests that COIN is a promising approach for semantic segmentation of tumors in CT images, and presents a step forward in making deep learning applications more accessible and effective in healthcare, where annotated data is scarce.
<div id='section'>Paperid: <span id='pid'>653, <a href='https://arxiv.org/pdf/2404.10242.pdf' target='_blank'>https://arxiv.org/pdf/2404.10242.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Oren Kraus, Kian Kenyon-Dean, Saber Saberian, Maryam Fallah, Peter McLean, Jess Leung, Vasudev Sharma, Ayla Khan, Jia Balakrishnan, Safiye Celik, Dominique Beaini, Maciej Sypetkowski, Chi Vicky Cheng, Kristen Morse, Maureen Makes, Ben Mabey, Berton Earnshaw
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10242">Masked Autoencoders for Microscopy are Scalable Learners of Cellular Biology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Featurizing microscopy images for use in biological research remains a significant challenge, especially for large-scale experiments spanning millions of images. This work explores the scaling properties of weakly supervised classifiers and self-supervised masked autoencoders (MAEs) when training with increasingly larger model backbones and microscopy datasets. Our results show that ViT-based MAEs outperform weakly supervised classifiers on a variety of tasks, achieving as much as a 11.5% relative improvement when recalling known biological relationships curated from public databases. Additionally, we develop a new channel-agnostic MAE architecture (CA-MAE) that allows for inputting images of different numbers and orders of channels at inference time. We demonstrate that CA-MAEs effectively generalize by inferring and evaluating on a microscopy image dataset (JUMP-CP) generated under different experimental conditions with a different channel structure than our pretraining data (RPI-93M). Our findings motivate continued research into scaling self-supervised learning on microscopy data in order to create powerful foundation models of cellular biology that have the potential to catalyze advancements in drug discovery and beyond.
<div id='section'>Paperid: <span id='pid'>654, <a href='https://arxiv.org/pdf/2404.08531.pdf' target='_blank'>https://arxiv.org/pdf/2404.08531.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiwei Yang, Jing Liu, Peng Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.08531">Text Prompt with Normality Guidance for Weakly Supervised Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised video anomaly detection (WSVAD) is a challenging task. Generating fine-grained pseudo-labels based on weak-label and then self-training a classifier is currently a promising solution. However, since the existing methods use only RGB visual modality and the utilization of category text information is neglected, thus limiting the generation of more accurate pseudo-labels and affecting the performance of self-training. Inspired by the manual labeling process based on the event description, in this paper, we propose a novel pseudo-label generation and self-training framework based on Text Prompt with Normality Guidance (TPWNG) for WSVAD. Our idea is to transfer the rich language-visual knowledge of the contrastive language-image pre-training (CLIP) model for aligning the video event description text and corresponding video frames to generate pseudo-labels. Specifically, We first fine-tune the CLIP for domain adaptation by designing two ranking losses and a distributional inconsistency loss. Further, we propose a learnable text prompt mechanism with the assist of a normality visual prompt to further improve the matching accuracy of video event description text and video frames. Then, we design a pseudo-label generation module based on the normality guidance to infer reliable frame-level pseudo-labels. Finally, we introduce a temporal context self-adaptive learning module to learn the temporal dependencies of different video events more flexibly and accurately. Extensive experiments show that our method achieves state-of-the-art performance on two benchmark datasets, UCF-Crime and XD-Viole
<div id='section'>Paperid: <span id='pid'>655, <a href='https://arxiv.org/pdf/2403.04865.pdf' target='_blank'>https://arxiv.org/pdf/2403.04865.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gabriele Campanella, Eugene Fluder, Jennifer Zeng, Chad Vanderbilt, Thomas J. Fuchs
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.04865">Beyond Multiple Instance Learning: Full Resolution All-In-Memory End-To-End Pathology Slide Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Artificial Intelligence (AI) has great potential to improve health outcomes by training systems on vast digitized clinical datasets. Computational Pathology, with its massive amounts of microscopy image data and impact on diagnostics and biomarkers, is at the forefront of this development. Gigapixel pathology slides pose a unique challenge due to their enormous size and are usually divided into tens of thousands of smaller tiles for analysis. This results in a discontinuity in the machine learning process by separating the training of tile-level encoders from slide-level aggregators and the need to adopt weakly supervised learning strategies. Training models from entire pathology slides end-to-end has been largely unexplored due to its computational challenges. To overcome this problem, we propose a novel approach to jointly train both a tile encoder and a slide-aggregator fully in memory and end-to-end at high-resolution, bridging the gap between input and slide-level supervision. While more computationally expensive, detailed quantitative validation shows promise for large-scale pre-training and fine-tuning of pathology foundation models.
<div id='section'>Paperid: <span id='pid'>656, <a href='https://arxiv.org/pdf/2403.02746.pdf' target='_blank'>https://arxiv.org/pdf/2403.02746.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuohong Li, Wei He, Jiepan Li, Fangxiao Lu, Hongyan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.02746">Learning without Exact Guidance: Updating Large-scale High-resolution Land Cover Maps from Low-resolution Historical Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale high-resolution (HR) land-cover mapping is a vital task to survey the Earth's surface and resolve many challenges facing humanity. However, it is still a non-trivial task hindered by complex ground details, various landforms, and the scarcity of accurate training labels over a wide-span geographic area. In this paper, we propose an efficient, weakly supervised framework (Paraformer) to guide large-scale HR land-cover mapping with easy-access historical land-cover data of low resolution (LR). Specifically, existing land-cover mapping approaches reveal the dominance of CNNs in preserving local ground details but still suffer from insufficient global modeling in various landforms. Therefore, we design a parallel CNN-Transformer feature extractor in Paraformer, consisting of a downsampling-free CNN branch and a Transformer branch, to jointly capture local and global contextual information. Besides, facing the spatial mismatch of training data, a pseudo-label-assisted training (PLAT) module is adopted to reasonably refine LR labels for weakly supervised semantic segmentation of HR images. Experiments on two large-scale datasets demonstrate the superiority of Paraformer over other state-of-the-art methods for automatically updating HR land-cover maps from LR historical labels.
<div id='section'>Paperid: <span id='pid'>657, <a href='https://arxiv.org/pdf/2403.01381.pdf' target='_blank'>https://arxiv.org/pdf/2403.01381.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Feng, Hao Huang, Junpeng Zhang, Weisheng Dong, Dingwen Zhang, Licheng Jiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.01381">SA-MixNet: Structure-aware Mixup and Invariance Learning for Scribble-supervised Road Extraction in Remote Sensing Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mainstreamed weakly supervised road extractors rely on highly confident pseudo-labels propagated from scribbles, and their performance often degrades gradually as the image scenes tend various. We argue that such degradation is due to the poor model's invariance to scenes with different complexities, whereas existing solutions to this problem are commonly based on crafted priors that cannot be derived from scribbles. To eliminate the reliance on such priors, we propose a novel Structure-aware Mixup and Invariance Learning framework (SA-MixNet) for weakly supervised road extraction that improves the model invariance in a data-driven manner. Specifically, we design a structure-aware Mixup scheme to paste road regions from one image onto another for creating an image scene with increased complexity while preserving the road's structural integrity. Then an invariance regularization is imposed on the predictions of constructed and origin images to minimize their conflicts, which thus forces the model to behave consistently on various scenes. Moreover, a discriminator-based regularization is designed for enhancing the connectivity meanwhile preserving the structure of roads. Combining these designs, our framework demonstrates superior performance on the DeepGlobe, Wuhan, and Massachusetts datasets outperforming the state-of-the-art techniques by 1.47%, 2.12%, 4.09% respectively in IoU metrics, and showing its potential of plug-and-play. The code will be made publicly available.
<div id='section'>Paperid: <span id='pid'>658, <a href='https://arxiv.org/pdf/2402.19116.pdf' target='_blank'>https://arxiv.org/pdf/2402.19116.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiamin Luo, Jianing Zhao, Jingjing Wang, Guodong Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.19116">How to Understand "Support"? An Implicit-enhanced Causal Inference Approach for Weakly-supervised Phrase Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised Phrase Grounding (WPG) is an emerging task of inferring the fine-grained phrase-region matching, while merely leveraging the coarse-grained sentence-image pairs for training. However, existing studies on WPG largely ignore the implicit phrase-region matching relations, which are crucial for evaluating the capability of models in understanding the deep multimodal semantics. To this end, this paper proposes an Implicit-Enhanced Causal Inference (IECI) approach to address the challenges of modeling the implicit relations and highlighting them beyond the explicit. Specifically, this approach leverages both the intervention and counterfactual techniques to tackle the above two challenges respectively. Furthermore, a high-quality implicit-enhanced dataset is annotated to evaluate IECI and detailed evaluations show the great advantages of IECI over the state-of-the-art baselines. Particularly, we observe an interesting finding that IECI outperforms the advanced multimodal LLMs by a large margin on this implicit-enhanced dataset, which may facilitate more research to evaluate the multimodal LLMs in this direction.
<div id='section'>Paperid: <span id='pid'>659, <a href='https://arxiv.org/pdf/2402.07685.pdf' target='_blank'>https://arxiv.org/pdf/2402.07685.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jacob Tyo, Zachary C. Lipton
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.07685">Contrastive Multiple Instance Learning for Weakly Supervised Person ReID</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The acquisition of large-scale, precisely labeled datasets for person re-identification (ReID) poses a significant challenge. Weakly supervised ReID has begun to address this issue, although its performance lags behind fully supervised methods. In response, we introduce Contrastive Multiple Instance Learning (CMIL), a novel framework tailored for more effective weakly supervised ReID. CMIL distinguishes itself by requiring only a single model and no pseudo labels while leveraging contrastive losses -- a technique that has significantly enhanced traditional ReID performance yet is absent in all prior MIL-based approaches. Through extensive experiments and analysis across three datasets, CMIL not only matches state-of-the-art performance on the large-scale SYSU-30k dataset with fewer assumptions but also consistently outperforms all baselines on the WL-market1501 and Weakly Labeled MUddy racer re-iDentification dataset (WL-MUDD) datasets. We introduce and release the WL-MUDD dataset, an extension of the MUDD dataset featuring naturally occurring weak labels from the real-world application at PerformancePhoto.co. All our code and data are accessible at https://drive.google.com/file/d/1rjMbWB6m-apHF3Wg_cfqc8QqKgQ21AsT/view?usp=drive_link.
<div id='section'>Paperid: <span id='pid'>660, <a href='https://arxiv.org/pdf/2512.06171.pdf' target='_blank'>https://arxiv.org/pdf/2512.06171.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jessica Plassmann, Nicolas Schuler, Michael Schuth, Georg von Freymann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.06171">Automated Annotation of Shearographic Measurements Enabling Weakly Supervised Defect Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Shearography is an interferometric technique sensitive to surface displacement gradients, providing high sensitivity for detecting subsurface defects in safety-critical components. A key limitation to industrial adoption is the lack of high-quality annotated datasets, since manual labeling remains labor-intensive, subjective, and difficult to standardize. We introduce an automated workflow that generates defect annotations from shearography measurements using deep learning, producing high-resolution segmentation and bounding-box labels. Evaluation against expert-labeled data demonstrates sufficient accuracy to enable weakly supervised training, reducing manual effort and supporting scalable dataset creation for robust defect detection.
<div id='section'>Paperid: <span id='pid'>661, <a href='https://arxiv.org/pdf/2512.05922.pdf' target='_blank'>https://arxiv.org/pdf/2512.05922.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Khang Le, Anh Mai Vu, Thi Kim Trang Vo, Ha Thach, Ngoc Bui Lam Quang, Thanh-Huy Nguyen, Minh H. N. Le, Zhu Han, Chandra Mohan, Hien Van Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.05922">LPD: Learnable Prototypes with Diversity Regularization for Weakly Supervised Histopathology Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation (WSSS) in histopathology reduces pixel-level labeling by learning from image-level labels, but it is hindered by inter-class homogeneity, intra-class heterogeneity, and CAM-induced region shrinkage (global pooling-based class activation maps whose activations highlight only the most distinctive areas and miss nearby class regions). Recent works address these challenges by constructing a clustering prototype bank and then refining masks in a separate stage; however, such two-stage pipelines are costly, sensitive to hyperparameters, and decouple prototype discovery from segmentation learning, limiting their effectiveness and efficiency. We propose a cluster-free, one-stage learnable-prototype framework with diversity regularization to enhance morphological intra-class heterogeneity coverage. Our approach achieves state-of-the-art (SOTA) performance on BCSS-WSSS, outperforming prior methods in mIoU and mDice. Qualitative segmentation maps show sharper boundaries and fewer mislabels, and activation heatmaps further reveal that, compared with clustering-based prototypes, our learnable prototypes cover more diverse and complementary regions within each class, providing consistent qualitative evidence for their effectiveness.
<div id='section'>Paperid: <span id='pid'>662, <a href='https://arxiv.org/pdf/2511.15396.pdf' target='_blank'>https://arxiv.org/pdf/2511.15396.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simon Boeder, Fabian Gigengack, Simon Roesler, Holger Caesar, Benjamin Risse
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.15396">ShelfOcc: Native 3D Supervision beyond LiDAR for Vision-Based Occupancy Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent progress in self- and weakly supervised occupancy estimation has largely relied on 2D projection or rendering-based supervision, which suffers from geometric inconsistencies and severe depth bleeding. We thus introduce ShelfOcc, a vision-only method that overcomes these limitations without relying on LiDAR. ShelfOcc brings supervision into native 3D space by generating metrically consistent semantic voxel labels from video, enabling true 3D supervision without any additional sensors or manual 3D annotations. While recent vision-based 3D geometry foundation models provide a promising source of prior knowledge, they do not work out of the box as a prediction due to sparse or noisy and inconsistent geometry, especially in dynamic driving scenes. Our method introduces a dedicated framework that mitigates these issues by filtering and accumulating static geometry consistently across frames, handling dynamic content and propagating semantic information into a stable voxel representation. This data-centric shift in supervision for weakly/shelf-supervised occupancy estimation allows the use of essentially any SOTA occupancy model architecture without relying on LiDAR data. We argue that such high-quality supervision is essential for robust occupancy learning and constitutes an important complementary avenue to architectural innovation. On the Occ3D-nuScenes benchmark, ShelfOcc substantially outperforms all previous weakly/shelf-supervised methods (up to a 34% relative improvement), establishing a new data-driven direction for LiDAR-free 3D scene understanding.
<div id='section'>Paperid: <span id='pid'>663, <a href='https://arxiv.org/pdf/2511.12269.pdf' target='_blank'>https://arxiv.org/pdf/2511.12269.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rupam Mukherjee, Rajkumar Daniel, Soujanya Hazra, Shirin Dasgupta, Subhamoy Mandal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.12269">RAA-MIL: A Novel Framework for Classification of Oral Cytology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cytology is a valuable tool for early detection of oral squamous cell carcinoma (OSCC). However, manual examination of cytology whole slide images (WSIs) is slow, subjective, and depends heavily on expert pathologists. To address this, we introduce the first weakly supervised deep learning framework for patient-level diagnosis of oral cytology whole slide images, leveraging the newly released Oral Cytology Dataset [1], which provides annotated cytology WSIs from ten medical centres across India. Each patient case is represented as a bag of cytology patches and assigned a diagnosis label (Healthy, Benign, Oral Potentially Malignant Disorders (OPMD), OSCC) by an in-house expert pathologist. These patient-level weak labels form a new extension to the dataset. We evaluate a baseline multiple-instance learning (MIL) model and a proposed Region-Affinity Attention MIL (RAA-MIL) that models spatial relationships between regions within each slide. The RAA-MIL achieves an average accuracy of 72.7%, weighted F1-score of 0.69 on an unseen test set, outperforming the baseline. This study establishes the first patient-level weakly supervised benchmark for oral cytology and moves toward reliable AI-assisted digital pathology.
<div id='section'>Paperid: <span id='pid'>664, <a href='https://arxiv.org/pdf/2510.12827.pdf' target='_blank'>https://arxiv.org/pdf/2510.12827.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md. Nayeem, Md Shamse Tabrej, Kabbojit Jit Deb, Shaonti Goswami, Md. Azizul Hakim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.12827">Automatic Speech Recognition in the Modern Era: Architectures, Training, and Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic Speech Recognition (ASR) has undergone a profound transformation over the past decade, driven by advances in deep learning. This survey provides a comprehensive overview of the modern era of ASR, charting its evolution from traditional hybrid systems, such as Gaussian Mixture Model-Hidden Markov Models (GMM-HMMs) and Deep Neural Network-HMMs (DNN-HMMs), to the now-dominant end-to-end neural architectures. We systematically review the foundational end-to-end paradigms: Connectionist Temporal Classification (CTC), attention-based encoder-decoder models, and the Recurrent Neural Network Transducer (RNN-T), which established the groundwork for fully integrated speech-to-text systems. We then detail the subsequent architectural shift towards Transformer and Conformer models, which leverage self-attention to capture long-range dependencies with high computational efficiency. A central theme of this survey is the parallel revolution in training paradigms. We examine the progression from fully supervised learning, augmented by techniques like SpecAugment, to the rise of self-supervised learning (SSL) with foundation models such as wav2vec 2.0, which drastically reduce the reliance on transcribed data. Furthermore, we analyze the impact of largescale, weakly supervised models like Whisper, which achieve unprecedented robustness through massive data diversity. The paper also covers essential ecosystem components, including key datasets and benchmarks (e.g., LibriSpeech, Switchboard, CHiME), standard evaluation metrics (e.g., Word Error Rate), and critical considerations for real-world deployment, such as streaming inference, on-device efficiency, and the ethical imperatives of fairness and robustness. We conclude by outlining open challenges and future research directions.
<div id='section'>Paperid: <span id='pid'>665, <a href='https://arxiv.org/pdf/2510.04477.pdf' target='_blank'>https://arxiv.org/pdf/2510.04477.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soo Yong Kim, Suin Cho, Vincent-Daniel Yun, Gyeongyeon Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04477">MedCLM: Learning to Localize and Reason via a CoT-Curriculum in Medical Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Bridging clinical diagnostic reasoning with AI remains a central challenge in medical imaging. We introduce MedCLM, an automated pipeline that converts detection datasets into large-scale medical visual question answering (VQA) data with Chain-of-Thought (CoT) reasoning by linking lesion boxes to organ segmentation and structured rationales. These contextual signals enable medical vision-language models to generate question-answer pairs with step-by-step reasoning. To utilize this data effectively, we propose an Integrated CoT-Curriculum Strategy composed of an Easy stage with explicit lesion boxes for visual grounding, a Medium stage that encourages implicit localization, and a Hard stage for weakly supervised reasoning. Experimental results demonstrate that MedCLM attains state-of-the-art performance on several medical VQA benchmarks, providing a scalable framework for developing clinically aligned medical vision-language models.
<div id='section'>Paperid: <span id='pid'>666, <a href='https://arxiv.org/pdf/2509.10184.pdf' target='_blank'>https://arxiv.org/pdf/2509.10184.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leen Almajed, Abeer ALdayel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10184">Incongruent Positivity: When Miscalibrated Positivity Undermines Online Supportive Conversations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In emotionally supportive conversations, well-intended positivity can sometimes misfire, leading to responses that feel dismissive, minimizing, or unrealistically optimistic. We examine this phenomenon of incongruent positivity as miscalibrated expressions of positive support in both human and LLM generated responses. To this end, we collected real user-assistant dialogues from Reddit across a range of emotional intensities and generated additional responses using large language models for the same context. We categorize these conversations by intensity into two levels: Mild, which covers relationship tension and general advice, and Severe, which covers grief and anxiety conversations. This level of categorization enables a comparative analysis of how supportive responses vary across lower and higher stakes contexts. Our analysis reveals that LLMs are more prone to unrealistic positivity through dismissive and minimizing tone, particularly in high-stakes contexts. To further study the underlying dimensions of this phenomenon, we finetune LLMs on datasets with strong and weak emotional reactions. Moreover, we developed a weakly supervised multilabel classifier ensemble (DeBERTa and MentalBERT) that shows improved detection of incongruent positivity types across two sorts of concerns (Mild and Severe). Our findings shed light on the need to move beyond merely generating generic positive responses and instead study the congruent support measures to balance positive affect with emotional acknowledgment. This approach offers insights into aligning large language models with affective expectations in the online supportive dialogue, paving the way toward context-aware and trust preserving online conversation systems.
<div id='section'>Paperid: <span id='pid'>667, <a href='https://arxiv.org/pdf/2509.04491.pdf' target='_blank'>https://arxiv.org/pdf/2509.04491.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinnian Zhao, Hugo Van Hamme
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04491">Refining Transcripts With TV Subtitles by Prompt-Based Weakly Supervised Training of ASR</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study proposes a novel approach to using TV subtitles within a weakly supervised (WS) Automatic Speech Recognition (ASR) framework. Although TV subtitles are readily available, their imprecise alignment with corresponding audio limits their applicability as supervised targets for verbatim transcription. Rather than using subtitles as direct supervision signals, our method reimagines them as context-rich prompts. This design enables the model to handle discrepancies between spoken audio and subtitle text. Instead, generated pseudo transcripts become the primary targets, with subtitles acting as guiding cues for iterative refinement. To further enhance the process, we introduce a weighted attention mechanism that emphasizes relevant subtitle tokens during inference. Our experiments demonstrate significant improvements in transcription accuracy, highlighting the effectiveness of the proposed method in refining transcripts. These enhanced pseudo-labeled datasets provide high-quality foundational resources for training robust ASR systems.
<div id='section'>Paperid: <span id='pid'>668, <a href='https://arxiv.org/pdf/2509.01214.pdf' target='_blank'>https://arxiv.org/pdf/2509.01214.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yizhe Yuan, Bingsen Xue, Bangzheng Pu, Chengxiang Wang, Cheng Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01214">PRINTER:Deformation-Aware Adversarial Learning for Virtual IHC Staining with In Situ Fidelity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tumor spatial heterogeneity analysis requires precise correlation between Hematoxylin and Eosin H&E morphology and immunohistochemical (IHC) biomarker expression, yet current methods suffer from spatial misalignment in consecutive sections, severely compromising in situ pathological interpretation. In order to obtain a more accurate virtual staining pattern, We propose PRINTER, a weakly-supervised framework that integrates PRototype-drIven content and staiNing patTERn decoupling and deformation-aware adversarial learning strategies designed to accurately learn IHC staining patterns while preserving H&E staining details. Our approach introduces three key innovations: (1) A prototype-driven staining pattern transfer with explicit content-style decoupling; and (2) A cyclic registration-synthesis framework GapBridge that bridges H&E and IHC domains through deformable structural alignment, where registered features guide cross-modal style transfer while synthesized outputs iteratively refine the registration;(3) Deformation-Aware Adversarial Learning: We propose a training framework where a generator and deformation-aware registration network jointly adversarially optimize a style-focused discriminator. Extensive experiments demonstrate that PRINTER effectively achieves superior performance in preserving H&E staining details and virtual staining fidelity, outperforming state-of-the-art methods. Our work provides a robust and scalable solution for virtual staining, advancing the field of computational pathology.
<div id='section'>Paperid: <span id='pid'>669, <a href='https://arxiv.org/pdf/2508.12290.pdf' target='_blank'>https://arxiv.org/pdf/2508.12290.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chor Boon Tan, Conghui Hu, Gim Hee Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12290">CLAIR: CLIP-Aided Weakly Supervised Zero-Shot Cross-Domain Image Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent growth of large foundation models that can easily generate pseudo-labels for huge quantity of unlabeled data makes unsupervised Zero-Shot Cross-Domain Image Retrieval (UZS-CDIR) less relevant. In this paper, we therefore turn our attention to weakly supervised ZS-CDIR (WSZS-CDIR) with noisy pseudo labels generated by large foundation models such as CLIP. To this end, we propose CLAIR to refine the noisy pseudo-labels with a confidence score from the similarity between the CLIP text and image features. Furthermore, we design inter-instance and inter-cluster contrastive losses to encode images into a class-aware latent space, and an inter-domain contrastive loss to alleviate domain discrepancies. We also learn a novel cross-domain mapping function in closed-form, using only CLIP text embeddings to project image features from one domain to another, thereby further aligning the image features for retrieval. Finally, we enhance the zero-shot generalization ability of our CLAIR to handle novel categories by introducing an extra set of learnable prompts. Extensive experiments are carried out using TUBerlin, Sketchy, Quickdraw, and DomainNet zero-shot datasets, where our CLAIR consistently shows superior performance compared to existing state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>670, <a href='https://arxiv.org/pdf/2505.18368.pdf' target='_blank'>https://arxiv.org/pdf/2505.18368.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yike Zhang, Jack H. Noble
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18368">Weakly-supervised Mamba-Based Mastoidectomy Shape Prediction for Cochlear Implant Surgery Using 3D T-Distribution Loss</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cochlear implant surgery is a treatment for individuals with severe hearing loss. It involves inserting an array of electrodes inside the cochlea to electrically stimulate the auditory nerve and restore hearing sensation. A crucial step in this procedure is mastoidectomy, a surgical intervention that removes part of the mastoid region of the temporal bone, providing a critical pathway to the cochlea for electrode placement. Accurate prediction of the mastoidectomy region from preoperative imaging assists presurgical planning, reduces surgical risks, and improves surgical outcomes. In previous work, a self-supervised network was introduced to predict the mastoidectomy region using only preoperative CT scans. While promising, the method suffered from suboptimal robustness, limiting its practical application. To address this limitation, we propose a novel weakly-supervised Mamba-based framework to predict accurate mastoidectomy regions directly from preoperative CT scans. Our approach utilizes a 3D T-Distribution loss function inspired by the Student-t distribution, which effectively handles the complex geometric variability inherent in mastoidectomy shapes. Weak supervision is achieved using the segmentation results from the prior self-supervised network to eliminate the need for manual data cleaning or labeling throughout the training process. The proposed method is extensively evaluated against state-of-the-art approaches, demonstrating superior performance in predicting accurate and clinically relevant mastoidectomy regions. Our findings highlight the robustness and efficiency of the weakly-supervised learning framework with the proposed novel 3D T-Distribution loss.
<div id='section'>Paperid: <span id='pid'>671, <a href='https://arxiv.org/pdf/2505.13911.pdf' target='_blank'>https://arxiv.org/pdf/2505.13911.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruijie Zhao, Zuopeng Tan, Xiao Xue, Longfei Zhao, Bing Li, Zicheng Liao, Ying Ming, Jiaru Wang, Ran Xiao, Sirong Piao, Rui Zhao, Qiqi Xu, Wei Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13911">Bronchovascular Tree-Guided Weakly Supervised Learning Method for Pulmonary Segment Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pulmonary segment segmentation is crucial for cancer localization and surgical planning. However, the pixel-wise annotation of pulmonary segments is laborious, as the boundaries between segments are indistinguishable in medical images. To this end, we propose a weakly supervised learning (WSL) method, termed Anatomy-Hierarchy Supervised Learning (AHSL), which consults the precise clinical anatomical definition of pulmonary segments to perform pulmonary segment segmentation. Since pulmonary segments reside within the lobes and are determined by the bronchovascular tree, i.e., artery, airway and vein, the design of the loss function is founded on two principles. First, segment-level labels are utilized to directly supervise the output of the pulmonary segments, ensuring that they accurately encompass the appropriate bronchovascular tree. Second, lobe-level supervision indirectly oversees the pulmonary segment, ensuring their inclusion within the corresponding lobe. Besides, we introduce a two-stage segmentation strategy that incorporates bronchovascular priori information. Furthermore, a consistency loss is proposed to enhance the smoothness of segment boundaries, along with an evaluation metric designed to measure the smoothness of pulmonary segment boundaries. Visual inspection and evaluation metrics from experiments conducted on a private dataset demonstrate the effectiveness of our method.
<div id='section'>Paperid: <span id='pid'>672, <a href='https://arxiv.org/pdf/2505.09615.pdf' target='_blank'>https://arxiv.org/pdf/2505.09615.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yung-Hsuan Lai, Janek Ebbers, Yu-Chiang Frank Wang, FranÃ§ois Germain, Michael Jeffrey Jones, Moitreya Chatterjee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09615">UWAV: Uncertainty-weighted Weakly-supervised Audio-Visual Video Parsing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-Visual Video Parsing (AVVP) entails the challenging task of localizing both uni-modal events (i.e., those occurring exclusively in either the visual or acoustic modality of a video) and multi-modal events (i.e., those occurring in both modalities concurrently). Moreover, the prohibitive cost of annotating training data with the class labels of all these events, along with their start and end times, imposes constraints on the scalability of AVVP techniques unless they can be trained in a weakly-supervised setting, where only modality-agnostic, video-level labels are available in the training data. To this end, recently proposed approaches seek to generate segment-level pseudo-labels to better guide model training. However, the absence of inter-segment dependencies when generating these pseudo-labels and the general bias towards predicting labels that are absent in a segment limit their performance. This work proposes a novel approach towards overcoming these weaknesses called Uncertainty-weighted Weakly-supervised Audio-visual Video Parsing (UWAV). Additionally, our innovative approach factors in the uncertainty associated with these estimated pseudo-labels and incorporates a feature mixup based training regularization for improved training. Empirical results show that UWAV outperforms state-of-the-art methods for the AVVP task on multiple metrics, across two different datasets, attesting to its effectiveness and generalizability.
<div id='section'>Paperid: <span id='pid'>673, <a href='https://arxiv.org/pdf/2504.14300.pdf' target='_blank'>https://arxiv.org/pdf/2504.14300.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Liang, Hao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14300">Learning and Generating Diverse Residential Load Patterns Using GAN with Weakly-Supervised Training and Weight Selection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The scarcity of high-quality residential load data can pose obstacles for decarbonizing the residential sector as well as effective grid planning and operation. The above challenges have motivated research into generating synthetic load data, but existing methods faced limitations in terms of scalability, diversity, and similarity. This paper proposes a Generative Adversarial Network-based Synthetic Residential Load Pattern (RLP-GAN) generation model, a novel weakly-supervised GAN framework, leveraging an over-complete autoencoder to capture dependencies within complex and diverse load patterns and learn household-level data distribution at scale. We incorporate a model weight selection method to address the mode collapse problem and generate load patterns with high diversity. We develop a holistic evaluation method to validate the effectiveness of RLP-GAN using real-world data of 417 households. The results demonstrate that RLP-GAN outperforms state-of-the-art models in capturing temporal dependencies and generating load patterns with higher similarity to real data. Furthermore, we have publicly released the RLP-GAN generated synthetic dataset, which comprises one million synthetic residential load pattern profiles.
<div id='section'>Paperid: <span id='pid'>674, <a href='https://arxiv.org/pdf/2503.18509.pdf' target='_blank'>https://arxiv.org/pdf/2503.18509.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nijesh Upreti, Vaishak Belle
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18509">Neuro-symbolic Weak Supervision: Theory and Semantics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weak supervision allows machine learning models to learn from limited or noisy labels, but it introduces challenges in interpretability and reliability - particularly in multi-instance partial label learning (MI-PLL), where models must resolve both ambiguous labels and uncertain instance-label mappings. We propose a semantics for neuro-symbolic framework that integrates Inductive Logic Programming (ILP) to improve MI-PLL by providing structured relational constraints that guide learning. Within our semantic characterization, ILP defines a logical hypothesis space for label transitions, clarifies classifier semantics, and establishes interpretable performance standards. This hybrid approach improves robustness, transparency, and accountability in weakly supervised settings, ensuring neural predictions align with domain knowledge. By embedding weak supervision into a logical framework, we enhance both interpretability and learning, making weak supervision more suitable for real-world, high-stakes applications.
<div id='section'>Paperid: <span id='pid'>675, <a href='https://arxiv.org/pdf/2503.18384.pdf' target='_blank'>https://arxiv.org/pdf/2503.18384.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Gao, Shaobo Xia, Pu Wang, Xiaohuan Xi, Sheng Nie, Cheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18384">LiDAR Remote Sensing Meets Weak Supervision: Concepts, Methods, and Perspectives</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LiDAR (Light Detection and Ranging) enables rapid and accurate acquisition of three-dimensional spatial data, widely applied in remote sensing areas such as surface mapping, environmental monitoring, urban modeling, and forestry inventory. LiDAR remote sensing primarily includes data interpretation and LiDAR-based inversion. However, LiDAR interpretation typically relies on dense and precise annotations, which are costly and time-consuming. Similarly, LiDAR inversion depends on scarce supervisory signals and expensive field surveys for annotations. To address this challenge, weakly supervised learning has gained significant attention in recent years, with many methods emerging to tackle LiDAR remote sensing tasks using incomplete, inaccurate, and inexact annotations, as well as annotations from other domains. Existing review articles treat LiDAR interpretation and inversion as separate tasks. This review, for the first time, adopts a unified weakly supervised learning perspective to systematically examine research on both LiDAR interpretation and inversion. We summarize the latest advancements, provide a comprehensive review of the development and application of weakly supervised techniques in LiDAR remote sensing, and discuss potential future research directions in this field.
<div id='section'>Paperid: <span id='pid'>676, <a href='https://arxiv.org/pdf/2503.16546.pdf' target='_blank'>https://arxiv.org/pdf/2503.16546.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Saddam Hussain Khan, Rashid Iqbal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16546">A Comprehensive Survey on Architectural Advances in Deep CNNs: Challenges, Applications, and Emerging Research Directions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep Convolutional Neural Networks (CNNs) have significantly advanced deep learning, driving breakthroughs in computer vision, natural language processing, medical diagnosis, object detection, and speech recognition. Architectural innovations including 1D, 2D, and 3D convolutional models, dilated and grouped convolutions, depthwise separable convolutions, and attention mechanisms address domain-specific challenges and enhance feature representation and computational efficiency. Structural refinements such as spatial-channel exploitation, multi-path design, and feature-map enhancement contribute to robust hierarchical feature extraction and improved generalization, particularly through transfer learning. Efficient preprocessing strategies, including Fourier transforms, structured transforms, low-precision computation, and weight compression, optimize inference speed and facilitate deployment in resource-constrained environments. This survey presents a unified taxonomy that classifies CNN architectures based on spatial exploitation, multi-path structures, depth, width, dimensionality expansion, channel boosting, and attention mechanisms. It systematically reviews CNN applications in face recognition, pose estimation, action recognition, text classification, statistical language modeling, disease diagnosis, radiological analysis, cryptocurrency sentiment prediction, 1D data processing, video analysis, and speech recognition. In addition to consolidating architectural advancements, the review highlights emerging learning paradigms such as few-shot, zero-shot, weakly supervised, federated learning frameworks and future research directions include hybrid CNN-transformer models, vision-language integration, generative learning, etc. This review provides a comprehensive perspective on CNN's evolution from 2015 to 2025, outlining key innovations, challenges, and opportunities.
<div id='section'>Paperid: <span id='pid'>677, <a href='https://arxiv.org/pdf/2503.13693.pdf' target='_blank'>https://arxiv.org/pdf/2503.13693.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eitan Shaar, Ariel Shaulov, Gal Chechik, Lior Wolf
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13693">Adapting to the Unknown: Training-Free Audio-Visual Event Perception with Dynamic Thresholds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the domain of audio-visual event perception, which focuses on the temporal localization and classification of events across distinct modalities (audio and visual), existing approaches are constrained by the vocabulary available in their training data. This limitation significantly impedes their capacity to generalize to novel, unseen event categories. Furthermore, the annotation process for this task is labor-intensive, requiring extensive manual labeling across modalities and temporal segments, limiting the scalability of current methods. Current state-of-the-art models ignore the shifts in event distributions over time, reducing their ability to adjust to changing video dynamics. Additionally, previous methods rely on late fusion to combine audio and visual information. While straightforward, this approach results in a significant loss of multimodal interactions. To address these challenges, we propose Audio-Visual Adaptive Video Analysis ($\text{AV}^2\text{A}$), a model-agnostic approach that requires no further training and integrates a score-level fusion technique to retain richer multimodal interactions. $\text{AV}^2\text{A}$ also includes a within-video label shift algorithm, leveraging input video data and predictions from prior frames to dynamically adjust event distributions for subsequent frames. Moreover, we present the first training-free, open-vocabulary baseline for audio-visual event perception, demonstrating that $\text{AV}^2\text{A}$ achieves substantial improvements over naive training-free baselines. We demonstrate the effectiveness of $\text{AV}^2\text{A}$ on both zero-shot and weakly-supervised state-of-the-art methods, achieving notable improvements in performance metrics over existing approaches.
<div id='section'>Paperid: <span id='pid'>678, <a href='https://arxiv.org/pdf/2502.18883.pdf' target='_blank'>https://arxiv.org/pdf/2502.18883.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanfu Yan, Viet Duong, Huajie Shao, Denys Poshyvanyk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18883">Towards More Trustworthy Deep Code Models by Enabling Out-of-Distribution Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Numerous machine learning (ML) models have been developed, including those for software engineering (SE) tasks, under the assumption that training and testing data come from the same distribution. However, training and testing distributions often differ, as training datasets rarely encompass the entire distribution, while testing distribution tends to shift over time. Hence, when confronted with out-of-distribution (OOD) instances that differ from the training data, a reliable and trustworthy SE ML model must be capable of detecting them to either abstain from making predictions, or potentially forward these OODs to appropriate models handling other categories or tasks.
  In this paper, we develop two types of SE-specific OOD detection models, unsupervised and weakly-supervised OOD detection for code. The unsupervised OOD detection approach is trained solely on in-distribution samples while the weakly-supervised approach utilizes a tiny number of OOD samples to further enhance the detection performance in various OOD scenarios. Extensive experimental results demonstrate that our proposed methods significantly outperform the baselines in detecting OOD samples from four different scenarios simultaneously and also positively impact a main code understanding task.
<div id='section'>Paperid: <span id='pid'>679, <a href='https://arxiv.org/pdf/2502.17288.pdf' target='_blank'>https://arxiv.org/pdf/2502.17288.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simon Boeder, Fabian Gigengack, Benjamin Risse
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17288">GaussianFlowOcc: Sparse and Weakly Supervised Occupancy Estimation using Gaussian Splatting and Temporal Flow</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Occupancy estimation has become a prominent task in 3D computer vision, particularly within the autonomous driving community. In this paper, we present a novel approach to occupancy estimation, termed GaussianFlowOcc, which is inspired by Gaussian Splatting and replaces traditional dense voxel grids with a sparse 3D Gaussian representation. Our efficient model architecture based on a Gaussian Transformer significantly reduces computational and memory requirements by eliminating the need for expensive 3D convolutions used with inefficient voxel-based representations that predominantly represent empty 3D spaces. GaussianFlowOcc effectively captures scene dynamics by estimating temporal flow for each Gaussian during the overall network training process, offering a straightforward solution to a complex problem that is often neglected by existing methods. Moreover, GaussianFlowOcc is designed for scalability, as it employs weak supervision and does not require costly dense 3D voxel annotations based on additional data (e.g., LiDAR). Through extensive experimentation, we demonstrate that GaussianFlowOcc significantly outperforms all previous methods for weakly supervised occupancy estimation on the nuScenes dataset while featuring an inference speed that is 50 times faster than current SOTA.
<div id='section'>Paperid: <span id='pid'>680, <a href='https://arxiv.org/pdf/2502.12484.pdf' target='_blank'>https://arxiv.org/pdf/2502.12484.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junrui Wen, Yifei Li, Bart Selman, Kun He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.12484">LocalEscaper: A Weakly-supervised Framework with Regional Reconstruction for Scalable Neural TSP Solvers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural solvers have shown significant potential in solving the Traveling Salesman Problem (TSP), yet current approaches face significant challenges. Supervised learning (SL)-based solvers require large amounts of high-quality labeled data, while reinforcement learning (RL)-based solvers, though less dependent on such data, often suffer from inefficiencies. To address these limitations, we propose LocalEscaper, a novel weakly-supervised learning framework for large-scale TSP. LocalEscaper effectively combines the advantages of both SL and RL, enabling effective training on datasets with low-quality labels. To further enhance solution quality, we introduce a regional reconstruction strategy, which is the key technique of this paper and mitigates the local-optima problem common in existing local reconstruction methods. Experimental results on both synthetic and real-world datasets demonstrate that LocalEscaper outperforms existing neural solvers, achieving remarkable results.
<div id='section'>Paperid: <span id='pid'>681, <a href='https://arxiv.org/pdf/2502.05129.pdf' target='_blank'>https://arxiv.org/pdf/2502.05129.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Van Brunt, Justin Kay, Timm Haucke, Pietro Perona, Grant Van Horn, Sara Beery
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05129">Counting Fish with Temporal Representations of Sonar Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate estimates of salmon escapement - the number of fish migrating upstream to spawn - are key data for conservation and fishery management. Existing methods for salmon counting using high-resolution imaging sonar hardware are non-invasive and compatible with computer vision processing. Prior work in this area has utilized object detection and tracking based methods for automated salmon counting. However, these techniques remain inaccessible to many sonar deployment sites due to limited compute and connectivity in the field. We propose an alternative lightweight computer vision method for fish counting based on analyzing echograms - temporal representations that compress several hundred frames of imaging sonar video into a single image. We predict upstream and downstream counts within 200-frame time windows directly from echograms using a ResNet-18 model, and propose a set of domain-specific image augmentations and a weakly-supervised training protocol to further improve results. We achieve a count error of 23% on representative data from the Kenai River in Alaska, demonstrating the feasibility of our approach.
<div id='section'>Paperid: <span id='pid'>682, <a href='https://arxiv.org/pdf/2502.03212.pdf' target='_blank'>https://arxiv.org/pdf/2502.03212.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jakob Poncelet, Hugo Van hamme
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03212">Leveraging Broadcast Media Subtitle Transcripts for Automatic Speech Recognition and Subtitling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent advancement of speech recognition technology has been driven by large-scale datasets and attention-based architectures, but many challenges still remain, especially for low-resource languages and dialects. This paper explores the integration of weakly supervised transcripts from TV subtitles into automatic speech recognition (ASR) systems, aiming to improve both verbatim transcriptions and automatically generated subtitles. To this end, verbatim data and subtitles are regarded as different domains or languages, due to their distinct characteristics. We propose and compare several end-to-end architectures that are designed to jointly model both modalities with separate or shared encoders and decoders. The proposed methods are able to jointly generate a verbatim transcription and a subtitle. Evaluation on Flemish (Belgian Dutch) demonstrates that a model with cascaded encoders and separate decoders allows to represent the differences between the two data types most efficiently while improving on both domains. Despite differences in domain and linguistic variations, combining verbatim transcripts with subtitle data leads to notable ASR improvements without the need for extensive preprocessing. Additionally, experiments with a large-scale subtitle dataset show the scalability of the proposed approach. The methods not only improve ASR accuracy but also generate subtitles that closely match standard written text, offering several potential applications.
<div id='section'>Paperid: <span id='pid'>683, <a href='https://arxiv.org/pdf/2501.04666.pdf' target='_blank'>https://arxiv.org/pdf/2501.04666.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nannan Li, Kevin J. Shih, Bryan A. Plummer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04666">Enhancing Virtual Try-On with Synthetic Pairs and Error-Aware Noise Scheduling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Given an isolated garment image in a canonical product view and a separate image of a person, the virtual try-on task aims to generate a new image of the person wearing the target garment. Prior virtual try-on works face two major challenges in achieving this goal: a) the paired (human, garment) training data has limited availability; b) generating textures on the human that perfectly match that of the prompted garment is difficult, often resulting in distorted text and faded textures. Our work explores ways to tackle these issues through both synthetic data as well as model refinement. We introduce a garment extraction model that generates (human, synthetic garment) pairs from a single image of a clothed individual. The synthetic pairs can then be used to augment the training of virtual try-on. We also propose an Error-Aware Refinement-based SchrÃ¶dinger Bridge (EARSB) that surgically targets localized generation errors for correcting the output of a base virtual try-on model. To identify likely errors, we propose a weakly-supervised error classifier that localizes regions for refinement, subsequently augmenting the SchrÃ¶dinger Bridge's noise schedule with its confidence heatmap. Experiments on VITON-HD and DressCode-Upper demonstrate that our synthetic data augmentation enhances the performance of prior work, while EARSB improves the overall image quality. In user studies, our model is preferred by the users in an average of 59% of cases.
<div id='section'>Paperid: <span id='pid'>684, <a href='https://arxiv.org/pdf/2411.18475.pdf' target='_blank'>https://arxiv.org/pdf/2411.18475.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuze Wang, Aoran Hu, Ji Qi, Yang Liu, Chao Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18475">Weakly Supervised Framework Considering Multi-temporal Information for Large-scale Cropland Mapping with Satellite Imagery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately mapping large-scale cropland is crucial for agricultural production management and planning. Currently, the combination of remote sensing data and deep learning techniques has shown outstanding performance in cropland mapping. However, those approaches require massive precise labels, which are labor-intensive. To reduce the label cost, this study presented a weakly supervised framework considering multi-temporal information for large-scale cropland mapping. Specifically, we extract high-quality labels according to their consistency among global land cover (GLC) products to construct the supervised learning signal. On the one hand, to alleviate the overfitting problem caused by the model's over-trust of remaining errors in high-quality labels, we encode the similarity/aggregation of cropland in the visual/spatial domain to construct the unsupervised learning signal, and take it as the regularization term to constrain the supervised part. On the other hand, to sufficiently leverage the plentiful information in the samples without high-quality labels, we also incorporate the unsupervised learning signal in these samples, enriching the diversity of the feature space. After that, to capture the phenological features of croplands, we introduce dense satellite image time series (SITS) to extend the proposed framework in the temporal dimension. We also visualized the high dimensional phenological features to uncover how multi-temporal information benefits cropland extraction, and assessed the method's robustness under conditions of data scarcity. The proposed framework has been experimentally validated for strong adaptability across three study areas (Hunan Province, Southeast France, and Kansas) in large-scale cropland mapping, and the internal mechanism and temporal generalizability are also investigated.
<div id='section'>Paperid: <span id='pid'>685, <a href='https://arxiv.org/pdf/2411.11636.pdf' target='_blank'>https://arxiv.org/pdf/2411.11636.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiman Li, Jiayue Zhao, Shaolei Liu, Xiaokun Dai, Chenxi Zhang, Zhijian Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11636">SP${ }^3$ : Superpixel-propagated pseudo-label learning for weakly semi-supervised medical image segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning-based medical image segmentation helps assist diagnosis and accelerate the treatment process while the model training usually requires large-scale dense annotation datasets. Weakly semi-supervised medical image segmentation is an essential application because it only requires a small amount of scribbles and a large number of unlabeled data to train the model, which greatly reduces the clinician's effort to fully annotate images. To handle the inadequate supervisory information challenge in weakly semi-supervised segmentation (WSSS), a SuperPixel-Propagated Pseudo-label (SP${}^3$) learning method is proposed, using the structural information contained in superpixel for supplemental information. Specifically, the annotation of scribbles is propagated to superpixels and thus obtains a dense annotation for supervised training. Since the quality of pseudo-labels is limited by the low-quality annotation, the beneficial superpixels selected by dynamic thresholding are used to refine pseudo-labels. Furthermore, aiming to alleviate the negative impact of noise in pseudo-label, superpixel-level uncertainty is incorporated to guide the pseudo-label supervision for stable learning. Our method achieves state-of-the-art performance on both tumor and organ segmentation datasets under the WSSS setting, using only 3\% of the annotation workload compared to fully supervised methods and attaining approximately 80\% Dice score. Additionally, our method outperforms eight weakly and semi-supervised methods under both weakly supervised and semi-supervised settings. Results of extensive experiments validate the effectiveness and annotation efficiency of our weakly semi-supervised segmentation, which can assist clinicians in achieving automated segmentation for organs or tumors quickly and ultimately benefit patients.
<div id='section'>Paperid: <span id='pid'>686, <a href='https://arxiv.org/pdf/2409.19600.pdf' target='_blank'>https://arxiv.org/pdf/2409.19600.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayu Hu, Senlin Shu, Beibei Li, Tao Xiang, Zhongshi He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19600">An Unbiased Risk Estimator for Partial Label Learning with Augmented Classes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partial Label Learning (PLL) is a typical weakly supervised learning task, which assumes each training instance is annotated with a set of candidate labels containing the ground-truth label. Recent PLL methods adopt identification-based disambiguation to alleviate the influence of false positive labels and achieve promising performance. However, they require all classes in the test set to have appeared in the training set, ignoring the fact that new classes will keep emerging in real applications. To address this issue, in this paper, we focus on the problem of Partial Label Learning with Augmented Class (PLLAC), where one or more augmented classes are not visible in the training stage but appear in the inference stage. Specifically, we propose an unbiased risk estimator with theoretical guarantees for PLLAC, which estimates the distribution of augmented classes by differentiating the distribution of known classes from unlabeled data and can be equipped with arbitrary PLL loss functions. Besides, we provide a theoretical analysis of the estimation error bound of the estimator, which guarantees the convergence of the empirical risk minimizer to the true risk minimizer as the number of training data tends to infinity. Furthermore, we add a risk-penalty regularization term in the optimization objective to alleviate the influence of the over-fitting issue caused by negative empirical risk. Extensive experiments on benchmark, UCI and real-world datasets demonstrate the effectiveness of the proposed approach.
<div id='section'>Paperid: <span id='pid'>687, <a href='https://arxiv.org/pdf/2409.18434.pdf' target='_blank'>https://arxiv.org/pdf/2409.18434.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siru Li, Ziyang Hong, Yushuai Chen, Liang Hu, Jiahu Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18434">Get It For Free: Radar Segmentation without Expert Labels and Its Application in Odometry and Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel weakly supervised semantic segmentation method for radar segmentation, where the existing LiDAR semantic segmentation models are employed to generate semantic labels, which then serve as supervision signals for training a radar semantic segmentation model. The obtained radar semantic segmentation model outperforms LiDAR-based models, providing more consistent and robust segmentation under all-weather conditions, particularly in the snow, rain and fog. To mitigate potential errors in LiDAR semantic labels, we design a dedicated refinement scheme that corrects erroneous labels based on structural features and distribution patterns. The semantic information generated by our radar segmentation model is used in two downstream tasks, achieving significant performance improvements. In large-scale radar-based localization using OpenStreetMap, it leads to localization error reduction by 20.55\% over prior methods. For the odometry task, it improves translation accuracy by 16.4\% compared to the second-best method, securing the first place in the radar odometry competition at the Radar in Robotics workshop of ICRA 2024, Japan
<div id='section'>Paperid: <span id='pid'>688, <a href='https://arxiv.org/pdf/2409.09616.pdf' target='_blank'>https://arxiv.org/pdf/2409.09616.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cagri Gungor, Adriana Kovashka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09616">Enhancing Weakly-Supervised Object Detection on Static Images through (Hallucinated) Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While motion has garnered attention in various tasks, its potential as a modality for weakly-supervised object detection (WSOD) in static images remains unexplored. Our study introduces an approach to enhance WSOD methods by integrating motion information. This method involves leveraging hallucinated motion from static images to improve WSOD on image datasets, utilizing a Siamese network for enhanced representation learning with motion, addressing camera motion through motion normalization, and selectively training images based on object motion. Experimental validation on the COCO and YouTube-BB datasets demonstrates improvements over a state-of-the-art method.
<div id='section'>Paperid: <span id='pid'>689, <a href='https://arxiv.org/pdf/2408.05562.pdf' target='_blank'>https://arxiv.org/pdf/2408.05562.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Utkarsh Tiwari, Snehashis Majhi, Michal Balazia, FranÃ§ois BrÃ©mond
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.05562">What Matters in Autonomous Driving Anomaly Detection: A Weakly Supervised Horizon</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video anomaly detection (VAD) in autonomous driving scenario is an important task, however it involves several challenges due to the ego-centric views and moving camera. Due to this, it remains largely under-explored. While recent developments in weakly-supervised VAD methods have shown remarkable progress in detecting critical real-world anomalies in static camera scenario, the development and validation of such methods are yet to be explored for moving camera VAD. This is mainly due to existing datasets like DoTA not following training pre-conditions of weakly-supervised learning. In this paper, we aim to promote weakly-supervised method development for autonomous driving VAD. We reorganize the DoTA dataset and aim to validate recent powerful weakly-supervised VAD methods on moving camera scenarios. Further, we provide a detailed analysis of what modifications on state-of-the-art methods can significantly improve the detection performance. Towards this, we propose a "feature transformation block" and through experimentation we show that our propositions can empower existing weakly-supervised VAD methods significantly in improving the VAD in autonomous driving. Our codes/dataset/demo will be released at github.com/ut21/WSAD-Driving
<div id='section'>Paperid: <span id='pid'>690, <a href='https://arxiv.org/pdf/2408.02039.pdf' target='_blank'>https://arxiv.org/pdf/2408.02039.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ye Du, Zehua Fu, Qingjie Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.02039">Pixel-Level Domain Adaptation: A New Perspective for Enhancing Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent attention has been devoted to the pursuit of learning semantic segmentation models exclusively from image tags, a paradigm known as image-level Weakly Supervised Semantic Segmentation (WSSS). Existing attempts adopt the Class Activation Maps (CAMs) as priors to mine object regions yet observe the imbalanced activation issue, where only the most discriminative object parts are located. In this paper, we argue that the distribution discrepancy between the discriminative and the non-discriminative parts of objects prevents the model from producing complete and precise pseudo masks as ground truths. For this purpose, we propose a Pixel-Level Domain Adaptation (PLDA) method to encourage the model in learning pixel-wise domain-invariant features. Specifically, a multi-head domain classifier trained adversarially with the feature extraction is introduced to promote the emergence of pixel features that are invariant with respect to the shift between the source (i.e., the discriminative object parts) and the target (\textit{i.e.}, the non-discriminative object parts) domains. In addition, we come up with a Confident Pseudo-Supervision strategy to guarantee the discriminative ability of each pixel for the segmentation task, which serves as a complement to the intra-image domain adversarial training. Our method is conceptually simple, intuitive and can be easily integrated into existing WSSS methods. Taking several strong baseline models as instances, we experimentally demonstrate the effectiveness of our approach under a wide range of settings.
<div id='section'>Paperid: <span id='pid'>691, <a href='https://arxiv.org/pdf/2407.19821.pdf' target='_blank'>https://arxiv.org/pdf/2407.19821.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianhang Nan, Hao Quan, Yong Ding, Xingyu Li, Kai Yang, Xiaoyu Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.19821">Distilling High Diagnostic Value Patches for Whole Slide Image Classification Using Attention Mechanism</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple Instance Learning (MIL) has garnered widespread attention in the field of Whole Slide Image (WSI) classification as it replaces pixel-level manual annotation with diagnostic reports as labels, significantly reducing labor costs. Recent research has shown that bag-level MIL methods often yield better results because they can consider all patches of the WSI as a whole. However, a drawback of such methods is the incorporation of more redundant patches, leading to interference. To extract patches with high diagnostic value while excluding interfering patches to address this issue, we developed an attention-based feature distillation multi-instance learning (AFD-MIL) approach. This approach proposed the exclusion of redundant patches as a preprocessing operation in weakly supervised learning, directly mitigating interference from extensive noise. It also pioneers the use of attention mechanisms to distill features with high diagnostic value, as opposed to the traditional practice of indiscriminately and forcibly integrating all patches. Additionally, we introduced global loss optimization to finely control the feature distillation module. AFD-MIL is orthogonal to many existing MIL methods, leading to consistent performance improvements. This approach has surpassed the current state-of-the-art method, achieving 91.47% ACC (accuracy) and 94.29% AUC (area under the curve) on the Camelyon16 (Camelyon Challenge 2016, breast cancer), while 93.33% ACC and 98.17% AUC on the TCGA-NSCLC (The Cancer Genome Atlas Program: non-small cell lung cancer). Different feature distillation methods were used for the two datasets, tailored to the specific diseases, thereby improving performance and interpretability.
<div id='section'>Paperid: <span id='pid'>692, <a href='https://arxiv.org/pdf/2407.12307.pdf' target='_blank'>https://arxiv.org/pdf/2407.12307.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufei Zhang, Jeffrey O. Kephart, Qiang Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.12307">Weakly-Supervised 3D Hand Reconstruction with Knowledge Prior and Uncertainty Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fully-supervised monocular 3D hand reconstruction is often difficult because capturing the requisite 3D data entails deploying specialized equipment in a controlled environment. We introduce a weakly-supervised method that avoids such requirements by leveraging fundamental principles well-established in the understanding of the human hand's unique structure and functionality. Specifically, we systematically study hand knowledge from different sources, including biomechanics, functional anatomy, and physics. We effectively incorporate these valuable foundational insights into 3D hand reconstruction models through an appropriate set of differentiable training losses. This enables training solely with readily-obtainable 2D hand landmark annotations and eliminates the need for expensive 3D supervision. Moreover, we explicitly model the uncertainty that is inherent in image observations. We enhance the training process by exploiting a simple yet effective Negative Log Likelihood (NLL) loss that incorporates uncertainty into the loss function. Through extensive experiments, we demonstrate that our method significantly outperforms state-of-the-art weakly-supervised methods. For example, our method achieves nearly a 21\% performance improvement on the widely adopted FreiHAND dataset.
<div id='section'>Paperid: <span id='pid'>693, <a href='https://arxiv.org/pdf/2407.12206.pdf' target='_blank'>https://arxiv.org/pdf/2407.12206.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amit Roth, Arnon Turetzky, Yossi Adi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.12206">A Language Modeling Approach to Diacritic-Free Hebrew TTS</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We tackle the task of text-to-speech (TTS) in Hebrew. Traditional Hebrew contains Diacritics, which dictate the way individuals should pronounce given words, however, modern Hebrew rarely uses them. The lack of diacritics in modern Hebrew results in readers expected to conclude the correct pronunciation and understand which phonemes to use based on the context. This imposes a fundamental challenge on TTS systems to accurately map between text-to-speech. In this work, we propose to adopt a language modeling Diacritics-Free approach, for the task of Hebrew TTS. The model operates on discrete speech representations and is conditioned on a word-piece tokenizer. We optimize the proposed method using in-the-wild weakly supervised data and compare it to several diacritic-based TTS systems. Results suggest the proposed method is superior to the evaluated baselines considering both content preservation and naturalness of the generated speech. Samples can be found under the following link: pages.cs.huji.ac.il/adiyoss-lab/HebTTS/
<div id='section'>Paperid: <span id='pid'>694, <a href='https://arxiv.org/pdf/2407.10000.pdf' target='_blank'>https://arxiv.org/pdf/2407.10000.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaifu Wang, Efthymia Tsamoura, Dan Roth
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10000">On Characterizing and Mitigating Imbalances in Multi-Instance Partial Label Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>*Multi-Instance Partial Label Learning* (MI-PLL) is a weakly-supervised learning setting encompassing *partial label learning*, *latent structural learning*, and *neurosymbolic learning*. Unlike supervised learning, in MI-PLL, the inputs to the classifiers at training-time are tuples of instances $\mathbf{x}$. At the same time, the supervision signal is generated by a function $Ï$ over the (hidden) gold labels of $\mathbf{x}$. In this work, we make multiple contributions towards addressing a problem that hasn't been studied so far in the context of MI-PLL: that of characterizing and mitigating *learning imbalances*, i.e., major differences in the errors occurring when classifying instances of different classes (aka *class-specific risks*). In terms of theory, we derive class-specific risk bounds for MI-PLL, while making minimal assumptions. Our theory reveals a unique phenomenon: that $Ï$ can greatly impact learning imbalances. This result is in sharp contrast with previous research on supervised and weakly-supervised learning, which only studies learning imbalances under the prism of data imbalances. On the practical side, we introduce a technique for estimating the marginal of the hidden labels using only MI-PLL data. Then, we introduce algorithms that mitigate imbalances at training- and testing-time, by treating the marginal of the hidden labels as a constraint. We demonstrate the effectiveness of our techniques using strong baselines from neurosymbolic and long-tail learning, suggesting performance improvements of up to 14\%.
<div id='section'>Paperid: <span id='pid'>695, <a href='https://arxiv.org/pdf/2407.02389.pdf' target='_blank'>https://arxiv.org/pdf/2407.02389.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sayan Nag, Koustava Goswami, Srikrishna Karanam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.02389">SafaRi:Adaptive Sequence Transformer for Weakly Supervised Referring Expression Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring Expression Segmentation (RES) aims to provide a segmentation mask of the target object in an image referred to by the text (i.e., referring expression). Existing methods require large-scale mask annotations. Moreover, such approaches do not generalize well to unseen/zero-shot scenarios. To address the aforementioned issues, we propose a weakly-supervised bootstrapping architecture for RES with several new algorithmic innovations. To the best of our knowledge, ours is the first approach that considers only a fraction of both mask and box annotations (shown in Figure 1 and Table 1) for training. To enable principled training of models in such low-annotation settings, improve image-text region-level alignment, and further enhance spatial localization of the target object in the image, we propose Cross-modal Fusion with Attention Consistency module. For automatic pseudo-labeling of unlabeled samples, we introduce a novel Mask Validity Filtering routine based on a spatially aware zero-shot proposal scoring approach. Extensive experiments show that with just 30% annotations, our model SafaRi achieves 59.31 and 48.26 mIoUs as compared to 58.93 and 48.19 mIoUs obtained by the fully-supervised SOTA method SeqTR respectively on RefCOCO+@testA and RefCOCO+testB datasets. SafaRi also outperforms SeqTR by 11.7% (on RefCOCO+testA) and 19.6% (on RefCOCO+testB) in a fully-supervised setting and demonstrates strong generalization capabilities in unseen/zero-shot tasks.
<div id='section'>Paperid: <span id='pid'>696, <a href='https://arxiv.org/pdf/2406.18576.pdf' target='_blank'>https://arxiv.org/pdf/2406.18576.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Zhang, Chuang Zhu, Guoqing Yang, Siqi Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.18576">Negative Prototypes Guided Contrastive Learning for WSOD</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Object Detection (WSOD) with only image-level annotation has recently attracted wide attention. Many existing methods ignore the inter-image relationship of instances which share similar characteristics while can certainly be determined not to belong to the same category. Therefore, in order to make full use of the weak label, we propose the Negative Prototypes Guided Contrastive learning (NPGC) architecture. Firstly, we define Negative Prototype as the proposal with the highest confidence score misclassified for the category that does not appear in the label. Unlike other methods that only utilize category positive feature, we construct an online updated global feature bank to store both positive prototypes and negative prototypes. Meanwhile, we propose a pseudo label sampling module to mine reliable instances and discard the easily misclassified instances based on the feature similarity with corresponding prototypes in global feature bank. Finally, we follow the contrastive learning paradigm to optimize the proposal's feature representation by attracting same class samples closer and pushing different class samples away in the embedding space. Extensive experiments have been conducted on VOC07, VOC12 datasets, which shows that our proposed method achieves the state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>697, <a href='https://arxiv.org/pdf/2406.09147.pdf' target='_blank'>https://arxiv.org/pdf/2406.09147.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xu Tan, Junqi Chen, Sylwan Rahardja, Jiawei Yang, Susanto Rahardja
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.09147">Weakly-supervised anomaly detection for multimodal data distributions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised anomaly detection can outperform existing unsupervised methods with the assistance of a very small number of labeled anomalies, which attracts increasing attention from researchers. However, existing weakly-supervised anomaly detection methods are limited as these methods do not factor in the multimodel nature of the real-world data distribution. To mitigate this, we propose the Weakly-supervised Variational-mixture-model-based Anomaly Detector (WVAD). WVAD excels in multimodal datasets. It consists of two components: a deep variational mixture model, and an anomaly score estimator. The deep variational mixture model captures various features of the data from different clusters, then these features are delivered to the anomaly score estimator to assess the anomaly levels. Experimental results on three real-world datasets demonstrate WVAD's superiority.
<div id='section'>Paperid: <span id='pid'>698, <a href='https://arxiv.org/pdf/2406.04933.pdf' target='_blank'>https://arxiv.org/pdf/2406.04933.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>AhcÃ¨ne Boubekki, Samuel G. Fadel, Sebastian Mair
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04933">Leveraging Activations for Superpixel Explanations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Saliency methods have become standard in the explanation toolkit of deep neural networks. Recent developments specific to image classifiers have investigated region-based explanations with either new methods or by adapting well-established ones using ad-hoc superpixel algorithms. In this paper, we aim to avoid relying on these segmenters by extracting a segmentation from the activations of a deep neural network image classifier without fine-tuning the network. Our so-called Neuro-Activated Superpixels (NAS) can isolate the regions of interest in the input relevant to the model's prediction, which boosts high-threshold weakly supervised object localization performance. This property enables the semi-supervised semantic evaluation of saliency methods. The aggregation of NAS with existing saliency methods eases their interpretation and reveals the inconsistencies of the widely used area under the relevance curve metric.
<div id='section'>Paperid: <span id='pid'>699, <a href='https://arxiv.org/pdf/2405.14334.pdf' target='_blank'>https://arxiv.org/pdf/2405.14334.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yitao Peng, Lianghua He, Die Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14334">Hierarchical Salient Patch Identification for Interpretable Fundus Disease Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the widespread application of deep learning technology in medical image analysis, the effective explanation of model predictions and improvement of diagnostic accuracy have become urgent problems that need to be solved. Attribution methods have become key tools to help doctors better understand the diagnostic basis of models, and are used to explain and localize diseases in medical images. However, previous methods suffer from inaccurate and incomplete localization problems for fundus diseases with complex and diverse structures. To solve these problems, we propose a weakly supervised interpretable fundus disease localization method called hierarchical salient patch identification (HSPI) that can achieve interpretable disease localization using only image-level labels and a neural network classifier (NNC). First, we propose salient patch identification (SPI), which divides the image into several patches and optimizes consistency loss to identify which patch in the input image is most important for the network's prediction, in order to locate the disease. Second, we propose a hierarchical identification strategy to force SPI to analyze the importance of different areas to neural network classifier's prediction to comprehensively locate disease areas. Conditional peak focusing is then introduced to ensure that the mask vector can accurately locate the disease area. Finally, we propose patch selection based on multi-sized intersections to filter out incorrectly or additionally identified non-disease regions. We conduct disease localization experiments on fundus image datasets and achieve the best performance on multiple evaluation metrics compared to previous interpretable attribution methods. Additional ablation studies are conducted to verify the effectiveness of each method.
<div id='section'>Paperid: <span id='pid'>700, <a href='https://arxiv.org/pdf/2405.03726.pdf' target='_blank'>https://arxiv.org/pdf/2405.03726.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andac Demir, Elizaveta Solovyeva, James Boylan, Mei Xiao, Fabrizio Serluca, Sebastian Hoersch, Jeremy Jenkins, Murthy Devarakonda, Bulent Kiziltan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.03726">sc-OTGM: Single-Cell Perturbation Modeling by Solving Optimal Mass Transport on the Manifold of Gaussian Mixtures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Influenced by breakthroughs in LLMs, single-cell foundation models are emerging. While these models show successful performance in cell type clustering, phenotype classification, and gene perturbation response prediction, it remains to be seen if a simpler model could achieve comparable or better results, especially with limited data. This is important, as the quantity and quality of single-cell data typically fall short of the standards in textual data used for training LLMs. Single-cell sequencing often suffers from technical artifacts, dropout events, and batch effects. These challenges are compounded in a weakly supervised setting, where the labels of cell states can be noisy, further complicating the analysis. To tackle these challenges, we present sc-OTGM, streamlined with less than 500K parameters, making it approximately 100x more compact than the foundation models, offering an efficient alternative. sc-OTGM is an unsupervised model grounded in the inductive bias that the scRNAseq data can be generated from a combination of the finite multivariate Gaussian distributions. The core function of sc-OTGM is to create a probabilistic latent space utilizing a GMM as its prior distribution and distinguish between distinct cell populations by learning their respective marginal PDFs. It uses a Hit-and-Run Markov chain sampler to determine the OT plan across these PDFs within the GMM framework. We evaluated our model against a CRISPR-mediated perturbation dataset, called CROP-seq, consisting of 57 one-gene perturbations. Our results demonstrate that sc-OTGM is effective in cell state classification, aids in the analysis of differential gene expression, and ranks genes for target identification through a recommender system. It also predicts the effects of single-gene perturbations on downstream gene regulation and generates synthetic scRNA-seq data conditioned on specific cell states.
<div id='section'>Paperid: <span id='pid'>701, <a href='https://arxiv.org/pdf/2404.11998.pdf' target='_blank'>https://arxiv.org/pdf/2404.11998.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiyuan Dai, Sibei Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.11998">Curriculum Point Prompting for Weakly-Supervised Referring Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring image segmentation (RIS) aims to precisely segment referents in images through corresponding natural language expressions, yet relying on cost-intensive mask annotations. Weakly supervised RIS thus learns from image-text pairs to pixel-level semantics, which is challenging for segmenting fine-grained masks. A natural approach to enhancing segmentation precision is to empower weakly supervised RIS with the image segmentation foundation model SAM. Nevertheless, we observe that simply integrating SAM yields limited benefits and can even lead to performance regression due to the inevitable noise issues and challenges in excessive focus on object parts. In this paper, we present an innovative framework, Point PrompTing (PPT), incorporated with the proposed multi-source curriculum learning strategy to address these challenges. Specifically, the core of PPT is a point generator that not only harnesses CLIP's text-image alignment capability and SAM's powerful mask generation ability but also generates negative point prompts to address the noisy and excessive focus issues inherently and effectively. In addition, we introduce a curriculum learning strategy with object-centric images to help PPT gradually learn from simpler yet precise semantic alignment to more complex RIS. Experiments demonstrate that our PPT significantly and consistently outperforms prior weakly supervised techniques on mIoU by 11.34%, 14.14%, and 6.97% across RefCOCO, RefCOCO+, and G-Ref, respectively.
<div id='section'>Paperid: <span id='pid'>702, <a href='https://arxiv.org/pdf/2404.00918.pdf' target='_blank'>https://arxiv.org/pdf/2404.00918.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Beomyoung Kim, Donghyun Kim, Sung Ju Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00918">Rethinking Saliency-Guided Weakly-Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a fresh perspective on the role of saliency maps in weakly-supervised semantic segmentation (WSSS) and offers new insights and research directions based on our empirical findings. We conduct comprehensive experiments and observe that the quality of the saliency map is a critical factor in saliency-guided WSSS approaches. Nonetheless, we find that the saliency maps used in previous works are often arbitrarily chosen, despite their significant impact on WSSS. Additionally, we observe that the choice of the threshold, which has received less attention before, is non-trivial in WSSS. To facilitate more meaningful and rigorous research for saliency-guided WSSS, we introduce \texttt{WSSS-BED}, a standardized framework for conducting research under unified conditions. \texttt{WSSS-BED} provides various saliency maps and activation maps for seven WSSS methods, as well as saliency maps from unsupervised salient object detection models.
<div id='section'>Paperid: <span id='pid'>703, <a href='https://arxiv.org/pdf/2401.10578.pdf' target='_blank'>https://arxiv.org/pdf/2401.10578.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lintai Wu, Junhui Hou, Linqi Song, Yong Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.10578">3D Shape Completion on Unseen Categories:A Weakly-supervised Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D shapes captured by scanning devices are often incomplete due to occlusion. 3D shape completion methods have been explored to tackle this limitation. However, most of these methods are only trained and tested on a subset of categories, resulting in poor generalization to unseen categories. In this paper, we introduce a novel weakly-supervised framework to reconstruct the complete shapes from unseen categories. We first propose an end-to-end prior-assisted shape learning network that leverages data from the seen categories to infer a coarse shape. Specifically, we construct a prior bank consisting of representative shapes from the seen categories. Then, we design a multi-scale pattern correlation module for learning the complete shape of the input by analyzing the correlation between local patterns within the input and the priors at various scales. In addition, we propose a self-supervised shape refinement model to further refine the coarse shape. Considering the shape variability of 3D objects across categories, we construct a category-specific prior bank to facilitate shape refinement. Then, we devise a voxel-based partial matching loss and leverage the partial scans to drive the refinement process. Extensive experimental results show that our approach is superior to state-of-the-art methods by a large margin.
<div id='section'>Paperid: <span id='pid'>704, <a href='https://arxiv.org/pdf/2401.10011.pdf' target='_blank'>https://arxiv.org/pdf/2401.10011.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinpeng Zhao, Yanwei Zheng, Chuanlin Lan, Xiaowei Zhang, Bowen Huang, Jibin Yang, Dongxiao Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.10011">CPCL: Cross-Modal Prototypical Contrastive Learning for Weakly Supervised Text-based Person Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised text-based person retrieval seeks to retrieve images of a target person using textual descriptions, without relying on identity annotations and is more challenging and practical. The primary challenge is the intra-class differences, encompassing intra-modal feature variations and cross-modal semantic gaps. Prior works have focused on instance-level samples and ignored prototypical features of each person which are intrinsic and invariant. Toward this, we propose a Cross-Modal Prototypical Contrastive Learning (CPCL) method. In practice, the CPCL introduces the CLIP model to weakly supervised text-based person retrieval to map visual and textual instances into a shared latent space. Subsequently, the proposed Prototypical Multi-modal Memory (PMM) module captures associations between heterogeneous modalities of image-text pairs belonging to the same person through the Hybrid Cross-modal Matching (HCM) module in a many-to-many mapping fashion. Moreover, the Outlier Pseudo Label Mining (OPLM) module further distinguishes valuable outlier samples from each modality, enhancing the creation of more reliable clusters by mining implicit relationships between image-text pairs. We conduct extensive experiments on popular benchmarks of weakly supervised text-based person retrieval, which validate the effectiveness, generalizability of CPCL.
<div id='section'>Paperid: <span id='pid'>705, <a href='https://arxiv.org/pdf/2512.22203.pdf' target='_blank'>https://arxiv.org/pdf/2512.22203.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiang Guo, Rubo Zhang, Bingbing Zhang, Junjie Liu, Jianqing Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.22203">TCFormer: A 5M-Parameter Transformer with Density-Guided Aggregation for Weakly-Supervised Crowd Counting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Crowd counting typically relies on labor-intensive point-level annotations and computationally intensive backbones, restricting its scalability and deployment in resource-constrained environments. To address these challenges, this paper proposes the TCFormer, a tiny, ultra-lightweight, weakly-supervised transformer-based crowd counting framework with only 5 million parameters that achieves competitive performance. Firstly, a powerful yet efficient vision transformer is adopted as the feature extractor, the global context-aware capabilities of which provides semantic meaningful crowd features with a minimal memory footprint. Secondly, to compensate for the lack of spatial supervision, we design a feature aggregation mechanism termed the Learnable Density-Weighted Averaging module. This module dynamically re-weights local tokens according to predicted density scores, enabling the network to adaptively modulate regional features based on their specific density characteristics without the need for additional annotations. Furthermore, this paper introduces a density-level classification loss, which discretizes crowd density into distinct grades, thereby regularizing the training process and enhancing the model's classification power across varying levels of crowd density. Therefore, although TCformer is trained under a weakly-supervised paradigm utilizing only image-level global counts, the joint optimization of count and density-level losses enables the framework to achieve high estimation accuracy. Extensive experiments on four benchmarks including ShanghaiTech A/B, UCF-QNRF, and NWPU datasets demonstrate that our approach strikes a superior trade-off between parameter efficiency and counting accuracy and can be a good solution for crowd counting tasks in edge devices.
<div id='section'>Paperid: <span id='pid'>706, <a href='https://arxiv.org/pdf/2512.15061.pdf' target='_blank'>https://arxiv.org/pdf/2512.15061.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pandega Abyan Zumarsyah, Igi Ardiyanto, Hanung Adi Nugroho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.15061">Meta-learners for few-shot weakly-supervised optic disc and cup segmentation on fundus images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study develops meta-learners for few-shot weakly-supervised segmentation (FWS) to address the challenge of optic disc (OD) and optic cup (OC) segmentation for glaucoma diagnosis with limited labeled fundus images. We significantly improve existing meta-learners by introducing Omni meta-training which balances data usage and diversifies the number of shots. We also develop their efficient versions that reduce computational costs. In addition, we develop sparsification techniques that generate more customizable and representative scribbles and other sparse labels. After evaluating multiple datasets, we find that Omni and efficient versions outperform the original versions, with the best meta-learner being Efficient Omni ProtoSeg (EO-ProtoSeg). It achieves intersection over union (IoU) scores of 88.15% for OD and 71.17% for OC on the REFUGE dataset using just one sparsely labeled image, outperforming few-shot and semi-supervised methods which require more labeled images. Its best performance reaches 86.80% for OD and 71.78%for OC on DRISHTIGS, 88.21% for OD and 73.70% for OC on REFUGE, 80.39% for OD and 52.65% for OC on REFUGE. EO-ProtoSeg is comparable to unsupervised domain adaptation methods yet much lighter with less than two million parameters and does not require any retraining.
<div id='section'>Paperid: <span id='pid'>707, <a href='https://arxiv.org/pdf/2511.17392.pdf' target='_blank'>https://arxiv.org/pdf/2511.17392.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runxun Zhang, Yizhou Liu, Li Dongrui, Bo XU, Jingwei Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17392">MorphSeek: Fine-grained Latent Representation-Level Policy Optimization for Deformable Image Registration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deformable image registration (DIR) remains a fundamental yet challenging problem in medical image analysis, largely due to the prohibitively high-dimensional deformation space of dense displacement fields and the scarcity of voxel-level supervision. Existing reinforcement learning frameworks often project this space into coarse, low-dimensional representations, limiting their ability to capture spatially variant deformations. We propose MorphSeek, a fine-grained representation-level policy optimization paradigm that reformulates DIR as a spatially continuous optimization process in the latent feature space. MorphSeek introduces a stochastic Gaussian policy head atop the encoder to model a distribution over latent features, facilitating efficient exploration and coarse-to-fine refinement. The framework integrates unsupervised warm-up with weakly supervised fine-tuning through Group Relative Policy Optimization, where multi-trajectory sampling stabilizes training and improves label efficiency. Across three 3D registration benchmarks (OASIS brain MRI, LiTS liver CT, and Abdomen MR-CT), MorphSeek achieves consistent Dice improvements over competitive baselines while maintaining high label efficiency with minimal parameter cost and low step-level latency overhead. Beyond optimizer specifics, MorphSeek advances a representation-level policy learning paradigm that achieves spatially coherent and data-efficient deformation optimization, offering a principled, backbone-agnostic, and optimizer-agnostic solution for scalable visual alignment in high-dimensional settings.
<div id='section'>Paperid: <span id='pid'>708, <a href='https://arxiv.org/pdf/2511.13276.pdf' target='_blank'>https://arxiv.org/pdf/2511.13276.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Noam Tsfaty, Avishai Weizman, Liav Cohen, Moshe Tshuva, Yehudit Aperstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13276">Recognition of Abnormal Events in Surveillance Videos using Weakly Supervised Dual-Encoder Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address the challenge of detecting rare and diverse anomalies in surveillance videos using only video-level supervision. Our dual-backbone framework combines convolutional and transformer representations through top-k pooling, achieving 90.7% area under the curve (AUC) on the UCF-Crime dataset.
<div id='section'>Paperid: <span id='pid'>709, <a href='https://arxiv.org/pdf/2511.13204.pdf' target='_blank'>https://arxiv.org/pdf/2511.13204.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junhee Lee, ChaeBeen Bang, MyoungChul Kim, MyeongAh Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13204">RefineVAD: Semantic-Guided Feature Recalibration for Weakly Supervised Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-Supervised Video Anomaly Detection aims to identify anomalous events using only video-level labels, balancing annotation efficiency with practical applicability. However, existing methods often oversimplify the anomaly space by treating all abnormal events as a single category, overlooking the diverse semantic and temporal characteristics intrinsic to real-world anomalies. Inspired by how humans perceive anomalies, by jointly interpreting temporal motion patterns and semantic structures underlying different anomaly types, we propose RefineVAD, a novel framework that mimics this dual-process reasoning. Our framework integrates two core modules. The first, Motion-aware Temporal Attention and Recalibration (MoTAR), estimates motion salience and dynamically adjusts temporal focus via shift-based attention and global Transformer-based modeling. The second, Category-Oriented Refinement (CORE), injects soft anomaly category priors into the representation space by aligning segment-level features with learnable category prototypes through cross-attention. By jointly leveraging temporal dynamics and semantic structure, explicitly models both "how" motion evolves and "what" semantic category it resembles. Extensive experiments on WVAD benchmark validate the effectiveness of RefineVAD and highlight the importance of integrating semantic context to guide feature refinement toward anomaly-relevant patterns.
<div id='section'>Paperid: <span id='pid'>710, <a href='https://arxiv.org/pdf/2510.25134.pdf' target='_blank'>https://arxiv.org/pdf/2510.25134.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingdong Cai, Charith Abhayaratne
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.25134">Region-CAM: Towards Accurate Object Regions in Class Activation Maps for Weakly Supervised Learning Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Class Activation Mapping (CAM) methods are widely applied in weakly supervised learning tasks due to their ability to highlight object regions. However, conventional CAM methods highlight only the most discriminative regions of the target. These highlighted regions often fail to cover the entire object and are frequently misaligned with object boundaries, thereby limiting the performance of downstream weakly supervised learning tasks, particularly Weakly Supervised Semantic Segmentation (WSSS), which demands pixel-wise accurate activation maps to get the best results. To alleviate the above problems, we propose a novel activation method, Region-CAM. Distinct from network feature weighting approaches, Region-CAM generates activation maps by extracting semantic information maps (SIMs) and performing semantic information propagation (SIP) by considering both gradients and features in each of the stages of the baseline classification model. Our approach highlights a greater proportion of object regions while ensuring activation maps to have precise boundaries that align closely with object edges. Region-CAM achieves 60.12% and 58.43% mean intersection over union (mIoU) using the baseline model on the PASCAL VOC training and validation datasets, respectively, which are improvements of 13.61% and 13.13% over the original CAM (46.51% and 45.30%). On the MS COCO validation set, Region-CAM achieves 36.38%, a 16.23% improvement over the original CAM (20.15%). We also demonstrate the superiority of Region-CAM in object localization tasks, using the ILSVRC2012 validation set. Region-CAM achieves 51.7% in Top-1 Localization accuracy Loc1. Compared with LayerCAM, an activation method designed for weakly supervised object localization, Region-CAM achieves 4.5% better performance in Loc1.
<div id='section'>Paperid: <span id='pid'>711, <a href='https://arxiv.org/pdf/2510.12182.pdf' target='_blank'>https://arxiv.org/pdf/2510.12182.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Youngju Yoo, Seho Kim, Changick Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.12182">BEEP3D: Box-Supervised End-to-End Pseudo-Mask Generation for 3D Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D instance segmentation is crucial for understanding complex 3D environments, yet fully supervised methods require dense point-level annotations, resulting in substantial annotation costs and labor overhead. To mitigate this, box-level annotations have been explored as a weaker but more scalable form of supervision. However, box annotations inherently introduce ambiguity in overlapping regions, making accurate point-to-instance assignment challenging. Recent methods address this ambiguity by generating pseudo-masks through training a dedicated pseudo-labeler in an additional training stage. However, such two-stage pipelines often increase overall training time and complexity, hinder end-to-end optimization. To overcome these challenges, we propose BEEP3D-Box-supervised End-to-End Pseudo-mask generation for 3D instance segmentation. BEEP3D adopts a student-teacher framework, where the teacher model serves as a pseudo-labeler and is updated by the student model via an Exponential Moving Average. To better guide the teacher model to generate precise pseudo-masks, we introduce an instance center-based query refinement that enhances position query localization and leverages features near instance centers. Additionally, we design two novel losses-query consistency loss and masked feature consistency loss-to align semantic and geometric signals between predictions and pseudo-masks. Extensive experiments on ScanNetV2 and S3DIS datasets demonstrate that BEEP3D achieves competitive or superior performance compared to state-of-the-art weakly supervised methods while remaining computationally efficient.
<div id='section'>Paperid: <span id='pid'>712, <a href='https://arxiv.org/pdf/2508.18958.pdf' target='_blank'>https://arxiv.org/pdf/2508.18958.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matteo Contini, Victor Illien, Sylvain Poulain, Serge Bernard, Julien Barde, Sylvain Bonhommeau, Alexis Joly
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18958">The point is the mask: scaling coral reef segmentation with weak supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monitoring coral reefs at large spatial scales remains an open challenge, essential for assessing ecosystem health and informing conservation efforts. While drone-based aerial imagery offers broad spatial coverage, its limited resolution makes it difficult to reliably distinguish fine-scale classes, such as coral morphotypes. At the same time, obtaining pixel-level annotations over large spatial extents is costly and labor-intensive, limiting the scalability of deep learning-based segmentation methods for aerial imagery. We present a multi-scale weakly supervised semantic segmentation framework that addresses this challenge by transferring fine-scale ecological information from underwater imagery to aerial data. Our method enables large-scale coral reef mapping from drone imagery with minimal manual annotation, combining classification-based supervision, spatial interpolation and self-distillation techniques. We demonstrate the efficacy of the approach, enabling large-area segmentation of coral morphotypes and demonstrating flexibility for integrating new classes. This study presents a scalable, cost-effective methodology for high-resolution reef monitoring, combining low-cost data collection, weakly supervised deep learning and multi-scale remote sensing.
<div id='section'>Paperid: <span id='pid'>713, <a href='https://arxiv.org/pdf/2508.11826.pdf' target='_blank'>https://arxiv.org/pdf/2508.11826.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dehn Xu, Tim Katzke, Emmanuel MÃ¼ller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11826">From Pixels to Graphs: Deep Graph-Level Anomaly Detection on Dermoscopic Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph Neural Networks (GNNs) have emerged as a powerful approach for graph-based machine learning tasks. Previous work applied GNNs to image-derived graph representations for various downstream tasks such as classification or anomaly detection. These transformations include segmenting images, extracting features from segments, mapping them to nodes, and connecting them. However, to the best of our knowledge, no study has rigorously compared the effectiveness of the numerous potential image-to-graph transformation approaches for GNN-based graph-level anomaly detection (GLAD). In this study, we systematically evaluate the efficacy of multiple segmentation schemes, edge construction strategies, and node feature sets based on color, texture, and shape descriptors to produce suitable image-derived graph representations to perform graph-level anomaly detection. We conduct extensive experiments on dermoscopic images using state-of-the-art GLAD models, examining performance and efficiency in purely unsupervised, weakly supervised, and fully supervised regimes. Our findings reveal, for example, that color descriptors contribute the best standalone performance, while incorporating shape and texture features consistently enhances detection efficacy. In particular, our best unsupervised configuration using OCGTL achieves a competitive AUC-ROC score of up to 0.805 without relying on pretrained backbones like comparable image-based approaches. With the inclusion of sparse labels, the performance increases substantially to 0.872 and with full supervision to 0.914 AUC-ROC.
<div id='section'>Paperid: <span id='pid'>714, <a href='https://arxiv.org/pdf/2508.08095.pdf' target='_blank'>https://arxiv.org/pdf/2508.08095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chun Wang, Chenyang Liu, Wenze Xu, Weihong Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08095">Dual Information Speech Language Models for Emotional Conversations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conversational systems relying on text-based large language models (LLMs) often overlook paralinguistic cues, essential for understanding emotions and intentions. Speech-language models (SLMs), which use speech as input, are emerging as a promising solution. However, SLMs built by extending frozen LLMs struggle to capture paralinguistic information and exhibit reduced context understanding. We identify entangled information and improper training strategies as key issues. To address these issues, we propose two heterogeneous adapters and suggest a weakly supervised training strategy. Our approach disentangles paralinguistic and linguistic information, enabling SLMs to interpret speech through structured representations. It also preserves contextual understanding by avoiding the generation of task-specific vectors through controlled randomness. This approach trains only the adapters on common datasets, ensuring parameter and data efficiency. Experiments demonstrate competitive performance in emotional conversation tasks, showcasing the model's ability to effectively integrate both paralinguistic and linguistic information within contextual settings.
<div id='section'>Paperid: <span id='pid'>715, <a href='https://arxiv.org/pdf/2508.05019.pdf' target='_blank'>https://arxiv.org/pdf/2508.05019.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sadia Kamal, Tim Oates, Joy Wan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05019">Skin-SOAP: A Weakly Supervised Framework for Generating Structured SOAP Notes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skin carcinoma is the most prevalent form of cancer globally, accounting for over $8 billion in annual healthcare expenditures. Early diagnosis, accurate and timely treatment are critical to improving patient survival rates. In clinical settings, physicians document patient visits using detailed SOAP (Subjective, Objective, Assessment, and Plan) notes. However, manually generating these notes is labor-intensive and contributes to clinician burnout. In this work, we propose skin-SOAP, a weakly supervised multimodal framework to generate clinically structured SOAP notes from limited inputs, including lesion images and sparse clinical text. Our approach reduces reliance on manual annotations, enabling scalable, clinically grounded documentation while alleviating clinician burden and reducing the need for large annotated data. Our method achieves performance comparable to GPT-4o, Claude, and DeepSeek Janus Pro across key clinical relevance metrics. To evaluate this clinical relevance, we introduce two novel metrics MedConceptEval and Clinical Coherence Score (CCS) which assess semantic alignment with expert medical concepts and input features, respectively.
<div id='section'>Paperid: <span id='pid'>716, <a href='https://arxiv.org/pdf/2508.03724.pdf' target='_blank'>https://arxiv.org/pdf/2508.03724.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jia Li, Yapeng Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03724">From Waveforms to Pixels: A Survey on Audio-Visual Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-Visual Segmentation (AVS) aims to identify and segment sound-producing objects in videos by leveraging both visual and audio modalities. It has emerged as a significant research area in multimodal perception, enabling fine-grained object-level understanding. In this survey, we present a comprehensive overview of the AVS field, covering its problem formulation, benchmark datasets, evaluation metrics, and the progression of methodologies. We analyze a wide range of approaches, including architectures for unimodal and multimodal encoding, key strategies for audio-visual fusion, and various decoder designs. Furthermore, we examine major training paradigms, from fully supervised learning to weakly supervised and training-free methods. Notably, we provide an extensive comparison of AVS methods across standard benchmarks, highlighting the impact of different architectural choices, fusion strategies, and training paradigms on performance. Finally, we outline the current challenges, such as limited temporal modeling, modality bias toward vision, lack of robustness in complex environments, and high computational demands, and propose promising future directions, including improving temporal reasoning and multimodal fusion, leveraging foundation models for better generalization and few-shot learning, reducing reliance on labeled data through selfand weakly supervised learning, and incorporating higher-level reasoning for more intelligent AVS systems.
<div id='section'>Paperid: <span id='pid'>717, <a href='https://arxiv.org/pdf/2508.02844.pdf' target='_blank'>https://arxiv.org/pdf/2508.02844.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anghong Du, Nay Aung, Theodoros N. Arvanitis, Stefan K. Piechnik, Joao A C Lima, Steffen E. Petersen, Le Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02844">RefineSeg: Dual Coarse-to-Fine Learning for Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-quality pixel-level annotations of medical images are essential for supervised segmentation tasks, but obtaining such annotations is costly and requires medical expertise. To address this challenge, we propose a novel coarse-to-fine segmentation framework that relies entirely on coarse-level annotations, encompassing both target and complementary drawings, despite their inherent noise. The framework works by introducing transition matrices in order to model the inaccurate and incomplete regions in the coarse annotations. By jointly training on multiple sets of coarse annotations, it progressively refines the network's outputs and infers the true segmentation distribution, achieving a robust approximation of precise labels through matrix-based modeling. To validate the flexibility and effectiveness of the proposed method, we demonstrate the results on two public cardiac imaging datasets, ACDC and MSCMRseg, and further evaluate its performance on the UK Biobank dataset. Experimental results indicate that our approach surpasses the state-of-the-art weakly supervised methods and closely matches the fully supervised approach.
<div id='section'>Paperid: <span id='pid'>718, <a href='https://arxiv.org/pdf/2507.06848.pdf' target='_blank'>https://arxiv.org/pdf/2507.06848.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joelle Hanna, Damian Borth
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06848">Know Your Attention Maps: Class-specific Token Masking for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly Supervised Semantic Segmentation (WSSS) is a challenging problem that has been extensively studied in recent years. Traditional approaches often rely on external modules like Class Activation Maps to highlight regions of interest and generate pseudo segmentation masks. In this work, we propose an end-to-end method that directly utilizes the attention maps learned by a Vision Transformer (ViT) for WSSS. We propose training a sparse ViT with multiple [CLS] tokens (one for each class), using a random masking strategy to promote [CLS] token - class assignment. At inference time, we aggregate the different self-attention maps of each [CLS] token corresponding to the predicted labels to generate pseudo segmentation masks. Our proposed approach enhances the interpretability of self-attention maps and ensures accurate class assignments. Extensive experiments on two standard benchmarks and three specialized datasets demonstrate that our method generates accurate pseudo-masks, outperforming related works. Those pseudo-masks can be used to train a segmentation model which achieves results comparable to fully-supervised models, significantly reducing the need for fine-grained labeled data.
<div id='section'>Paperid: <span id='pid'>719, <a href='https://arxiv.org/pdf/2507.01721.pdf' target='_blank'>https://arxiv.org/pdf/2507.01721.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongwen Zhang, Yuri Boykov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01721">Soft Self-labeling and Potts Relaxations for Weakly-Supervised Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider weakly supervised segmentation where only a fraction of pixels have ground truth labels (scribbles) and focus on a self-labeling approach optimizing relaxations of the standard unsupervised CRF/Potts loss on unlabeled pixels. While WSSS methods can directly optimize such losses via gradient descent, prior work suggests that higher-order optimization can improve network training by introducing hidden pseudo-labels and powerful CRF sub-problem solvers, e.g. graph cut. However, previously used hard pseudo-labels can not represent class uncertainty or errors, which motivates soft self-labeling. We derive a principled auxiliary loss and systematically evaluate standard and new CRF relaxations (convex and non-convex), neighborhood systems, and terms connecting network predictions with soft pseudo-labels. We also propose a general continuous sub-problem solver. Using only standard architectures, soft self-labeling consistently improves scribble-based training and outperforms significantly more complex specialized WSSS systems. It can outperform full pixel-precise supervision. Our general ideas apply to other weakly-supervised problems/systems.
<div id='section'>Paperid: <span id='pid'>720, <a href='https://arxiv.org/pdf/2506.10328.pdf' target='_blank'>https://arxiv.org/pdf/2506.10328.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sadia Kamal, Tim Oates, Joy Wan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10328">Towards Scalable SOAP Note Generation: A Weakly Supervised Multimodal Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skin carcinoma is the most prevalent form of cancer globally, accounting for over $8 billion in annual healthcare expenditures. In clinical settings, physicians document patient visits using detailed SOAP (Subjective, Objective, Assessment, and Plan) notes. However, manually generating these notes is labor-intensive and contributes to clinician burnout. In this work, we propose a weakly supervised multimodal framework to generate clinically structured SOAP notes from limited inputs, including lesion images and sparse clinical text. Our approach reduces reliance on manual annotations, enabling scalable, clinically grounded documentation while alleviating clinician burden and reducing the need for large annotated data. Our method achieves performance comparable to GPT-4o, Claude, and DeepSeek Janus Pro across key clinical relevance metrics. To evaluate clinical quality, we introduce two novel metrics MedConceptEval and Clinical Coherence Score (CCS) which assess semantic alignment with expert medical concepts and input features, respectively.
<div id='section'>Paperid: <span id='pid'>721, <a href='https://arxiv.org/pdf/2506.07076.pdf' target='_blank'>https://arxiv.org/pdf/2506.07076.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyi Wu, Haohong Wang, Aggelos K. Katsaggelos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07076">Harmony-Aware Music-driven Motion Synthesis with Perceptual Constraint on UGC Datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the popularity of video-based user-generated content (UGC) on social media, harmony, as dictated by human perceptual principles, is critical in assessing the rhythmic consistency of audio-visual UGCs for better user engagement. In this work, we propose a novel harmony-aware GAN framework, following a specifically designed harmony evaluation strategy to enhance rhythmic synchronization in the automatic music-to-motion synthesis using a UGC dance dataset. This harmony strategy utilizes refined cross-modal beat detection to capture closely correlated audio and visual rhythms in an audio-visual pair. To mimic human attention mechanism, we introduce saliency-based beat weighting and interval-driven beat alignment, which ensures accurate harmony score estimation consistent with human perception. Building on this strategy, our model, employing efficient encoder-decoder and depth-lifting designs, is adversarially trained based on categorized musical meter segments to generate realistic and rhythmic 3D human motions. We further incorporate our harmony evaluation strategy as a weakly supervised perceptual constraint to flexibly guide the synchronized audio-visual rhythms during the generation process. Experimental results show that our proposed model significantly outperforms other leading music-to-motion methods in rhythmic harmony, both quantitatively and qualitatively, even with limited UGC training data. Live samples 15 can be watched at: https://youtu.be/tWwz7yq4aUs
<div id='section'>Paperid: <span id='pid'>722, <a href='https://arxiv.org/pdf/2506.03570.pdf' target='_blank'>https://arxiv.org/pdf/2506.03570.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lin Sun, Chuang Liu, Xiaofeng Ma, Tao Yang, Weijia Lu, Ning Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03570">FreePRM: Training Process Reward Models Without Ground Truth Process Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in Large Language Models (LLMs) have demonstrated that Process Reward Models (PRMs) play a crucial role in enhancing model performance. However, training PRMs typically requires step-level labels, either manually annotated or automatically generated, which can be costly and difficult to obtain at scale. To address this challenge, we introduce FreePRM, a weakly supervised framework for training PRMs without access to ground-truth step-level labels. FreePRM first generates pseudo step-level labels based on the correctness of final outcome, and then employs Buffer Probability to eliminate impact of noise inherent in pseudo labeling. Experimental results show that FreePRM achieves an average F1 score of 53.0% on ProcessBench, outperforming fully supervised PRM trained on Math-Shepherd by +24.1%. Compared to other open-source PRMs, FreePRM outperforms upon RLHFlow-PRM-Mistral-8B (28.4%) by +24.6%, EurusPRM (31.3%) by +21.7%, and Skywork-PRM-7B (42.1%) by +10.9%. This work introduces a new paradigm in PRM training, significantly reducing reliance on costly step-level annotations while maintaining strong performance.
<div id='section'>Paperid: <span id='pid'>723, <a href='https://arxiv.org/pdf/2506.02451.pdf' target='_blank'>https://arxiv.org/pdf/2506.02451.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pratheeksha Nair, Reihaneh Rabbany
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02451">Weak Supervision for Real World Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Node classification in real world graphs often suffers from label scarcity and noise, especially in high stakes domains like human trafficking detection and misinformation monitoring. While direct supervision is limited, such graphs frequently contain weak signals, noisy or indirect cues, that can still inform learning. We propose WSNET, a novel weakly supervised graph contrastive learning framework that leverages these weak signals to guide robust representation learning. WSNET integrates graph structure, node features, and multiple noisy supervision sources through a contrastive objective tailored for weakly labeled data. Across three real world datasets and synthetic benchmarks with controlled noise, WSNET consistently outperforms state of the art contrastive and noisy label learning methods by up to 15% in F1 score. Our results highlight the effectiveness of contrastive learning under weak supervision and the promise of exploiting imperfect labels in graph based settings.
<div id='section'>Paperid: <span id='pid'>724, <a href='https://arxiv.org/pdf/2505.09955.pdf' target='_blank'>https://arxiv.org/pdf/2505.09955.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaeho Kim, Seulki Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09955">TransPL: VQ-Code Transition Matrices for Pseudo-Labeling of Time Series Unsupervised Domain Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsupervised domain adaptation (UDA) for time series data remains a critical challenge in deep learning, with traditional pseudo-labeling strategies failing to capture temporal patterns and channel-wise shifts between domains, producing sub-optimal pseudo-labels. As such, we introduce TransPL, a novel approach that addresses these limitations by modeling the joint distribution $P(\mathbf{X}, y)$ of the source domain through code transition matrices, where the codes are derived from vector quantization (VQ) of time series patches. Our method constructs class- and channel-wise code transition matrices from the source domain and employs Bayes' rule for target domain adaptation, generating pseudo-labels based on channel-wise weighted class-conditional likelihoods. TransPL offers three key advantages: explicit modeling of temporal transitions and channel-wise shifts between different domains, versatility towards different UDA scenarios (e.g., weakly-supervised UDA), and explainable pseudo-label generation. We validate TransPL's effectiveness through extensive analysis on four time series UDA benchmarks and confirm that it consistently outperforms state-of-the-art pseudo-labeling methods by a strong margin (6.1% accuracy improvement, 4.9% F1 improvement), while providing interpretable insights into the domain adaptation process through its learned code transition matrices.
<div id='section'>Paperid: <span id='pid'>725, <a href='https://arxiv.org/pdf/2505.09011.pdf' target='_blank'>https://arxiv.org/pdf/2505.09011.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Antonio Candito, Matthew D Blackledge, Richard Holbrey, Nuria Porta, Ana Ribeiro, Fabio Zugni, Luca D'Erme, Francesca Castagnoli, Alina Dragan, Ricardo Donners, Christina Messiou, Nina Tunariu, Dow-Mu Koh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09011">Signal-based AI-driven software solution for automated quantification of metastatic bone disease and treatment response assessment using Whole-Body Diffusion-Weighted MRI (WB-DWI) biomarkers in Advanced Prostate Cancer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We developed an AI-driven software solution to quantify metastatic bone disease from WB-DWI scans. Core technologies include: (i) a weakly-supervised Residual U-Net model generating a skeleton probability map to isolate bone; (ii) a statistical framework for WB-DWI intensity normalisation, obtaining a signal-normalised b=900s/mm^2 (b900) image; and (iii) a shallow convolutional neural network that processes outputs from (i) and (ii) to generate a mask of suspected bone lesions, characterised by higher b900 signal intensity due to restricted water diffusion. This mask is applied to the gADC map to extract TDV and gADC statistics. We tested the tool using expert-defined metastatic bone disease delineations on 66 datasets, assessed repeatability of imaging biomarkers (N=10), and compared software-based response assessment with a construct reference standard based on clinical, laboratory and imaging assessments (N=118). Dice score between manual and automated delineations was 0.6 for lesions within pelvis and spine, with an average surface distance of 2mm. Relative differences for log-transformed TDV (log-TDV) and median gADC were below 9% and 5%, respectively. Repeatability analysis showed coefficients of variation of 4.57% for log-TDV and 3.54% for median gADC, with intraclass correlation coefficients above 0.9. The software achieved 80.5% accuracy, 84.3% sensitivity, and 85.7% specificity in assessing response to treatment compared to the construct reference standard. Computation time generating a mask averaged 90 seconds per scan. Our software enables reproducible TDV and gADC quantification from WB-DWI scans for monitoring metastatic bone disease response, thus providing potentially useful measurements for clinical decision-making in APC patients.
<div id='section'>Paperid: <span id='pid'>726, <a href='https://arxiv.org/pdf/2504.09430.pdf' target='_blank'>https://arxiv.org/pdf/2504.09430.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruiwen Ding, Lin Li, Rajath Soans, Tosha Shah, Radha Krishnan, Marc Alexander Sze, Sasha Lukyanov, Yash Deshpande, Antong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09430">Predicting ulcer in H&E images of inflammatory bowel disease using domain-knowledge-driven graph neural network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inflammatory bowel disease (IBD) involves chronic inflammation of the digestive tract, with treatment options often burdened by adverse effects. Identifying biomarkers for personalized treatment is crucial. While immune cells play a key role in IBD, accurately identifying ulcer regions in whole slide images (WSIs) is essential for characterizing these cells and exploring potential therapeutics. Multiple instance learning (MIL) approaches have advanced WSI analysis but they lack spatial context awareness. In this work, we propose a weakly-supervised model called DomainGCN that employs a graph convolution neural network (GCN) and incorporates domain-specific knowledge of ulcer features, specifically, the presence of epithelium, lymphocytes, and debris for WSI-level ulcer prediction in IBD. We demonstrate that DomainGCN outperforms various state-of-the-art (SOTA) MIL methods and show the added value of domain knowledge.
<div id='section'>Paperid: <span id='pid'>727, <a href='https://arxiv.org/pdf/2504.05700.pdf' target='_blank'>https://arxiv.org/pdf/2504.05700.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seth Z. Zhao, Reza Ghoddoosian, Isht Dwivedi, Nakul Agarwal, Behzad Dariush
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.05700">Pose-Aware Weakly-Supervised Action Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding human behavior is an important problem in the pursuit of visual intelligence. A challenge in this endeavor is the extensive and costly effort required to accurately label action segments. To address this issue, we consider learning methods that demand minimal supervision for segmentation of human actions in long instructional videos. Specifically, we introduce a weakly-supervised framework that uniquely incorporates pose knowledge during training while omitting its use during inference, thereby distilling pose knowledge pertinent to each action component. We propose a pose-inspired contrastive loss as a part of the whole weakly-supervised framework which is trained to distinguish action boundaries more effectively. Our approach, validated through extensive experiments on representative datasets, outperforms previous state-of-the-art (SOTA) in segmenting long instructional videos under both online and offline settings. Additionally, we demonstrate the framework's adaptability to various segmentation backbones and pose extractors across different datasets.
<div id='section'>Paperid: <span id='pid'>728, <a href='https://arxiv.org/pdf/2503.21616.pdf' target='_blank'>https://arxiv.org/pdf/2503.21616.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahui Chen, Yang Huan, Runhua Shi, Chanfan Ding, Xiaoqi Mo, Siyu Xiong, Yinong He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.21616">Audio-driven Gesture Generation via Deviation Feature in the Latent Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gestures are essential for enhancing co-speech communication, offering visual emphasis and complementing verbal interactions. While prior work has concentrated on point-level motion or fully supervised data-driven methods, we focus on co-speech gestures, advocating for weakly supervised learning and pixel-level motion deviations. We introduce a weakly supervised framework that learns latent representation deviations, tailored for co-speech gesture video generation. Our approach employs a diffusion model to integrate latent motion features, enabling more precise and nuanced gesture representation. By leveraging weakly supervised deviations in latent space, we effectively generate hand gestures and mouth movements, crucial for realistic video production. Experiments show our method significantly improves video quality, surpassing current state-of-the-art techniques.
<div id='section'>Paperid: <span id='pid'>729, <a href='https://arxiv.org/pdf/2503.14395.pdf' target='_blank'>https://arxiv.org/pdf/2503.14395.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Wang, Ruirui Liu, Yu Lei, Michael J. Baine, Tian Liu, Yang Lei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14395">Weakly Supervised Spatial Implicit Neural Representation Learning for 3D MRI-Ultrasound Deformable Image Registration in HDR Prostate Brachytherapy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Purpose: Accurate 3D MRI-ultrasound (US) deformable registration is critical for real-time guidance in high-dose-rate (HDR) prostate brachytherapy. We present a weakly supervised spatial implicit neural representation (SINR) method to address modality differences and pelvic anatomy challenges.
  Methods: The framework uses sparse surface supervision from MRI/US segmentations instead of dense intensity matching. SINR models deformations as continuous spatial functions, with patient-specific surface priors guiding a stationary velocity field for biologically plausible deformations. Validation included 20 public Prostate-MRI-US-Biopsy cases and 10 institutional HDR cases, evaluated via Dice similarity coefficient (DSC), mean surface distance (MSD), and 95% Hausdorff distance (HD95).
  Results: The proposed method achieved robust registration. For the public dataset, prostate DSC was $0.93 \pm 0.05$, MSD $0.87 \pm 0.10$ mm, and HD95 $1.58 \pm 0.37$ mm. For the institutional dataset, prostate CTV achieved DSC $0.88 \pm 0.09$, MSD $1.21 \pm 0.38$ mm, and HD95 $2.09 \pm 1.48$ mm. Bladder and rectum performance was lower due to ultrasound's limited field of view. Visual assessments confirmed accurate alignment with minimal discrepancies.
  Conclusion: This study introduces a novel weakly supervised SINR-based approach for 3D MRI-US deformable registration. By leveraging sparse surface supervision and spatial priors, it achieves accurate, robust, and computationally efficient registration, enhancing real-time image guidance in HDR prostate brachytherapy and improving treatment precision.
<div id='section'>Paperid: <span id='pid'>730, <a href='https://arxiv.org/pdf/2501.07020.pdf' target='_blank'>https://arxiv.org/pdf/2501.07020.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anh Thi-Hoang Nguyen, Dung Ha Nguyen, Kiet Van Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07020">ViSoLex: An Open-Source Repository for Vietnamese Social Media Lexical Normalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>ViSoLex is an open-source system designed to address the unique challenges of lexical normalization for Vietnamese social media text. The platform provides two core services: Non-Standard Word (NSW) Lookup and Lexical Normalization, enabling users to retrieve standard forms of informal language and standardize text containing NSWs. ViSoLex's architecture integrates pre-trained language models and weakly supervised learning techniques to ensure accurate and efficient normalization, overcoming the scarcity of labeled data in Vietnamese. This paper details the system's design, functionality, and its applications for researchers and non-technical users. Additionally, ViSoLex offers a flexible, customizable framework that can be adapted to various datasets and research requirements. By publishing the source code, ViSoLex aims to contribute to the development of more robust Vietnamese natural language processing tools and encourage further research in lexical normalization. Future directions include expanding the system's capabilities for additional languages and improving the handling of more complex non-standard linguistic patterns.
<div id='section'>Paperid: <span id='pid'>731, <a href='https://arxiv.org/pdf/2501.05933.pdf' target='_blank'>https://arxiv.org/pdf/2501.05933.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Olivier Morelle, Justus Bisten, Maximilian W. M. Wintergerst, Robert P. Finger, Thomas Schultz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.05933">Weakly Supervised Segmentation of Hyper-Reflective Foci with Compact Convolutional Transformers and SAM2</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised segmentation has the potential to greatly reduce the annotation effort for training segmentation models for small structures such as hyper-reflective foci (HRF) in optical coherence tomography (OCT). However, most weakly supervised methods either involve a strong downsampling of input images, or only achieve localization at a coarse resolution, both of which are unsatisfactory for small structures. We propose a novel framework that increases the spatial resolution of a traditional attention-based Multiple Instance Learning (MIL) approach by using Layer-wise Relevance Propagation (LRP) to prompt the Segment Anything Model (SAM~2), and increases recall with iterative inference. Moreover, we demonstrate that replacing MIL with a Compact Convolutional Transformer (CCT), which adds a positional encoding, and permits an exchange of information between different regions of the OCT image, leads to a further and substantial increase in segmentation accuracy.
<div id='section'>Paperid: <span id='pid'>732, <a href='https://arxiv.org/pdf/2412.12829.pdf' target='_blank'>https://arxiv.org/pdf/2412.12829.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elena Bueno-Benito, Mariella Dimiccoli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12829">2by2: Weakly-Supervised Learning for Global Action Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a simple yet effective approach for the poorly investigated task of global action segmentation, aiming at grouping frames capturing the same action across videos of different activities. Unlike the case of videos depicting all the same activity, the temporal order of actions is not roughly shared among all videos, making the task even more challenging. We propose to use activity labels to learn, in a weakly-supervised fashion, action representations suitable for global action segmentation. For this purpose, we introduce a triadic learning approach for video pairs, to ensure intra-video action discrimination, as well as inter-video and inter-activity action association. For the backbone architecture, we use a Siamese network based on sparse transformers that takes as input video pairs and determine whether they belong to the same activity. The proposed approach is validated on two challenging benchmark datasets: Breakfast and YouTube Instructions, outperforming state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>733, <a href='https://arxiv.org/pdf/2412.07384.pdf' target='_blank'>https://arxiv.org/pdf/2412.07384.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Florin Condrea, Saikiran Rapaka, Marius Leordeanu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07384">Label up: Learning Pulmonary Embolism Segmentation from Image Level Annotation through Model Explainability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pulmonary Embolisms (PE) are a leading cause of cardiovascular death. Computed tomographic pulmonary angiography (CTPA) stands as the gold standard for diagnosing pulmonary embolisms (PE) and there has been a lot of interest in developing AI-based models for assisting in PE diagnosis. Performance of these algorithms has been hindered by the scarcity of annotated data, especially those with fine-grained delineation of the thromboembolic burden. In this paper we attempt to address this issue by introducing a weakly supervised learning pipeline, that leverages model explainability to generate fine-grained (pixel level) masks for embolisms starting from more coarse-grained (binary, image level) PE annotations. Furthermore, we show that training models using the automatically generated pixel annotations yields good PE localization performance. We demonstrate the effectiveness of our pipeline on the large-scale, multi-center RSPECT augmented dataset for PE detection and localization.
<div id='section'>Paperid: <span id='pid'>734, <a href='https://arxiv.org/pdf/2412.02250.pdf' target='_blank'>https://arxiv.org/pdf/2412.02250.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Javier UreÃ±a Santiago, Thomas StrÃ¶hle, Antonio RodrÃ­guez-SÃ¡nchez, Ruth Breu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02250">Vision Transformers for Weakly-Supervised Microorganism Enumeration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Microorganism enumeration is an essential task in many applications, such as assessing contamination levels or ensuring health standards when evaluating surface cleanliness. However, it's traditionally performed by human-supervised methods that often require manual counting, making it tedious and time-consuming. Previous research suggests automating this task using computer vision and machine learning methods, primarily through instance segmentation or density estimation techniques. This study conducts a comparative analysis of vision transformers (ViTs) for weakly-supervised counting in microorganism enumeration, contrasting them with traditional architectures such as ResNet and investigating ViT-based models such as TransCrowd. We trained different versions of ViTs as the architectural backbone for feature extraction using four microbiology datasets to determine potential new approaches for total microorganism enumeration in images. Results indicate that while ResNets perform better overall, ViTs performance demonstrates competent results across all datasets, opening up promising lines of research in microorganism enumeration. This comparative study contributes to the field of microbial image analysis by presenting innovative approaches to the recurring challenge of microorganism enumeration and by highlighting the capabilities of ViTs in the task of regression counting.
<div id='section'>Paperid: <span id='pid'>735, <a href='https://arxiv.org/pdf/2412.00426.pdf' target='_blank'>https://arxiv.org/pdf/2412.00426.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ayoub Hammal, Benno Uthayasooriyar, Caio Corro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.00426">Few-Shot Domain Adaptation for Named-Entity Recognition via Joint Constrained k-Means and Subspace Selection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Named-entity recognition (NER) is a task that typically requires large annotated datasets, which limits its applicability across domains with varying entity definitions. This paper addresses few-shot NER, aiming to transfer knowledge to new domains with minimal supervision. Unlike previous approaches that rely solely on limited annotated data, we propose a weakly supervised algorithm that combines small labeled datasets with large amounts of unlabeled data. Our method extends the k-means algorithm with label supervision, cluster size constraints and domain-specific discriminative subspace selection. This unified framework achieves state-of-the-art results in few-shot NER on several English datasets.
<div id='section'>Paperid: <span id='pid'>736, <a href='https://arxiv.org/pdf/2410.09999.pdf' target='_blank'>https://arxiv.org/pdf/2410.09999.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sandeep Sricharan Mukku, Abinesh Kanagarajan, Pushpendu Ghosh, Chetan Aggarwal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.09999">Leveraging Customer Feedback for Multi-modal Insight Extraction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Businesses can benefit from customer feedback in different modalities, such as text and images, to enhance their products and services. However, it is difficult to extract actionable and relevant pairs of text segments and images from customer feedback in a single pass. In this paper, we propose a novel multi-modal method that fuses image and text information in a latent space and decodes it to extract the relevant feedback segments using an image-text grounded text decoder. We also introduce a weakly-supervised data generation technique that produces training data for this task. We evaluate our model on unseen data and demonstrate that it can effectively mine actionable insights from multi-modal customer feedback, outperforming the existing baselines by $14$ points in F1 score.
<div id='section'>Paperid: <span id='pid'>737, <a href='https://arxiv.org/pdf/2410.04789.pdf' target='_blank'>https://arxiv.org/pdf/2410.04789.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>MÃ³nica Apellaniz Portos, Roberto Labadie-Tamayo, Claudius Stemmler, Erwin Feyersinger, Andreas Babic, Franziska Bruckner, VrÃ¤Ã¤th Ãhner, Matthias Zeppelzauer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.04789">Analysis of Hybrid Compositions in Animation Film with Weakly Supervised Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present an approach for the analysis of hybrid visual compositions in animation in the domain of ephemeral film. We combine ideas from semi-supervised and weakly supervised learning to train a model that can segment hybrid compositions without requiring pre-labeled segmentation masks. We evaluate our approach on a set of ephemeral films from 13 film archives. Results demonstrate that the proposed learning strategy yields a performance close to a fully supervised baseline. On a qualitative level the performed analysis provides interesting insights on hybrid compositions in animation film.
<div id='section'>Paperid: <span id='pid'>738, <a href='https://arxiv.org/pdf/2409.20467.pdf' target='_blank'>https://arxiv.org/pdf/2409.20467.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dung Ha Nguyen, Anh Thi Hoang Nguyen, Kiet Van Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.20467">A Weakly Supervised Data Labeling Framework for Machine Lexical Normalization in Vietnamese Social Media</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study introduces an innovative automatic labeling framework to address the challenges of lexical normalization in social media texts for low-resource languages like Vietnamese. Social media data is rich and diverse, but the evolving and varied language used in these contexts makes manual labeling labor-intensive and expensive. To tackle these issues, we propose a framework that integrates semi-supervised learning with weak supervision techniques. This approach enhances the quality of training dataset and expands its size while minimizing manual labeling efforts. Our framework automatically labels raw data, converting non-standard vocabulary into standardized forms, thereby improving the accuracy and consistency of the training data. Experimental results demonstrate the effectiveness of our weak supervision framework in normalizing Vietnamese text, especially when utilizing Pre-trained Language Models. The proposed framework achieves an impressive F1-score of 82.72% and maintains vocabulary integrity with an accuracy of up to 99.22%. Additionally, it effectively handles undiacritized text under various conditions. This framework significantly enhances natural language normalization quality and improves the accuracy of various NLP tasks, leading to an average accuracy increase of 1-3%.
<div id='section'>Paperid: <span id='pid'>739, <a href='https://arxiv.org/pdf/2409.15491.pdf' target='_blank'>https://arxiv.org/pdf/2409.15491.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyu Su, Yongxin Guo, Robert Wesolowski, Gary Tozbikian, Nathaniel S. O'Connell, M. Khalid Khan Niazi, Metin N. Gurcan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.15491">Computational Pathology for Accurate Prediction of Breast Cancer Recurrence: Development and Validation of a Deep Learning-based Tool</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate recurrence risk stratification is crucial for optimizing treatment plans for breast cancer patients. Current prognostic tools like Oncotype DX (ODX) offer valuable genomic insights for HR+/HER2- patients but are limited by cost and accessibility, particularly in underserved populations. In this study, we present Deep-BCR-Auto, a deep learning-based computational pathology approach that predicts breast cancer recurrence risk from routine H&E-stained whole slide images (WSIs). Our methodology was validated on two independent cohorts: the TCGA-BRCA dataset and an in-house dataset from The Ohio State University (OSU). Deep-BCR-Auto demonstrated robust performance in stratifying patients into low- and high-recurrence risk categories. On the TCGA-BRCA dataset, the model achieved an area under the receiver operating characteristic curve (AUROC) of 0.827, significantly outperforming existing weakly supervised models (p=0.041). In the independent OSU dataset, Deep-BCR-Auto maintained strong generalizability, achieving an AUROC of 0.832, along with 82.0% accuracy, 85.0% specificity, and 67.7% sensitivity. These findings highlight the potential of computational pathology as a cost-effective alternative for recurrence risk assessment, broadening access to personalized treatment strategies. This study underscores the clinical utility of integrating deep learning-based computational pathology into routine pathological assessment for breast cancer prognosis across diverse clinical settings.
<div id='section'>Paperid: <span id='pid'>740, <a href='https://arxiv.org/pdf/2409.03236.pdf' target='_blank'>https://arxiv.org/pdf/2409.03236.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenglizhao Chen, Xinyu Liu, Mengke Song, Luming Li, Xu Yu, Shanchen Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.03236">Unveiling Context-Related Anomalies: Knowledge Graph Empowered Decoupling of Scene and Action for Human-Related Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detecting anomalies in human-related videos is crucial for surveillance applications. Current methods primarily include appearance-based and action-based techniques. Appearance-based methods rely on low-level visual features such as color, texture, and shape. They learn a large number of pixel patterns and features related to known scenes during training, making them effective in detecting anomalies within these familiar contexts. However, when encountering new or significantly changed scenes, i.e., unknown scenes, they often fail because existing SOTA methods do not effectively capture the relationship between actions and their surrounding scenes, resulting in low generalization. In contrast, action-based methods focus on detecting anomalies in human actions but are usually less informative because they tend to overlook the relationship between actions and their scenes, leading to incorrect detection. For instance, the normal event of running on the beach and the abnormal event of running on the street might both be considered normal due to the lack of scene information. In short, current methods struggle to integrate low-level visual and high-level action features, leading to poor anomaly detection in varied and complex scenes. To address this challenge, we propose a novel decoupling-based architecture for human-related video anomaly detection (DecoAD). DecoAD significantly improves the integration of visual and action features through the decoupling and interweaving of scenes and actions, thereby enabling a more intuitive and accurate understanding of complex behaviors and scenes. DecoAD supports fully supervised, weakly supervised, and unsupervised settings.
<div id='section'>Paperid: <span id='pid'>741, <a href='https://arxiv.org/pdf/2409.01676.pdf' target='_blank'>https://arxiv.org/pdf/2409.01676.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenyang Hu, Gaetan Frusque, Tianyang Wang, Fulei Chu, Olga Fink
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01676">Classifier-Free Diffusion-Based Weakly-Supervised Approach for Health Indicator Derivation in Rotating Machines: Advancing Early Fault Detection and Condition Monitoring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deriving health indicators of rotating machines is crucial for their maintenance. However, this process is challenging for the prevalent adopted intelligent methods since they may take the whole data distributions, not only introducing noise interference but also lacking the explainability. To address these issues, we propose a diffusion-based weakly-supervised approach for deriving health indicators of rotating machines, enabling early fault detection and continuous monitoring of condition evolution. This approach relies on a classifier-free diffusion model trained using healthy samples and a few anomalies. This model generates healthy samples. and by comparing the differences between the original samples and the generated ones in the envelope spectrum, we construct an anomaly map that clearly identifies faults. Health indicators are then derived, which can explain the fault types and mitigate noise interference. Comparative studies on two cases demonstrate that the proposed method offers superior health monitoring effectiveness and robustness compared to baseline models.
<div id='section'>Paperid: <span id='pid'>742, <a href='https://arxiv.org/pdf/2408.14498.pdf' target='_blank'>https://arxiv.org/pdf/2408.14498.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhijin Dong, Hongzhi Liu, Boyuan Ren, Weimin Xiong, Zhonghai Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.14498">Multi-Normal Prototypes Learning for Weakly Supervised Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anomaly detection is a crucial task in various domains. Most of the existing methods assume the normal sample data clusters around a single central prototype while the real data may consist of multiple categories or subgroups. In addition, existing methods always assume all unlabeled samples are normal while some of them are inevitably being anomalies. To address these issues, we propose a novel anomaly detection framework that can efficiently work with limited labeled anomalies. Specifically, we assume the normal sample data may consist of multiple subgroups, and propose to learn multi-normal prototypes to represent them with deep embedding clustering and contrastive learning. Additionally, we propose a method to estimate the likelihood of each unlabeled sample being normal during model training, which can help to learn more efficient data encoder and normal prototypes for anomaly detection. Extensive experiments on various datasets demonstrate the superior performance of our method compared to state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>743, <a href='https://arxiv.org/pdf/2408.09952.pdf' target='_blank'>https://arxiv.org/pdf/2408.09952.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ik Jun Moon, Junho Moon, Ikbeom Jang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09952">Weakly Supervised Pretraining and Multi-Annotator Supervised Finetuning for Facial Wrinkle Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>1. Research question: With the growing interest in skin diseases and skin aesthetics, the ability to predict facial wrinkles is becoming increasingly important. This study aims to evaluate whether a computational model, convolutional neural networks (CNN), can be trained for automated facial wrinkle segmentation. 2. Findings: Our study presents an effective technique for integrating data from multiple annotators and illustrates that transfer learning can enhance performance, resulting in dependable segmentation of facial wrinkles. 3. Meaning: This approach automates intricate and time-consuming tasks of wrinkle analysis with a deep learning framework. It could be used to facilitate skin treatments and diagnostics.
<div id='section'>Paperid: <span id='pid'>744, <a href='https://arxiv.org/pdf/2408.05191.pdf' target='_blank'>https://arxiv.org/pdf/2408.05191.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yashika Jain, Ali Dabouei, Min Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.05191">Cross-Domain Learning for Video Anomaly Detection with Limited Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Anomaly Detection (VAD) automates the identification of unusual events, such as security threats in surveillance videos. In real-world applications, VAD models must effectively operate in cross-domain settings, identifying rare anomalies and scenarios not well-represented in the training data. However, existing cross-domain VAD methods focus on unsupervised learning, resulting in performance that falls short of real-world expectations. Since acquiring weak supervision, i.e., video-level labels, for the source domain is cost-effective, we conjecture that combining it with external unlabeled data has notable potential to enhance cross-domain performance. To this end, we introduce a novel weakly-supervised framework for Cross-Domain Learning (CDL) in VAD that incorporates external data during training by estimating its prediction bias and adaptively minimizing that using the predicted uncertainty. We demonstrate the effectiveness of the proposed CDL framework through comprehensive experiments conducted in various configurations on two large-scale VAD datasets: UCF-Crime and XD-Violence. Our method significantly surpasses the state-of-the-art works in cross-domain evaluations, achieving an average absolute improvement of 19.6% on UCF-Crime and 12.87% on XD-Violence.
<div id='section'>Paperid: <span id='pid'>745, <a href='https://arxiv.org/pdf/2408.04482.pdf' target='_blank'>https://arxiv.org/pdf/2408.04482.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sriram Mandalika, Athira Nambiar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.04482">SegXAL: Explainable Active Learning for Semantic Segmentation in Driving Scene Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most of the sophisticated AI models utilize huge amounts of annotated data and heavy training to achieve high-end performance. However, there are certain challenges that hinder the deployment of AI models "in-the-wild" scenarios, i.e., inefficient use of unlabeled data, lack of incorporation of human expertise, and lack of interpretation of the results. To mitigate these challenges, we propose a novel Explainable Active Learning (XAL) model, XAL-based semantic segmentation model "SegXAL", that can (i) effectively utilize the unlabeled data, (ii) facilitate the "Human-in-the-loop" paradigm, and (iii) augment the model decisions in an interpretable way. In particular, we investigate the application of the SegXAL model for semantic segmentation in driving scene scenarios. The SegXAL model proposes the image regions that require labeling assistance from Oracle by dint of explainable AI (XAI) and uncertainty measures in a weakly-supervised manner. Specifically, we propose a novel Proximity-aware Explainable-AI (PAE) module and Entropy-based Uncertainty (EBU) module to get an Explainable Error Mask, which enables the machine teachers/human experts to provide intuitive reasoning behind the results and to solicit feedback to the AI system via an active learning strategy. Such a mechanism bridges the semantic gap between man and machine through collaborative intelligence, where humans and AI actively enhance each other's complementary strengths. A novel high-confidence sample selection technique based on the DICE similarity coefficient is also presented within the SegXAL framework. Extensive quantitative and qualitative analyses are carried out in the benchmarking Cityscape dataset. Results show the outperformance of our proposed SegXAL against other state-of-the-art models.
<div id='section'>Paperid: <span id='pid'>746, <a href='https://arxiv.org/pdf/2407.10274.pdf' target='_blank'>https://arxiv.org/pdf/2407.10274.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinsheng He, Xingyu Li, Roger J. Zemp
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10274">Enhancing Weakly-Supervised Histopathology Image Segmentation with Knowledge Distillation on MIL-Based Pseudo-Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Segmenting tumors in histological images is vital for cancer diagnosis. While fully supervised models excel with pixel-level annotations, creating such annotations is labor-intensive and costly. Accurate histopathology image segmentation under weakly-supervised conditions with coarse-grained image labels is still a challenging problem. Although multiple instance learning (MIL) has shown promise in segmentation tasks, surprisingly, no previous pseudo-supervision methods have used MIL-based outputs as pseudo-masks for training. We suspect this stems from concerns over noises in MIL results affecting pseudo supervision quality. To explore the potential of leveraging MIL-based segmentation for pseudo supervision, we propose a novel distillation framework for histopathology image segmentation. This framework introduces a iterative fusion-knowledge distillation strategy, enabling the student model to learn directly from the teacher's comprehensive outcomes. Through dynamic role reversal between the fixed teacher and learnable student models and the incorporation of weighted cross-entropy loss for model optimization, our approach prevents performance deterioration and noise amplification during knowledge distillation. Experimental results on public histopathology datasets, Camelyon16 and Digestpath2019, demonstrate that our approach not only complements various MIL-based segmentation methods but also significantly enhances their performance. Additionally, our method achieves new SOTA in the field.
<div id='section'>Paperid: <span id='pid'>747, <a href='https://arxiv.org/pdf/2406.08396.pdf' target='_blank'>https://arxiv.org/pdf/2406.08396.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yoshiaki Bando, Tomohiko Nakamura, Shinji Watanabe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.08396">Neural Blind Source Separation and Diarization for Distant Speech Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a neural method for distant speech recognition (DSR) that jointly separates and diarizes speech mixtures without supervision by isolated signals. A standard separation method for multi-talker DSR is a statistical multichannel method called guided source separation (GSS). While GSS does not require signal-level supervision, it relies on speaker diarization results to handle unknown numbers of active speakers. To overcome this limitation, we introduce and train a neural inference model in a weakly-supervised manner, employing the objective function of a statistical separation method. This training requires only multichannel mixtures and their temporal annotations of speaker activities. In contrast to GSS, the trained model can jointly separate and diarize speech mixtures without any auxiliary information. The experiments with the AMI corpus show that our method outperforms GSS with oracle diarization results regarding word error rates. The code is available online.
<div id='section'>Paperid: <span id='pid'>748, <a href='https://arxiv.org/pdf/2406.06723.pdf' target='_blank'>https://arxiv.org/pdf/2406.06723.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Enshuo Hsu, Kirk Roberts
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.06723">Leveraging Large Language Models for Knowledge-free Weak Supervision in Clinical Natural Language Processing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The performance of deep learning-based natural language processing systems is based on large amounts of labeled training data which, in the clinical domain, are not easily available or affordable. Weak supervision and in-context learning offer partial solutions to this issue, particularly using large language models (LLMs), but their performance still trails traditional supervised methods with moderate amounts of gold-standard data. In particular, inferencing with LLMs is computationally heavy. We propose an approach leveraging fine-tuning LLMs and weak supervision with virtually no domain knowledge that still achieves consistently dominant performance. Using a prompt-based approach, the LLM is used to generate weakly-labeled data for training a downstream BERT model. The weakly supervised model is then further fine-tuned on small amounts of gold standard data. We evaluate this approach using Llama2 on three different n2c2 datasets. With no more than 10 gold standard notes, our final BERT models weakly supervised by fine-tuned Llama2-13B consistently outperformed out-of-the-box PubMedBERT by 4.7% to 47.9% in F1 scores. With only 50 gold standard notes, our models achieved close performance to fully fine-tuned systems.
<div id='section'>Paperid: <span id='pid'>749, <a href='https://arxiv.org/pdf/2406.02831.pdf' target='_blank'>https://arxiv.org/pdf/2406.02831.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jash Dalvi, Ali Dabouei, Gunjan Dhanuka, Min Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.02831">Distilling Aggregated Knowledge for Weakly-Supervised Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video anomaly detection aims to develop automated models capable of identifying abnormal events in surveillance videos. The benchmark setup for this task is extremely challenging due to: i) the limited size of the training sets, ii) weak supervision provided in terms of video-level labels, and iii) intrinsic class imbalance induced by the scarcity of abnormal events. In this work, we show that distilling knowledge from aggregated representations of multiple backbones into a single-backbone Student model achieves state-of-the-art performance. In particular, we develop a bi-level distillation approach along with a novel disentangled cross-attention-based feature aggregation network. Our proposed approach, DAKD (Distilling Aggregated Knowledge with Disentangled Attention), demonstrates superior performance compared to existing methods across multiple benchmark datasets. Notably, we achieve significant improvements of 1.36%, 0.78%, and 7.02% on the UCF-Crime, ShanghaiTech, and XD-Violence datasets, respectively.
<div id='section'>Paperid: <span id='pid'>750, <a href='https://arxiv.org/pdf/2406.00164.pdf' target='_blank'>https://arxiv.org/pdf/2406.00164.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huixin Zhan, Zijun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00164">DYNA: Disease-Specific Language Model for Variant Pathogenicity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Clinical variant classification of pathogenic versus benign genetic variants remains a challenge in clinical genetics. Recently, the proposition of genomic foundation models has improved the generic variant effect prediction (VEP) accuracy via weakly-supervised or unsupervised training. However, these VEPs are not disease-specific, limiting their adaptation at the point of care. To address this problem, we propose DYNA: Disease-specificity fine-tuning via a Siamese neural network broadly applicable to all genomic foundation models for more effective variant effect predictions in disease-specific contexts. We evaluate DYNA in two distinct disease-relevant tasks. For coding VEPs, we focus on various cardiovascular diseases, where gene-disease relationships of loss-of-function vs. gain-of-function dictate disease-specific VEP. For non-coding VEPs, we apply DYNA to an essential post-transcriptional regulatory axis of RNA splicing, the most common non-coding pathogenic mechanism in established clinical VEP guidelines. In both cases, DYNA fine-tunes various pre-trained genomic foundation models on small, rare variant sets. The DYNA fine-tuned models show superior performance in the held-out rare variant testing set and are further replicated in large, clinically-relevant variant annotations in ClinVAR. Thus, DYNA offers a potent disease-specific variant effect prediction method, excelling in intra-gene generalization and generalization to unseen genetic variants, making it particularly valuable for disease associations and clinical applicability.
<div id='section'>Paperid: <span id='pid'>751, <a href='https://arxiv.org/pdf/2405.10456.pdf' target='_blank'>https://arxiv.org/pdf/2405.10456.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammed Patel, Xinwei Chen, Linlin Xu, Yuhao Chen, K Andrea Scott, David A. Clausi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.10456">Region-level labels in ice charts can produce pixel-level segmentation for Sea Ice types</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fully supervised deep learning approaches have demonstrated impressive accuracy in sea ice classification, but their dependence on high-resolution labels presents a significant challenge due to the difficulty of obtaining such data. In response, our weakly supervised learning method provides a compelling alternative by utilizing lower-resolution regional labels from expert-annotated ice charts. This approach achieves exceptional pixel-level classification performance by introducing regional loss representations during training to measure the disparity between predicted and ice chart-derived sea ice type distributions. Leveraging the AI4Arctic Sea Ice Challenge Dataset, our method outperforms the fully supervised U-Net benchmark, the top solution of the AutoIce challenge, in both mapping resolution and class-wise accuracy, marking a significant advancement in automated operational sea ice mapping.
<div id='section'>Paperid: <span id='pid'>752, <a href='https://arxiv.org/pdf/2405.04913.pdf' target='_blank'>https://arxiv.org/pdf/2405.04913.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Lai, Chi-Man Vong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04913">Weakly-supervised Semantic Segmentation via Dual-stream Contrastive Learning of Cross-image Contextual Information</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised semantic segmentation (WSSS) aims at learning a semantic segmentation model with only image-level tags. Despite intensive research on deep learning approaches over a decade, there is still a significant performance gap between WSSS and full semantic segmentation. Most current WSSS methods always focus on a limited single image (pixel-wise) information while ignoring the valuable inter-image (semantic-wise) information. From this perspective, a novel end-to-end WSSS framework called DSCNet is developed along with two innovations: i) pixel-wise group contrast and semantic-wise graph contrast are proposed and introduced into the WSSS framework; ii) a novel dual-stream contrastive learning (DSCL) mechanism is designed to jointly handle pixel-wise and semantic-wise context information for better WSSS performance. Specifically, the pixel-wise group contrast learning (PGCL) and semantic-wise graph contrast learning (SGCL) tasks form a more comprehensive solution. Extensive experiments on PASCAL VOC and MS COCO benchmarks verify the superiority of DSCNet over SOTA approaches and baseline models.
<div id='section'>Paperid: <span id='pid'>753, <a href='https://arxiv.org/pdf/2405.00260.pdf' target='_blank'>https://arxiv.org/pdf/2405.00260.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yamato Okamoto, Youngmin Baek, Geewook Kim, Ryota Nakao, DongHyun Kim, Moon Bin Yim, Seunghyun Park, Bado Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.00260">CREPE: Coordinate-Aware End-to-End Document Parser</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study, we formulate an OCR-free sequence generation model for visual document understanding (VDU). Our model not only parses text from document images but also extracts the spatial coordinates of the text based on the multi-head architecture. Named as Coordinate-aware End-to-end Document Parser (CREPE), our method uniquely integrates these capabilities by introducing a special token for OCR text, and token-triggered coordinate decoding. We also proposed a weakly-supervised framework for cost-efficient training, requiring only parsing annotations without high-cost coordinate annotations. Our experimental evaluations demonstrate CREPE's state-of-the-art performances on document parsing tasks. Beyond that, CREPE's adaptability is further highlighted by its successful usage in other document understanding tasks such as layout analysis, document visual question answering, and so one. CREPE's abilities including OCR and semantic parsing not only mitigate error propagation issues in existing OCR-dependent methods, it also significantly enhance the functionality of sequence generation models, ushering in a new era for document understanding studies.
<div id='section'>Paperid: <span id='pid'>754, <a href='https://arxiv.org/pdf/2403.02932.pdf' target='_blank'>https://arxiv.org/pdf/2403.02932.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Miaomiao Li, Jiaqi Zhu, Yang Wang, Yi Yang, Yilin Li, Hongan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.02932">RulePrompt: Weakly Supervised Text Classification with Prompting PLMs and Self-Iterative Logical Rules</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised text classification (WSTC), also called zero-shot or dataless text classification, has attracted increasing attention due to its applicability in classifying a mass of texts within the dynamic and open Web environment, since it requires only a limited set of seed words (label names) for each category instead of labeled data. With the help of recently popular prompting Pre-trained Language Models (PLMs), many studies leveraged manually crafted and/or automatically identified verbalizers to estimate the likelihood of categories, but they failed to differentiate the effects of these category-indicative words, let alone capture their correlations and realize adaptive adjustments according to the unlabeled corpus. In this paper, in order to let the PLM effectively understand each category, we at first propose a novel form of rule-based knowledge using logical expressions to characterize the meanings of categories. Then, we develop a prompting PLM-based approach named RulePrompt for the WSTC task, consisting of a rule mining module and a rule-enhanced pseudo label generation module, plus a self-supervised fine-tuning module to make the PLM align with this task. Within this framework, the inaccurate pseudo labels assigned to texts and the imprecise logical rules associated with categories mutually enhance each other in an alternative manner. That establishes a self-iterative closed loop of knowledge (rule) acquisition and utilization, with seed words serving as the starting point. Extensive experiments validate the effectiveness and robustness of our approach, which markedly outperforms state-of-the-art weakly supervised methods. What is more, our approach yields interpretable category rules, proving its advantage in disambiguating easily-confused categories.
<div id='section'>Paperid: <span id='pid'>755, <a href='https://arxiv.org/pdf/2402.10887.pdf' target='_blank'>https://arxiv.org/pdf/2402.10887.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyang Wang, Chao Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.10887">Weak-Mamba-UNet: Visual Mamba Makes CNN and ViT Work Better for Scribble-based Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical image segmentation is increasingly reliant on deep learning techniques, yet the promising performance often come with high annotation costs. This paper introduces Weak-Mamba-UNet, an innovative weakly-supervised learning (WSL) framework that leverages the capabilities of Convolutional Neural Network (CNN), Vision Transformer (ViT), and the cutting-edge Visual Mamba (VMamba) architecture for medical image segmentation, especially when dealing with scribble-based annotations. The proposed WSL strategy incorporates three distinct architecture but same symmetrical encoder-decoder networks: a CNN-based UNet for detailed local feature extraction, a Swin Transformer-based SwinUNet for comprehensive global context understanding, and a VMamba-based Mamba-UNet for efficient long-range dependency modeling. The key concept of this framework is a collaborative and cross-supervisory mechanism that employs pseudo labels to facilitate iterative learning and refinement across the networks. The effectiveness of Weak-Mamba-UNet is validated on a publicly available MRI cardiac segmentation dataset with processed scribble annotations, where it surpasses the performance of a similar WSL framework utilizing only UNet or SwinUNet. This highlights its potential in scenarios with sparse or imprecise annotations. The source code is made publicly accessible.
<div id='section'>Paperid: <span id='pid'>756, <a href='https://arxiv.org/pdf/2402.05373.pdf' target='_blank'>https://arxiv.org/pdf/2402.05373.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingxin Liu, Yunzan Liu, Pengbo Xu, Jiquan Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.05373">Unleashing the Infinity Power of Geometry: A Novel Geometry-Aware Transformer (GOAT) for Whole Slide Histopathology Image Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The histopathology analysis is of great significance for the diagnosis and prognosis of cancers, however, it has great challenges due to the enormous heterogeneity of gigapixel whole slide images (WSIs) and the intricate representation of pathological features. However, recent methods have not adequately exploited geometrical representation in WSIs which is significant in disease diagnosis. Therefore, we proposed a novel weakly-supervised framework, Geometry-Aware Transformer (GOAT), in which we urge the model to pay attention to the geometric characteristics within the tumor microenvironment which often serve as potent indicators. In addition, a context-aware attention mechanism is designed to extract and enhance the morphological features within WSIs.
<div id='section'>Paperid: <span id='pid'>757, <a href='https://arxiv.org/pdf/2402.04835.pdf' target='_blank'>https://arxiv.org/pdf/2402.04835.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Darshana Saravanan, Naresh Manwani, Vineet Gandhi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04835">Pseudo-labelling meets Label Smoothing for Noisy Partial Label Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We motivate weakly supervised learning as an effective learning paradigm for problems where curating perfectly annotated datasets is expensive and may require domain expertise such as fine-grained classification. We focus on Partial Label Learning (PLL), a weakly-supervised learning paradigm where each training instance is paired with a set of candidate labels (partial label), one of which is the true label. Noisy PLL (NPLL) relaxes this constraint by allowing some partial labels to not contain the true label, enhancing the practicality of the problem. Our work centres on NPLL and presents a framework that initially assigns pseudo-labels to images by exploiting the noisy partial labels through a weighted nearest neighbour algorithm. These pseudo-label and image pairs are then used to train a deep neural network classifier with label smoothing. The classifier's features and predictions are subsequently employed to refine and enhance the accuracy of pseudo-labels. We perform thorough experiments on seven datasets and compare against nine NPLL and PLL methods. We achieve state-of-the-art results in all studied settings from the prior literature, obtaining substantial gains in the simulated fine-grained benchmarks. Further, we show the promising generalisation capability of our framework in realistic, fine-grained, crowd-sourced datasets.
<div id='section'>Paperid: <span id='pid'>758, <a href='https://arxiv.org/pdf/2401.13998.pdf' target='_blank'>https://arxiv.org/pdf/2401.13998.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haitao Gan, Lingchao Fu, Ran Zhou, Weiyan Gan, Furong Wang, Xiaoyan Wu, Zhi Yang, Zhongwei Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.13998">WAL-Net: Weakly supervised auxiliary task learning network for carotid plaques classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The classification of carotid artery ultrasound images is a crucial means for diagnosing carotid plaques, holding significant clinical relevance for predicting the risk of stroke. Recent research suggests that utilizing plaque segmentation as an auxiliary task for classification can enhance performance by leveraging the correlation between segmentation and classification tasks. However, this approach relies on obtaining a substantial amount of challenging-to-acquire segmentation annotations. This paper proposes a novel weakly supervised auxiliary task learning network model (WAL-Net) to explore the interdependence between carotid plaque classification and segmentation tasks. The plaque classification task is primary task, while the plaque segmentation task serves as an auxiliary task, providing valuable information to enhance the performance of the primary task. Weakly supervised learning is adopted in the auxiliary task to completely break away from the dependence on segmentation annotations. Experiments and evaluations are conducted on a dataset comprising 1270 carotid plaque ultrasound images from Wuhan University Zhongnan Hospital. Results indicate that the proposed method achieved an approximately 1.3% improvement in carotid plaque classification accuracy compared to the baseline network. Specifically, the accuracy of mixed-echoic plaques classification increased by approximately 3.3%, demonstrating the effectiveness of our approach.
<div id='section'>Paperid: <span id='pid'>759, <a href='https://arxiv.org/pdf/2512.16700.pdf' target='_blank'>https://arxiv.org/pdf/2512.16700.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>John M. Statheros, Hairong Wang, Richard Klein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.16700">CLARiTy: A Vision Transformer for Multi-Label Classification and Weakly-Supervised Localization of Chest X-ray Pathologies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The interpretation of chest X-rays (CXRs) poses significant challenges, particularly in achieving accurate multi-label pathology classification and spatial localization. These tasks demand different levels of annotation granularity but are frequently constrained by the scarcity of region-level (dense) annotations. We introduce CLARiTy (Class Localizing and Attention Refining Image Transformer), a vision transformer-based model for joint multi-label classification and weakly-supervised localization of thoracic pathologies. CLARiTy employs multiple class-specific tokens to generate discriminative attention maps, and a SegmentCAM module for foreground segmentation and background suppression using explicit anatomical priors. Trained on image-level labels from the NIH ChestX-ray14 dataset, it leverages distillation from a ConvNeXtV2 teacher for efficiency. Evaluated on the official NIH split, the CLARiTy-S-16-512 (a configuration of CLARiTy), achieves competitive classification performance across 14 pathologies, and state-of-the-art weakly-supervised localization performance on 8 pathologies, outperforming prior methods by 50.7%. In particular, pronounced gains occur for small pathologies like nodules and masses. The lower-resolution variant of CLARiTy, CLARiTy-S-16-224, offers high efficiency while decisively surpassing baselines, thereby having the potential for use in low-resource settings. An ablation study confirms contributions of SegmentCAM, DINO pretraining, orthogonal class token loss, and attention pooling. CLARiTy advances beyond CNN-ViT hybrids by harnessing ViT self-attention for global context and class-specific localization, refined through convolutional background suppression for precise, noise-reduced heatmaps.
<div id='section'>Paperid: <span id='pid'>760, <a href='https://arxiv.org/pdf/2511.16343.pdf' target='_blank'>https://arxiv.org/pdf/2511.16343.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chi-Han Chen, Chieh-Ming Chen, Wen-Huang Cheng, Ching-Chun Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.16343">Aerial View River Landform Video segmentation: A Weakly Supervised Context-aware Temporal Consistency Distillation Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The study of terrain and landform classification through UAV remote sensing diverges significantly from ground vehicle patrol tasks. Besides grappling with the complexity of data annotation and ensuring temporal consistency, it also confronts the scarcity of relevant data and the limitations imposed by the effective range of many technologies. This research substantiates that, in aerial positioning tasks, both the mean Intersection over Union (mIoU) and temporal consistency (TC) metrics are of paramount importance. It is demonstrated that fully labeled data is not the optimal choice, as selecting only key data lacks the enhancement in TC, leading to failures. Hence, a teacher-student architecture, coupled with key frame selection and key frame updating algorithms, is proposed. This framework successfully performs weakly supervised learning and TC knowledge distillation, overcoming the deficiencies of traditional TC training in aerial tasks. The experimental results reveal that our method utilizing merely 30\% of labeled data, concurrently elevates mIoU and temporal consistency ensuring stable localization of terrain objects. Result demo : https://gitlab.com/prophet.ai.inc/drone-based-riverbed-inspection
<div id='section'>Paperid: <span id='pid'>761, <a href='https://arxiv.org/pdf/2511.15393.pdf' target='_blank'>https://arxiv.org/pdf/2511.15393.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kunyu Zhang, Mingxuan Wang, Xiangjie Shi, Haoxing Xu, Chao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.15393">EVA-Net: Interpretable Anomaly Detection for Brain Health via Learning Continuous Aging Prototypes from One-Class EEG Cohorts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The brain age is a key indicator of brain health. While electroencephalography (EEG) is a practical tool for this task, existing models struggle with the common challenge of imperfect medical data, such as learning a ``normal'' baseline from weakly supervised, healthy-only cohorts. This is a critical anomaly detection task for identifying disease, but standard models are often black boxes lacking an interpretable structure. We propose EVA-Net, a novel framework that recasts brain age as an interpretable anomaly detection problem. EVA-Net uses an efficient, sparsified-attention Transformer to model long EEG sequences. To handle noise and variability in imperfect data, it employs a Variational Information Bottleneck to learn a robust, compressed representation. For interpretability, this representation is aligned to a continuous prototype network that explicitly learns the normative healthy aging manifold. Trained on 1297 healthy subjects, EVA-Net achieves state-of-the-art accuracy. We validated its anomaly detection capabilities on an unseen cohort of 27 MCI and AD patients. This pathological group showed significantly higher brain-age gaps and a novel Prototype Alignment Error, confirming their deviation from the healthy manifold. EVA-Net provides an interpretable framework for healthcare intelligence using imperfect medical data.
<div id='section'>Paperid: <span id='pid'>762, <a href='https://arxiv.org/pdf/2511.04892.pdf' target='_blank'>https://arxiv.org/pdf/2511.04892.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vasileios Magoulianitis, Catherine A. Alexander, Jiaxin Yang, C. -C. Jay Kuo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.04892">LG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nuclei segmentation is the cornerstone task in histology image reading, shedding light on the underlying molecular patterns and leading to disease or cancer diagnosis. Yet, it is a laborious task that requires expertise from trained physicians. The large nuclei variability across different organ tissues and acquisition processes challenges the automation of this task. On the other hand, data annotations are expensive to obtain, and thus, Deep Learning (DL) models are challenged to generalize to unseen organs or different domains. This work proposes Local-to-Global NuSegHop (LG-NuSegHop), a self-supervised pipeline developed on prior knowledge of the problem and molecular biology. There are three distinct modules: (1) a set of local processing operations to generate a pseudolabel, (2) NuSegHop a novel data-driven feature extraction model and (3) a set of global operations to post-process the predictions of NuSegHop. Notably, even though the proposed pipeline uses { no manually annotated training data} or domain adaptation, it maintains a good generalization performance on other datasets. Experiments in three publicly available datasets show that our method outperforms other self-supervised and weakly supervised methods while having a competitive standing among fully supervised methods. Remarkably, every module within LG-NuSegHop is transparent and explainable to physicians.
<div id='section'>Paperid: <span id='pid'>763, <a href='https://arxiv.org/pdf/2508.15646.pdf' target='_blank'>https://arxiv.org/pdf/2508.15646.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Swann Emilien CÃ©leste Destouches, Jesse Lahaye, Laurent Valentin Jospin, Jan Skaloud
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15646">Weakly-Supervised Learning for Tree Instances Segmentation in Airborne Lidar Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tree instance segmentation of airborne laser scanning (ALS) data is of utmost importance for forest monitoring, but remains challenging due to variations in the data caused by factors such as sensor resolution, vegetation state at acquisition time, terrain characteristics, etc. Moreover, obtaining a sufficient amount of precisely labeled data to train fully supervised instance segmentation methods is expensive. To address these challenges, we propose a weakly supervised approach where labels of an initial segmentation result obtained either by a non-finetuned model or a closed form algorithm are provided as a quality rating by a human operator. The labels produced during the quality assessment are then used to train a rating model, whose task is to classify a segmentation output into the same classes as specified by the human operator. Finally, the segmentation model is finetuned using feedback from the rating model. This in turn improves the original segmentation model by 34\% in terms of correctly identified tree instances while considerably reducing the number of non-tree instances predicted. Challenges still remain in data over sparsely forested regions characterized by small trees (less than two meters in height) or within complex surroundings containing shrubs, boulders, etc. which can be confused as trees where the performance of the proposed method is reduced.
<div id='section'>Paperid: <span id='pid'>764, <a href='https://arxiv.org/pdf/2508.09665.pdf' target='_blank'>https://arxiv.org/pdf/2508.09665.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmed Alharbi, Hai Dong, Xun Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09665">Social-Sensor Identity Cloning Detection Using Weakly Supervised Deep Forest and Cryptographic Authentication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent years have witnessed a rising trend in social-sensor cloud identity cloning incidents. However, existing approaches suffer from unsatisfactory performance, a lack of solutions for detecting duplicated accounts, and a lack of large-scale evaluations on real-world datasets. We introduce a novel method for detecting identity cloning in social-sensor cloud service providers. Our proposed technique consists of two primary components: 1) a similar identity detection method and 2) a cryptography-based authentication protocol. Initially, we developed a weakly supervised deep forest model to identify similar identities using non-privacy-sensitive user profile features provided by the service. Subsequently, we designed a cryptography-based authentication protocol to verify whether similar identities were generated by the same provider. Our extensive experiments on a large real-world dataset demonstrate the feasibility and superior performance of our technique compared to current state-of-the-art identity clone detection methods.
<div id='section'>Paperid: <span id='pid'>765, <a href='https://arxiv.org/pdf/2508.08912.pdf' target='_blank'>https://arxiv.org/pdf/2508.08912.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mahmoud Salhab, Shameed Sait, Mohammad Abusheikh, Hasan Abusheikh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08912">Munsit at NADI 2025 Shared Task 2: Pushing the Boundaries of Multidialectal Arabic ASR with Weakly Supervised Pretraining and Continual Supervised Fine-tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic speech recognition (ASR) plays a vital role in enabling natural human-machine interaction across applications such as virtual assistants, industrial automation, customer support, and real-time transcription. However, developing accurate ASR systems for low-resource languages like Arabic remains a significant challenge due to limited labeled data and the linguistic complexity introduced by diverse dialects. In this work, we present a scalable training pipeline that combines weakly supervised learning with supervised fine-tuning to develop a robust Arabic ASR model. In the first stage, we pretrain the model on 15,000 hours of weakly labeled speech covering both Modern Standard Arabic (MSA) and various Dialectal Arabic (DA) variants. In the subsequent stage, we perform continual supervised fine-tuning using a mixture of filtered weakly labeled data and a small, high-quality annotated dataset. Our approach achieves state-of-the-art results, ranking first in the multi-dialectal Arabic ASR challenge. These findings highlight the effectiveness of weak supervision paired with fine-tuning in overcoming data scarcity and delivering high-quality ASR for low-resource, dialect-rich languages.
<div id='section'>Paperid: <span id='pid'>766, <a href='https://arxiv.org/pdf/2508.06819.pdf' target='_blank'>https://arxiv.org/pdf/2508.06819.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ayaan Nooruddin Siddiqui, Mahnoor Zaidi, Ayesha Nazneen Shahbaz, Priyadarshini Chatterjee, Krishnan Menon Iyer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06819">VesselRW: Weakly Supervised Subcutaneous Vessel Segmentation via Learned Random Walk Propagation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The task of parsing subcutaneous vessels in clinical images is often hindered by the high cost and limited availability of ground truth data, as well as the challenge of low contrast and noisy vessel appearances across different patients and imaging modalities. In this work, we propose a novel weakly supervised training framework specifically designed for subcutaneous vessel segmentation. This method utilizes low-cost, sparse annotations such as centerline traces, dot markers, or short scribbles to guide the learning process. These sparse annotations are expanded into dense probabilistic supervision through a differentiable random walk label propagation model, which integrates vesselness cues and tubular continuity priors driven by image data. The label propagation process results in per-pixel hitting probabilities and uncertainty estimates, which are incorporated into an uncertainty-weighted loss function to prevent overfitting in ambiguous areas. Notably, the label propagation model is trained jointly with a CNN-based segmentation network, allowing the system to learn vessel boundaries and continuity constraints without the need for explicit edge supervision. Additionally, we introduce a topology-aware regularizer that encourages centerline connectivity and penalizes irrelevant branches, further enhancing clinical applicability. Our experiments on clinical subcutaneous imaging datasets demonstrate that our approach consistently outperforms both naive sparse-label training and traditional dense pseudo-labeling methods, yielding more accurate vascular maps and better-calibrated uncertainty, which is crucial for clinical decision-making. This method significantly reduces the annotation workload while maintaining clinically relevant vessel topology.
<div id='section'>Paperid: <span id='pid'>767, <a href='https://arxiv.org/pdf/2507.22002.pdf' target='_blank'>https://arxiv.org/pdf/2507.22002.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yida Tao, Yen-Chia Hsu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22002">Bridging Synthetic and Real-World Domains: A Human-in-the-Loop Weakly-Supervised Framework for Industrial Toxic Emission Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industrial smoke segmentation is critical for air-quality monitoring and environmental protection but is often hampered by the high cost and scarcity of pixel-level annotations in real-world settings. We introduce CEDANet, a human-in-the-loop, class-aware domain adaptation framework that uniquely integrates weak, citizen-provided video-level labels with adversarial feature alignment. Specifically, we refine pseudo-labels generated by a source-trained segmentation model using citizen votes, and employ class-specific domain discriminators to transfer rich source-domain representations to the industrial domain. Comprehensive experiments on SMOKE5K and custom IJmond datasets demonstrate that CEDANet achieves an F1-score of 0.414 and a smoke-class IoU of 0.261 with citizen feedback, vastly outperforming the baseline model, which scored 0.083 and 0.043 respectively. This represents a five-fold increase in F1-score and a six-fold increase in smoke-class IoU. Notably, CEDANet with citizen-constrained pseudo-labels achieves performance comparable to the same architecture trained on limited 100 fully annotated images with F1-score of 0.418 and IoU of 0.264, demonstrating its ability to reach small-sampled fully supervised-level accuracy without target-domain annotations. Our research validates the scalability and cost-efficiency of combining citizen science with weakly supervised domain adaptation, offering a practical solution for complex, data-scarce environmental monitoring applications.
<div id='section'>Paperid: <span id='pid'>768, <a href='https://arxiv.org/pdf/2507.03292.pdf' target='_blank'>https://arxiv.org/pdf/2507.03292.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pattaramanee Arsomngern, Sasikarn Khwanmuang, Matthias NieÃner, Supasorn Suwajanakorn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03292">Zero-shot Inexact CAD Model Alignment from a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One practical approach to infer 3D scene structure from a single image is to retrieve a closely matching 3D model from a database and align it with the object in the image. Existing methods rely on supervised training with images and pose annotations, which limits them to a narrow set of object categories. To address this, we propose a weakly supervised 9-DoF alignment method for inexact 3D models that requires no pose annotations and generalizes to unseen categories. Our approach derives a novel feature space based on foundation features that ensure multi-view consistency and overcome symmetry ambiguities inherent in foundation features using a self-supervised triplet loss. Additionally, we introduce a texture-invariant pose refinement technique that performs dense alignment in normalized object coordinates, estimated through the enhanced feature space. We conduct extensive evaluations on the real-world ScanNet25k dataset, where our method outperforms SOTA weakly supervised baselines by +4.3% mean alignment accuracy and is the only weakly supervised approach to surpass the supervised ROCA by +2.7%. To assess generalization, we introduce SUN2CAD, a real-world test set with 20 novel object categories, where our method achieves SOTA results without prior training on them.
<div id='section'>Paperid: <span id='pid'>769, <a href='https://arxiv.org/pdf/2506.16745.pdf' target='_blank'>https://arxiv.org/pdf/2506.16745.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi-Ying Sun, Wan-Lei Zhao, Yi-Bo Miao, Chong-Wah Ngo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.16745">Class Agnostic Instance-level Descriptor for Visual Instance Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the great success of the deep features in content-based image retrieval, the visual instance search remains challenging due to the lack of effective instance level feature representation. Supervised or weakly supervised object detection methods are not among the options due to their poor performance on the unknown object categories. In this paper, based on the feature set output from self-supervised ViT, the instance level region discovery is modeled as detecting the compact feature subsets in a hierarchical fashion. The hierarchical decomposition results in a hierarchy of feature subsets. The non-leaf nodes and leaf nodes on the hierarchy correspond to the various instance regions in an image of different semantic scales. The hierarchical decomposition well addresses the problem of object embedding and occlusions, which are widely observed in the real scenarios. The features derived from the nodes on the hierarchy make up a comprehensive representation for the latent instances in the image. Our instance-level descriptor remains effective on both the known and unknown object categories. Empirical studies on three instance search benchmarks show that it outperforms state-of-the-art methods considerably.
<div id='section'>Paperid: <span id='pid'>770, <a href='https://arxiv.org/pdf/2504.14860.pdf' target='_blank'>https://arxiv.org/pdf/2504.14860.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyi Liu, Yangcen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14860">Bridge the Gap: From Weak to Full Supervision for Temporal Action Localization with PseudoFormer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised Temporal Action Localization (WTAL) has achieved notable success but still suffers from a lack of temporal annotations, leading to a performance and framework gap compared with fully-supervised methods. While recent approaches employ pseudo labels for training, three key challenges: generating high-quality pseudo labels, making full use of different priors, and optimizing training methods with noisy labels remain unresolved. Due to these perspectives, we propose PseudoFormer, a novel two-branch framework that bridges the gap between weakly and fully-supervised Temporal Action Localization (TAL). We first introduce RickerFusion, which maps all predicted action proposals to a global shared space to generate pseudo labels with better quality. Subsequently, we leverage both snippet-level and proposal-level labels with different priors from the weak branch to train the regression-based model in the full branch. Finally, the uncertainty mask and iterative refinement mechanism are applied for training with noisy pseudo labels. PseudoFormer achieves state-of-the-art WTAL results on the two commonly used benchmarks, THUMOS14 and ActivityNet1.3. Besides, extensive ablation studies demonstrate the contribution of each component of our method.
<div id='section'>Paperid: <span id='pid'>771, <a href='https://arxiv.org/pdf/2504.12254.pdf' target='_blank'>https://arxiv.org/pdf/2504.12254.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mahmoud Salhab, Marwan Elghitany, Shameed Sait, Syed Sibghat Ullah, Mohammad Abusheikh, Hasan Abusheikh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12254">Advancing Arabic Speech Recognition Through Large-Scale Weakly Supervised Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic speech recognition (ASR) is crucial for human-machine interaction in diverse applications like conversational agents, industrial robotics, call center automation, and automated subtitling. However, developing high-performance ASR models remains challenging, particularly for low-resource languages like Arabic, due to the scarcity of large, labeled speech datasets, which are costly and labor-intensive to produce. In this work, we employ weakly supervised learning to train an Arabic ASR model using the Conformer architecture. Our model is trained from scratch on 15,000 hours of weakly annotated speech data covering both Modern Standard Arabic (MSA) and Dialectal Arabic (DA), eliminating the need for costly manual transcriptions. Despite the absence of human-verified labels, our approach achieves state-of-the-art (SOTA) results in Arabic ASR, surpassing both open and closed-source models on standard benchmarks. By demonstrating the effectiveness of weak supervision as a scalable, cost-efficient alternative to traditional supervised approaches, paving the way for improved ASR systems in low resource settings.
<div id='section'>Paperid: <span id='pid'>772, <a href='https://arxiv.org/pdf/2504.10613.pdf' target='_blank'>https://arxiv.org/pdf/2504.10613.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xing David Wang, Ulf Leser
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10613">Enhancing Document Retrieval for Curating N-ary Relations in Knowledge Bases</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Curation of biomedical knowledge bases (KBs) relies on extracting accurate multi-entity relational facts from the literature - a process that remains largely manual and expert-driven. An essential step in this workflow is retrieving documents that can support or complete partially observed n-ary relations. We present a neural retrieval model designed to assist KB curation by identifying documents that help fill in missing relation arguments and provide relevant contextual evidence.
  To reduce dependence on scarce gold-standard training data, we exploit existing KB records to construct weakly supervised training sets. Our approach introduces two key technical contributions: (i) a layered contrastive loss that enables learning from noisy and incomplete relational structures, and (ii) a balanced sampling strategy that generates high-quality negatives from diverse KB records. On two biomedical retrieval benchmarks, our approach achieves state-of-the-art performance, outperforming strong baselines in NDCG@10 by 5.7 and 3.7 percentage points, respectively.
<div id='section'>Paperid: <span id='pid'>773, <a href='https://arxiv.org/pdf/2503.22668.pdf' target='_blank'>https://arxiv.org/pdf/2503.22668.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sindhu B Hegde, K R Prajwal, Taein Kwon, Andrew Zisserman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22668">Understanding Co-speech Gestures in-the-wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Co-speech gestures play a vital role in non-verbal communication. In this paper, we introduce a new framework for co-speech gesture understanding in the wild. Specifically, we propose three new tasks and benchmarks to evaluate a model's capability to comprehend gesture-speech-text associations: (i) gesture based retrieval, (ii) gesture word spotting, and (iii) active speaker detection using gestures. We present a new approach that learns a tri-modal video-gesture-speech-text representation to solve these tasks. By leveraging a combination of global phrase contrastive loss and local gesture-word coupling loss, we demonstrate that a strong gesture representation can be learned in a weakly supervised manner from videos in the wild. Our learned representations outperform previous methods, including large vision-language models (VLMs). Further analysis reveals that speech and text modalities capture distinct gesture related signals, underscoring the advantages of learning a shared tri-modal embedding space. The dataset, model, and code are available at: https://www.robots.ox.ac.uk/~vgg/research/jegal.
<div id='section'>Paperid: <span id='pid'>774, <a href='https://arxiv.org/pdf/2502.17836.pdf' target='_blank'>https://arxiv.org/pdf/2502.17836.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Nawaz, Basma Nasir, Tehseen Zia, Zawar Hussain, Catarina Moreira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17836">TagGAN: A Generative Model for Data Tagging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Precise identification and localization of disease-specific features at the pixel-level are particularly important for early diagnosis, disease progression monitoring, and effective treatment in medical image analysis. However, conventional diagnostic AI systems lack decision transparency and cannot operate well in environments where there is a lack of pixel-level annotations. In this study, we propose a novel Generative Adversarial Networks (GANs)-based framework, TagGAN, which is tailored for weakly-supervised fine-grained disease map generation from purely image-level labeled data. TagGAN generates a pixel-level disease map during domain translation from an abnormal image to a normal representation. Later, this map is subtracted from the input abnormal image to convert it into its normal counterpart while preserving all the critical anatomical details. Our method is first to generate fine-grained disease maps to visualize disease lesions in a weekly supervised setting without requiring pixel-level annotations. This development enhances the interpretability of diagnostic AI by providing precise visualizations of disease-specific regions. It also introduces automated binary mask generation to assist radiologists. Empirical evaluations carried out on the benchmark datasets, CheXpert, TBX11K, and COVID-19, demonstrate the capability of TagGAN to outperform current top models in accurately identifying disease-specific pixels. This outcome highlights the capability of the proposed model to tag medical images, significantly reducing the workload for radiologists by eliminating the need for binary masks during training.
<div id='section'>Paperid: <span id='pid'>775, <a href='https://arxiv.org/pdf/2502.17824.pdf' target='_blank'>https://arxiv.org/pdf/2502.17824.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Basma Nasir, Tehseen Zia, Muhammad Nawaz, Catarina Moreira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17824">Weakly Supervised Pixel-Level Annotation with Visual Interpretability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical image annotation is essential for diagnosing diseases, yet manual annotation is time-consuming, costly, and prone to variability among experts. To address these challenges, we propose an automated explainable annotation system that integrates ensemble learning, visual explainability, and uncertainty quantification. Our approach combines three pre-trained deep learning models - ResNet50, EfficientNet, and DenseNet - enhanced with XGrad-CAM for visual explanations and Monte Carlo Dropout for uncertainty quantification. This ensemble mimics the consensus of multiple radiologists by intersecting saliency maps from models that agree on the diagnosis while uncertain predictions are flagged for human review. We evaluated our system using the TBX11K medical imaging dataset and a Fire segmentation dataset, demonstrating its robustness across different domains. Experimental results show that our method outperforms baseline models, achieving 93.04% accuracy on TBX11K and 96.4% accuracy on the Fire dataset. Moreover, our model produces precise pixel-level annotations despite being trained with only image-level labels, achieving Intersection over Union IoU scores of 36.07% and 64.7%, respectively. By enhancing the accuracy and interpretability of image annotations, our approach offers a reliable and transparent solution for medical diagnostics and other image analysis tasks.
<div id='section'>Paperid: <span id='pid'>776, <a href='https://arxiv.org/pdf/2501.04582.pdf' target='_blank'>https://arxiv.org/pdf/2501.04582.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Miaoyang He, Shuyong Gao, Tsui Qin Mok, Weifeng Ge, Wengqiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04582">Boosting Salient Object Detection with Knowledge Distillated from Large Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Salient Object Detection (SOD) aims to identify and segment prominent regions within a scene. Traditional models rely on manually annotated pseudo labels with precise pixel-level accuracy, which is time-consuming. We developed a low-cost, high-precision annotation method by leveraging large foundation models to address the challenges. Specifically, we use a weakly supervised approach to guide large models in generating pseudo-labels through textual prompts. Since large models do not effectively focus on the salient regions of images, we manually annotate a subset of text to fine-tune the model. Based on this approach, which enables precise and rapid generation of pseudo-labels, we introduce a new dataset, BDS-TR. Compared to the previous DUTS-TR dataset, BDS-TR is more prominent in scale and encompasses a wider variety of categories and scenes. This expansion will enhance our model's applicability across a broader range of scenarios and provide a more comprehensive foundational dataset for future SOD research. Additionally, we present an edge decoder based on dynamic upsampling, which focuses on object edges while gradually recovering image feature resolution. Comprehensive experiments on five benchmark datasets demonstrate that our method significantly outperforms state-of-the-art approaches and also surpasses several existing fully-supervised SOD methods. The code and results will be made available.
<div id='section'>Paperid: <span id='pid'>777, <a href='https://arxiv.org/pdf/2412.14561.pdf' target='_blank'>https://arxiv.org/pdf/2412.14561.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jintao Huang, Yiu-ming Cheung, Chi-man Vong, Wenbin Qian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14561">GBRIP: Granular Ball Representation for Imbalanced Partial Label Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partial label learning (PLL) is a complicated weakly supervised multi-classification task compounded by class imbalance. Currently, existing methods only rely on inter-class pseudo-labeling from inter-class features, often overlooking the significant impact of the intra-class imbalanced features combined with the inter-class. To address these limitations, we introduce Granular Ball Representation for Imbalanced PLL (GBRIP), a novel framework for imbalanced PLL. GBRIP utilizes coarse-grained granular ball representation and multi-center loss to construct a granular ball-based nfeature space through unsupervised learning, effectively capturing the feature distribution within each class. GBRIP mitigates the impact of confusing features by systematically refining label disambiguation and estimating imbalance distributions. The novel multi-center loss function enhances learning by emphasizing the relationships between samples and their respective centers within the granular balls. Extensive experiments on standard benchmarks demonstrate that GBRIP outperforms existing state-of-the-art methods, offering a robust solution to the challenges of imbalanced PLL.
<div id='section'>Paperid: <span id='pid'>778, <a href='https://arxiv.org/pdf/2412.12331.pdf' target='_blank'>https://arxiv.org/pdf/2412.12331.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>PhÃºc H. Le Khac, Graham Healy, Alan F. Smeaton
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12331">Efficient Object-centric Representation Learning with Pre-trained Geometric Prior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses key challenges in object-centric representation learning of video. While existing approaches struggle with complex scenes, we propose a novel weakly-supervised framework that emphasises geometric understanding and leverages pre-trained vision models to enhance object discovery. Our method introduces an efficient slot decoder specifically designed for object-centric learning, enabling effective representation of multi-object scenes without requiring explicit depth information. Results on synthetic video benchmarks with increasing complexity in terms of objects and their movement, object occlusion and camera motion demonstrate that our approach achieves comparable performance to supervised methods while maintaining computational efficiency. This advances the field towards more practical applications in complex real-world scenarios.
<div id='section'>Paperid: <span id='pid'>779, <a href='https://arxiv.org/pdf/2412.11467.pdf' target='_blank'>https://arxiv.org/pdf/2412.11467.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuyang Xie, Yan Yang, Yankai Yu, Jie Wang, Yongquan Jiang, Xiao Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11467">Exploring Temporal Event Cues for Dense Video Captioning in Cyclic Co-learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dense video captioning aims to detect and describe all events in untrimmed videos. This paper presents a dense video captioning network called Multi-Concept Cyclic Learning (MCCL), which aims to: (1) detect multiple concepts at the frame level, using these concepts to enhance video features and provide temporal event cues; and (2) design cyclic co-learning between the generator and the localizer within the captioning network to promote semantic perception and event localization. Specifically, we perform weakly supervised concept detection for each frame, and the detected concept embeddings are integrated into the video features to provide event cues. Additionally, video-level concept contrastive learning is introduced to obtain more discriminative concept embeddings. In the captioning network, we establish a cyclic co-learning strategy where the generator guides the localizer for event localization through semantic matching, while the localizer enhances the generator's event semantic perception through location matching, making semantic perception and event localization mutually beneficial. MCCL achieves state-of-the-art performance on the ActivityNet Captions and YouCook2 datasets. Extensive experiments demonstrate its effectiveness and interpretability.
<div id='section'>Paperid: <span id='pid'>780, <a href='https://arxiv.org/pdf/2411.13528.pdf' target='_blank'>https://arxiv.org/pdf/2411.13528.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>James Willoughby, Irina Voiculescu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13528">Entropy Bootstrapping for Weakly Supervised Nuclei Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Microscopy structure segmentation, such as detecting cells or nuclei, generally requires a human to draw a ground truth contour around each instance. Weakly supervised approaches (e.g. consisting of only single point labels) have the potential to reduce this workload significantly. Our approach uses individual point labels for an entropy estimation to approximate an underlying distribution of cell pixels. We infer full cell masks from this distribution, and use Mask-RCNN to produce an instance segmentation output. We compare this point--annotated approach with training on the full ground truth masks. We show that our method achieves a comparatively good level of performance, despite a 95% reduction in pixel labels.
<div id='section'>Paperid: <span id='pid'>781, <a href='https://arxiv.org/pdf/2410.19332.pdf' target='_blank'>https://arxiv.org/pdf/2410.19332.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianning Chi, Zelan Li, Huixuan Wu, Wenjun Zhang, Ying Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.19332">Beyond Point Annotation: A Weakly Supervised Network Guided by Multi-Level Labels Generated from Four-Point Annotation for Thyroid Nodule Segmentation in Ultrasound Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised methods typically guided the pixel-wise training by comparing the predictions to single-level labels containing diverse segmentation-related information at once, but struggled to represent delicate feature differences between nodule and background regions and confused incorrect information, resulting in underfitting or overfitting in the segmentation predictions. In this work, we propose a weakly-supervised network that generates multi-level labels from four-point annotation to refine diverse constraints for delicate nodule segmentation. The Distance-Similarity Fusion Prior referring to the points annotations filters out information irrelevant to nodules. The bounding box and pure foreground/background labels, generated from the point annotation, guarantee the rationality of the prediction in the arrangement of target localization and the spatial distribution of target/background regions, respectively. Our proposed network outperforms existing weakly-supervised methods on two public datasets with respect to the accuracy and robustness, improving the applicability of deep-learning based segmentation in the clinical practice of thyroid nodule diagnosis.
<div id='section'>Paperid: <span id='pid'>782, <a href='https://arxiv.org/pdf/2408.13639.pdf' target='_blank'>https://arxiv.org/pdf/2408.13639.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Yuan, Tania Stathaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.13639">Size Aware Cross-shape Scribble Supervision for Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scribble supervision, a common form of weakly supervised learning, involves annotating pixels using hand-drawn curve lines, which helps reduce the cost of manual labelling. This technique has been widely used in medical image segmentation tasks to fasten network training. However, scribble supervision has limitations in terms of annotation consistency across samples and the availability of comprehensive groundtruth information. Additionally, it often grapples with the challenge of accommodating varying scale targets, particularly in the context of medical images. In this paper, we propose three novel methods to overcome these challenges, namely, 1) the cross-shape scribble annotation method; 2) the pseudo mask method based on cross shapes; and 3) the size-aware multi-branch method. The parameter and structure design are investigated in depth. Experimental results show that the proposed methods have achieved significant improvement in mDice scores across multiple polyp datasets. Notably, the combination of these methods outperforms the performance of state-of-the-art scribble supervision methods designed for medical image segmentation.
<div id='section'>Paperid: <span id='pid'>783, <a href='https://arxiv.org/pdf/2406.00891.pdf' target='_blank'>https://arxiv.org/pdf/2406.00891.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin-Yi Tong, Runmin Dong, Xiao Xiang Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00891">Global High Categorical Resolution Land Cover Mapping via Weak Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Land cover information is indispensable for advancing the United Nations' sustainable development goals, and land cover mapping under a more detailed category system would significantly contribute to economic livelihood tracking and environmental degradation measurement. However, the substantial difficulty in acquiring fine-grained training data makes the implementation of this task particularly challenging. Here, we propose to combine fully labeled source domain and weakly labeled target domain for weakly supervised domain adaptation (WSDA). This is beneficial as the utilization of sparse and coarse weak labels can considerably alleviate the labor required for precise and detailed land cover annotation. Specifically, we introduce the Prototype-based pseudo-label Rectification and Expansion (PRE) approach, which leverages the prototypes (i.e., the class-wise feature centroids) as the bridge to connect sparse labels and global feature distributions. According to the feature distances to the prototypes, the confidence of pseudo-labels predicted in the unlabeled regions of the target domain is assessed. This confidence is then utilized to guide the dynamic expansion and rectification of pseudo-labels. Based on PRE, we carry out high categorical resolution land cover mapping for 10 cities in different regions around the world, severally using PlanetScope, Gaofen-1, and Sentinel-2 satellite images. In the study areas, we achieve cross-sensor, cross-category, and cross-continent WSDA, with the overall accuracy exceeding 80%. The promising results indicate that PRE is capable of reducing the dependency of land cover classification on high-quality annotations, thereby improving label efficiency. We expect our work to enable global fine-grained land cover mapping, which in turn promote Earth observation to provide more precise and thorough information for environmental monitoring.
<div id='section'>Paperid: <span id='pid'>784, <a href='https://arxiv.org/pdf/2405.09142.pdf' target='_blank'>https://arxiv.org/pdf/2405.09142.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jenthe Thienpondt, Kris Demuynck
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.09142">Speaker Embeddings With Weakly Supervised Voice Activity Detection For Efficient Speaker Diarization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current speaker diarization systems rely on an external voice activity detection model prior to speaker embedding extraction on the detected speech segments. In this paper, we establish that the attention system of a speaker embedding extractor acts as a weakly supervised internal VAD model and performs equally or better than comparable supervised VAD systems. Subsequently, speaker diarization can be performed efficiently by extracting the VAD logits and corresponding speaker embedding simultaneously, alleviating the need and computational overhead of an external VAD model. We provide an extensive analysis of the behavior of the frame-level attention system in current speaker verification models and propose a novel speaker diarization pipeline using ECAPA2 speaker embeddings for both VAD and embedding extraction. The proposed strategy gains state-of-the-art performance on the AMI, VoxConverse and DIHARD III diarization benchmarks.
<div id='section'>Paperid: <span id='pid'>785, <a href='https://arxiv.org/pdf/2404.04983.pdf' target='_blank'>https://arxiv.org/pdf/2404.04983.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>AurÃ©lie BeaufrÃ¨re, Nora Ouzir, Paul Emile Zafar, Astrid Laurent-Bellue, Miguel Albuquerque, Gwladys Lubuela, Jules GrÃ©gory, Catherine Guettier, KÃ©vin Mondet, Jean-Christophe Pesquet, ValÃ©rie Paradis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.04983">Primary liver cancer classification from routine tumour biopsy using weakly supervised deep learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The diagnosis of primary liver cancers (PLCs) can be challenging, especially on biopsies and for combined hepatocellular-cholangiocarcinoma (cHCC-CCA). We automatically classified PLCs on routine-stained biopsies using a weakly supervised learning method. Weak tumour/non-tumour annotations served as labels for training a Resnet18 neural network, and the network's last convolutional layer was used to extract new tumour tile features. Without knowledge of the precise labels of the malignancies, we then applied an unsupervised clustering algorithm. Our model identified specific features of hepatocellular carcinoma (HCC) and intrahepatic cholangiocarcinoma (iCCA). Despite no specific features of cHCC-CCA being recognized, the identification of HCC and iCCA tiles within a slide could facilitate the diagnosis of primary liver cancers, particularly cHCC-CCA.
  Method and results: 166 PLC biopsies were divided into training, internal and external validation sets: 90, 29 and 47 samples. Two liver pathologists reviewed each whole-slide hematein eosin saffron (HES)-stained image (WSI). After annotating the tumour/non-tumour areas, 256x256 pixel tiles were extracted from the WSIs and used to train a ResNet18. The network was used to extract new tile features. An unsupervised clustering algorithm was then applied to the new tile features. In a two-cluster model, Clusters 0 and 1 contained mainly HCC and iCCA histological features. The diagnostic agreement between the pathological diagnosis and the model predictions in the internal and external validation sets was 100% (11/11) and 96% (25/26) for HCC and 78% (7/9) and 87% (13/15) for iCCA, respectively. For cHCC-CCA, we observed a highly variable proportion of tiles from each cluster (Cluster 0: 5-97%; Cluster 1: 2-94%).
<div id='section'>Paperid: <span id='pid'>786, <a href='https://arxiv.org/pdf/2404.01446.pdf' target='_blank'>https://arxiv.org/pdf/2404.01446.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Martim Afonso, Praphulla M. S. Bhawsar, Monjoy Saha, Jonas S. Almeida, Arlindo L. Oliveira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01446">Finding Regions of Interest in Whole Slide Images Using Multiple Instance Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Whole Slide Images (WSI), obtained by high-resolution digital scanning of microscope slides at multiple scales, are the cornerstone of modern Digital Pathology. However, they represent a particular challenge to AI-based/AI-mediated analysis because pathology labeling is typically done at slide-level, instead of tile-level. It is not just that medical diagnostics is recorded at the specimen level, the detection of oncogene mutation is also experimentally obtained, and recorded by initiatives like The Cancer Genome Atlas (TCGA), at the slide level. This configures a dual challenge: a) accurately predicting the overall cancer phenotype and b) finding out what cellular morphologies are associated with it at the tile level. To address these challenges, a weakly supervised Multiple Instance Learning (MIL) approach was explored for two prevalent cancer types, Invasive Breast Carcinoma (TCGA-BRCA) and Lung Squamous Cell Carcinoma (TCGA-LUSC). This approach was explored for tumor detection at low magnification levels and TP53 mutations at various levels. Our results show that a novel additive implementation of MIL matched the performance of reference implementation (AUC 0.96), and was only slightly outperformed by Attention MIL (AUC 0.97). More interestingly from the perspective of the molecular pathologist, these different AI architectures identify distinct sensitivities to morphological features (through the detection of Regions of Interest, RoI) at different amplification levels. Tellingly, TP53 mutation was most sensitive to features at the higher applications where cellular morphology is resolved.
<div id='section'>Paperid: <span id='pid'>787, <a href='https://arxiv.org/pdf/2404.00626.pdf' target='_blank'>https://arxiv.org/pdf/2404.00626.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minyoung Oh, Duhyun Kim, Jae-Young Sim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00626">Domain Generalizable Person Search Using Unreal Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collecting and labeling real datasets to train the person search networks not only requires a lot of time and effort, but also accompanies privacy issues. The weakly-supervised and unsupervised domain adaptation methods have been proposed to alleviate the labeling burden for target datasets, however, their generalization capability is limited. We introduce a novel person search method based on the domain generalization framework, that uses an automatically labeled unreal dataset only for training but is applicable to arbitrary unseen real datasets. To alleviate the domain gaps when transferring the knowledge from the unreal source dataset to the real target datasets, we estimate the fidelity of person instances which is then used to train the end-to-end network adaptively. Moreover, we devise a domain-invariant feature learning scheme to encourage the network to suppress the domain-related features. Experimental results demonstrate that the proposed method provides the competitive performance to existing person search methods even though it is applicable to arbitrary unseen datasets without any prior knowledge and re-training burdens.
<div id='section'>Paperid: <span id='pid'>788, <a href='https://arxiv.org/pdf/2403.18600.pdf' target='_blank'>https://arxiv.org/pdf/2403.18600.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Zare, Yulei Niu, Hammad Ayyubi, Shih-fu Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.18600">RAP: Retrieval-Augmented Planner for Adaptive Procedure Planning in Instructional Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Procedure Planning in instructional videos entails generating a sequence of action steps based on visual observations of the initial and target states. Despite the rapid progress in this task, there remain several critical challenges to be solved: (1) Adaptive procedures: Prior works hold an unrealistic assumption that the number of action steps is known and fixed, leading to non-generalizable models in real-world scenarios where the sequence length varies. (2) Temporal relation: Understanding the step temporal relation knowledge is essential in producing reasonable and executable plans. (3) Annotation cost: Annotating instructional videos with step-level labels (i.e., timestamp) or sequence-level labels (i.e., action category) is demanding and labor-intensive, limiting its generalizability to large-scale datasets. In this work, we propose a new and practical setting, called adaptive procedure planning in instructional videos, where the procedure length is not fixed or pre-determined. To address these challenges, we introduce Retrieval-Augmented Planner (RAP) model. Specifically, for adaptive procedures, RAP adaptively determines the conclusion of actions using an auto-regressive model architecture. For temporal relation, RAP establishes an external memory module to explicitly retrieve the most relevant state-action pairs from the training videos and revises the generated procedures. To tackle high annotation cost, RAP utilizes a weakly-supervised learning manner to expand the training dataset to other task-relevant, unannotated videos by generating pseudo labels for action steps. Experiments on CrossTask and COIN benchmarks show the superiority of RAP over traditional fixed-length models, establishing it as a strong baseline solution for adaptive procedure planning.
<div id='section'>Paperid: <span id='pid'>789, <a href='https://arxiv.org/pdf/2403.15238.pdf' target='_blank'>https://arxiv.org/pdf/2403.15238.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abhinav Sharma, Bojing Liu, Mattias Rantalainen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15238">WEEP: A method for spatial interpretation of weakly supervised CNN models in computational pathology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning enables the modelling of high-resolution histopathology whole-slide images (WSI). Weakly supervised learning of tile-level data is typically applied for tasks where labels only exist on the patient or WSI level (e.g. patient outcomes or histological grading). In this context, there is a need for improved spatial interpretability of predictions from such models. We propose a novel method, Wsi rEgion sElection aPproach (WEEP), for model interpretation. It provides a principled yet straightforward way to establish the spatial area of WSI required for assigning a particular prediction label. We demonstrate WEEP on a binary classification task in the area of breast cancer computational pathology. WEEP is easy to implement, is directly connected to the model-based decision process, and offers information relevant to both research and diagnostic applications.
<div id='section'>Paperid: <span id='pid'>790, <a href='https://arxiv.org/pdf/2402.08333.pdf' target='_blank'>https://arxiv.org/pdf/2402.08333.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Antoine Habis, Roy Rosman Nathanson, Vannary Meas-Yedid, Elsa D. Angelini, Jean-Christophe Olivo-Marin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.08333">Scribble-based fast weak-supervision and interactive corrections for segmenting whole slide images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a dynamic interactive and weakly supervised segmentation method with minimal user interactions to address two major challenges in the segmentation of whole slide histopathology images. First, the lack of hand-annotated datasets to train algorithms. Second, the lack of interactive paradigms to enable a dialogue between the pathologist and the machine, which can be a major obstacle for use in clinical routine.
  We therefore propose a fast and user oriented method to bridge this gap by giving the pathologist control over the final result while limiting the number of interactions needed to achieve a good result (over 90\% on all our metrics with only 4 correction scribbles).
<div id='section'>Paperid: <span id='pid'>791, <a href='https://arxiv.org/pdf/2402.03492.pdf' target='_blank'>https://arxiv.org/pdf/2402.03492.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qixiang Ma, Antoine Åucas, Huazhong Shu, Adrien Kaladji, Pascal Haigron
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.03492">Beyond Strong labels: Weakly-supervised Learning Based on Gaussian Pseudo Labels for The Segmentation of Ellipse-like Vascular Structures in Non-contrast CTs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep-learning-based automated segmentation of vascular structures in preoperative CT scans contributes to computer-assisted diagnosis and intervention procedure in vascular diseases. While CT angiography (CTA) is the common standard, non-contrast CT imaging is significant as a contrast-risk-free alternative, avoiding complications associated with contrast agents. However, the challenges of labor-intensive labeling and high labeling variability due to the ambiguity of vascular boundaries hinder conventional strong-label-based, fully-supervised learning in non-contrast CTs. This paper introduces a weakly-supervised framework using ellipses' topology in slices, including 1) an efficient annotation process based on predefined standards, 2) ellipse-fitting processing, 3) the generation of 2D Gaussian heatmaps serving as pseudo labels, 4) a training process through a combination of voxel reconstruction loss and distribution loss with the pseudo labels. We assess the effectiveness of the proposed method on one local and two public datasets comprising non-contrast CT scans, particularly focusing on the abdominal aorta. On the local dataset, our weakly-supervised learning approach based on pseudo labels outperforms strong-label-based fully-supervised learning (1.54\% of Dice score on average), reducing labeling time by around 82.0\%. The efficiency in generating pseudo labels allows the inclusion of label-agnostic external data in the training set, leading to an additional improvement in performance (2.74\% of Dice score on average) with a reduction of 66.3\% labeling time, where the labeling time remains considerably less than that of strong labels. On the public dataset, the pseudo labels achieve an overall improvement of 1.95\% in Dice score for 2D models while a reduction of 11.65 voxel spacing in Hausdorff distance for 3D model.
<div id='section'>Paperid: <span id='pid'>792, <a href='https://arxiv.org/pdf/2401.11313.pdf' target='_blank'>https://arxiv.org/pdf/2401.11313.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Isaac J. Sledge, Dominic M. Byrne, Jonathan L. King, Steven H. Ostertag, Denton L. Woods, James L. Prater, Jermaine L. Kennedy, Timothy M. Marston, Jose C. Principe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11313">Weakly-Supervised Semantic Segmentation of Circular-Scan, Synthetic-Aperture-Sonar Imagery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a weakly-supervised framework for the semantic segmentation of circular-scan synthetic-aperture-sonar (CSAS) imagery. The first part of our framework is trained in a supervised manner, on image-level labels, to uncover a set of semi-sparse, spatially-discriminative regions in each image. The classification uncertainty of each region is then evaluated. Those areas with the lowest uncertainties are then chosen to be weakly labeled segmentation seeds, at the pixel level, for the second part of the framework. Each of the seed extents are progressively resized according to an unsupervised, information-theoretic loss with structured-prediction regularizers. This reshaping process uses multi-scale, adaptively-weighted features to delineate class-specific transitions in local image content. Content-addressable memories are inserted at various parts of our framework so that it can leverage features from previously seen images to improve segmentation performance for related images.
  We evaluate our weakly-supervised framework using real-world CSAS imagery that contains over ten seafloor classes and ten target classes. We show that our framework performs comparably to nine fully-supervised deep networks. Our framework also outperforms eleven of the best weakly-supervised deep networks. We achieve state-of-the-art performance when pre-training on natural imagery. The average absolute performance gap to the next-best weakly-supervised network is well over ten percent for both natural imagery and sonar imagery. This gap is found to be statistically significant.
<div id='section'>Paperid: <span id='pid'>793, <a href='https://arxiv.org/pdf/2401.05416.pdf' target='_blank'>https://arxiv.org/pdf/2401.05416.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifeng Wang, Yi Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.05416">Wavelet Dynamic Selection Network for Inertial Sensor Signal Enhancement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As attitude and motion sensing components, inertial sensors are widely used in various portable devices. But the severe errors of inertial sensors restrain their function, especially the trajectory recovery and semantic recognition. As a mainstream signal processing method, wavelet is hailed as the mathematical microscope of signal due to the plentiful and diverse wavelet basis functions. However, complicated noise types and application scenarios of inertial sensors make selecting wavelet basis perplexing. To this end, we propose a wavelet dynamic selection network (WDSNet), which intelligently selects the appropriate wavelet basis for variable inertial signals. In addition, existing deep learning architectures excel at extracting features from input data but neglect to learn the characteristics of target categories, which is essential to enhance the category awareness capability, thereby improving the selection of wavelet basis. Therefore, we propose a category representation mechanism (CRM), which enables the network to extract and represent category features without increasing trainable parameters. Furthermore, CRM transforms the common fully connected network into category representations, which provide closer supervision to the feature extractor than the far and trivial one-hot classification labels. We call this process of imposing interpretability on a network and using it to supervise the feature extractor the feature supervision mechanism, and its effectiveness is demonstrated experimentally and theoretically in this paper. The enhanced inertial signal can perform impracticable tasks with regard to the original signal, such as trajectory reconstruction. Both quantitative and visual results show that WDSNet outperforms the existing methods. Remarkably, WDSNet, as a weakly-supervised method, achieves the state-of-the-art performance of all the compared fully-supervised methods.
<div id='section'>Paperid: <span id='pid'>794, <a href='https://arxiv.org/pdf/2512.19550.pdf' target='_blank'>https://arxiv.org/pdf/2512.19550.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Naresh Manwani, M Elamparithy, Tanish Taneja
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.19550">DFORD: Directional Feedback based Online Ordinal Regression Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce directional feedback in the ordinal regression setting, in which the learner receives feedback on whether the predicted label is on the left or the right side of the actual label. This is a weak supervision setting for ordinal regression compared to the full information setting, where the learner can access the labels. We propose an online algorithm for ordinal regression using directional feedback. The proposed algorithm uses an exploration-exploitation scheme to learn from directional feedback efficiently. Furthermore, we introduce its kernel-based variant to learn non-linear ordinal regression models in an online setting. We use a truncation trick to make the kernel implementation more memory efficient. The proposed algorithm maintains the ordering of the thresholds in the expected sense. Moreover, it achieves the expected regret of $\mathcal{O}(\log T)$. We compare our approach with a full information and a weakly supervised algorithm for ordinal regression on synthetic and real-world datasets. The proposed approach, which learns using directional feedback, performs comparably (sometimes better) to its full information counterpart.
<div id='section'>Paperid: <span id='pid'>795, <a href='https://arxiv.org/pdf/2512.11057.pdf' target='_blank'>https://arxiv.org/pdf/2512.11057.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marshal Ashif Shawkat, Moidul Hasan, Taufiq Hasan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.11057">Weakly Supervised Tuberculosis Localization in Chest X-rays through Knowledge Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tuberculosis (TB) remains one of the leading causes of mortality worldwide, particularly in resource-limited countries. Chest X-ray (CXR) imaging serves as an accessible and cost-effective diagnostic tool but requires expert interpretation, which is often unavailable. Although machine learning models have shown high performance in TB classification, they often depend on spurious correlations and fail to generalize. Besides, building large datasets featuring high-quality annotations for medical images demands substantial resources and input from domain specialists, and typically involves several annotators reaching agreement, which results in enormous financial and logistical expenses. This study repurposes knowledge distillation technique to train CNN models reducing spurious correlations and localize TB-related abnormalities without requiring bounding-box annotations. By leveraging a teacher-student framework with ResNet50 architecture, the proposed method trained on TBX11k dataset achieve impressive 0.2428 mIOU score. Experimental results further reveal that the student model consistently outperforms the teacher, underscoring improved robustness and potential for broader clinical deployment in diverse settings.
<div id='section'>Paperid: <span id='pid'>796, <a href='https://arxiv.org/pdf/2512.05364.pdf' target='_blank'>https://arxiv.org/pdf/2512.05364.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ananth Hariharan, David Mortensen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.05364">Transformer-Enabled Diachronic Analysis of Vedic Sanskrit: Neural Methods for Quantifying Types of Language Change</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study demonstrates how hybrid neural-symbolic methods can yield significant new insights into the evolution of a morphologically rich, low-resource language. We challenge the naive assumption that linguistic change is simplification by quantitatively analyzing over 2,000 years of Sanskrit, demonstrating how weakly-supervised hybrid methods can yield new insights into the evolution of morphologically rich, low-resource languages. Our approach addresses data scarcity through weak supervision, using 100+ high-precision regex patterns to generate pseudo-labels for fine-tuning a multilingual BERT. We then fuse symbolic and neural outputs via a novel confidence-weighted ensemble, creating a system that is both scalable and interpretable. Applying this framework to a 1.47-million-word diachronic corpus, our ensemble achieves a 52.4% overall feature detection rate. Our findings reveal that Sanskrit's overall morphological complexity does not decrease but is instead dynamically redistributed: while earlier verbal features show cyclical patterns of decline, complexity shifts to other domains, evidenced by a dramatic expansion in compounding and the emergence of new philosophical terminology. Critically, our system produces well-calibrated uncertainty estimates, with confidence strongly correlating with accuracy (Pearson r = 0.92) and low overall calibration error (ECE = 0.043), bolstering the reliability of these findings for computational philology.
<div id='section'>Paperid: <span id='pid'>797, <a href='https://arxiv.org/pdf/2512.02344.pdf' target='_blank'>https://arxiv.org/pdf/2512.02344.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyuan Sun, Yongping Zhang, Hongcheng Zeng, Yamin Wang, Wei Yang, Wanting Yang, Jie Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.02344">A multi-weight self-matching visual explanation for cnns on sar images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, convolutional neural networks (CNNs) have achieved significant success in various synthetic aperture radar (SAR) tasks. However, the complexity and opacity of their internal mechanisms hinder the fulfillment of high-reliability requirements, thereby limiting their application in SAR. Improving the interpretability of CNNs is thus of great importance for their development and deployment in SAR. In this paper, a visual explanation method termed multi-weight self-matching class activation mapping (MS-CAM) is proposed. MS-CAM matches SAR images with the feature maps and corresponding gradients extracted by the CNN, and combines both channel-wise and element-wise weights to visualize the decision basis learned by the model in SAR images. Extensive experiments conducted on a self-constructed SAR target classification dataset demonstrate that MS-CAM more accurately highlights the network's regions of interest and captures detailed target feature information, thereby enhancing network interpretability. Furthermore, the feasibility of applying MS-CAM to weakly-supervised obiect localization is validated. Key factors affecting localization accuracy, such as pixel thresholds, are analyzed in depth to inform future work.
<div id='section'>Paperid: <span id='pid'>798, <a href='https://arxiv.org/pdf/2511.18012.pdf' target='_blank'>https://arxiv.org/pdf/2511.18012.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaying Zhou, Qingchao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.18012">State and Scene Enhanced Prototypes for Weakly Supervised Open-Vocabulary Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-Vocabulary Object Detection (OVOD) aims to generalize object recognition to novel categories, while Weakly Supervised OVOD (WS-OVOD) extends this by combining box-level annotations with image-level labels. Despite recent progress, two critical challenges persist in this setting. First, existing semantic prototypes, even when enriched by LLMs, are static and limited, failing to capture the rich intra-class visual variations induced by different object states (e.g., a cat's pose). Second, the standard pseudo-box generation introduces a semantic mismatch between visual region proposals (which contain context) and object-centric text embeddings. To tackle these issues, we introduce two complementary prototype enhancement strategies. To capture intra-class variations in appearance and state, we propose the State-Enhanced Semantic Prototypes (SESP), which generates state-aware textual descriptions (e.g., "a sleeping cat") to capture diverse object appearances, yielding more discriminative prototypes. Building on this, we further introduce Scene-Augmented Pseudo Prototypes (SAPP) to address the semantic mismatch. SAPP incorporates contextual semantics (e.g., "cat lying on sofa") and utilizes a soft alignment mechanism to promote contextually consistent visual-textual representations. By integrating SESP and SAPP, our method effectively enhances both the richness of semantic prototypes and the visual-textual alignment, achieving notable improvements.
<div id='section'>Paperid: <span id='pid'>799, <a href='https://arxiv.org/pdf/2511.03361.pdf' target='_blank'>https://arxiv.org/pdf/2511.03361.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gabriel Pirlogeanu, Alexandru-Lucian Georgescu, Horia Cucu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.03361">Open Source State-Of-the-Art Solution for Romanian Speech Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we present a new state-of-the-art Romanian Automatic Speech Recognition (ASR) system based on NVIDIA's FastConformer architecture--explored here for the first time in the context of Romanian. We train our model on a large corpus of, mostly, weakly supervised transcriptions, totaling over 2,600 hours of speech. Leveraging a hybrid decoder with both Connectionist Temporal Classification (CTC) and Token-Duration Transducer (TDT) branches, we evaluate a range of decoding strategies including greedy, ALSD, and CTC beam search with a 6-gram token-level language model. Our system achieves state-of-the-art performance across all Romanian evaluation benchmarks, including read, spontaneous, and domain-specific speech, with up to 27% relative WER reduction compared to previous best-performing systems. In addition to improved transcription accuracy, our approach demonstrates practical decoding efficiency, making it suitable for both research and deployment in low-latency ASR applications.
<div id='section'>Paperid: <span id='pid'>800, <a href='https://arxiv.org/pdf/2511.02576.pdf' target='_blank'>https://arxiv.org/pdf/2511.02576.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alix de Langlais, Benjamin Billot, Théo Aguilar Vidal, Marc-Olivier Gauci, Hervé Delingette
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.02576">Resource-efficient Automatic Refinement of Segmentations via Weak Supervision from Light Feedback</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Delineating anatomical regions is a key task in medical image analysis. Manual segmentation achieves high accuracy but is labor-intensive and prone to variability, thus prompting the development of automated approaches. Recently, a breadth of foundation models has enabled automated segmentations across diverse anatomies and imaging modalities, but these may not always meet the clinical accuracy standards. While segmentation refinement strategies can improve performance, current methods depend on heavy user interactions or require fully supervised segmentations for training. Here, we present SCORE (Segmentation COrrection from Regional Evaluations), a weakly supervised framework that learns to refine mask predictions only using light feedback during training. Specifically, instead of relying on dense training image annotations, SCORE introduces a novel loss that leverages region-wise quality scores and over/under-segmentation error labels. We demonstrate SCORE on humerus CT scans, where it considerably improves initial predictions from TotalSegmentator, and achieves performance on par with existing refinement methods, while greatly reducing their supervision requirements and annotation time. Our code is available at: https://gitlab.inria.fr/adelangl/SCORE.
<div id='section'>Paperid: <span id='pid'>801, <a href='https://arxiv.org/pdf/2507.21959.pdf' target='_blank'>https://arxiv.org/pdf/2507.21959.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheyuan Zhang, Yen-chia Hsu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21959">Mitigating Spurious Correlations in Weakly Supervised Semantic Segmentation via Cross-architecture Consistency Regularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scarcity of pixel-level labels is a significant challenge in practical scenarios. In specific domains like industrial smoke, acquiring such detailed annotations is particularly difficult and often requires expert knowledge. To alleviate this, weakly supervised semantic segmentation (WSSS) has emerged as a promising approach. However, due to the supervision gap and inherent bias in models trained with only image level labels, existing WSSS methods suffer from limitations such as incomplete foreground coverage, inaccurate object boundaries, and spurious correlations, especially in our domain, where emissions are always spatially coupled with chimneys.
  Previous solutions typically rely on additional priors or external knowledge to mitigate these issues, but they often lack scalability and fail to address the model's inherent bias toward co-occurring context. To address this, we propose a novel WSSS framework that directly targets the co-occurrence problem without relying on external supervision. Unlike prior methods that adopt a single network, we employ a teacher-student framework that combines CNNs and ViTs. We introduce a knowledge transfer loss that enforces cross-architecture consistency by aligning internal representations. Additionally, we incorporate post-processing techniques to address partial coverage and further improve pseudo mask quality.
<div id='section'>Paperid: <span id='pid'>802, <a href='https://arxiv.org/pdf/2507.21587.pdf' target='_blank'>https://arxiv.org/pdf/2507.21587.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheyuan Zhang, Wang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21587">Emerging Trends in Pseudo-Label Refinement for Weakly Supervised Semantic Segmentation with Image-Level Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unlike fully supervised semantic segmentation, weakly supervised semantic segmentation (WSSS) relies on weaker forms of supervision to perform dense prediction tasks. Among the various types of weak supervision, WSSS with image level annotations is considered both the most challenging and the most practical, attracting significant research attention. Therefore, in this review, we focus on WSSS with image level annotations. Additionally, this review concentrates on mainstream research directions, deliberately omitting less influential branches.
  Given the rapid development of new methods and the limitations of existing surveys in capturing recent trends, there is a pressing need for an updated and comprehensive review. Our goal is to fill this gap by synthesizing the latest advancements and state-of-the-art techniques in WSSS with image level labels.
  Basically, we provide a comprehensive review of recent advancements in WSSS with image level labels, categorizing existing methods based on the types and levels of additional supervision involved. We also examine the challenges of applying advanced methods to domain specific datasets in WSSS,a topic that remains underexplored. Finally, we discuss the current challenges, evaluate the limitations of existing approaches, and outline several promising directions for future research. This review is intended for researchers who are already familiar with the fundamental concepts of WSSS and are seeking to deepen their understanding of current advances and methodological innovations.
<div id='section'>Paperid: <span id='pid'>803, <a href='https://arxiv.org/pdf/2507.16849.pdf' target='_blank'>https://arxiv.org/pdf/2507.16849.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi-Shan Chu, Hsuan-Cheng Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16849">Post-Disaster Affected Area Segmentation with a Vision Transformer (ViT)-based EVAP Model using Sentinel-2 and Formosat-5 Imagery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a vision transformer (ViT)-based deep learning framework to refine disaster-affected area segmentation from remote sensing imagery, aiming to support and enhance the Emergent Value Added Product (EVAP) developed by the Taiwan Space Agency (TASA). The process starts with a small set of manually annotated regions. We then apply principal component analysis (PCA)-based feature space analysis and construct a confidence index (CI) to expand these labels, producing a weakly supervised training set. These expanded labels are then used to train ViT-based encoder-decoder models with multi-band inputs from Sentinel-2 and Formosat-5 imagery. Our architecture supports multiple decoder variants and multi-stage loss strategies to improve performance under limited supervision. During the evaluation, model predictions are compared with higher-resolution EVAP output to assess spatial coherence and segmentation consistency. Case studies on the 2022 Poyang Lake drought and the 2023 Rhodes wildfire demonstrate that our framework improves the smoothness and reliability of segmentation results, offering a scalable approach for disaster mapping when accurate ground truth is unavailable.
<div id='section'>Paperid: <span id='pid'>804, <a href='https://arxiv.org/pdf/2507.06013.pdf' target='_blank'>https://arxiv.org/pdf/2507.06013.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kushal Gajjar, Harshit Sikchi, Arpit Singh Gautam, Marc Hammons, Saurabh Jha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06013">CogniSQL-R1-Zero: Lightweight Reinforced Reasoning for Efficient SQL Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Translating natural language into SQL (Text-to-SQL) remains a core challenge at the intersection of language understanding and structured data access. Although large language models (LLMs) have improved fluency, generating correct and executable SQL, especially for complex queries, continues to be challenging. We introduce CogniSQL-R1-Zero, a reinforcement learning (RL) framework and model that produces accurate SQL using a lightweight reward signal based on execution correctness and format-tag compliance. By avoiding intermediate supervision, hybrid pipelines and complex reward shaping, our method encourages stable learning and stronger alignment with the ultimate task objective-producing executable programs. CogniSQL-R1-Zero achieves state-of-the-art execution accuracy on Text2SQL benchmark; BIRD bench, outperforming prior supervised and instruction-tuned baselines including SFT CodeS-7B, DeepSeek-Coder 236B, and Mistral 123B-despite being trained on a significantly smaller 7B backbone. This result underscores the scalability and efficiency of our RL-based approach when trained on just four NVIDIA A100 GPUs (40 GB VRAM each). To support further research in efficient and interpretable Text-to-SQL modeling, we release two curated datasets: (i) a collection of 5,024 reasoning traces with varying context lengths, and (ii) a positive-sampled corpus of 36,356 corpus of weakly supervised queries, each annotated with six semantically diverse reasoning paths. Together, these contributions advance scalable, execution-aligned Text-to-SQL generation.
<div id='section'>Paperid: <span id='pid'>805, <a href='https://arxiv.org/pdf/2507.02308.pdf' target='_blank'>https://arxiv.org/pdf/2507.02308.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pei Guo, Ryan Farrell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02308">LMPNet for Weakly-supervised Keypoint Discovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we explore the task of semantic object keypoint discovery weakly-supervised by only category labels. This is achieved by transforming discriminatively-trained intermediate layer filters into keypoint detectors. We begin by identifying three preferred characteristics of keypoint detectors: (i) spatially sparse activations, (ii) consistency and (iii) diversity. Instead of relying on hand-crafted loss terms, a novel computationally-efficient leaky max pooling (LMP) layer is proposed to explicitly encourage final conv-layer filters to learn "non-repeatable local patterns" that are well aligned with object keypoints. Informed by visualizations, a simple yet effective selection strategy is proposed to ensure consistent filter activations and attention mask-out is then applied to force the network to distribute its attention to the whole object instead of just the most discriminative region. For the final keypoint prediction, a learnable clustering layer is proposed to group keypoint proposals into keypoint predictions. The final model, named LMPNet, is highly interpretable in that it directly manipulates network filters to detect predefined concepts. Our experiments show that LMPNet can (i) automatically discover semantic keypoints that are robust to object pose and (ii) achieves strong prediction accuracy comparable to a supervised pose estimation model.
<div id='section'>Paperid: <span id='pid'>806, <a href='https://arxiv.org/pdf/2506.13095.pdf' target='_blank'>https://arxiv.org/pdf/2506.13095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Wang, Shiwei Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13095">Learning Event Completeness for Weakly Supervised Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised video anomaly detection (WS-VAD) is tasked with pinpointing temporal intervals containing anomalous events within untrimmed videos, utilizing only video-level annotations. However, a significant challenge arises due to the absence of dense frame-level annotations, often leading to incomplete localization in existing WS-VAD methods. To address this issue, we present a novel LEC-VAD, Learning Event Completeness for Weakly Supervised Video Anomaly Detection, which features a dual structure designed to encode both category-aware and category-agnostic semantics between vision and language. Within LEC-VAD, we devise semantic regularities that leverage an anomaly-aware Gaussian mixture to learn precise event boundaries, thereby yielding more complete event instances. Besides, we develop a novel memory bank-based prototype learning mechanism to enrich concise text descriptions associated with anomaly-event categories. This innovation bolsters the text's expressiveness, which is crucial for advancing WS-VAD. Our LEC-VAD demonstrates remarkable advancements over the current state-of-the-art methods on two benchmark datasets XD-Violence and UCF-Crime.
<div id='section'>Paperid: <span id='pid'>807, <a href='https://arxiv.org/pdf/2506.02166.pdf' target='_blank'>https://arxiv.org/pdf/2506.02166.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arnav Rustagi, Satvik Bajpai, Nimrat Kaur, Siddharth Siddharth
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02166">Dhvani: A Weakly-supervised Phonemic Error Detection and Personalized Feedback System for Hindi</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computer-Assisted Pronunciation Training (CAPT) has been extensively studied for English. However, there remains a critical gap in its application to Indian languages with a base of 1.5 billion speakers. Pronunciation tools tailored to Indian languages are strikingly lacking despite the fact that millions learn them every year. With over 600 million speakers and being the fourth most-spoken language worldwide, improving Hindi pronunciation is a vital first step toward addressing this gap. This paper proposes 1) Dhvani -- a novel CAPT system for Hindi, 2) synthetic speech generation for Hindi mispronunciations, and 3) a novel methodology for providing personalized feedback to learners. While the system often interacts with learners using Devanagari graphemes, its core analysis targets phonemic distinctions, leveraging Hindi's highly phonetic orthography to analyze mispronounced speech and provide targeted feedback.
<div id='section'>Paperid: <span id='pid'>808, <a href='https://arxiv.org/pdf/2505.23586.pdf' target='_blank'>https://arxiv.org/pdf/2505.23586.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyong Wang, Charith Abhayaratne
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23586">Weakly-supervised Localization of Manipulated Image Regions Using Multi-resolution Learned Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The explosive growth of digital images and the widespread availability of image editing tools have made image manipulation detection an increasingly critical challenge. Current deep learning-based manipulation detection methods excel in achieving high image-level classification accuracy, they often fall short in terms of interpretability and localization of manipulated regions. Additionally, the absence of pixel-wise annotations in real-world scenarios limits the existing fully-supervised manipulation localization techniques. To address these challenges, we propose a novel weakly-supervised approach that integrates activation maps generated by image-level manipulation detection networks with segmentation maps from pre-trained models. Specifically, we build on our previous image-level work named WCBnet to produce multi-view feature maps which are subsequently fused for coarse localization. These coarse maps are then refined using detailed segmented regional information provided by pre-trained segmentation models (such as DeepLab, SegmentAnything and PSPnet), with Bayesian inference employed to enhance the manipulation localization. Experimental results demonstrate the effectiveness of our approach, highlighting the feasibility to localize image manipulations without relying on pixel-level labels.
<div id='section'>Paperid: <span id='pid'>809, <a href='https://arxiv.org/pdf/2505.15438.pdf' target='_blank'>https://arxiv.org/pdf/2505.15438.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianyuan Guo, Peike Li, Trevor Cohn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.15438">Bridging Sign and Spoken Languages: Pseudo Gloss Generation for Sign Language Translation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sign Language Translation (SLT) aims to map sign language videos to spoken language text. A common approach relies on gloss annotations as an intermediate representation, decomposing SLT into two sub-tasks: video-to-gloss recognition and gloss-to-text translation. While effective, this paradigm depends on expert-annotated gloss labels, which are costly and rarely available in existing datasets, limiting its scalability. To address this challenge, we propose a gloss-free pseudo gloss generation framework that eliminates the need for human-annotated glosses while preserving the structured intermediate representation. Specifically, we prompt a Large Language Model (LLM) with a few example text-gloss pairs using in-context learning to produce draft sign glosses from spoken language text. To enhance the correspondence between LLM-generated pseudo glosses and the sign sequences in video, we correct the ordering in the pseudo glosses for better alignment via a weakly supervised learning process. This reordering facilitates the incorporation of auxiliary alignment objectives, and allows for the use of efficient supervision via a Connectionist Temporal Classification (CTC) loss. We train our SLT mode, which consists of a vision encoder and a translator, through a three-stage pipeline, which progressively narrows the modality gap between sign language and spoken language. Despite its simplicity, our approach outperforms previous state-of-the-art gloss-free frameworks on two SLT benchmarks and achieves competitive results compared to gloss-based methods.
<div id='section'>Paperid: <span id='pid'>810, <a href='https://arxiv.org/pdf/2505.10781.pdf' target='_blank'>https://arxiv.org/pdf/2505.10781.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Minkwan Kim, Soeun Lee, Byeongkeun Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10781">Completely Weakly Supervised Class-Incremental Learning for Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work addresses the task of completely weakly supervised class-incremental learning for semantic segmentation to learn segmentation for both base and additional novel classes using only image-level labels. While class-incremental semantic segmentation (CISS) is crucial for handling diverse and newly emerging objects in the real world, traditional CISS methods require expensive pixel-level annotations for training. To overcome this limitation, partially weakly-supervised approaches have recently been proposed. However, to the best of our knowledge, this is the first work to introduce a completely weakly-supervised method for CISS. To achieve this, we propose to generate robust pseudo-labels by combining pseudo-labels from a localizer and a sequence of foundation models based on their uncertainty. Moreover, to mitigate catastrophic forgetting, we introduce an exemplar-guided data augmentation method that generates diverse images containing both previous and novel classes with guidance. Finally, we conduct experiments in three common experimental settings: 15-5 VOC, 10-10 VOC, and COCO-to-VOC, and in two scenarios: disjoint and overlap. The experimental results demonstrate that our completely weakly supervised method outperforms even partially weakly supervised methods in the 15-5 VOC and 10-10 VOC settings while achieving competitive accuracy in the COCO-to-VOC setting.
<div id='section'>Paperid: <span id='pid'>811, <a href='https://arxiv.org/pdf/2505.07440.pdf' target='_blank'>https://arxiv.org/pdf/2505.07440.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rituraj Singh, Sachin Pawar, Girish Palshikar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07440">Matching Tasks with Industry Groups for Augmenting Commonsense Knowledge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Commonsense knowledge bases (KB) are a source of specialized knowledge that is widely used to improve machine learning applications. However, even for a large KB such as ConceptNet, capturing explicit knowledge from each industry domain is challenging. For example, only a few samples of general {\em tasks} performed by various industries are available in ConceptNet. Here, a task is a well-defined knowledge-based volitional action to achieve a particular goal. In this paper, we aim to fill this gap and present a weakly-supervised framework to augment commonsense KB with tasks carried out by various industry groups (IG). We attempt to {\em match} each task with one or more suitable IGs by training a neural model to learn task-IG affinity and apply clustering to select the top-k tasks per IG. We extract a total of 2339 triples of the form $\langle IG, is~capable~of, task \rangle$ from two publicly available news datasets for 24 IGs with the precision of 0.86. This validates the reliability of the extracted task-IG pairs that can be directly added to existing KBs.
<div id='section'>Paperid: <span id='pid'>812, <a href='https://arxiv.org/pdf/2505.04150.pdf' target='_blank'>https://arxiv.org/pdf/2505.04150.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Yamaoka, Weng Ian Chan, Shigeto Seno, Soichiro Fukada, Hideo Matsuda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04150">Learning from Similarity Proportion Loss for Classifying Skeletal Muscle Recovery Stages</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Evaluating the regeneration process of damaged muscle tissue is a fundamental analysis in muscle research to measure experimental effect sizes and uncover mechanisms behind muscle weakness due to aging and disease. The conventional approach to assessing muscle tissue regeneration involves whole-slide imaging and expert visual inspection of the recovery stages based on the morphological information of cells and fibers. There is a need to replace these tasks with automated methods incorporating machine learning techniques to ensure a quantitative and objective analysis. Given the limited availability of fully labeled data, a possible approach is Learning from Label Proportions (LLP), a weakly supervised learning method using class label proportions. However, current LLP methods have two limitations: (1) they cannot adapt the feature extractor for muscle tissues, and (2) they treat the classes representing recovery stages and cell morphological changes as nominal, resulting in the loss of ordinal information. To address these issues, we propose Ordinal Scale Learning from Similarity Proportion (OSLSP), which uses a similarity proportion loss derived from two bag combinations. OSLSP can update the feature extractor by using class proportion attention to the ordinal scale of the class. Our model with OSLSP outperforms large-scale pre-trained and fine-tuning models in classification tasks of skeletal muscle recovery stages.
<div id='section'>Paperid: <span id='pid'>813, <a href='https://arxiv.org/pdf/2504.14302.pdf' target='_blank'>https://arxiv.org/pdf/2504.14302.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yogev Kriger, Shai Fine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14302">Learning to Score</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Common machine learning settings range from supervised tasks, where accurately labeled data is accessible, through semi-supervised and weakly-supervised tasks, where target labels are scant or noisy, to unsupervised tasks where labels are unobtainable. In this paper we study a scenario where the target labels are not available but additional related information is at hand. This information, referred to as Side Information, is either correlated with the unknown labels or imposes constraints on the feature space. We formulate the problem as an ensemble of three semantic components: representation learning, side information and metric learning. The proposed scoring model is advantageous for multiple use-cases. For example, in the healthcare domain it can be used to create a severity score for diseases where the symptoms are known but the criteria for the disease progression are not well defined. We demonstrate the utility of the suggested scoring system on well-known benchmark data-sets and bio-medical patient records.
<div id='section'>Paperid: <span id='pid'>814, <a href='https://arxiv.org/pdf/2504.13927.pdf' target='_blank'>https://arxiv.org/pdf/2504.13927.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>F. Herrera, U. A. Rozikov, M. V. Velasco
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13927">Ising Models with Hidden Markov Structure: Applications to Probabilistic Inference in Machine Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we investigate tree-indexed Markov chains (Gibbs measures) defined by a Hamiltonian that couples two Ising layers: hidden spins \(s(x) \in \{\pm 1\}\) and observed spins \(Ï(x) \in \{\pm 1\}\) on a Cayley tree. The Hamiltonian incorporates Ising interactions within each layer and site-wise emission couplings between layers, extending hidden Markov models to a bilayer Markov random field.
  Specifically, we explore translation-invariant Gibbs measures (TIGM) of this Hamiltonian on Cayley trees.
  Under certain explicit conditions on the model's parameters, we demonstrate that there can be up to three distinct TIGMs. Each of these measures represents an equilibrium state of the spin system. These measures provide a structured approach to inference on hierarchical data in machine learning. They have practical applications in tasks such as denoising, weakly supervised learning, and anomaly detection. The Cayley tree structure is particularly advantageous for exact inference due to its tractability.
<div id='section'>Paperid: <span id='pid'>815, <a href='https://arxiv.org/pdf/2504.13297.pdf' target='_blank'>https://arxiv.org/pdf/2504.13297.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andreas Lau Hansen, Lukas Wanzeck, Dim P. Papadopoulos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13297">Weak Cube R-CNN: Weakly Supervised 3D Detection using only 2D Bounding Boxes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monocular 3D object detection is an essential task in computer vision, and it has several applications in robotics and virtual reality. However, 3D object detectors are typically trained in a fully supervised way, relying extensively on 3D labeled data, which is labor-intensive and costly to annotate. This work focuses on weakly-supervised 3D detection to reduce data needs using a monocular method that leverages a singlecamera system over expensive LiDAR sensors or multi-camera setups. We propose a general model Weak Cube R-CNN, which can predict objects in 3D at inference time, requiring only 2D box annotations for training by exploiting the relationship between 2D projections of 3D cubes. Our proposed method utilizes pre-trained frozen foundation 2D models to estimate depth and orientation information on a training set. We use these estimated values as pseudo-ground truths during training. We design loss functions that avoid 3D labels by incorporating information from the external models into the loss. In this way, we aim to implicitly transfer knowledge from these large foundation 2D models without having access to 3D bounding box annotations. Experimental results on the SUN RGB-D dataset show increased performance in accuracy compared to an annotation time equalized Cube R-CNN baseline. While not precise for centimetre-level measurements, this method provides a strong foundation for further research.
<div id='section'>Paperid: <span id='pid'>816, <a href='https://arxiv.org/pdf/2504.04435.pdf' target='_blank'>https://arxiv.org/pdf/2504.04435.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tatiana Merkulova, Bharani Jayakumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04435">Evaluation framework for Image Segmentation Algorithms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a comprehensive evaluation framework for image segmentation algorithms, encompassing naive methods, machine learning approaches, and deep learning techniques. We begin by introducing the fundamental concepts and importance of image segmentation, and the role of interactive segmentation in enhancing accuracy. A detailed background theory section explores various segmentation methods, including thresholding, edge detection, region growing, feature extraction, random forests, support vector machines, convolutional neural networks, U-Net, and Mask R-CNN. The implementation and experimental setup are thoroughly described, highlighting three primary approaches: algorithm assisting user, user assisting algorithm, and hybrid methods. Evaluation metrics such as Intersection over Union (IoU), computation time, and user interaction time are employed to measure performance. A comparative analysis presents detailed results, emphasizing the strengths, limitations, and trade-offs of each method. The paper concludes with insights into the practical applicability of these approaches across various scenarios and outlines future work, focusing on expanding datasets, developing more representative approaches, integrating real-time feedback, and exploring weakly supervised and self-supervised learning paradigms to enhance segmentation accuracy and efficiency. Keywords: Image Segmentation, Interactive Segmentation, Machine Learning, Deep Learning, Computer Vision
<div id='section'>Paperid: <span id='pid'>817, <a href='https://arxiv.org/pdf/2503.23181.pdf' target='_blank'>https://arxiv.org/pdf/2503.23181.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sunoh Kim, Daeho Um
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.23181">Enhancing Weakly Supervised Video Grounding via Diverse Inference Strategies for Boundary and Prediction Selection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised video grounding aims to localize temporal boundaries relevant to a given query without explicit ground-truth temporal boundaries. While existing methods primarily use Gaussian-based proposals, they overlook the importance of (1) boundary prediction and (2) top-1 prediction selection during inference. In their boundary prediction, boundaries are simply set at half a standard deviation away from a Gaussian mean on both sides, which may not accurately capture the optimal boundaries. In the top-1 prediction process, these existing methods rely heavily on intersections with other proposals, without considering the varying quality of each proposal. To address these issues, we explore various inference strategies by introducing (1) novel boundary prediction methods to capture diverse boundaries from multiple Gaussians and (2) new selection methods that take proposal quality into account. Extensive experiments on the ActivityNet Captions and Charades-STA datasets validate the effectiveness of our inference strategies, demonstrating performance improvements without requiring additional training.
<div id='section'>Paperid: <span id='pid'>818, <a href='https://arxiv.org/pdf/2503.22856.pdf' target='_blank'>https://arxiv.org/pdf/2503.22856.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shanshan Bai, Anna Kruspe, Xiaoxiang Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22856">Generating Synthetic Oracle Datasets to Analyze Noise Impact: A Study on Building Function Classification Using Tweets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tweets provides valuable semantic context for earth observation tasks and serves as a complementary modality to remote sensing imagery. In building function classification (BFC), tweets are often collected using geographic heuristics and labeled via external databases, an inherently weakly supervised process that introduces both label noise and sentence level feature noise (e.g., irrelevant or uninformative tweets). While label noise has been widely studied, the impact of sentence level feature noise remains underexplored, largely due to the lack of clean benchmark datasets for controlled analysis. In this work, we propose a method for generating a synthetic oracle dataset using LLM, designed to contain only tweets that are both correctly labeled and semantically relevant to their associated buildings. This oracle dataset enables systematic investigation of noise impacts that are otherwise difficult to isolate in real-world data. To assess its utility, we compare model performance using Naive Bayes and mBERT classifiers under three configurations: real vs. synthetic training data, and cross-domain generalization. Results show that noise in real tweets significantly degrades the contextual learning capacity of mBERT, reducing its performance to that of a simple keyword-based model. In contrast, the clean synthetic dataset allows mBERT to learn effectively, outperforming Naive Bayes Bayes by a large margin. These findings highlight that addressing feature noise is more critical than model complexity in this task. Our synthetic dataset offers a novel experimental environment for future noise injection studies and is publicly available on GitHub.
<div id='section'>Paperid: <span id='pid'>819, <a href='https://arxiv.org/pdf/2503.13925.pdf' target='_blank'>https://arxiv.org/pdf/2503.13925.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Da Kuang, Guanwen Qiu, Junhyong Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13925">Reconstructing Cell Lineage Trees from Phenotypic Features with Metric Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>How a single fertilized cell gives rise to a complex array of specialized cell types in development is a central question in biology. The cells grow, divide, and acquire differentiated characteristics through poorly understood molecular processes. A key approach to studying developmental processes is to infer the tree graph of cell lineage division and differentiation histories, providing an analytical framework for dissecting individual cells' molecular decisions during replication and differentiation. Although genetically engineered lineage-tracing methods have advanced the field, they are either infeasible or ethically constrained in many organisms. In contrast, modern single-cell technologies can measure high-content molecular profiles (e.g., transcriptomes) in a wide range of biological systems.
  Here, we introduce CellTreeQM, a novel deep learning method based on transformer architectures that learns an embedding space with geometric properties optimized for tree-graph inference. By formulating lineage reconstruction as a tree-metric learning problem, we have systematically explored supervised, weakly supervised, and unsupervised training settings and present a Lineage Reconstruction Benchmark to facilitate comprehensive evaluation of our learning method. We benchmarked the method on (1) synthetic data modeled via Brownian motion with independent noise and spurious signals and (2) lineage-resolved single-cell RNA sequencing datasets. Experimental results show that CellTreeQM recovers lineage structures with minimal supervision and limited data, offering a scalable framework for uncovering cell lineage relationships in challenging animal models. To our knowledge, this is the first method to cast cell lineage inference explicitly as a metric learning task, paving the way for future computational models aimed at uncovering the molecular dynamics of cell lineage.
<div id='section'>Paperid: <span id='pid'>820, <a href='https://arxiv.org/pdf/2503.04342.pdf' target='_blank'>https://arxiv.org/pdf/2503.04342.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ivan Oleksiyuk, Svyatoslav Voloshynovskiy, Tobias Golling
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04342">TRANSIT your events into a new mass: Fast background interpolation for weakly-supervised anomaly searches</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a new model for conditional and continuous data morphing called TRansport Adversarial Network for Smooth InTerpolation (TRANSIT). We apply it to create a background data template for weakly-supervised searches at the LHC. The method smoothly transforms sideband events to match signal region mass distributions. We demonstrate the performance of TRANSIT using the LHC Olympics R\&D dataset. The model captures non-linear mass correlations of features and produces a template that offers a competitive anomaly sensitivity compared to state-of-the-art transport-based template generators. Moreover, the computational training time required for TRANSIT is an order of magnitude lower than that of competing deep learning methods. This makes it ideal for analyses that iterate over many signal regions and signal models. Unlike generative models, which must learn a full probability density distribution, i.e., the correlations between all the variables, the proposed transport model only has to learn a smooth conditional shift of the distribution. This allows for a simpler, more efficient residual architecture, enabling mass uncorrelated features to pass the network unchanged while the mass correlated features are adjusted accordingly. Furthermore, we show that the latent space of the model provides a set of mass decorrelated features useful for anomaly detection without background sculpting.
<div id='section'>Paperid: <span id='pid'>821, <a href='https://arxiv.org/pdf/2502.00629.pdf' target='_blank'>https://arxiv.org/pdf/2502.00629.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Wu, Hideki Nakayama
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.00629">Advanced Weakly-Supervised Formula Exploration for Neuro-Symbolic Mathematical Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, neuro-symbolic methods have become a popular and powerful approach that augments artificial intelligence systems with the capability to perform abstract, logical, and quantitative deductions with enhanced precision and controllability. Recent studies successfully performed symbolic reasoning by leveraging various machine learning models to explicitly or implicitly predict intermediate labels that provide symbolic instructions. However, these intermediate labels are not always prepared for every task as a part of training data, and pre-trained models, represented by Large Language Models (LLMs), also do not consistently generate valid symbolic instructions with their intrinsic knowledge. On the other hand, existing work developed alternative learning techniques that allow the learning system to autonomously uncover optimal symbolic instructions. Nevertheless, their performance also exhibits limitations when faced with relatively huge search spaces or more challenging reasoning problems. In view of this, in this work, we put forward an advanced practice for neuro-symbolic reasoning systems to explore the intermediate labels with weak supervision from problem inputs and final outputs. Our experiments on the Mathematics dataset illustrated the effectiveness of our proposals from multiple aspects.
<div id='section'>Paperid: <span id='pid'>822, <a href='https://arxiv.org/pdf/2501.03891.pdf' target='_blank'>https://arxiv.org/pdf/2501.03891.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyi Wu, Hong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.03891">Superpixel Boundary Correction for Weakly-Supervised Semantic Segmentation on Histopathology Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancement of deep learning, computational pathology has made significant progress in cancer diagnosis and subtyping. Tissue segmentation is a core challenge, essential for prognosis and treatment decisions. Weakly supervised semantic segmentation (WSSS) reduces the annotation requirement by using image-level labels instead of pixel-level ones. However, Class Activation Map (CAM)-based methods still suffer from low spatial resolution and unclear boundaries. To address these issues, we propose a multi-level superpixel correction algorithm that refines CAM boundaries using superpixel clustering and floodfill. Experimental results show that our method achieves great performance on breast cancer segmentation dataset with mIoU of 71.08%, significantly improving tumor microenvironment boundary delineation.
<div id='section'>Paperid: <span id='pid'>823, <a href='https://arxiv.org/pdf/2501.01658.pdf' target='_blank'>https://arxiv.org/pdf/2501.01658.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wang Lituan, Zhang Lei, Wang Yan, Wang Zhenbin, Zhang Zhenwei, Zhang Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01658">EAUWSeg: Eliminating annotation uncertainty in weakly-supervised medical image segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised medical image segmentation is gaining traction as it requires only rough annotations rather than accurate pixel-to-pixel labels, thereby reducing the workload for specialists. Although some progress has been made, there is still a considerable performance gap between the label-efficient methods and fully-supervised one, which can be attributed to the uncertainty nature of these weak labels. To address this issue, we propose a novel weak annotation method coupled with its learning framework EAUWSeg to eliminate the annotation uncertainty. Specifically, we first propose the Bounded Polygon Annotation (BPAnno) by simply labeling two polygons for a lesion. Then, the tailored learning mechanism that explicitly treat bounded polygons as two separated annotations is proposed to learn invariant feature by providing adversarial supervision signal for model training. Subsequently, a confidence-auxiliary consistency learner incorporates with a classification-guided confidence generator is designed to provide reliable supervision signal for pixels in uncertain region by leveraging the feature presentation consistency across pixels within the same category as well as class-specific information encapsulated in bounded polygons annotation. Experimental results demonstrate that EAUWSeg outperforms existing weakly-supervised segmentation methods. Furthermore, compared to fully-supervised counterparts, the proposed method not only delivers superior performance but also costs much less annotation workload. This underscores the superiority and effectiveness of our approach.
<div id='section'>Paperid: <span id='pid'>824, <a href='https://arxiv.org/pdf/2412.19563.pdf' target='_blank'>https://arxiv.org/pdf/2412.19563.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongbiao Gao, Xiangcheng Sun, Guohua Lv, Deng Yu, Sijiu Niu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19563">Reinforced Label Denoising for Weakly-Supervised Audio-Visual Video Parsing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-visual video parsing (AVVP) aims to recognize audio and visual event labels with precise temporal boundaries, which is quite challenging since audio or visual modality might include only one event label with only the overall video labels available. Existing label denoising models often treat the denoising process as a separate preprocessing step, leading to a disconnect between label denoising and AVVP tasks. To bridge this gap, we present a novel joint reinforcement learning-based label denoising approach (RLLD). This approach enables simultaneous training of both label denoising and video parsing models through a joint optimization strategy. We introduce a novel AVVP-validation and soft inter-reward feedback mechanism that directly guides the learning of label denoising policy. Extensive experiments on AVVP tasks demonstrate the superior performance of our proposed method compared to label denoising techniques. Furthermore, by incorporating our label denoising method into other AVVP models, we find that it can further enhance parsing results.
<div id='section'>Paperid: <span id='pid'>825, <a href='https://arxiv.org/pdf/2412.14870.pdf' target='_blank'>https://arxiv.org/pdf/2412.14870.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Isabelle Tingzon, Utku Can Ozturk, Ivan Dotu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14870">Large-scale School Mapping using Weakly Supervised Deep Learning for Universal School Connectivity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Improving global school connectivity is critical for ensuring inclusive and equitable quality education. To reliably estimate the cost of connecting schools, governments and connectivity providers require complete and accurate school location data - a resource that is often scarce in many low- and middle-income countries. To address this challenge, we propose a cost-effective, scalable approach to locating schools in high-resolution satellite images using weakly supervised deep learning techniques. Our best models, which combine vision transformers and convolutional neural networks, achieve AUPRC values above 0.96 across 10 pilot African countries. Leveraging explainable AI techniques, our approach can approximate the precise geographical coordinates of the school locations using only low-cost, classification-level annotations. To demonstrate the scalability of our method, we generate nationwide maps of school location predictions in African countries and present a detailed analysis of our results, using Senegal as our case study. Finally, we demonstrate the immediate usability of our work by introducing an interactive web mapping tool to streamline human-in-the-loop model validation efforts by government partners. This work successfully showcases the real-world utility of deep learning and satellite images for planning regional infrastructure and accelerating universal school connectivity.
<div id='section'>Paperid: <span id='pid'>826, <a href='https://arxiv.org/pdf/2412.11237.pdf' target='_blank'>https://arxiv.org/pdf/2412.11237.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Max Riffi-Aslett, Christina Fell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11237">On the Generalizability of Iterative Patch Selection for Memory-Efficient High-Resolution Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Classifying large images with small or tiny regions of interest (ROI) is challenging due to computational and memory constraints. Weakly supervised memory-efficient patch selectors have achieved results comparable with strongly supervised methods. However, low signal-to-noise ratios and low entropy attention still cause overfitting. We explore these issues using a novel testbed on a memory-efficient cross-attention transformer with Iterative Patch Selection (IPS) as the patch selection module. Our testbed extends the megapixel MNIST benchmark to four smaller O2I (object-to-image) ratios ranging from 0.01% to 0.14% while keeping the canvas size fixed and introducing a noise generation component based on BÃ©zier curves. Experimental results generalize the observations made on CNNs to IPS whereby the O2I threshold below which the classifier fails to generalize is affected by the training dataset size. We further observe that the magnitude of this interaction differs for each task of the Megapixel MNIST. For tasks "Maj" and "Top", the rate is at its highest, followed by tasks "Max" and "Multi" where in the latter, this rate is almost at 0. Moreover, results show that in a low data setting, tuning the patch size to be smaller relative to the ROI improves generalization, resulting in an improvement of + 15% for the megapixel MNIST and + 5% for the Swedish traffic signs dataset compared to the original object-to-patch ratios in IPS. Further outcomes indicate that the similarity between the thickness of the noise component and the digits in the megapixel MNIST gradually causes IPS to fail to generalize, contributing to previous suspicions.
<div id='section'>Paperid: <span id='pid'>827, <a href='https://arxiv.org/pdf/2412.00077.pdf' target='_blank'>https://arxiv.org/pdf/2412.00077.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nima Sedaghat, Tanawan Chatchadanoraset, Colin Orion Chandler, Ashish Mahabal, Maryam Eslami
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.00077">Selfish Evolution: Making Discoveries in Extreme Label Noise with the Help of Overfitting Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motivated by the scarcity of proper labels in an astrophysical application, we have developed a novel technique, called Selfish Evolution, which allows for the detection and correction of corrupted labels in a weakly supervised fashion. Unlike methods based on early stopping, we let the model train on the noisy dataset. Only then do we intervene and allow the model to overfit to individual samples. The ``evolution'' of the model during this process reveals patterns with enough information about the noisiness of the label, as well as its correct version. We train a secondary network on these spatiotemporal ``evolution cubes'' to correct potentially corrupted labels. We incorporate the technique in a closed-loop fashion, allowing for automatic convergence towards a mostly clean dataset, without presumptions about the state of the network in which we intervene. We evaluate on the main task of the Supernova-hunting dataset but also demonstrate efficiency on the more standard MNIST dataset.
<div id='section'>Paperid: <span id='pid'>828, <a href='https://arxiv.org/pdf/2411.19547.pdf' target='_blank'>https://arxiv.org/pdf/2411.19547.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dihong Gong, Pu Lu, Zelong Wang, Meng Zhou, Xiuqiang He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19547">Training Agents with Weakly Supervised Feedback from Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) offer a promising basis for creating agents that can tackle complex tasks through iterative environmental interaction. Existing methods either require these agents to mimic expert-provided trajectories or rely on definitive environmental feedback for reinforcement learning which limits their application to specific scenarios like gaming or code generation. This paper introduces a novel training method for LLM-based agents using weakly supervised signals from a critic LLM, bypassing the need for expert trajectories or definitive feedback. Our agents are trained in iterative manner, where they initially generate trajectories through environmental interaction. Subsequently, a critic LLM selects a subset of good trajectories, which are then used to update the agents, enabling them to generate improved trajectories in the next iteration. Extensive tests on the API-bank dataset show consistent improvement in our agents' capabilities and comparable performance to GPT-4, despite using open-source models with much fewer parameters.
<div id='section'>Paperid: <span id='pid'>829, <a href='https://arxiv.org/pdf/2411.18915.pdf' target='_blank'>https://arxiv.org/pdf/2411.18915.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vishnou Vinayagame, Gregory Senay, Luis MartÃ­
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18915">MATATA: Weakly Supervised End-to-End MAthematical Tool-Augmented Reasoning for Tabular Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Business documents often contain substantial tabular and textual information with numerical values, requiring mathematical reasoning for effective document understanding. While Small Language Models (SLMs) still struggle at this task, tool-augmented multi-step agents perform better, at the cost of relying on closed-source or larger models, external data, or extensive prompt-engineering. This work introduces MATATA, a novel weakly supervised end-to-end approach to train multi-step reasoning language agents for document tabular applications. MATATA presents an annotation-free paradigm for each agent to enhance 3.8B/8B SLMs. During its two-stage training, MATATA uses the final outcome of the multi-step reasoning chain as weak supervision. This approach avoids having to individually supervise each intermediate agent in the reasoning chain. By employing an adaptive planner and shared tools across different datasets, MATATA shows robust performance. Experiments demonstrate that MATATA achieves state-of-the-art on FinQA, and on TAT-QA among reasoning methods based on open-source SLMs. Although being SLM-based, MATATA closely matches GPT-4-based frameworks on TabMWP. This novel weakly supervised approach enables training an end-to-end multi-step reasoning agent without intermediate supervision, supporting future developments of cost-effective powerful agentic systems.
<div id='section'>Paperid: <span id='pid'>830, <a href='https://arxiv.org/pdf/2411.08755.pdf' target='_blank'>https://arxiv.org/pdf/2411.08755.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sareh Soltani Nejad, Anwar Haque
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08755">Weakly-Supervised Anomaly Detection in Surveillance Videos Based on Two-Stream I3D Convolution Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The widespread implementation of urban surveillance systems has necessitated more sophisticated techniques for anomaly detection to ensure enhanced public safety. This paper presents a significant advancement in the field of anomaly detection through the application of Two-Stream Inflated 3D (I3D) Convolutional Networks. These networks substantially outperform traditional 3D Convolutional Networks (C3D) by more effectively extracting spatial and temporal features from surveillance videos, thus improving the precision of anomaly detection. Our research advances the field by implementing a weakly supervised learning framework based on Multiple Instance Learning (MIL), which uniquely conceptualizes surveillance videos as collections of 'bags' that contain instances (video clips). Each instance is innovatively processed through a ranking mechanism that prioritizes clips based on their potential to display anomalies. This novel strategy not only enhances the accuracy and precision of anomaly detection but also significantly diminishes the dependency on extensive manual annotations. Moreover, through meticulous optimization of model settings, including the choice of optimizer, our approach not only establishes new benchmarks in the performance of anomaly detection systems but also offers a scalable and efficient solution for real-world surveillance applications. This paper contributes significantly to the field of computer vision by delivering a more adaptable, efficient, and context-aware anomaly detection system, which is poised to redefine practices in urban surveillance.
<div id='section'>Paperid: <span id='pid'>831, <a href='https://arxiv.org/pdf/2411.03082.pdf' target='_blank'>https://arxiv.org/pdf/2411.03082.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Irum Mehboob, Li Sun, Alireza Astegarpanah, Rustam Stolkin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03082">Self-supervised cross-modality learning for uncertainty-aware object detection and recognition in applications which lack pre-labelled training data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper shows how an uncertainty-aware, deep neural network can be trained to detect, recognise and localise objects in 2D RGB images, in applications lacking annotated train-ng datasets. We propose a self-supervising teacher-student pipeline, in which a relatively simple teacher classifier, trained with only a few labelled 2D thumbnails, automatically processes a larger body of unlabelled RGB-D data to teach a student network based on a modified YOLOv3 architecture. Firstly, 3D object detection with back projection is used to automatically extract and teach 2D detection and localisation information to the student network. Secondly, a weakly supervised 2D thumbnail classifier, with minimal training on a small number of hand-labelled images, is used to teach object category recognition. Thirdly, we use a Gaussian Process GP to encode and teach a robust uncertainty estimation functionality, so that the student can output confidence scores with each categorization. The resulting student significantly outperforms the same YOLO architecture trained directly on the same amount of labelled data. Our GP-based approach yields robust and meaningful uncertainty estimations for complex industrial object classifications. The end-to-end network is also capable of real-time processing, needed for robotics applications. Our method can be applied to many important industrial tasks, where labelled datasets are typically unavailable. In this paper, we demonstrate an example of detection, localisation, and object category recognition of nuclear mixed-waste materials in highly cluttered and unstructured scenes. This is critical for robotic sorting and handling of legacy nuclear waste, which poses complex environmental remediation challenges in many nuclearised nations.
<div id='section'>Paperid: <span id='pid'>832, <a href='https://arxiv.org/pdf/2410.07460.pdf' target='_blank'>https://arxiv.org/pdf/2410.07460.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Wen, Evgenia Roussinova, Olivier Brina, Paolo Machi, Mohamed Bouri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07460">Generalizing Segmentation Foundation Model Under Sim-to-real Domain-shift for Guidewire Segmentation in X-ray Fluoroscopy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Guidewire segmentation during endovascular interventions holds the potential to significantly enhance procedural accuracy, improving visualization and providing critical feedback that can support both physicians and robotic systems in navigating complex vascular pathways. Unlike supervised segmentation networks, which need many expensive expert-annotated labels, sim-to-real domain adaptation approaches utilize synthetic data from simulations, offering a cost-effective solution. The success of models like Segment-Anything (SAM) has driven advancements in image segmentation foundation models with strong zero/few-shot generalization through prompt engineering. However, they struggle with medical images like X-ray fluoroscopy and the domain-shifts of the data. Given the challenges of acquiring annotation and the accessibility of labeled simulation data, we propose a sim-to-real domain adaption framework with a coarse-to-fine strategy to adapt SAM to X-ray fluoroscopy guidewire segmentation without any annotation on the target domain. We first generate the pseudo-labels by utilizing a simple source image style transfer technique that preserves the guidewire structure. Then, we develop a weakly supervised self-training architecture to fine-tune an end-to-end student SAM with the coarse labels by imposing consistency regularization and supervision from the teacher SAM network. We validate the effectiveness of the proposed method on a publicly available Cardiac dataset and an in-house Neurovascular dataset, where our method surpasses both pre-trained SAM and many state-of-the-art domain adaptation techniques by a large margin. Our code will be made public on GitHub soon.
<div id='section'>Paperid: <span id='pid'>833, <a href='https://arxiv.org/pdf/2409.01330.pdf' target='_blank'>https://arxiv.org/pdf/2409.01330.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Iulian Emil Tampu, Per Nyman, Christoforos Spyretos, Ida Blystad, Alia Shamikh, Gabriela Prochazka, Teresita DÃ­az de StÃ¥hl, Johanna Sandgren, Peter Lundberg, Neda Haj-Hosseini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01330">Pediatric brain tumor classification using digital histopathology and deep learning: evaluation of SOTA methods on a multi-center Swedish cohort</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Brain tumors are the most common solid tumors in children and young adults, but the scarcity of large histopathology datasets has limited the application of computational pathology in this group. This study implements two weakly supervised multiple-instance learning (MIL) approaches on patch-features obtained from state-of-the-art histology-specific foundation models to classify pediatric brain tumors in hematoxylin and eosin whole slide images (WSIs) from a multi-center Swedish cohort. WSIs from 540 subjects (age 8.5$\pm$4.9 years) diagnosed with brain tumor were gathered from the six Swedish university hospitals. Instance (patch)-level features were obtained from WSIs using three pre-trained feature extractors: ResNet50, UNI, and CONCH. Instances were aggregated using attention-based MIL (ABMIL) or clustering-constrained attention MIL (CLAM) for patient-level classification. Models were evaluated on three classification tasks based on the hierarchical classification of pediatric brain tumors: tumor category, family, and type. Model generalization was assessed by training on data from two of the centers and testing on data from four other centers. Model interpretability was evaluated through attention mapping. The highest classification performance was achieved using UNI features and ABMIL aggregation, with Matthew's correlation coefficient of 0.76$\pm$0.04, 0.63$\pm$0.04, and 0.60$\pm$0.05 for tumor category, family, and type classification, respectively. When evaluating generalization, models utilizing UNI and CONCH features outperformed those using ResNet50. However, the drop in performance from the in-site to out-of-site testing was similar across feature extractors. These results show the potential of state-of-the-art computational pathology methods in diagnosing pediatric brain tumors at different hierarchical levels with fair generalizability on a multi-center national dataset.
<div id='section'>Paperid: <span id='pid'>834, <a href='https://arxiv.org/pdf/2407.20600.pdf' target='_blank'>https://arxiv.org/pdf/2407.20600.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunfeng Zhao, Huiyu Zhou, Fei Wu, Xifeng Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.20600">Categorical Knowledge Fused Recognition: Fusing Hierarchical Knowledge with Image Classification through Aligning and Deep Metric Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image classification is a fundamental computer vision task and an important baseline for deep metric learning. In decades efforts have been made on enhancing image classification accuracy by using deep learning models while less attention has been paid on the reasoning aspect of the recognition, i.e., predictions could be made because of background or other surrounding objects rather than the target object. Hierarchical knowledge about image categories depicts inter-class similarities or dissimilarities. Effective fusion of such knowledge with deep learning image classification models is promising in improving target object identification and enhancing the reasoning aspect of the recognition. In this paper, we propose a novel deep metric learning based method to effectively fuse prior knowledge about image categories with mainstream backbone image classification models and enhance the reasoning aspect of the recognition in an end-to-end manner. Existing deep metric learning incorporated image classification methods mainly focus on whether sampled images are from the same class. A new triplet loss function term that aligns distances in the model latent space with those in knowledge space is presented and incorporated in the proposed method to facilitate the dual-modality fusion. Extensive experiments on the CIFAR-10, CIFAR-100, Mini-ImageNet, and ImageNet-1K datasets evaluated the proposed method, and results indicate that the proposed method is effective in enhancing the reasoning aspect of image recognition in terms of weakly-supervised object localization performance.
<div id='section'>Paperid: <span id='pid'>835, <a href='https://arxiv.org/pdf/2407.15170.pdf' target='_blank'>https://arxiv.org/pdf/2407.15170.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhu Huang, Gang Pan, Chao Kang, YaoZhi Lv
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.15170">Semi-Supervised Pipe Video Temporal Defect Interval Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In sewer pipe Closed-Circuit Television (CCTV) inspection, accurate temporal defect localization is essential for effective defect classification, detection, segmentation and quantification. Industry standards typically do not require time-interval annotations, even though they are more informative than time-point annotations for defect localization, resulting in additional annotation costs when fully supervised methods are used. Additionally, differences in scene types and camera motion patterns between pipe inspections and Temporal Action Localization (TAL) hinder the effective transfer of point-supervised TAL methods. Therefore, this study introduces a Semi-supervised multi-Prototype-based method incorporating visual Odometry for enhanced attention guidance (PipeSPO). PipeSPO fully leverages unlabeled data through unsupervised pretext tasks and utilizes time-point annotated data with a weakly supervised multi-prototype-based method, relying on visual odometry features to capture camera pose information. Experiments on real-world datasets demonstrate that PipeSPO achieves 41.89% average precision across Intersection over Union (IoU) thresholds of 0.1-0.7, improving by 8.14% over current state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>836, <a href='https://arxiv.org/pdf/2407.13292.pdf' target='_blank'>https://arxiv.org/pdf/2407.13292.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lukuan Dong, Donghong Qin, Fengbo Bai, Fanhua Song, Yan Liu, Chen Xu, Zhijian Ou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.13292">Low-Resourced Speech Recognition for Iu Mien Language via Weakly-Supervised Phoneme-based Multilingual Pre-training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The mainstream automatic speech recognition (ASR) technology usually requires hundreds to thousands of hours of annotated speech data. Three approaches to low-resourced ASR are phoneme or subword based supervised pre-training, and self-supervised pre-training over multilingual data. The Iu Mien language is the main ethnic language of the Yao ethnic group in China and is low-resourced in the sense that the annotated speech is very limited. With less than 10 hours of transcribed Iu Mien language, this paper investigates and compares the three approaches for Iu Mien speech recognition. Our experiments are based on the recently released, three backbone models pretrained over the 10 languages from the CommonVoice dataset (CV-Lang10), which correspond to the three approaches for low-resourced ASR. It is found that phoneme supervision can achieve better results compared to subword supervision and self-supervision, thereby providing higher data-efficiency. Particularly, the Whistle models, i.e., obtained by the weakly-supervised phoneme-based multilingual pre-training, obtain the most competitive results.
<div id='section'>Paperid: <span id='pid'>837, <a href='https://arxiv.org/pdf/2406.05794.pdf' target='_blank'>https://arxiv.org/pdf/2406.05794.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kiseung Kim, Jay-Yoon Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.05794">RE-RAG: Improving Open-Domain QA Performance and Interpretability with Relevance Estimator in Retrieval-Augmented Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Retrieval Augmented Generation (RAG) framework utilizes a combination of parametric knowledge and external knowledge to demonstrate state-of-the-art performance on open-domain question answering tasks. However, the RAG framework suffers from performance degradation when the query is accompanied by irrelevant contexts. In this work, we propose the RE-RAG framework, which introduces a relevance estimator (RE) that not only provides relative relevance between contexts as previous rerankers did, but also provides confidence, which can be used to classify whether given context is useful for answering the given question. We propose a weakly supervised method for training the RE simply utilizing question-answer data without any labels for correct contexts. We show that RE trained with a small generator (sLM) can not only improve the sLM fine-tuned together with RE but also improve previously unreferenced large language models (LLMs). Furthermore, we investigate new decoding strategies that utilize the proposed confidence measured by RE such as choosing to let the user know that it is "unanswerable" to answer the question given the retrieved contexts or choosing to rely on LLM's parametric knowledge rather than unrelated contexts.
<div id='section'>Paperid: <span id='pid'>838, <a href='https://arxiv.org/pdf/2406.02653.pdf' target='_blank'>https://arxiv.org/pdf/2406.02653.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Reza Babaei, Samuel Cheng, Theresa Thai, Shangqing Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.02653">Pancreatic Tumor Segmentation as Anomaly Detection in CT Images Using Denoising Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the advances in medicine, cancer has remained a formidable challenge. Particularly in the case of pancreatic tumors, characterized by their diversity and late diagnosis, early detection poses a significant challenge crucial for effective treatment. The advancement of deep learning techniques, particularly supervised algorithms, has significantly propelled pancreatic tumor detection in the medical field. However, supervised deep learning approaches necessitate extensive labeled medical images for training, yet acquiring such annotations is both limited and costly. Conversely, weakly supervised anomaly detection methods, requiring only image-level annotations, have garnered interest. Existing methodologies predominantly hinge on generative adversarial networks (GANs) or autoencoder models, which can pose complexity in training and, these models may face difficulties in accurately preserving fine image details. This research presents a novel approach to pancreatic tumor detection, employing weak supervision anomaly detection through denoising diffusion algorithms. By incorporating a deterministic iterative process of adding and removing noise along with classifier guidance, the method enables seamless translation of images between diseased and healthy subjects, resulting in detailed anomaly maps without requiring complex training protocols and segmentation masks. This study explores denoising diffusion models as a recent advancement over traditional generative models like GANs, contributing to the field of pancreatic tumor detection. Recognizing the low survival rates of pancreatic cancer, this study emphasizes the need for continued research to leverage diffusion models' efficiency in medical segmentation tasks.
<div id='section'>Paperid: <span id='pid'>839, <a href='https://arxiv.org/pdf/2405.12850.pdf' target='_blank'>https://arxiv.org/pdf/2405.12850.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jjahao Zhang, Yin Gu, Deyu Sun, Yuhua Gao, Ming Gao, Ming Cui, Teng Zhang, He Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.12850">Weakly supervised alignment and registration of MR-CT for cervical cancer radiotherapy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cervical cancer is one of the leading causes of death in women, and brachytherapy is currently the primary treatment method. However, it is important to precisely define the extent of paracervical tissue invasion to improve cancer diagnosis and treatment options. The fusion of the information characteristics of both computed tomography (CT) and magnetic resonance imaging(MRI) modalities may be useful in achieving a precise outline of the extent of paracervical tissue invasion. Registration is the initial step in information fusion. However, when aligning multimodal images with varying depths, manual alignment is prone to large errors and is time-consuming. Furthermore, the variations in the size of the Region of Interest (ROI) and the shape of multimodal images pose a significant challenge for achieving accurate registration.In this paper, we propose a preliminary spatial alignment algorithm and a weakly supervised multimodal registration network. The spatial position alignment algorithm efficiently utilizes the limited annotation information in the two modal images provided by the doctor to automatically align multimodal images with varying depths. By utilizing aligned multimodal images for weakly supervised registration and incorporating pyramidal features and cost volume to estimate the optical flow, the results indicate that the proposed method outperforms traditional volume rendering alignment methods and registration networks in various evaluation metrics. This demonstrates the effectiveness of our model in multimodal image registration.
<div id='section'>Paperid: <span id='pid'>840, <a href='https://arxiv.org/pdf/2405.04093.pdf' target='_blank'>https://arxiv.org/pdf/2405.04093.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Da Fu, Mingfei Rong, Eun-Hu Kim, Hao Huang, Witold Pedrycz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04093">DCNN: Dual Cross-current Neural Networks Realized Using An Interactive Deep Learning Discriminator for Fine-grained Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate classification of fine-grained images remains a challenge in backbones based on convolutional operations or self-attention mechanisms. This study proposes novel dual-current neural networks (DCNN), which combine the advantages of convolutional operations and self-attention mechanisms to improve the accuracy of fine-grained image classification. The main novel design features for constructing a weakly supervised learning backbone model DCNN include (a) extracting heterogeneous data, (b) keeping the feature map resolution unchanged, (c) expanding the receptive field, and (d) fusing global representations and local features. Experimental results demonstrated that using DCNN as the backbone network for classifying certain fine-grained benchmark datasets achieved performance advantage improvements of 13.5--19.5% and 2.2--12.9%, respectively, compared to other advanced convolution or attention-based fine-grained backbones.
<div id='section'>Paperid: <span id='pid'>841, <a href='https://arxiv.org/pdf/2404.16474.pdf' target='_blank'>https://arxiv.org/pdf/2404.16474.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhihao Shuai, Yinan Chen, Shunqiang Mao, Yihan Zho, Xiaohong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.16474">DiffSeg: A Segmentation Model for Skin Lesions Based on Diffusion Difference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly supervised medical image segmentation (MIS) using generative models is crucial for clinical diagnosis. However, the accuracy of the segmentation results is often limited by insufficient supervision and the complex nature of medical imaging. Existing models also only provide a single outcome, which does not allow for the measurement of uncertainty. In this paper, we introduce DiffSeg, a segmentation model for skin lesions based on diffusion difference which exploits diffusion model principles to ex-tract noise-based features from images with diverse semantic information. By discerning difference between these noise features, the model identifies diseased areas. Moreover, its multi-output capability mimics doctors' annotation behavior, facilitating the visualization of segmentation result consistency and ambiguity. Additionally, it quantifies output uncertainty using Generalized Energy Distance (GED), aiding interpretability and decision-making for physicians. Finally, the model integrates outputs through the Dense Conditional Random Field (DenseCRF) algorithm to refine the segmentation boundaries by considering inter-pixel correlations, which improves the accuracy and optimizes the segmentation results. We demonstrate the effectiveness of DiffSeg on the ISIC 2018 Challenge dataset, outperforming state-of-the-art U-Net-based methods.
<div id='section'>Paperid: <span id='pid'>842, <a href='https://arxiv.org/pdf/2404.13103.pdf' target='_blank'>https://arxiv.org/pdf/2404.13103.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marius Schmidt-Mengin, Alexis Benichoux, Shibeshih Belachew, Nikos Komodakis, Nikos Paragios
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.13103">ToNNO: Tomographic Reconstruction of a Neural Network's Output for Weakly Supervised Segmentation of 3D Medical Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Annotating lots of 3D medical images for training segmentation models is time-consuming. The goal of weakly supervised semantic segmentation is to train segmentation models without using any ground truth segmentation masks. Our work addresses the case where only image-level categorical labels, indicating the presence or absence of a particular region of interest (such as tumours or lesions), are available. Most existing methods rely on class activation mapping (CAM). We propose a novel approach, ToNNO, which is based on the Tomographic reconstruction of a Neural Network's Output. Our technique extracts stacks of slices with different angles from the input 3D volume, feeds these slices to a 2D encoder, and applies the inverse Radon transform in order to reconstruct a 3D heatmap of the encoder's predictions. This generic method allows to perform dense prediction tasks on 3D volumes using any 2D image encoder. We apply it to weakly supervised medical image segmentation by training the 2D encoder to output high values for slices containing the regions of interest. We test it on four large scale medical image datasets and outperform 2D CAM methods. We then extend ToNNO by combining tomographic reconstruction with CAM methods, proposing Averaged CAM and Tomographic CAM, which obtain even better results.
<div id='section'>Paperid: <span id='pid'>843, <a href='https://arxiv.org/pdf/2404.07594.pdf' target='_blank'>https://arxiv.org/pdf/2404.07594.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Olatunji Mumini Omisore, Toluwanimi Akinyemi, Anh Nguyen, Lei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.07594">Weakly-Supervised Learning via Multi-Lateral Decoder Branching for Tool Segmentation in Robot-Assisted Cardiovascular Catheterization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot-assisted catheterization has garnered a good attention for its potentials in treating cardiovascular diseases. However, advancing surgeon-robot collaboration still requires further research, particularly on task-specific automation. For instance, automated tool segmentation can assist surgeons in visualizing and tracking of endovascular tools during cardiac procedures. While learning-based models have demonstrated state-of-the-art segmentation performances, generating ground-truth labels for fully-supervised methods is both labor-intensive time consuming, and costly. In this study, we propose a weakly-supervised learning method with multi-lateral pseudo labeling for tool segmentation in cardiovascular angiogram datasets. The method utilizes a modified U-Net architecture featuring one encoder and multiple laterally branched decoders. The decoders generate diverse pseudo labels under different perturbations, augmenting available partial labels. The pseudo labels are self-generated using a mixed loss function with shared consistency across the decoders. The weakly-supervised model was trained end-to-end and validated using partially annotated angiogram data from three cardiovascular catheterization procedures. Validation results show that the model could perform closer to fully-supervised models. Also, the proposed weakly-supervised multi-lateral method outperforms three well known methods used for weakly-supervised learning, offering the highest segmentation performance across the three angiogram datasets. Furthermore, numerous ablation studies confirmed the model's consistent performance under different parameters. Finally, the model was applied for tool segmentation in a robot-assisted catheterization experiments. The model enhanced visualization with high connectivity indices for guidewire and catheter, and a mean processing time of 35 ms per frame.
<div id='section'>Paperid: <span id='pid'>844, <a href='https://arxiv.org/pdf/2404.03394.pdf' target='_blank'>https://arxiv.org/pdf/2404.03394.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Izumi Fujimori, Masaki Oono, Masami Shishibori
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.03394">Background Noise Reduction of Attention Map for Weakly Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In weakly-supervised semantic segmentation (WSSS) using only image-level class labels, a problem with CNN-based Class Activation Maps (CAM) is that they tend to activate the most discriminative local regions of objects. On the other hand, methods based on Transformers learn global features but suffer from the issue of background noise contamination. This paper focuses on addressing the issue of background noise in attention weights within the existing WSSS method based on Conformer, known as TransCAM. The proposed method successfully reduces background noise, leading to improved accuracy of pseudo labels. Experimental results demonstrate that our model achieves segmentation performance of 70.5% on the PASCAL VOC 2012 validation data, 71.1% on the test data, and 45.9% on MS COCO 2014 data, outperforming TransCAM in terms of segmentation performance.
<div id='section'>Paperid: <span id='pid'>845, <a href='https://arxiv.org/pdf/2403.19306.pdf' target='_blank'>https://arxiv.org/pdf/2403.19306.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuyang Shang, Tian Ma, Wanzhu Ren, Yuancheng Li, Jiayi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.19306">Sparse Generation: Making Pseudo Labels Sparse for Point Weakly Supervised Object Detection on Low Data Volume</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing pseudo label generation methods for point weakly supervised object detection are inadequate in low data volume and dense object detection tasks. We consider the generation of weakly supervised pseudo labels as the model's sparse output, and propose Sparse Generation as a solution to make pseudo labels sparse. The method employs three processing stages (Mapping, Mask, Regression), constructs dense tensors through the relationship between data and detector model, optimizes three of its parameters, and obtains a sparse tensor, thereby indirectly obtaining higher quality pseudo labels, and addresses the model's density problem on low data volume. Additionally, we propose perspective-based matching, which provides more rational pseudo boxes for prediction missed on instances. In comparison to the SOTA method, on four datasets (MS COCO-val, RSOD, SIMD, Bullet-Hole), the experimental results demonstrated a significant advantage.
<div id='section'>Paperid: <span id='pid'>846, <a href='https://arxiv.org/pdf/2403.19225.pdf' target='_blank'>https://arxiv.org/pdf/2403.19225.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Angchi Xu, Wei-Shi Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.19225">Efficient and Effective Weakly-Supervised Action Segmentation via Action-Transition-Aware Boundary Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised action segmentation is a task of learning to partition a long video into several action segments, where training videos are only accompanied by transcripts (ordered list of actions). Most of existing methods need to infer pseudo segmentation for training by serial alignment between all frames and the transcript, which is time-consuming and hard to be parallelized while training. In this work, we aim to escape from this inefficient alignment with massive but redundant frames, and instead to directly localize a few action transitions for pseudo segmentation generation, where a transition refers to the change from an action segment to its next adjacent one in the transcript. As the true transitions are submerged in noisy boundaries due to intra-segment visual variation, we propose a novel Action-Transition-Aware Boundary Alignment (ATBA) framework to efficiently and effectively filter out noisy boundaries and detect transitions. In addition, to boost the semantic learning in the case that noise is inevitably present in the pseudo segmentation, we also introduce video-level losses to utilize the trusted video-level supervision. Extensive experiments show the effectiveness of our approach on both performance and training speed.
<div id='section'>Paperid: <span id='pid'>847, <a href='https://arxiv.org/pdf/2403.18134.pdf' target='_blank'>https://arxiv.org/pdf/2403.18134.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhan Shi, Jingwei Zhang, Jun Kong, Fusheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.18134">Integrative Graph-Transformer Framework for Histopathology Whole Slide Image Representation and Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In digital pathology, the multiple instance learning (MIL) strategy is widely used in the weakly supervised histopathology whole slide image (WSI) classification task where giga-pixel WSIs are only labeled at the slide level. However, existing attention-based MIL approaches often overlook contextual information and intrinsic spatial relationships between neighboring tissue tiles, while graph-based MIL frameworks have limited power to recognize the long-range dependencies. In this paper, we introduce the integrative graph-transformer framework that simultaneously captures the context-aware relational features and global WSI representations through a novel Graph Transformer Integration (GTI) block. Specifically, each GTI block consists of a Graph Convolutional Network (GCN) layer modeling neighboring relations at the local instance level and an efficient global attention model capturing comprehensive global information from extensive feature embeddings. Extensive experiments on three publicly available WSI datasets: TCGA-NSCLC, TCGA-RCC and BRIGHT, demonstrate the superiority of our approach over current state-of-the-art MIL methods, achieving an improvement of 1.0% to 2.6% in accuracy and 0.7%-1.6% in AUROC.
<div id='section'>Paperid: <span id='pid'>848, <a href='https://arxiv.org/pdf/2403.14829.pdf' target='_blank'>https://arxiv.org/pdf/2403.14829.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>F. M. Castro-MacÃ­as, P. Morales-Ãlvarez, Y. Wu, R. Molina, A. K. Katsaggelos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.14829">Hyperbolic Secant representation of the logistic function: Application to probabilistic Multiple Instance Learning for CT intracranial hemorrhage detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple Instance Learning (MIL) is a weakly supervised paradigm that has been successfully applied to many different scientific areas and is particularly well suited to medical imaging. Probabilistic MIL methods, and more specifically Gaussian Processes (GPs), have achieved excellent results due to their high expressiveness and uncertainty quantification capabilities. One of the most successful GP-based MIL methods, VGPMIL, resorts to a variational bound to handle the intractability of the logistic function. Here, we formulate VGPMIL using PÃ³lya-Gamma random variables. This approach yields the same variational posterior approximations as the original VGPMIL, which is a consequence of the two representations that the Hyperbolic Secant distribution admits. This leads us to propose a general GP-based MIL method that takes different forms by simply leveraging distributions other than the Hyperbolic Secant one. Using the Gamma distribution we arrive at a new approach that obtains competitive or superior predictive performance and efficiency. This is validated in a comprehensive experimental study including one synthetic MIL dataset, two well-known MIL benchmarks, and a real-world medical problem. We expect that this work provides useful ideas beyond MIL that can foster further research in the field.
<div id='section'>Paperid: <span id='pid'>849, <a href='https://arxiv.org/pdf/2403.13429.pdf' target='_blank'>https://arxiv.org/pdf/2403.13429.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaushalya Kularatnam, Tania Stathaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13429">Detecting and Triaging Spoofing using Temporal Convolutional Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As algorithmic trading and electronic markets continue to transform the landscape of financial markets, detecting and deterring rogue agents to maintain a fair and efficient marketplace is crucial. The explosion of large datasets and the continually changing tricks of the trade make it difficult to adapt to new market conditions and detect bad actors. To that end, we propose a framework that can be adapted easily to various problems in the space of detecting market manipulation. Our approach entails initially employing a labelling algorithm which we use to create a training set to learn a weakly supervised model to identify potentially suspicious sequences of order book states. The main goal here is to learn a representation of the order book that can be used to easily compare future events. Subsequently, we posit the incorporation of expert assessment to scrutinize specific flagged order book states. In the event of an expert's unavailability, recourse is taken to the application of a more complex algorithm on the identified suspicious order book states. We then conduct a similarity search between any new representation of the order book against the expert labelled representations to rank the results of the weak learner. We show some preliminary results that are promising to explore further in this direction
<div id='section'>Paperid: <span id='pid'>850, <a href='https://arxiv.org/pdf/2403.12212.pdf' target='_blank'>https://arxiv.org/pdf/2403.12212.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ramon Abilio, Guilherme Palermo Coelho, Ana Estela Antunes da Silva
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.12212">Evaluating Named Entity Recognition: A comparative analysis of mono- and multilingual transformer models on a novel Brazilian corporate earnings call transcripts dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Since 2018, when the Transformer architecture was introduced, Natural Language Processing has gained significant momentum with pre-trained Transformer-based models that can be fine-tuned for various tasks. Most models are pre-trained on large English corpora, making them less applicable to other languages, such as Brazilian Portuguese. In our research, we identified two models pre-trained in Brazilian Portuguese (BERTimbau and PTT5) and two multilingual models (mBERT and mT5). BERTimbau and mBERT use only the Encoder module, while PTT5 and mT5 use both the Encoder and Decoder. Our study aimed to evaluate their performance on a financial Named Entity Recognition (NER) task and determine the computational requirements for fine-tuning and inference. To this end, we developed the Brazilian Financial NER (BraFiNER) dataset, comprising sentences from Brazilian banks' earnings calls transcripts annotated using a weakly supervised approach. Additionally, we introduced a novel approach that reframes the token classification task as a text generation problem. After fine-tuning the models, we evaluated them using performance and error metrics. Our findings reveal that BERT-based models consistently outperform T5-based models. While the multilingual models exhibit comparable macro F1-scores, BERTimbau demonstrates superior performance over PTT5. In terms of error metrics, BERTimbau outperforms the other models. We also observed that PTT5 and mT5 generated sentences with changes in monetary and percentage values, highlighting the importance of accuracy and consistency in the financial domain. Our findings provide insights into the differing performance of BERT- and T5-based models for the NER task.
<div id='section'>Paperid: <span id='pid'>851, <a href='https://arxiv.org/pdf/2403.01811.pdf' target='_blank'>https://arxiv.org/pdf/2403.01811.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Felix KÃ¼nnecke, Anna Filighera, Colin Leong, Tim Steuer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.01811">Enhancing Multi-Domain Automatic Short Answer Grading through an Explainable Neuro-Symbolic Pipeline</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Grading short answer questions automatically with interpretable reasoning behind the grading decision is a challenging goal for current transformer approaches. Justification cue detection, in combination with logical reasoners, has shown a promising direction for neuro-symbolic architectures in ASAG. But, one of the main challenges is the requirement of annotated justification cues in the students' responses, which only exist for a few ASAG datasets. To overcome this challenge, we contribute (1) a weakly supervised annotation procedure for justification cues in ASAG datasets, and (2) a neuro-symbolic model for explainable ASAG based on justification cues. Our approach improves upon the RMSE by 0.24 to 0.3 compared to the state-of-the-art on the Short Answer Feedback dataset in a bilingual, multi-domain, and multi-question training setup. This result shows that our approach provides a promising direction for generating high-quality grades and accompanying explanations for future research in ASAG and educational NLP.
<div id='section'>Paperid: <span id='pid'>852, <a href='https://arxiv.org/pdf/2402.17792.pdf' target='_blank'>https://arxiv.org/pdf/2402.17792.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Leite, Alisson Silva, Gabriella Casalino, Arnab Sharma, Danielle Fortunato, Axel-Cyrille Ngomo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17792">EGNN-C+: Interpretable Evolving Granular Neural Network and Application in Classification of Weakly-Supervised EEG Data Streams</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a modified incremental learning algorithm for evolving Granular Neural Network Classifiers (eGNN-C+). We use double-boundary hyper-boxes to represent granules, and customize the adaptation procedures to enhance the robustness of outer boxes for data coverage and noise suppression, while ensuring that inner boxes remain flexible to capture drifts. The classifier evolves from scratch, incorporates new classes on the fly, and performs local incremental feature weighting. As an application, we focus on the classification of emotion-related patterns within electroencephalogram (EEG) signals. Emotion recognition is crucial for enhancing the realism and interactivity of computer systems. We extract features from the Fourier spectrum of EEG signals obtained from 28 individuals engaged in playing computer games -- a public dataset. Each game elicits a different predominant emotion: boredom, calmness, horror, or joy. We analyze individual electrodes, time window lengths, and frequency bands to assess the accuracy and interpretability of resulting user-independent neural models. The findings indicate that both brain hemispheres assist classification, especially electrodes on the temporal (T8) and parietal (P7) areas, alongside contributions from frontal and occipital electrodes. While patterns may manifest in any band, the Alpha (8-13Hz), Delta (1-4Hz), and Theta (4-8Hz) bands, in this order, exhibited higher correspondence with the emotion classes. The eGNN-C+ demonstrates effectiveness in learning EEG data. It achieves an accuracy of 81.7% and a 0.0029 II interpretability using 10-second time windows, even in face of a highly-stochastic time-varying 4-class classification problem.
<div id='section'>Paperid: <span id='pid'>853, <a href='https://arxiv.org/pdf/2401.14074.pdf' target='_blank'>https://arxiv.org/pdf/2401.14074.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Y. Liu, L. Lin, K. K. Y. Wong, X. Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.14074">ProCNS: Progressive Prototype Calibration and Noise Suppression for Weakly-Supervised Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly-supervised segmentation (WSS) has emerged as a solution to mitigate the conflict between annotation cost and model performance by adopting sparse annotation formats (e.g., point, scribble, block, etc.). Typical approaches attempt to exploit anatomy and topology priors to directly expand sparse annotations into pseudo-labels. However, due to a lack of attention to the ambiguous edges in medical images and insufficient exploration of sparse supervision, existing approaches tend to generate erroneous and overconfident pseudo proposals in noisy regions, leading to cumulative model error and performance degradation. In this work, we propose a novel WSS approach, named ProCNS, encompassing two synergistic modules devised with the principles of progressive prototype calibration and noise suppression. Specifically, we design a Prototype-based Regional Spatial Affinity (PRSA) loss to maximize the pair-wise affinities between spatial and semantic elements, providing our model of interest with more reliable guidance. The affinities are derived from the input images and the prototype-refined predictions. Meanwhile, we propose an Adaptive Noise Perception and Masking (ANPM) module to obtain more enriched and representative prototype representations, which adaptively identifies and masks noisy regions within the pseudo proposals, reducing potential erroneous interference during prototype computation. Furthermore, we generate specialized soft pseudo-labels for the noisy regions identified by ANPM, providing supplementary supervision. Extensive experiments on six medical image segmentation tasks involving different modalities demonstrate that the proposed framework significantly outperforms representative state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>854, <a href='https://arxiv.org/pdf/2401.03312.pdf' target='_blank'>https://arxiv.org/pdf/2401.03312.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arjun Bhalla, Daniel Levenson, Jan Bernhard, Anton Abilov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.03312">Exploiting Data Hierarchy as a New Modality for Contrastive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work investigates how hierarchically structured data can help neural networks learn conceptual representations of cathedrals. The underlying WikiScenes dataset provides a spatially organized hierarchical structure of cathedral components. We propose a novel hierarchical contrastive training approach that leverages a triplet margin loss to represent the data's spatial hierarchy in the encoder's latent space. As such, the proposed approach investigates if the dataset structure provides valuable information for self-supervised learning. We apply t-SNE to visualize the resultant latent space and evaluate the proposed approach by comparing it with other dataset-specific contrastive learning methods using a common downstream classification task. The proposed method outperforms the comparable weakly-supervised and baseline methods. Our findings suggest that dataset structure is a valuable modality for weakly-supervised learning.
<div id='section'>Paperid: <span id='pid'>855, <a href='https://arxiv.org/pdf/2512.01145.pdf' target='_blank'>https://arxiv.org/pdf/2512.01145.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Riyadh Mohammed Almushrafy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.01145">Weakly Supervised Continuous Micro-Expression Intensity Estimation Using Temporal Deep Neural Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Micro-facial expressions are brief and involuntary facial movements that reflect genuine emotional states. While most prior work focuses on classifying discrete micro-expression categories, far fewer studies address the continuous evolution of intensity over time. Progress in this direction is limited by the lack of frame-level intensity labels, which makes fully supervised regression impractical. We propose a unified framework for continuous micro-expression intensity estimation using only weak temporal labels (onset, apex, offset). A simple triangular prior converts sparse temporal landmarks into dense pseudo-intensity trajectories, and a lightweight temporal regression model that combines a ResNet18 encoder with a bidirectional GRU predicts frame-wise intensity directly from image sequences. The method requires no frame-level annotation effort and is applied consistently across datasets through a single preprocessing and temporal alignment pipeline. Experiments on SAMM and CASME II show strong temporal agreement with the pseudo-intensity trajectories. On SAMM, the model reaches a Spearman correlation of 0.9014 and a Kendall correlation of 0.7999, outperforming a frame-wise baseline. On CASME II, it achieves up to 0.9116 and 0.8168, respectively, when trained without the apex-ranking term. Ablation studies confirm that temporal modeling and structured pseudo labels are central to capturing the rise-apex-fall dynamics of micro-facial movements. To our knowledge, this is the first unified approach for continuous micro-expression intensity estimation using only sparse temporal annotations.
<div id='section'>Paperid: <span id='pid'>856, <a href='https://arxiv.org/pdf/2511.07429.pdf' target='_blank'>https://arxiv.org/pdf/2511.07429.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hari Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.07429">Knowledge-Guided Textual Reasoning for Explainable Video Anomaly Detection via LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Text-based Explainable Video Anomaly Detection (TbVAD), a language-driven framework for weakly supervised video anomaly detection that performs anomaly detection and explanation entirely within the textual domain. Unlike conventional WSVAD models that rely on explicit visual features, TbVAD represents video semantics through language, enabling interpretable and knowledge-grounded reasoning. The framework operates in three stages: (1) transforming video content into fine-grained captions using a vision-language model, (2) constructing structured knowledge by organizing the captions into four semantic slots (action, object, context, environment), and (3) generating slot-wise explanations that reveal which semantic factors contribute most to the anomaly decision. We evaluate TbVAD on two public benchmarks, UCF-Crime and XD-Violence, demonstrating that textual knowledge reasoning provides interpretable and reliable anomaly detection for real-world surveillance scenarios.
<div id='section'>Paperid: <span id='pid'>857, <a href='https://arxiv.org/pdf/2510.25075.pdf' target='_blank'>https://arxiv.org/pdf/2510.25075.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Keisuke Imoto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.25075">Joint Analysis of Acoustic Scenes and Sound Events Based on Semi-Supervised Training of Sound Events With Partial Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Annotating time boundaries of sound events is labor-intensive, limiting the scalability of strongly supervised learning in audio detection. To reduce annotation costs, weakly-supervised learning with only clip-level labels has been widely adopted. As an alternative, partial label learning offers a cost-effective approach, where a set of possible labels is provided instead of exact weak annotations. However, partial label learning for audio analysis remains largely unexplored. Motivated by the observation that acoustic scenes provide contextual information for constructing a set of possible sound events, we utilize acoustic scene information to construct partial labels of sound events. On the basis of this idea, in this paper, we propose a multitask learning framework that jointly performs acoustic scene classification and sound event detection with partial labels of sound events. While reducing annotation costs, weakly-supervised and partial label learning often suffer from decreased detection performance due to lacking the precise event set and their temporal annotations. To better balance between annotation cost and detection performance, we also explore a semi-supervised framework that leverages both strong and partial labels. Moreover, to refine partial labels and achieve better model training, we propose a label refinement method based on self-distillation for the proposed approach with partial labels.
<div id='section'>Paperid: <span id='pid'>858, <a href='https://arxiv.org/pdf/2510.03606.pdf' target='_blank'>https://arxiv.org/pdf/2510.03606.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mattia Scardecchia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03606">Unsupervised Transformer Pre-Training for Images: Self-Distillation, Mean Teachers, and Random Crops</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in self-supervised learning (SSL) have made it possible to learn general-purpose visual features that capture both the high-level semantics and the fine-grained spatial structure of images. Most notably, the recent DINOv2 has established a new state of the art by surpassing weakly supervised methods (WSL) like OpenCLIP on most benchmarks. In this survey, we examine the core ideas behind its approach, multi-crop view augmentation and self-distillation with a mean teacher, and trace their development in previous work. We then compare the performance of DINO and DINOv2 with other SSL and WSL methods across various downstream tasks, and highlight some remarkable emergent properties of their learned features with transformer backbones. We conclude by briefly discussing DINOv2's limitations, its impact, and future research directions.
<div id='section'>Paperid: <span id='pid'>859, <a href='https://arxiv.org/pdf/2509.26145.pdf' target='_blank'>https://arxiv.org/pdf/2509.26145.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yukun Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26145">LMILAtt: A Deep Learning Model for Depression Detection from Social Media Users Enhanced by Multi-Instance Learning Based on Attention Mechanism</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Depression is a major global public health challenge and its early identification is crucial. Social media data provides a new perspective for depression detection, but existing methods face limitations such as insufficient accuracy, insufficient utilization of time series features, and high annotation costs. To this end, this study proposes the LMILAtt model, which innovatively integrates Long Short-Term Memory autoencoders and attention mechanisms: firstly, the temporal dynamic features of user tweets (such as depressive tendency evolution patterns) are extracted through unsupervised LSTM autoencoders. Secondly, the attention mechanism is used to dynamically weight key texts (such as early depression signals) and construct a multi-example learning architecture to improve the accuracy of user-level detection. Finally, the performance was verified on the WU3D dataset labeled by professional medicine. Experiments show that the model is significantly better than the baseline model in terms of accuracy, recall and F1 score. In addition, the weakly supervised learning strategy significantly reduces the cost of labeling and provides an efficient solution for large-scale social media depression screening.
<div id='section'>Paperid: <span id='pid'>860, <a href='https://arxiv.org/pdf/2509.14554.pdf' target='_blank'>https://arxiv.org/pdf/2509.14554.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoming Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14554">Generative Large Language Models for Knowledge Representation: A Systematic Review of Concept Map Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rise of generative large language models (LLMs) has opened new opportunities for automating knowledge representation through concept maps, a long-standing pedagogical tool valued for fostering meaningful learning and higher-order thinking. Traditional construction of concept maps is labor-intensive, requiring significant expertise and time, limiting their scalability in education. This review systematically synthesizes the emerging body of research on LLM-enabled concept map generation, focusing on two guiding questions: (a) What methods and technical features of LLMs are employed to construct concept maps? (b) What empirical evidence exists to validate their educational utility? Through a comprehensive search across major databases and AI-in-education conference proceedings, 28 studies meeting rigorous inclusion criteria were analyzed using thematic synthesis. Findings reveal six major methodological categories: human-in-the-loop systems, weakly supervised learning models, fine-tuned domain-specific LLMs, pre-trained LLMs with prompt engineering, hybrid systems integrating knowledge bases, and modular frameworks combining symbolic and statistical tools. Validation strategies ranged from quantitative metrics (precision, recall, F1-score, semantic similarity) to qualitative evaluations (expert review, learner feedback). Results indicate LLM-generated maps hold promise for scalable, adaptive, and pedagogically relevant knowledge visualization, though challenges remain regarding validity, interpretability, multilingual adaptability, and classroom integration. Future research should prioritize interdisciplinary co-design, empirical classroom trials, and alignment with instructional practices to realize their full educational potential.
<div id='section'>Paperid: <span id='pid'>861, <a href='https://arxiv.org/pdf/2509.08698.pdf' target='_blank'>https://arxiv.org/pdf/2509.08698.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thorsten Wittkopp
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08698">A layered architecture for log analysis in complex IT systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the evolving IT landscape, stability and reliability of systems are essential, yet their growing complexity challenges DevOps teams in implementation and maintenance. Log analysis, a core element of AIOps, provides critical insights into complex behaviors and failures. This dissertation introduces a three-layered architecture to support DevOps in failure resolution. The first layer, Log Investigation, performs autonomous log labeling and anomaly classification. We propose a method that labels log data without manual effort, enabling supervised training and precise evaluation of anomaly detection. Additionally, we define a taxonomy that groups anomalies into three categories, ensuring appropriate method selection. The second layer, Anomaly Detection, detects behaviors deviating from the norm. We propose a flexible Anomaly Detection method adaptable to unsupervised, weakly supervised, and supervised training. Evaluations on public and industry datasets show F1-scores between 0.98 and 1.0, ensuring reliable anomaly detection. The third layer, Root Cause Analysis, identifies minimal log sets describing failures, their origin, and event sequences. By balancing training data and identifying key services, our Root Cause Analysis method consistently detects 90-98% of root cause log lines within the top 10 candidates, providing actionable insights for mitigation. Our research addresses how log analysis methods can be designed and optimized to help DevOps resolve failures efficiently. By integrating these three layers, the architecture equips teams with robust methods to enhance IT system reliability.
<div id='section'>Paperid: <span id='pid'>862, <a href='https://arxiv.org/pdf/2508.15973.pdf' target='_blank'>https://arxiv.org/pdf/2508.15973.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minh-Tan Pham
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15973">Contributions to Label-Efficient Learning in Computer Vision and Remote Sensing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This manuscript presents a series of my selected contributions to the topic of label-efficient learning in computer vision and remote sensing. The central focus of this research is to develop and adapt methods that can learn effectively from limited or partially annotated data, and can leverage abundant unlabeled data in real-world applications. The contributions span both methodological developments and domain-specific adaptations, in particular addressing challenges unique to Earth observation data such as multi-modality, spatial resolution variability, and scene heterogeneity. The manuscript is organized around four main axes including (1) weakly supervised learning for object discovery and detection based on anomaly-aware representations learned from large amounts of background images; (2) multi-task learning that jointly trains on multiple datasets with disjoint annotations to improve performance on object detection and semantic segmentation; (3) self-supervised and supervised contrastive learning with multimodal data to enhance scene classification in remote sensing; and (4) few-shot learning for hierarchical scene classification using both explicit and implicit modeling of class hierarchies. These contributions are supported by extensive experimental results across natural and remote sensing datasets, reflecting the outcomes of several collaborative research projects. The manuscript concludes by outlining ongoing and future research directions focused on scaling and enhancing label-efficient learning for real-world applications.
<div id='section'>Paperid: <span id='pid'>863, <a href='https://arxiv.org/pdf/2507.00297.pdf' target='_blank'>https://arxiv.org/pdf/2507.00297.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Ifeoluwa Adelani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00297">Natural language processing for African languages</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in word embeddings and language models use large-scale, unlabelled data and self-supervised learning to boost NLP performance. Multilingual models, often trained on web-sourced data like Wikipedia, face challenges: few low-resource languages are included, their data is often noisy, and lack of labeled datasets makes it hard to evaluate performance outside high-resource languages like English. In this dissertation, we focus on languages spoken in Sub-Saharan Africa where all the indigenous languages in this region can be regarded as low-resourced in terms of the availability of labelled data for NLP tasks and unlabelled data found on the web. We analyse the noise in the publicly available corpora, and curate a high-quality corpus, demonstrating that the quality of semantic representations learned in word embeddings does not only depend on the amount of data but on the quality of pre-training data. We demonstrate empirically the limitations of word embeddings, and the opportunities the multilingual pre-trained language model (PLM) offers especially for languages unseen during pre-training and low-resource scenarios. We further study how to adapt and specialize multilingual PLMs to unseen African languages using a small amount of monolingual texts. To address the under-representation of the African languages in NLP research, we developed large scale human-annotated labelled datasets for 21 African languages in two impactful NLP tasks: named entity recognition and machine translation. We conduct an extensive empirical evaluation using state-of-the-art methods across supervised, weakly-supervised, and transfer learning settings.
<div id='section'>Paperid: <span id='pid'>864, <a href='https://arxiv.org/pdf/2505.09129.pdf' target='_blank'>https://arxiv.org/pdf/2505.09129.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09129">WSCIF: A Weakly-Supervised Color Intelligence Framework for Tactical Anomaly Detection in Surveillance Keyframes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The deployment of traditional deep learning models in high-risk security tasks in an unlabeled, data-non-exploitable video intelligence environment faces significant challenges. In this paper, we propose a lightweight anomaly detection framework based on color features for surveillance video clips in a high sensitivity tactical mission, aiming to quickly identify and interpret potential threat events under resource-constrained and data-sensitive conditions. The method fuses unsupervised KMeans clustering with RGB channel histogram modeling to achieve composite detection of structural anomalies and color mutation signals in key frames. The experiment takes an operation surveillance video occurring in an African country as a research sample, and successfully identifies multiple highly anomalous frames related to high-energy light sources, target presence, and reflective interference under the condition of no access to the original data. The results show that this method can be effectively used for tactical assassination warning, suspicious object screening and environmental drastic change monitoring with strong deployability and tactical interpretation value. The study emphasizes the importance of color features as low semantic battlefield signal carriers, and its battlefield intelligent perception capability will be further extended by combining graph neural networks and temporal modeling in the future.
<div id='section'>Paperid: <span id='pid'>865, <a href='https://arxiv.org/pdf/2505.09083.pdf' target='_blank'>https://arxiv.org/pdf/2505.09083.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dominic Zaun Eu Jones
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09083">Ornithologist: Towards Trustworthy "Reasoning" about Central Bank Communications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>I develop Ornithologist, a weakly-supervised textual classification system and measure the hawkishness and dovishness of central bank text. Ornithologist uses ``taxonomy-guided reasoning'', guiding a large language model with human-authored decision trees. This increases the transparency and explainability of the system and makes it accessible to non-experts. It also reduces hallucination risk. Since it requires less supervision than traditional classification systems, it can more easily be applied to other problems or sources of text (e.g. news) without much modification. Ornithologist measurements of hawkishness and dovishness of RBA communication carry information about the future of the cash rate path and of market expectations.
<div id='section'>Paperid: <span id='pid'>866, <a href='https://arxiv.org/pdf/2501.02021.pdf' target='_blank'>https://arxiv.org/pdf/2501.02021.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditya Prakash
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.02021">Weakly Supervised Learning on Large Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph classification plays a pivotal role in various domains, including pathology, where images can be represented as graphs. In this domain, images can be represented as graphs, where nodes might represent individual nuclei, and edges capture the spatial or functional relationships between them. Often, the overall label of the graph, such as a cancer type or disease state, is determined by patterns within smaller, localized regions of the image. This work introduces a weakly-supervised graph classification framework leveraging two subgraph extraction techniques: (1) Sliding-window approach (2) BFS-based approach. Subgraphs are processed using a Graph Attention Network (GAT), which employs attention mechanisms to identify the most informative subgraphs for classification. Weak supervision is achieved by propagating graph-level labels to subgraphs, eliminating the need for detailed subgraph annotations.
<div id='section'>Paperid: <span id='pid'>867, <a href='https://arxiv.org/pdf/2406.11014.pdf' target='_blank'>https://arxiv.org/pdf/2406.11014.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luca Moschella
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.11014">Latent Communication in Artificial Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As NNs permeate various scientific and industrial domains, understanding the universality and reusability of their representations becomes crucial. At their core, these networks create intermediate neural representations, indicated as latent spaces, of the input data and subsequently leverage them to perform specific downstream tasks. This dissertation focuses on the universality and reusability of neural representations. Do the latent representations crafted by a NN remain exclusive to a particular trained instance, or can they generalize across models, adapting to factors such as randomness during training, model architecture, or even data domain? This adaptive quality introduces the notion of Latent Communication -- a phenomenon that describes when representations can be unified or reused across neural spaces. A salient observation from our research is the emergence of similarities in latent representations, even when these originate from distinct or seemingly unrelated NNs. By exploiting a partial correspondence between the two data distributions that establishes a semantic link, we found that these representations can either be projected into a universal representation, coined as Relative Representation, or be directly translated from one space to another. Latent Communication allows for a bridge between independently trained NN, irrespective of their training regimen, architecture, or the data modality they were trained on -- as long as the data semantic content stays the same (e.g., images and their captions). This holds true for both generation, classification and retrieval downstream tasks; in supervised, weakly supervised, and unsupervised settings; and spans various data modalities including images, text, audio, and graphs -- showcasing the universality of the Latent Communication phenomenon. [...]
<div id='section'>Paperid: <span id='pid'>868, <a href='https://arxiv.org/pdf/2406.05605.pdf' target='_blank'>https://arxiv.org/pdf/2406.05605.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sayan Mandal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.05605">Deep Learning to Predict Glaucoma Progression using Structural Changes in the Eye</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Glaucoma is a chronic eye disease characterized by optic neuropathy, leading to irreversible vision loss. It progresses gradually, often remaining undiagnosed until advanced stages. Early detection is crucial to monitor atrophy and develop treatment strategies to prevent further vision impairment. Data-centric methods have enabled computer-aided algorithms for precise glaucoma diagnosis.
  In this study, we use deep learning models to identify complex disease traits and progression criteria, detecting subtle changes indicative of glaucoma. We explore the structure-function relationship in glaucoma progression and predict functional impairment from structural eye deterioration. We analyze statistical and machine learning methods, including deep learning techniques with optical coherence tomography (OCT) scans for accurate progression prediction.
  Addressing challenges like age variability, data imbalances, and noisy labels, we develop novel semi-supervised time-series algorithms:
  1. Weakly-Supervised Time-Series Learning: We create a CNN-LSTM model to encode spatiotemporal features from OCT scans. This approach uses age-related progression and positive-unlabeled data to establish robust pseudo-progression criteria, bypassing gold-standard labels.
  2. Semi-Supervised Time-Series Learning: Using labels from Guided Progression Analysis (GPA) in a contrastive learning scheme, the CNN-LSTM architecture learns from potentially mislabeled data to improve prediction accuracy.
  Our methods outperform conventional and state-of-the-art techniques.
<div id='section'>Paperid: <span id='pid'>869, <a href='https://arxiv.org/pdf/2405.17444.pdf' target='_blank'>https://arxiv.org/pdf/2405.17444.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Min Hun Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.17444">Towards Gradient-based Time-Series Explanations through a SpatioTemporal Attention Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we explore the feasibility of using a transformer-based, spatiotemporal attention network (STAN) for gradient-based time-series explanations. First, we trained the STAN model for video classifications using the global and local views of data and weakly supervised labels on time-series data (i.e. the type of an activity). We then leveraged a gradient-based XAI technique (e.g. saliency map) to identify salient frames of time-series data. According to the experiments using the datasets of four medically relevant activities, the STAN model demonstrated its potential to identify important frames of videos.
<div id='section'>Paperid: <span id='pid'>870, <a href='https://arxiv.org/pdf/2405.01583.pdf' target='_blank'>https://arxiv.org/pdf/2405.01583.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nadia Saeed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.01583">MediFact at MEDIQA-M3G 2024: Medical Question Answering in Dermatology with Multimodal Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The MEDIQA-M3G 2024 challenge necessitates novel solutions for Multilingual & Multimodal Medical Answer Generation in dermatology (wai Yim et al., 2024a). This paper addresses the limitations of traditional methods by proposing a weakly supervised learning approach for open-ended medical question-answering (QA). Our system leverages readily available MEDIQA-M3G images via a VGG16-CNN-SVM model, enabling multilingual (English, Chinese, Spanish) learning of informative skin condition representations. Using pre-trained QA models, we further bridge the gap between visual and textual information through multimodal fusion. This approach tackles complex, open-ended questions even without predefined answer choices. We empower the generation of comprehensive answers by feeding the ViT-CLIP model with multiple responses alongside images. This work advances medical QA research, paving the way for clinical decision support systems and ultimately improving healthcare delivery.
<div id='section'>Paperid: <span id='pid'>871, <a href='https://arxiv.org/pdf/2403.14678.pdf' target='_blank'>https://arxiv.org/pdf/2403.14678.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Romeo Valentin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.14678">Towards a Framework for Deep Learning Certification in Safety-Critical Applications Using Inherently Safe Design and Run-Time Error Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although an ever-growing number of applications employ deep learning based systems for prediction, decision-making, or state estimation, almost no certification processes have been established that would allow such systems to be deployed in safety-critical applications. In this work we consider real-world problems arising in aviation and other safety-critical areas, and investigate their requirements for a certified model. To this end, we investigate methodologies from the machine learning research community aimed towards verifying robustness and reliability of deep learning systems, and evaluate these methodologies with regard to their applicability to real-world problems. Then, we establish a new framework towards deep learning certification based on (i) inherently safe design, and (ii) run-time error detection. Using a concrete use case from aviation, we show how deep learning models can recover disentangled variables through the use of weakly-supervised representation learning. We argue that such a system design is inherently less prone to common model failures, and can be verified to encode underlying mechanisms governing the data. Then, we investigate four techniques related to the run-time safety of a model, namely (i) uncertainty quantification, (ii) out-of-distribution detection, (iii) feature collapse, and (iv) adversarial attacks. We evaluate each for their applicability and formulate a set of desiderata that a certified model should fulfill. Finally, we propose a novel model structure that exhibits all desired properties discussed in this work, and is able to make regression and uncertainty predictions, as well as detect out-of-distribution inputs, while requiring no regression labels to train. We conclude with a discussion of the current state and expected future progress of deep learning certification, and its industrial and social implications.
